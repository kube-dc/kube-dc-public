{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What Is Kube-DC?","text":"<p>Kube-DC is an advanced, enterprise-grade platform that transforms Kubernetes into a comprehensive Data Center solution supporting both virtual machines and containerized workloads. It provides organizations with a unified management interface for all their infrastructure needs, from multi-tenancy and virtualization to networking and billing.</p>"},{"location":"#overview","title":"Overview","text":"<p>Kube-DC bridges the gap between traditional virtualization and modern container orchestration, allowing teams to run both legacy workloads and cloud-native applications on the same platform. By leveraging Kubernetes as the foundation, Kube-DC inherits its robust ecosystem while extending functionality to support enterprise requirements.</p> <p></p>"},{"location":"#key-features-at-a-glance","title":"Key Features at a Glance","text":"<p>Kube-DC offers a comprehensive set of features designed for modern data center operations:</p> <ul> <li>Multi-Tenancy - Host multiple organizations with isolated environments and custom SSO integration</li> <li>Unified Workload Management - Run both VMs and containers on the same platform</li> <li>Advanced Networking - VPC per project, VLAN support, and software-defined networking</li> <li>Enterprise Virtualization - KubeVirt integration with GPU passthrough and live migration</li> <li>Infrastructure as Code - Kubernetes-native APIs with support for Terraform, Ansible, and more</li> <li>Integrated Billing - Track and allocate costs for all resources</li> <li>Managed Services Platform - Deploy databases, storage, and AI/ML infrastructure</li> </ul> <p>For detailed information about each feature, including capabilities and use cases, visit the Core Features page.</p>"},{"location":"#why-choose-kube-dc","title":"Why Choose Kube-DC?","text":""},{"location":"#for-enterprise-it","title":"For Enterprise IT","text":"<ul> <li>Run legacy VMs alongside modern containers</li> <li>Implement chargeback models for departmental resource usage</li> <li>Provide self-service infrastructure while maintaining governance</li> <li>Reduce operational costs by consolidating virtualization and container platforms</li> </ul>"},{"location":"#for-service-providers","title":"For Service Providers","text":"<ul> <li>Offer multi-tenant infrastructure with complete isolation</li> <li>Provide value-added services beyond basic IaaS</li> <li>Implement flexible billing based on actual resource usage</li> <li>Support diverse customer workloads on a single platform</li> </ul>"},{"location":"#for-devops-teams","title":"For DevOps Teams","text":"<ul> <li>Unify VM and container management workflows</li> <li>Implement infrastructure as code for all resources</li> <li>Integrate with existing CI/CD pipelines</li> <li>Enable developer self-service while maintaining control</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to explore Kube-DC? Check out our Quick Start guides to begin your journey.</p> <p>For a deeper understanding of the underlying architecture and concepts, visit the Architecture &amp; Concepts section.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Kube-DC is built with a focus on community collaboration. Visit our Community &amp; Support page to learn how to get involved, report issues, or seek assistance.</p>"},{"location":"CODEBASE_CONTEXT/","title":"Codebase Context for Kube-DC","text":"<p>This document summarizes the codebase structure of the Kube-DC project for future reference. It serves as a single source of truth to avoid repeating exploratory analysis.</p>"},{"location":"CODEBASE_CONTEXT/#top-level-organization","title":"Top-level Organization","text":"<pre><code>.github/             # GitHub workflows and issue templates\n.vscode/             # Editor settings\napi/                 # Kubernetes CRD API definitions for kube-dc.com\ncharts/              # Helm charts for deploying Kube-DC\ncmd/                 # Controller manager entry point (main.go)\ndocs/                # User-facing documentation and architecture guides\nexamples/            # Sample manifests and usage scenarios\nhack/                # Development and automation scripts\ninternal/            # Core libraries, controllers, and utilities\ninstaller/           # Installation manifests and scripts\nservices/            # Auxiliary Kubernetes services (DB, storage, etc.)\nui/                  # Web UI (frontend and backend)\n\nDockerfile           # Container image for controller manager\nDockerfile_manager   # Alternate Dockerfile for the manager image\n.dockerignore        # Files to ignore in Docker builds\n.gitignore           # Git ignore rules\n.golangci.yml        # GolangCI-Lint configuration\nMakefile             # Build and automation targets\nPROJECT              # Project metadata\nREADME.md            # Project overview and key features\ngo.mod, go.sum       # Go module definitions\npackage-lock.json    # Node/NPM dependency lock file for UI backend\nmkdocs.yml           # MkDocs configuration for documentation site\n</code></pre>"},{"location":"CODEBASE_CONTEXT/#detailed-directory-breakdown","title":"Detailed Directory Breakdown","text":""},{"location":"CODEBASE_CONTEXT/#cmd","title":"cmd/","text":"<p>Contains the <code>main.go</code> entry point for the Kube-DC controller manager that initializes and runs Kubernetes controllers.</p>"},{"location":"CODEBASE_CONTEXT/#internal","title":"internal/","text":"<p>Modular Go packages implementing business logic and controller patterns: - service_lb/: Load balancer and external IP management - organization/: Organization CRD and Keycloak integration - eip/, fip/: External/ floating IP resource controllers - project/, organizationgroup/: CRD controllers for multi-tenancy - client/, objmgr/, controller/, utils/: Core abstractions for resource management</p>"},{"location":"CODEBASE_CONTEXT/#ui-web-ui","title":"ui/ (Web UI)","text":"<p>The <code>ui</code> directory contains the Kube\u2011DC user interface, split into two subprojects:</p>"},{"location":"CODEBASE_CONTEXT/#frontend","title":"frontend/","text":"<p>React/TypeScript single\u2011page application scaffolded from PatternFly Seed: - Entry: <code>src/index.tsx</code> and <code>src/app/</code> for layout, routing, and components - Build: Webpack configs (<code>webpack.common.js</code>, <code>webpack.dev.js</code>, <code>webpack.prod.js</code>) and scripts in <code>package.json</code> - Assets &amp; manifests: <code>kubernetes/</code> holds deployment, service, and ingress YAML for UI - Dev tools: Jest tests, Storybook (<code>stories/</code>), ESLint/Prettier, bundle analyzer, and Surge deployment (<code>dr-surge.js</code>)</p> <p><pre><code>ui/frontend/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 index.tsx\n\u2502   \u2514\u2500\u2500 app/\n\u251c\u2500\u2500 kubernetes/\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 webpack.common.js\n\u2514\u2500\u2500 README.md\n</code></pre> \u3010F:ui/frontend/README.md\u2020L1-L6\u3011\u3010F:ui/frontend/package.json\u2020L9-L16\u3011\u3010F:ui/frontend/webpack.common.js\u2020L1-L7\u3011</p>"},{"location":"CODEBASE_CONTEXT/#backend","title":"backend/","text":"<p>Node.js/Express API server that provides UI endpoints and in\u2011cluster proxies: - Server: <code>app.js</code> sets up routes, CORS, body parsing, and WebSocket proxy for VNC - Controllers: <code>controllers/</code> contains modules for cloud-shell, VMs, volumes, network, projects, metrics, system functions, etc. - Proxy: HTTP and WebSocket proxy middleware to route VNC and other traffic via Kubernetes services - Kubernetes manifests: <code>kubernetes/</code> and <code>kubernetes_service/</code> directories for deployment YAML</p> <p><pre><code>ui/backend/\n\u251c\u2500\u2500 app.js\n\u251c\u2500\u2500 controllers/\n\u2502   \u251c\u2500\u2500 cloudShellModule.js\n\u2502   \u2514\u2500\u2500 volumeModule.js\n\u251c\u2500\u2500 kubernetes/\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 README.md\n</code></pre> \u3010F:ui/backend/app.js\u2020L1-L20\u3011\u3010F:ui/backend/controllers/cloudShellModule.js\u2020L1-L10\u3011</p>"},{"location":"CODEBASE_CONTEXT/#hack","title":"hack/","text":"<p>Utility scripts for: - cluster setup and bootstrap - UI code updates and build automation - integration tests and version management</p>"},{"location":"CODEBASE_CONTEXT/#charts","title":"charts/","text":"<p>Helm chart definitions to deploy Kube-DC components onto a Kubernetes cluster.</p>"},{"location":"CODEBASE_CONTEXT/#docs","title":"docs/","text":"<p>Markdown files for: - Tutorials (quickstart, kubeconfig, IP &amp; LB, VMs, user groups) - Architecture (networking, virtualization, multi-tenancy, overview) - Community and support guidelines</p>"},{"location":"CODEBASE_CONTEXT/#examples","title":"examples/","text":"<p>Sample manifests demonstrating cluster API integration, VM workloads, and organization/user configurations.</p>"},{"location":"CODEBASE_CONTEXT/#installer","title":"installer/","text":"<p>Installation scripts and YAML manifests for bootstrapping the control plane and CRDs.</p>"},{"location":"CODEBASE_CONTEXT/#services","title":"services/","text":"<p>Predefined Kubernetes objects for ancillary services such as database and storage provisioning.</p>"},{"location":"CODEBASE_CONTEXT/#go-controller-manager-architecture","title":"Go Controller Manager Architecture","text":""},{"location":"CODEBASE_CONTEXT/#cmdmaingo","title":"cmd/main.go","text":"<ul> <li>Registers schemes for Kubernetes core, kube-dc CRDs, OVN, and CNI types.</li> <li>Parses flags (metrics address, leader election, Keycloak debug, HTTP/2, config secret).</li> <li>Initializes controller-runtime Manager with metrics server, health/readiness probes, and webhook server.</li> <li>Sets global configuration (ConfigSecretName, KubeDcNamespace).</li> <li>Registers Reconcilers: OrganizationReconciler, ProjectReconciler, OrganizationGroupReconciler, EIpReconciler, FIpReconciler, ServiceReconciler.</li> </ul> <p><pre><code>// Add CRD schemes and plugins; initialize Manager and register controllers\nutilruntime.Must(clientgoscheme.AddToScheme(scheme))\nutilruntime.Must(kubedccomv1.AddToScheme(scheme))\n// ...\nmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{...})\n// ...\n(&amp;controller.OrganizationReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme(), Debug: debug}).SetupWithManager(mgr)\n(&amp;controller.ProjectReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme(), Debug: debug}).SetupWithManager(mgr)\n// ...\n(&amp;corecontroller.ServiceReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme()}).SetupWithManager(mgr)\n</code></pre> \u3010F:cmd/main.go\u2020L54-L60\u3011\u3010F:cmd/main.go\u2020L169-L212\u3011</p>"},{"location":"CODEBASE_CONTEXT/#crd-types-and-schemas-apikube-dccomv1","title":"CRD Types and Schemas (api/kube-dc.com/v1)","text":"<ul> <li>Defines custom resources: Organization, Project, OrganizationGroup, EIp, FIp.</li> <li><code>*_types.go</code> files describe Spec and Status fields; <code>*_extend.go</code> adds loader and helper methods.</li> </ul> <p><pre><code>api/kube-dc.com/v1/\n\u251c\u2500\u2500 organization_types.go\n\u251c\u2500\u2500 project_types.go\n\u251c\u2500\u2500 organizationgroup_types.go\n\u251c\u2500\u2500 eip_types.go\n\u251c\u2500\u2500 fip_types.go\n\u251c\u2500\u2500 organization_extend.go\n\u2514\u2500\u2500 project_extend.go\n</code></pre> \u3010F:api/kube-dc.com/v1/organization_types.go\u2020L1-L80\u3011\u3010F:api/kube-dc.com/v1/project_types.go\u2020L1-L80\u3011</p>"},{"location":"CODEBASE_CONTEXT/#controllers-internalcontroller","title":"Controllers (internal/controller)","text":"<ul> <li>OrganizationReconciler: Manages Organization CR, delegates to internal/organization for Keycloak realm, auth config, roles, and secrets.</li> <li>ProjectReconciler: Manages Project CR, orchestrates namespace, VPC, subnet, SNAT, EIP, keypairs, roles, and DNS via internal/project.</li> <li>OrganizationGroupReconciler: Syncs OrganizationGroup CR, handling Keycloak groups and Kubernetes rolebindings.</li> <li>EIpReconciler / FIpReconciler: Reconcile external/Floating IP CRs.</li> <li>ServiceReconciler: Reconciles <code>ServiceTypeLoadBalancer</code> Services and their Endpoints; loads Project context; manages external IPs via <code>NewSvcLbEIpRes</code> and OVN-based load balancers via <code>NewLoadBalancerRes</code> in <code>service_controller.go</code>.   \u3010F:internal/controller/core/service_controller.go\u2020L52-L83\u3011\u3010F:internal/controller/core/service_controller.go\u2020L116-L140\u3011</li> </ul> <p><pre><code>internal/controller/\n\u251c\u2500\u2500 kube-dc.com/\n\u2502   \u251c\u2500\u2500 organization_controller.go\n\u2502   \u251c\u2500\u2500 project_controller.go\n\u2502   \u251c\u2500\u2500 organizatongroup_controller.go\n\u2502   \u251c\u2500\u2500 eip_controller.go\n\u2502   \u2514\u2500\u2500 fip_controller.go\n\u2514\u2500\u2500 core/\n    \u2514\u2500\u2500 service_controller.go\n</code></pre> \u3010F:internal/controller/kube-dc.com/organization_controller.go\u2020L1-L30\u3011\u3010F:internal/controller/core/service_controller.go\u2020L1-L20\u3011</p>"},{"location":"CODEBASE_CONTEXT/#business-logic-internal-packages","title":"Business Logic (internal packages)","text":"<ul> <li>internal/organization: Orchestrates Organization CR synchronization by invoking resource controllers:</li> <li><code>organization.go</code>: Sync/Delete pipeline calling NewKeycloakRealm, NewKubeAuthConfig, NewRealmRole, NewRealmAccessSeret to manage Keycloak realms, Kubernetes auth secrets, realm roles, and access secrets.     \u3010F:internal/organization/organization.go\u2020L12-L58\u3011\u3010F:internal/organization/organization.go\u2020L61-L102\u3011</li> <li>internal/project: Orchestrates Project CR lifecycle, provisioning namespaces, networking, and identities:</li> <li><code>project.go</code>: Sync/Delete pipeline calling NewProjectNamespace, NewProjectVpc, NewProjectEip, NewProjectSubnet, NewProjectNad, NewProjectSnat, NewProjectKeyPairSeret, NewProjectAuthKeySecret, NewProjectKeycloakRole, NewProjectRole, NewProjectRoleBinding, NewProjectVpcDns.     \u3010F:internal/project/project.go\u2020L13-L58\u3011\u3010F:internal/project/project.go\u2020L59-L137\u3011</li> <li>internal/organizationgroup: Manages Keycloak group and Kubernetes bindings per project.</li> <li>internal/service_lb: Implements Service LoadBalancer logic using OVN and EIp CRD:</li> <li><code>service_lb.go</code>: Defines <code>LBResource</code> which configures OVN logical router/switch load balancers (VIPs\u2192backends) via <code>NewLoadBalancerRes</code>, with <code>Sync</code>/<code>Delete</code> methods to mutate OVN NB DB.</li> <li><code>eip_res.go</code>: Defines <code>NewSvcLbEIpRes</code> to reconcile external IP addresses (EIp CRD) for services, based on annotations or project gateway, with functions to Get/Create/Delete and update status.   \u3010F:internal/service_lb/service_lb.go\u2020L30-L41\u3011\u3010F:internal/service_lb/eip_res.go\u2020L18-L27\u3011</li> <li>internal/objmgr: Generic resource manager abstractions for creating/updating Kubernetes objects.</li> <li>internal/utils: Common utilities (random names, JSON copy, resource processor).</li> </ul>"},{"location":"CODEBASE_CONTEXT/#external-integrations","title":"External Integrations","text":"<ul> <li>Keycloak via gocloak for identity management.</li> <li>OVN via kube-ovn client for software\u2011defined networking.</li> <li>NetworkAttachmentDefinitions via CNI client for custom network attachments.</li> </ul>"},{"location":"CODEBASE_CONTEXT/#metrics-healthchecks-leader-election","title":"Metrics, Healthchecks &amp; Leader Election","text":"<ul> <li>Exposes secure metrics endpoint with authentication filters.</li> <li>Readiness and liveness probes via <code>/healthz</code> and <code>/readyz</code>.</li> <li>Optional leader election for HA controller managers.</li> </ul>"},{"location":"CODEBASE_CONTEXT/#installation-via-clusterdev-infrastructure-as-code","title":"Installation via cluster.dev Infrastructure as Code","text":"<p>Kube-DC leverages the cluster.dev IaC framework (v0.9.7) to provision and deploy its control plane and dependencies.</p> <p>Under <code>installer/kube-dc</code>: - stack.yaml: Defines a <code>Stack</code> using the <code>templates/kube-dc/</code> StackTemplate to orchestrate installation units.   <pre><code>name: cluster\ntemplate: \"./templates/kube-dc/\"\nkind: Stack\n</code></pre>   \u3010F:installer/kube-dc/stack.yaml\u2020L1-L4\u3011\u3010F:go.mod\u2020L16\u3011 - project.yaml: Defines a <code>Project</code> for cluster.dev, setting owner organization and project defaults.   <pre><code>name: dev\nkind: Project\n</code></pre>   \u3010F:installer/kube-dc/project.yaml\u2020L1-L3\u3011 - templates/kube-dc/template.yaml: StackTemplate with sequential units: Terraform install, password generators, CRDs, Helm charts (kube-ovn, multus-cni, kubevirt, Keycloak, cert-manager, ingress-nginx, monitoring stack, kube-dc core), and custom shell hooks.   \u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L17-L23\u3011</p> <p>The installer docs demonstrate bootstrapping cluster.dev CLI and deploying the stack: - Bootstrapping cluster.dev: <code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh</code>   \u3010F:docs/quickstart-hetzner.md\u2020L175-L176\u3011 - High-level install step: \u201cKube-DC Installation: Use cluster.dev to deploy Kube-DC components\u201d   \u3010F:docs/quickstart-overview.md\u2020L104-L105\u3011</p>"},{"location":"CODEBASE_CONTEXT/#cicd-testing","title":"CI/CD &amp; Testing","text":""},{"location":"CODEBASE_CONTEXT/#github-actions-workflows","title":"GitHub Actions workflows","text":"<ul> <li>release.yaml: on tag pushes, builds and pushes Helm charts via <code>hack/build.sh</code> inside Alpine/helm container.   \u3010F:.github/workflows/release.yaml\u2020L1-L20\u3011</li> <li>sync_to_public_repo.yaml: on <code>main</code> changes to charts/examples/docs/installer/hack, syncs to the public kube-dc-public repo.   \u3010F:.github/workflows/sync_to_public_repo.yaml\u2020L1-L55\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#local-ci-via-makefile-go-code","title":"Local CI via Makefile (Go code)","text":"<ul> <li><code>make test</code>: run unit tests with envtest and coverage.</li> <li><code>make test-e2e</code>: run end-to-end tests via Kind.</li> <li><code>make lint</code>, <code>make fmt</code>, <code>make vet</code>: lint, format, and vet Go code.</li> <li><code>make build</code>: compile the controller manager binary.   \u3010F:Makefile\u2020L14-L23\u3011\u3010F:Makefile\u2020L27-L38\u3011\u3010F:Makefile\u2020L95-L114\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#frontend-ci-reacttypescript","title":"Frontend CI (React/TypeScript)","text":"<ul> <li><code>npm run ci-checks</code>: type-check, ESLint lint, and Jest coverage tests.</li> <li>Additional scripts: <code>start:dev</code>, <code>build</code>, <code>test</code>, <code>storybook</code>, <code>bundle-profile:analyze</code>.   \u3010F:ui/frontend/package.json\u2020L21-L26\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#backend-ci-nodejsexpress","title":"Backend CI (Node.js/Express)","text":"<ul> <li><code>npm run lint</code>: run ESLint for backend controllers.   \u3010F:ui/backend/package.json\u2020L28-L33\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#usage","title":"Usage","text":"<p>Refer to this file for project structure insights to avoid redundant codebase exploration.</p>"},{"location":"architecture-multi-tenancy/","title":"Multi-Tenancy &amp; RBAC","text":"<p>Kube-DC implements a comprehensive multi-tenant architecture that leverages Kubernetes namespaces and Keycloak for identity and access management. This document explains how organizations, projects, and groups in Kube-DC are mapped to Kubernetes and Keycloak objects.</p>"},{"location":"architecture-multi-tenancy/#core-components-and-mapping-structure","title":"Core Components and Mapping Structure","text":"<p>The following diagram illustrates the mapping between Kube-DC structures and the underlying Kubernetes and Keycloak components:</p> <pre><code>graph TD\n    User[User 1] --&gt;|Authenticates| KC[Keycloak Realm]\n    User --&gt;|Obtains Group and Role| KCG[Keycloak Group]\n    User --&gt;|Obtains Group and Role| KCR[Keycloak Role]\n\n    ORG[Organization] --&gt;|Maps to| ORGNS[Organization Namespace]\n\n    ORGNS --&gt;|Contains| ORGGRP[Organization Group]\n\n    PROJ[Project A] --&gt;|Maps to| PNS[Project A NS]\n    PROJ2[Project B] --&gt;|Maps to| PNS2[Project B NS]\n\n    ORGGRP --&gt;|Maps to| K8GRPCRD[Group CRD]\n    ORGGRP --&gt;|Maps to| KCGRP[Keycloak Group]\n\n    KCGRP --&gt;|Maps to| K8SROLE[K8s RoleBinding]\n    KCR --&gt;|Maps to| K8SROLE\n\n    K8GRPCRD --&gt;|Defines permissions for| PNS\n    K8GRPCRD --&gt;|Defines permissions for| PNS2\n\n    KK[Keycloak Client Role] --&gt;|KK to K8s Role Mapping| K8R[K8s Role]</code></pre>"},{"location":"architecture-multi-tenancy/#organization-structure","title":"Organization Structure","text":""},{"location":"architecture-multi-tenancy/#organization","title":"Organization","text":"<p>An Organization is the top-level entity in Kube-DC that represents a company, department, or team.</p> <p>Example Organization YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Organization\nmetadata:\n  name: shalb\n  namespace: shalb\nspec: \n  description: \"Shalb organization\"\n  email: \"arti@shalb.com\"\n</code></pre> <p>Mapping:</p> <ul> <li>Each Organization maps to a dedicated Kubernetes namespace with the same name</li> <li>A corresponding Keycloak Client is created for the organization</li> <li>The Organization serves as a logical grouping for Projects and OrganizationGroups</li> </ul>"},{"location":"architecture-multi-tenancy/#project","title":"Project","text":"<p>A Project represents a logical grouping of resources within an Organization. Projects help segregate workloads and manage access control.</p> <p>Example Project YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre> <p>Mapping:</p> <ul> <li>Each Project maps to a dedicated Kubernetes namespace in the format: <code>{organization}-{project}</code> (e.g., <code>shalb-demo</code>)</li> <li>Projects receive their own network CIDR block for resource isolation</li> <li>Kubernetes namespaces provide the boundary for resource quotas and access control</li> </ul>"},{"location":"architecture-multi-tenancy/#organizationgroup","title":"OrganizationGroup","text":"<p>An OrganizationGroup maps users to roles within specific projects, defining what actions they can perform.</p> <p>Example OrganizationGroup YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - admin\n  - project: \"prod\"\n    roles:\n    - resource-manager\n</code></pre> <p>Mapping:</p> <ul> <li>OrganizationGroups are implemented as Kubernetes Custom Resource Definitions (CRDs)</li> <li>Each OrganizationGroup maps to a Keycloak Group</li> <li>The permissions defined in OrganizationGroups determine the Kubernetes RoleBindings that grant access to resources</li> <li>Different roles can be assigned for different projects</li> </ul>"},{"location":"architecture-multi-tenancy/#authentication-and-authorization-flow","title":"Authentication and Authorization Flow","text":"<p>User Authentication:</p> <ul> <li>Users authenticate through Keycloak</li> <li>Upon successful authentication, users receive JSON Web Tokens (JWTs)</li> </ul> <p>Group and Role Assignment:</p> <ul> <li>Users are assigned to Keycloak Groups based on their OrganizationGroup membership</li> <li>Keycloak maps these groups to corresponding roles</li> </ul> <p>Kubernetes Authorization:</p> <ul> <li>The Kubernetes API server validates the user's JWT</li> <li>RoleBindings determine what actions the user can perform within each namespace</li> <li>Resource access is controlled at the Project (namespace) level</li> </ul> <p>Resource Access:</p> <ul> <li>Users can only access resources in projects where they have appropriate role assignments</li> <li>Actions are restricted based on the permissions defined in their roles</li> </ul>"},{"location":"architecture-multi-tenancy/#role-based-access-control","title":"Role-Based Access Control","text":"<p>Kube-DC provides several built-in roles that can be assigned to users via OrganizationGroups:</p> <ul> <li>Admin: Full access to all resources within a project</li> <li>Resource Manager: Can create and manage resources, but cannot modify project settings</li> <li>Viewer: Read-only access to project resources</li> </ul> <p>Example Role YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Role\nmetadata:\n  name: resource-manager\n  namespace: shalb\nspec:\n  rules:\n  - apiGroups: [\"*\"]\n    resources: [\"pods\", \"services\", \"deployments\", \"statefulsets\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"kubevirt.io\"]\n    resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"architecture-multi-tenancy/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture-multi-tenancy/#kubernetes-components","title":"Kubernetes Components","text":"<ul> <li>Namespaces: Used to isolate Organizations and Projects</li> <li>RBAC: Role-Based Access Control for managing permissions</li> <li>CRDs: Custom Resource Definitions for Kube-DC specific resources</li> <li>NetworkPolicies: Ensure network isolation between Projects</li> </ul>"},{"location":"architecture-multi-tenancy/#keycloak-integration","title":"Keycloak Integration","text":"<ul> <li>Realm: Represents the authentication domain</li> <li>Clients: Each Organization has a dedicated client</li> <li>Groups: Map to OrganizationGroups in Kube-DC</li> <li>Roles: Define permissions that can be assigned to users</li> <li>Role Mappings: Connect Keycloak roles to Kubernetes RBAC</li> </ul>"},{"location":"architecture-multi-tenancy/#practical-application","title":"Practical Application","text":"<p>When a user is added to an organization group in Kube-DC:</p> <ol> <li>The corresponding Keycloak group membership is created</li> <li>The user inherits roles based on the group's permissions</li> <li>When the user accesses the Kubernetes API, their JWT contains the group and role information</li> <li>Kubernetes RBAC evaluates the JWT against RoleBindings to determine access</li> <li>The user can operate only within the boundaries of their assigned permissions</li> </ol> <p>This multi-layered approach ensures secure isolation between tenants while providing fine-grained access control within each project.</p>"},{"location":"architecture-networking/","title":"Networking (Kube-OVN, VLANs)","text":"<p>Kube-DC provides advanced networking capabilities through Kube-OVN, enabling multi-tenant network isolation, external connectivity, and flexible service exposure. This document explains the key networking components and how they operate within the Kube-DC architecture.</p>"},{"location":"architecture-networking/#network-architecture-overview","title":"Network Architecture Overview","text":"<p>Kube-DC's networking architecture is built on Kube-OVN, which supports both overlay and underlay networks and provides Virtual Private Cloud (VPC) capabilities for tenant isolation.</p> <pre><code>graph TB\n    Internet((Internet)) &lt;--&gt;|Ingress/Egress| FW[VPC Firewall]\n\n    subgraph \"Project A (Namespace + VPC)\"\n        PROJ_A_EIP[Project A EIP] &lt;--&gt; FW\n        PROJ_A_EIP --&gt;|Default NAT GW| PROJ_A_NET[Project A Network]\n\n        SVC_LB_A[Service LoadBalancer&lt;br/&gt;with annotation] --&gt;|Uses| PROJ_A_EIP\n        SVC_LB_A --&gt;|Routes to| POD_A[Pods]\n        SVC_LB_A --&gt;|Routes to| VM_A[VMs]\n\n        FIP_A[Floating IP] --&gt;|Maps to| VM_A_TARGET[Specific VM/Pod]\n    end\n\n    subgraph \"Project B (Namespace + VPC)\"\n        PROJ_B_EIP[Project B EIP] &lt;--&gt; FW\n        PROJ_B_EIP --&gt;|Default NAT GW| PROJ_B_NET[Project B Network]\n\n        SVC_LB_B[Service LoadBalancer&lt;br/&gt;with annotation] --&gt;|Uses| PROJ_B_EIP\n        SVC_LB_B --&gt;|Routes to| POD_B[Pods]\n        SVC_LB_B --&gt;|Routes to| VM_B[VMs]\n\n        DEDICATED_EIP[Dedicated EIP] &lt;--&gt; FW\n        SVC_LB_B_DEDICATED[Service LoadBalancer&lt;br/&gt;with dedicated EIP] --&gt;|Uses| DEDICATED_EIP\n    end</code></pre> <p>In this architecture: - Each project gets its own namespace and VPC - Each project receives its own EIP that acts as a NAT gateway for outbound traffic - Service LoadBalancers can route traffic to both Pods and VMs - Service LoadBalancers can use either the project's default EIP (via annotation) or a dedicated EIP - Floating IPs can map specific VMs or Pods to External IPs for direct access</p>"},{"location":"architecture-networking/#network-elements","title":"Network Elements","text":""},{"location":"architecture-networking/#user-visible-network-resources","title":"User-Visible Network Resources","text":""},{"location":"architecture-networking/#external-ip-eip","title":"External IP (EIP)","text":"<p>External IPs provide connectivity from the public internet to resources within Kube-DC. Each EIP is allocated from the provider network.</p> <p>Example EIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: ssh-arti\n  namespace: shalb-demo\nspec: {}  \n</code></pre>"},{"location":"architecture-networking/#floating-ip-fip","title":"Floating IP (FIP)","text":"<p>Floating IPs map an internal IP address (of a VM or pod) to an External IP, enabling direct access to specific resources.</p> <p>Example FIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: fedora-arti\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171\n  eip: ssh-arti\n</code></pre>"},{"location":"architecture-networking/#kubernetes-service","title":"Kubernetes Service","text":"<p>Standard Kubernetes Services for in-cluster service discovery and load balancing.</p>"},{"location":"architecture-networking/#service-type-loadbalancer","title":"Service Type LoadBalancer","text":"<p>Creates and maps an EIP to a service that routes traffic to pods or VMs. Can use either a dedicated EIP or the project's default EIP.</p> <p>Example Service LoadBalancer YAML with default gateway EIP:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n</code></pre> <p>Example Service LoadBalancer for VM SSH access:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: debian\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222\n      targetPort: 22\n</code></pre>"},{"location":"architecture-networking/#internal-network-resources","title":"Internal Network Resources","text":""},{"location":"architecture-networking/#dnat-rule","title":"DNAT Rule","text":"<p>Destination Network Address Translation rules proxy requests from the internet through an EIP to resources within the VPC network. These are created automatically when an EIP is associated with a resource.</p>"},{"location":"architecture-networking/#snat","title":"SNAT","text":"<p>Source Network Address Translation is used for outbound connections from VPC subnets through EIPs, allowing resources within the VPC to communicate with the internet.</p>"},{"location":"architecture-networking/#project-network-provisioning","title":"Project Network Provisioning","text":"<p>When a new project is created in Kube-DC:</p> <ol> <li>The project is allocated a dedicated subnet from the VPC CIDR range</li> <li>Each project connected to the internet receives an EIP</li> <li>All project outbound traffic is routed through its assigned EIP</li> <li>Project-specific network policies are applied for isolation</li> </ol> <p>Example project creation with CIDR allocation:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre>"},{"location":"architecture-networking/#load-balancer-implementation","title":"Load Balancer Implementation","text":"<p>Kube-DC uses a specialized implementation for Service LoadBalancers:</p> <ul> <li>When a Service with type <code>LoadBalancer</code> is created, an OVS-based LoadBalancer routes traffic to service endpoints</li> <li>Endpoints can be Kubernetes pods or KubeVirt VMs</li> <li>The LoadBalancer can use either:</li> <li>The project's default gateway EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"</code>)</li> <li>A dedicated EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-eip: \"eip-name\"</code>)</li> </ul>"},{"location":"architecture-networking/#automatic-external-endpoints-v0134","title":"Automatic External Endpoints (v0.1.34+)","text":"<p>Kube-DC automatically creates external endpoints for LoadBalancer services to enable cross-VPC communication.</p> <p>When a LoadBalancer receives an external IP, the controller creates: - External Service (<code>&lt;service-name&gt;-ext</code>): Headless service - Endpoints (<code>&lt;service-name&gt;-ext</code>): Points to the LoadBalancer's external IP</p> <p>This solves cross-VPC access by providing stable DNS names (e.g., <code>etcd-lb-ext.shalb-envoy.svc.cluster.local</code>) instead of hardcoded IPs. Endpoints are automatically updated when IPs change and deleted with the LoadBalancer service.</p> <p>External endpoints are labeled with <code>kube-dc.com/managed-by: service-lb-controller</code>.</p>"},{"location":"architecture-networking/#kube-ovn-for-vpc-management","title":"Kube-OVN for VPC Management","text":"<p>Kube-OVN is a key component of Kube-DC's networking architecture, providing the foundation for multi-tenant network isolation through VPC networks.</p>"},{"location":"architecture-networking/#vpc-isolation","title":"VPC Isolation","text":"<p>Different VPC networks are independent of each other and can be separately configured with: - Subnet CIDRs - Routing policies - Security policies - Outbound gateways - EIP allocations</p>"},{"location":"architecture-networking/#overlay-vs-underlay-networks","title":"Overlay vs. Underlay Networks","text":"<p>Kube-DC supports both networking approaches:</p>"},{"location":"architecture-networking/#overlay-networks","title":"Overlay Networks","text":"<ul> <li>Software-defined networks that encapsulate packets</li> <li>Provide maximum flexibility for network segmentation</li> <li>Independent of physical network topology</li> <li>Managed entirely by Kube-OVN</li> <li>Ideal for multi-tenant environments</li> </ul>"},{"location":"architecture-networking/#underlay-networks","title":"Underlay Networks","text":"<ul> <li>Direct mapping to physical network infrastructure</li> <li>Better performance with reduced encapsulation overhead</li> <li>Requires coordination with physical network infrastructure</li> <li>Physical switches handle data-plane forwarding</li> <li>Cannot be isolated by VPCs as they are managed by physical switches</li> </ul>"},{"location":"architecture-networking/#network-security","title":"Network Security","text":"<p>Kube-DC implements multiple layers of network security:</p> <p>Project Isolation</p> <ul> <li>Each project receives its own subnet</li> <li>Traffic between projects is controlled by network policies</li> </ul> <p>VPC Segmentation</p> <ul> <li>Projects can be placed in different VPCs for stricter isolation</li> <li>Each VPC has its own network stack and routing tables</li> </ul> <p>Kubernetes Network Policies</p> <ul> <li>Fine-grained control over ingress and egress traffic</li> <li>Can be applied at the namespace, pod, or service level</li> </ul> <p>Subnet ACLs</p> <ul> <li>Control traffic at the subnet level</li> <li>Provide an additional layer of security beyond network policies</li> </ul>"},{"location":"architecture-overview/","title":"Overall Architecture","text":"<p>Kube-DC provides a comprehensive multi-tenant cloud infrastructure platform built on Kubernetes and enhanced with enterprise-grade features like virtualization, networking, and identity management.</p>"},{"location":"architecture-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC architecture consists of several key components that work together to deliver a complete cloud platform:</p> <p></p>"},{"location":"architecture-overview/#architectural-layers","title":"Architectural Layers","text":"<p>Kube-DC is organized into main architectural layers:</p> <pre><code>graph TD\n    K8s[Kubernetes] --&gt; KubeVirt[KubeVirt]    \n    K8s --&gt; KubeOVN[Kube-OVN]    \n    K8s --&gt; Keycloak[Keycloak]    \n    K8s --&gt; LBController[Kube-DC LB Controller]    \n    K8s --&gt; MultiTenant[Multi-Tenant Controller]\n\n    KubeVirt --&gt;|Provides| VMs[Virtual Machines]\n    KubeOVN --&gt;|Manages| Networking[Network VLANs/VPCs]\n    Keycloak --&gt;|Controls| IAM[Identity &amp; Access]\n    LBController --&gt;|Enables| LoadBalancing[Load Balancing, Floating IPs]\n    MultiTenant --&gt;|Organizes| Resources[Organization and Projects]</code></pre> <p>Infrastructure Layer</p> <ul> <li>Bare metal servers or cloud infrastructure</li> <li>Kubernetes core services</li> <li>Storage subsystems</li> </ul> <p>Virtualization Layer</p> <ul> <li>KubeVirt for VM provisioning and management</li> <li>Container workloads</li> <li>Hybrid application support</li> </ul> <p>Networking Layer</p> <ul> <li>Kube-OVN for software-defined networking</li> <li>Multi-tenant network isolation</li> <li>External IP addressing and service exposure</li> </ul> <p>Management Layer</p> <ul> <li>Multi-tenancy resource organization</li> <li>Identity and access management via Keycloak</li> <li>User interface and API access</li> </ul>"},{"location":"architecture-overview/#multi-tenant-organization","title":"Multi-Tenant Organization","text":"<p>Kube-DC introduces a hierarchical resource organization model:</p> <ul> <li>Organizations - Top-level entities representing companies or teams</li> <li>Projects - Logical groupings of resources within an organization</li> <li>Groups - Collections of users with defined roles and permissions</li> </ul> <p>This multi-tenant structure maps to Kubernetes and Keycloak components to provide isolation and access control. For detailed information on the multi-tenancy architecture, see the Multi-Tenancy &amp; RBAC documentation.</p>"},{"location":"architecture-overview/#network-architecture","title":"Network Architecture","text":"<p>Kube-DC leverages Kube-OVN to provide advanced networking capabilities:</p> <ul> <li>Virtual Private Clouds (VPCs) for network isolation</li> <li>External and Floating IPs for service exposure</li> <li>Load balancing and service routing</li> </ul> <p>For detailed information on the networking architecture, see the Networking (Kube-OVN, VLANs) documentation.</p>"},{"location":"architecture-overview/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC integrates KubeVirt to enable VM workloads alongside containers:</p> <ul> <li>VM lifecycle management through Kubernetes APIs</li> <li>Hardware passthrough capabilities</li> <li>Mixed container and VM environments</li> </ul> <p>For detailed information on the virtualization architecture, see the Virtualization (KubeVirt) documentation.</p>"},{"location":"architecture-overview/#key-benefits","title":"Key Benefits","text":"<ul> <li>Multi-tenant isolation: Secure separation between organizations and projects</li> <li>Unified management: Single platform for VMs and containers</li> <li>Network flexibility: Advanced SDN capabilities with Kube-OVN</li> <li>Enterprise security: Integrated identity management with Keycloak</li> <li>API-driven architecture: Consistent interfaces for automation and integration</li> </ul>"},{"location":"architecture-virtualization/","title":"Virtualization (KubeVirt)","text":"<p>Kube-DC leverages KubeVirt to provide powerful virtual machine capabilities alongside traditional container workloads. This document covers the virtualization architecture, features, and how VMs are managed within the platform.</p>"},{"location":"architecture-virtualization/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC's virtualization layer is built on KubeVirt, which extends Kubernetes to support virtual machine workloads. This architecture enables consistent management of both containers and VMs through the same API and tooling.</p> <pre><code>graph TD\n    K8s[Kubernetes API] --&gt; KV[KubeVirt Controller]\n    K8s --&gt; CDI[Containerized Data Importer]\n\n    KV --&gt; VMI[VM Instances]\n    CDI --&gt; DV[Data Volumes]\n\n    VMI --&gt; POD[VM Pods]\n    DV --&gt; PVC[Persistent Volumes]\n\n    subgraph \"VM Management\"\n        KV\n        VMI\n        POD\n    end\n\n    subgraph \"Storage Management\"\n        CDI\n        DV\n        PVC\n    end\n\n    UI[Kube-DC Dashboard] --&gt; K8s\n    CLI[kubectl/virtctl] --&gt; K8s</code></pre>"},{"location":"architecture-virtualization/#core-components","title":"Core Components","text":""},{"location":"architecture-virtualization/#kubevirt-controller","title":"KubeVirt Controller","text":"<p>The KubeVirt controller manages the lifecycle of virtual machines by:</p> <ul> <li>Translating VM specifications into Kubernetes resources</li> <li>Scheduling VMs on appropriate nodes</li> <li>Managing VM state (start, stop, pause, resume)</li> <li>Providing VM migration capabilities</li> <li>Handling VM monitoring and health checks</li> </ul>"},{"location":"architecture-virtualization/#containerized-data-importer-cdi","title":"Containerized Data Importer (CDI)","text":"<p>CDI handles storage provisioning for VMs by:</p> <ul> <li>Creating and managing Data Volumes</li> <li>Importing disk images from HTTP/S3 sources</li> <li>Converting disk formats as needed</li> <li>Cloning existing volumes</li> </ul>"},{"location":"architecture-virtualization/#data-volumes","title":"Data Volumes","text":"<p>Data Volumes serve as the storage backbone for VMs, providing:</p> <ul> <li>Storage allocation for VM disks</li> <li>Integration with Kubernetes storage classes</li> <li>Automated provisioning and cleanup</li> </ul>"},{"location":"architecture-virtualization/#vm-management-in-kube-dc","title":"VM Management in Kube-DC","text":""},{"location":"architecture-virtualization/#vm-creation-and-configuration","title":"VM Creation and Configuration","text":"<p>Kube-DC allows users to create VMs through YAML definitions or the web UI. VM configurations include:</p> <p>Example VM Definition:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\n  namespace: demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: default/ovn-demo\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n        cpu:\n          cores: 2\n        memory:\n          guest: 4G\n      volumes:\n      - dataVolume:\n          name: ubuntu-base-img\n        name: root-volume\n</code></pre>"},{"location":"architecture-virtualization/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Kube-DC provides templates for a variety of operating systems:</p> <ul> <li>Ubuntu (20.04, 22.04, 24.04)</li> <li>Debian</li> <li>CentOS/RHEL</li> <li>Fedora</li> <li>Alpine Linux</li> <li>FreeBSD</li> <li>openSUSE</li> <li>Minimal images (cirros)</li> </ul>"},{"location":"architecture-virtualization/#network-integration","title":"Network Integration","text":"<p>VMs in Kube-DC are integrated with the same network architecture as containers:</p> <ul> <li>Each VM can connect to VPC networks via Multus CNI</li> <li>VMs receive IP addresses from the project's CIDR block</li> <li>Network policies apply to VMs just like containers</li> <li>VMs can use floating IPs and load balancer services</li> </ul>"},{"location":"architecture-virtualization/#storage-management","title":"Storage Management","text":"<p>Kube-DC provides flexible storage options for VMs:</p> <ul> <li>Support for multiple storage classes</li> <li>Persistent storage using Kubernetes PVCs</li> <li>Live volume resizing</li> <li>Volume snapshots and cloning</li> </ul>"},{"location":"architecture-virtualization/#vm-customization","title":"VM Customization","text":"<p>VMs can be customized through cloud-init configurations:</p> <pre><code>cloudInitNoCloud:\n  userData: |-\n    #cloud-config\n    chpasswd: { expire: False }\n    password: securepassword\n    ssh_pwauth: True\n    package_update: true\n    package_upgrade: true\n    packages:\n    - qemu-guest-agent\n    runcmd:\n    - [ systemctl, start, qemu-guest-agent ]\n</code></pre> <p>This allows for: - Setting initial passwords - SSH key distribution - Software installation - Custom scripts execution - Network configuration</p>"},{"location":"architecture-virtualization/#health-monitoring","title":"Health Monitoring","text":"<p>VMs in Kube-DC support health checks through:</p> <pre><code>readinessProbe:\n  guestAgentPing: {}\n  failureThreshold: 10\n  initialDelaySeconds: 20\n  periodSeconds: 10\n</code></pre> <p>Health checks ensure: - VM is properly booted - Guest agent is responsive - Cloud-init has completed - Custom health check scripts pass</p>"},{"location":"architecture-virtualization/#web-ui-management","title":"Web UI Management","text":"<p>Kube-DC provides an intuitive web interface for VM management:</p> <p></p>"},{"location":"architecture-virtualization/#vm-dashboard-features","title":"VM Dashboard Features","text":"<p>The VM dashboard provides:</p> <ul> <li>VM Status Monitoring: Running status, uptime, and conditions</li> <li>Performance Metrics: Real-time CPU, memory, and storage usage</li> <li>VM Details: OS version, network configuration, and node placement</li> <li>Console Access: Direct web-based console access to VMs</li> <li>SSH Terminal: Direct SSH access from the browser</li> <li>Network Information: IP addresses and VPC subnet details</li> </ul>"},{"location":"architecture-virtualization/#vm-lifecycle-management","title":"VM Lifecycle Management","text":"<p>Through the UI, administrators and users can:</p> <ul> <li>Create VMs from templates or custom images</li> <li>Start, stop, pause, and restart VMs</li> <li>Adjust resource allocations (CPU, memory)</li> <li>Take snapshots for backup purposes</li> <li>Clone VMs to create new instances</li> <li>Migrate VMs between nodes</li> </ul>"},{"location":"architecture-virtualization/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture-virtualization/#gpu-passthrough","title":"GPU Passthrough","text":"<p>Kube-DC supports GPU passthrough for high-performance computing and AI workloads:</p> <pre><code>domain:\n  devices:\n    gpus:\n    - deviceName: nvidia.com/GP102GL_Tesla_P40\n      name: gpu1\n</code></pre>"},{"location":"architecture-virtualization/#live-migration","title":"Live Migration","text":"<p>VMs can be migrated between nodes without downtime:</p> <pre><code>spec:\n  strategy:\n    type: LiveMigrate\n</code></pre>"},{"location":"architecture-virtualization/#vm-snapshots","title":"VM Snapshots","text":"<p>Kube-DC supports VM snapshots for point-in-time recovery:</p> <pre><code>apiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: my-vm-snapshot\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: my-vm\n</code></pre>"},{"location":"architecture-virtualization/#vm-templates","title":"VM Templates","text":"<p>Organization administrators can create standardized VM templates for their users, ensuring consistent deployments and reducing configuration errors.</p>"},{"location":"architecture-virtualization/#integration-with-multi-tenancy","title":"Integration with Multi-Tenancy","text":"<p>VMs in Kube-DC operate within the same multi-tenant architecture as containers:</p> <ul> <li>VMs are created within specific projects</li> <li>Organization and project permissions control VM access</li> <li>Network isolation is enforced between projects</li> <li>VM metrics are included in project billing and quotas</li> </ul>"},{"location":"architecture-virtualization/#best-practices","title":"Best Practices","text":""},{"location":"architecture-virtualization/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Allocate sufficient memory for the guest OS (minimum 1GB for most Linux distributions)</li> <li>Consider CPU overcommit ratios when planning node capacity</li> <li>Use appropriate storage classes for VM performance requirements</li> </ul>"},{"location":"architecture-virtualization/#vm-optimization","title":"VM Optimization","text":"<ul> <li>Install guest agents for improved integration</li> <li>Use cloud-init for automated VM configuration</li> <li>Configure readiness probes for proper health monitoring</li> <li>Use virtio drivers for improved performance</li> </ul>"},{"location":"architecture-virtualization/#conclusion","title":"Conclusion","text":"<p>Kube-DC's integration of KubeVirt provides a seamless experience for managing both VMs and containers in a single platform. This unified approach simplifies infrastructure management, improves resource utilization, and enables hybrid application architectures that combine the benefits of both virtualization and containerization.</p>"},{"location":"community-support/","title":"Community &amp; Support","text":"<p>Kube-DC has multiple channels for support, community engagement, and professional services. Choose the option that best fits your needs.</p>"},{"location":"community-support/#community-support_1","title":"Community Support","text":""},{"location":"community-support/#github-discussions","title":"GitHub Discussions","text":"<ul> <li>Ask questions and engage with the community</li> <li>Share your experiences and solutions</li> <li>Report bugs and request features</li> <li>Access to public roadmap and project updates</li> <li>Visit GitHub Discussions</li> </ul>"},{"location":"community-support/#slack-community","title":"Slack Community","text":"<p>Join our active Slack community to:</p> <ul> <li>Get real-time help from community members</li> <li>Connect with other Kube-DC users</li> <li>Share your use cases and solutions</li> <li>Stay updated on latest developments</li> <li>Join Kube-DC Slack</li> </ul>"},{"location":"community-support/#documentation","title":"Documentation","text":"<ul> <li>Comprehensive guides and tutorials</li> <li>API reference documentation</li> <li>Best practices and examples</li> <li>Browse Documentation</li> </ul>"},{"location":"community-support/#professional-services","title":"Professional Services","text":""},{"location":"community-support/#commercial-support","title":"Commercial Support","text":"<p>We offer various tiers of commercial support:</p>"},{"location":"community-support/#basic-support","title":"Basic Support","text":"<ul> <li>Business hours support (9/5)</li> <li>Email support</li> <li>24-hour response time</li> <li>Bug fixes and security updates</li> <li>Access to knowledge base</li> </ul>"},{"location":"community-support/#enterprise-support","title":"Enterprise Support","text":"<ul> <li>24/7 support coverage</li> <li>Priority response (2-hour SLA for critical issues)</li> <li>Direct access to engineering team</li> <li>Custom feature development</li> <li>Dedicated support engineer</li> <li>Regular health checks and reviews</li> </ul>"},{"location":"community-support/#professional-services_1","title":"Professional Services","text":""},{"location":"community-support/#implementation-services","title":"Implementation Services","text":"<ul> <li>Architecture design and review</li> <li>Production deployment assistance</li> <li>Migration planning and execution</li> <li>Performance optimization</li> <li>Security hardening</li> </ul>"},{"location":"community-support/#training","title":"Training","text":"<ul> <li>Admin and operator training</li> <li>Developer workshops</li> <li>Custom training programs</li> <li>Certification programs</li> </ul>"},{"location":"community-support/#consulting","title":"Consulting","text":"<ul> <li>Technical architecture consulting</li> <li>Scalability planning</li> <li>High availability design</li> <li>Security assessment</li> <li>Performance optimization</li> <li>Custom integration development</li> </ul>"},{"location":"community-support/#getting-support","title":"Getting Support","text":""},{"location":"community-support/#for-community-support","title":"For Community Support","text":"<ol> <li>Check the documentation</li> <li>Search existing GitHub Issues</li> <li>Join our Slack community</li> <li>Post on GitHub Discussions</li> </ol>"},{"location":"community-support/#for-commercial-support","title":"For Commercial Support","text":"<p>Contact our sales team:</p> <ul> <li>Email: support@kube-dc.com</li> <li>Website: https://kube-dc.com/</li> <li>Phone: +380632441621</li> </ul>"},{"location":"community-support/#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Check our Contributing Guide to learn how you can:</p> <ul> <li>Submit bug reports and feature requests</li> <li>Contribute code</li> <li>Improve documentation</li> <li>Share use cases and examples</li> </ul>"},{"location":"controller_diagram/","title":"Controller Architecture Diagram","text":"<p>A high-level view of Kube-DC controller components (excluding UI) and external dependencies.</p> <pre><code>flowchart TB\n  subgraph Installer\n    CD[cluster.dev IaC]\n  end\n\n  subgraph K8sCluster[\"Kubernetes Cluster &amp; CRDs\"]\n    CRDs[[\"Org, Project, OrgGroup, EIp, FIp CRDs\"]]\n  end\n\n  subgraph Manager[\"Controller Manager\"]\n    OR(OrganizationReconciler)\n    PR(ProjectReconciler)\n    OGR(OrganizationGroupReconciler)\n    EIP(EIpReconciler)\n    FIP(FIpReconciler)\n    SR(ServiceReconciler)\n  end\n\n  subgraph Logic[\"Business Logic Packages\"]\n    OGi[\"internal/organization\"]\n    PI[\"internal/project\"]\n    OGG[\"internal/organizationgroup\"]\n    SLP[\"internal/service_lb\"]\n    OBJ[\"internal/objmgr\"]\n    UTL[\"internal/utils\"]\n  end\n\n  subgraph Ext[\"External Dependencies\"]\n    KC[Keycloak]\n    KO[Kube-OVN]\n    KV[KubeVirt]\n    ML[Multus CNI]\n    CM[Cert-Manager]\n    PM[Prometheus &amp; Loki]\n  end\n\n  CD --&gt; CRDs\n  CRDs --&gt; OR &amp; PR &amp; OGR &amp; EIP &amp; FIP &amp; SR\n\n  OR --&gt; OGi\n  PR --&gt; PI\n  OGR --&gt; OGG\n  EIP --&gt; SLP\n  FIP --&gt; SLP\n  SR --&gt; SLP\n\n  SLP --&gt; KO\n  OGi --&gt; KC\n  PI --&gt; KO &amp; KV &amp; ML\n  PI --&gt; PM &amp; CM\n  PI --&gt; KC\n\n  style CRDs fill:#f9f,stroke:#333,stroke-width:2px\n  style Manager fill:#bbf,stroke:#333,stroke-width:2px\n  style Logic fill:#bfb,stroke:#333,stroke-width:2px\n  style Ext fill:#ffb,stroke:#333,stroke-width:2px\n  style Installer fill:#fbb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"controller_diagram/#networking-integration-kube-ovn-multus","title":"Networking Integration (Kube-OVN &amp; Multus)","text":"<p>Below is a focused diagram showing how Kube-OVN and Multus CNI are installed and integrated via the Project NetworkAttachmentDefinition.</p> <pre><code>flowchart LR\n  subgraph Installer\n    KOV[\"Kube-OVN Helm Chart\"]\n    MULT[\"Multus CNI Helm Chart\"]\n  end\n\n  KOV --&gt; MULT\n\n  subgraph CNIInfra[\"CNI Infrastructure\"]\n    OVN[\"ovn-daemon (kube-ovn)\"]\n    MPods[\"Multus Pods\"]\n  end\n\n  MULT --&gt; MPods\n  KOV --&gt; OVN\n  OVN &amp; MPods --&gt; CNIInfra\n\n  NewNAD[\"NewProjectNad Controller\"]\n  NADCRD[\"NetworkAttachmentDefinition CR\"]\n  CNIConfig[\"Spec.Config: {type:'kube-ovn', server_socket:'/run/openvswitch/kube-ovn-daemon.sock', provider:&lt;proj&gt;} \"]\n  PodAttach[\"Pod annotation 'k8s.v1.cni.cncf.io/networks' = NAD\"]\n\n  NewNAD --&gt; NADCRD\n  NADCRD --&gt; CNIConfig\n  CNIConfig --&gt; PodAttach\n  PodAttach --&gt; CNIInfra\n\n  style Installer fill:#fbb,stroke:#333,stroke-width:1px\n  style CNIInfra fill:#ffb,stroke:#333,stroke-width:1px\n  style NewNAD fill:#bfb,stroke:#333,stroke-width:1px\n  style NADCRD fill:#f9f,stroke:#333,stroke-width:1px</code></pre> <p>Referenced code: - Scheme registration: \u3010F:cmd/main.go\u2020L57-L60\u3011 - NAD controller: \u3010F:internal/project/res_nad.go\u2020L12-L27\u3011   - Installer sequence: \u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L94-L102\u3011\u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L119-L127\u3011</p>"},{"location":"controller_diagram/#eip-fip-serviceloadbalancer-networking-flows","title":"EIP, FIP &amp; ServiceLoadBalancer Networking Flows","text":"<pre><code>flowchart TD\n  subgraph ProjectNet[\"Project Networking Controllers\"]\n    EIPdef[\"NewProjectEip (Default Gateway EIP)\"]\n    EIPcr[NewProjectEip CR]\n    EIPsync[EIpReconciler]\n    FIPsync[FIpReconciler]\n    LBsync[ServiceReconciler]\n  end\n\n  subgraph OVNNB[\"OVN Northbound DB &amp; OVS\"]\n    OVNNBdb[ovn-nb.db]\n    OVSOCK[ovs-db socket]\n  end\n\n  EIPdef --&gt; EIPcr\n  EIPcr --&gt; EIPsync\n  EIPsync --&gt; OVNNBdb\n\n  FIPsync --&gt;|Sync EIP + Floating IP| OVNNBdb\n\n  LBsync --&gt;|Ensure external IP via EIp CR| OVNNBdb\n  LBsync --&gt;|Configure Virtual IPs in LB| OVNNBdb\n\n  OVNNBdb --&gt; OVSOCK\n\n  classDef flow fill:#eef,stroke:#666,stroke-width:1px;\n  class EIPdef,EIPcr,EIPsync,FIPsync,LBsync flow;</code></pre>"},{"location":"controller_diagram/#detailed-network-stack-implementation","title":"Detailed Network Stack Implementation","text":"<ol> <li>Project VPC &amp; Subnet provisioning (<code>internal/project/res_vpc.go</code>)</li> <li>Creates an OVN Virtual Private Cloud via <code>OvnVpc</code> CR and logical switch.</li> <li>NetworkAttachmentDefinition (<code>internal/project/res_nad.go</code>)</li> <li>Defines a Multus NAD with CNI config for <code>kube-ovn</code>, pointing at the OVS socket and project provider.</li> <li>SNAT Rule (<code>internal/project/res_snat.go</code>)</li> <li>Installs an <code>OvnSnatRule</code> to translate pod-source IPs to the project gateway EIP for outbound internet.</li> <li>Default Gateway EIP (<code>internal/project/res_eip_default.go</code>)</li> <li>Ensures a project-scoped <code>EIp</code> CR representing the default gateway external IP, created via <code>NewEipDefault</code>.</li> <li>Floating IP (FIp) (<code>internal/fip/res_eip.go</code> &amp; <code>FIpReconciler</code>)</li> <li>Syncs or creates EIp owned by FIp, then updates <code>FIp.Status.ExternalIP</code> after attaching the EIp to pods via OVN.</li> <li>Service LoadBalancer (<code>internal/service_lb/service_lb.go</code>, <code>internal/service_lb/eip_res.go</code>, <code>ServiceReconciler</code>)</li> <li><code>NewSvcLbEIpRes</code> allocates or binds an external IP for the Service.</li> <li><code>NewLoadBalancerRes</code> uses OVN NB client to define load balancer VIP\u2192backend mappings and injects rules into logical router/switch.</li> <li>Extra External Subnets (<code>internal/project/res_vpc.go</code>)</li> <li>Adds <code>ExtraExternalSubnets</code> field to <code>Vpc.Spec</code> when <code>project.Spec.EgressNetworkType</code> differs from the default external subnet, enabling multi-network external connectivity.    <pre><code>if externalNetwork.Name != defaultExternalSubnet.Name {\n    vpc.Spec.ExtraExternalSubnets = []string{externalNetwork.Name}\n}\n</code></pre>    \u3010F:internal/project/res_vpc.go\u2020L45-L52\u3011</li> </ol> <p>-Refer to code for detailed behavior: - Preamble and flag parsing: \u3010F:cmd/main.go\u2020L117-L131\u3011 - NAD CNI config: \u3010F:internal/project/res_nad.go\u2020L14-L31\u3011 - SNAT via OVN: \u3010F:internal/project/res_snat.go\u2020L14-L45\u3011 - Default EIP creation: \u3010F:internal/project/res_eip_default.go\u2020L15-L42\u3011 - FIp EIP sync: \u3010F:internal/fip/res_eip.go\u2020L25-L50\u3011 - Service LB orchestration: \u3010F:internal/service_lb/service_lb.go\u2020L30-L58\u3011\u3010F:internal/service_lb/eip_res.go\u2020L18-L40\u3011</p>"},{"location":"controller_diagram/#public-vs-cloud-external-networking","title":"Public vs Cloud External Networking","text":"<p>Kube-DC supports two external network types: public (direct public IPs) and cloud (cloud-provider-backed). The type influences EIP/FIP provisioning and SNAT rules:</p> <p><pre><code>// ExternalNetworkType defines how external networks are treated:\ntype ExternalNetworkType string\nconst (\n  ExternalNetworkTypePublic ExternalNetworkType = \"public\"\n  ExternalNetworkTypeCloud  ExternalNetworkType = \"cloud\"\n)\n\n// MasterConfig defaults per resource if not overridden:\nDefaultGwNetworkType, DefaultEipNetworkType,\nDefaultFipNetworkType, DefaultSvcLbNetworkType\n</code></pre> \u3010F:api/kube-dc.com/v1/types.go\u2020L1-L18\u3011</p>"},{"location":"controller_diagram/#project-egress-network-selection","title":"Project Egress Network Selection","text":"<p>The project spec may set <code>egressNetworkType</code> to choose the external subnet for VPC/SNAT/EIP.</p> <p><code>go // GenerateProjectVpc picks externalSubnet based on project.Spec.EgressNetworkType: externalNetwork, _ := utils.SelectBestExternalSubnet(ctx, cli, project.Spec.EgressNetworkType)</code>\u3010F:internal/project/res_vpc.go\u2020L55-L61\u3011</p>"},{"location":"controller_diagram/#snat-rules-for-outbound-traffic","title":"SNAT Rules for Outbound Traffic","text":"<p>SNAT rules ensure pod egress to internet through the gateway EIP:</p> <p><code>go // NewProjectSnat creates OvnSnatRule linking project namespace to gateway EIP base.GeneratedObject = &amp;kubeovn.OvnSnatRule{   Spec: OvnSnatRuleSpec{     OvnEip: DefaultOvnEipName(project, externalSubnet.Name),     Vpc:    projectNamespace,     VpcSubnet: SubnetName(project),   }, }</code>\u3010F:internal/project/res_snat.go\u2020L14-L45\u3011</p>"},{"location":"controller_diagram/#default-gateway-eip-vs-floating-ip","title":"Default Gateway EIP vs Floating IP","text":"<ul> <li>Default Gateway EIP: A single EIp CR per project created by <code>NewProjectEip</code> when no explicit EIP exists. Used for SNAT and default outbound.</li> <li>Floating IP (FIp): EIp allocated per FIp CR to attach public IPs to specific workloads.</li> </ul> <p><code>go // NewProjectEip ensures default project gateway EIp exists WithGetFunction(func(...) {   eip, err := resourcesProcessor.GetProjectGwEip()   if IsNotFound(err) {     newEip, _ := NewEipDefault(...)     base.GeneratedObject = newEip   } })</code>\u3010F:internal/project/res_eip_default.go\u2020L15-L37\u3011</p> <p><code>go // SyncEip for FIp: derives EIp name from FIp and creates/gets it // then FIpReconciler attaches exclusive ownership in OVN</code>\u3010F:internal/fip/res_eip.go\u2020L25-L40\u3011</p>"},{"location":"controller_diagram/#service-loadbalancer-external-ip-binding","title":"Service LoadBalancer External IP Binding","text":"<p>ServiceReconciler uses annotations or defaults to bind EIp to Services:</p> <p>```go // Get or create EIp for Service LB via NewSvcLbEIpRes eipSyncer := NewSvcLbEIpRes(ctx, cli, svc, project) eipSyncer.Sync(ctx)</p> <p>// Configure OVN LB VRRP rules via NewLoadBalancerRes lbRes := NewLoadBalancerRes(ctx, cli, svc, endpoints, eipSyncer.Found(), project) lbRes.Sync(ctx) ```\u3010F:internal/service_lb/eip_res.go\u2020L18-L40\u3011\u3010F:internal/service_lb/service_lb.go\u2020L75-L98\u3011</p>"},{"location":"core-features/","title":"Core Features","text":"<p>Kube-DC extends Kubernetes with a robust set of features designed for enterprise data center operations. This page provides detailed technical specifications and use cases for each of Kube-DC's core capabilities.</p> <p>Looking for a Architectural details? Visit our architectural overview.</p>"},{"location":"core-features/#organization-management","title":"Organization Management","text":"<p>Foundation for Multi-Tenancy</p> <p>Organization Management provides the foundation for Kube-DC's multi-tenant capabilities, enabling complete isolation between different users and groups.</p> <p>Kube-DC's multi-tenant architecture allows service providers to host multiple organizations with complete isolation and customization.</p> <p>Capabilities:</p> <ul> <li>Multi-Organization Support: Host multiple organizations on a single Kube-DC installation with complete logical separation</li> <li>Custom SSO Integration: Each organization can configure its own identity provider:<ul> <li>Google Workspace / Gmail</li> <li>Microsoft Active Directory / Azure AD</li> <li>GitHub</li> <li>GitLab</li> <li>LDAP</li> <li>SAML 2.0 providers</li> <li>OpenID Connect providers</li> </ul> </li> <li>Hierarchical Group Management: Create and manage groups within organizations with inheritance of permissions</li> <li>Flexible RBAC: Assign fine-grained permissions to groups for specific projects or resources</li> <li>Organizational Quotas: Set resource limits at the organization level to ensure fair resource allocation</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Host multiple client organizations with separate authentication systems</li> <li>Enterprise IT: Separate departments with different authentication requirements</li> <li>Educational Institutions: Provide isolated environments for different departments or research groups</li> </ul>"},{"location":"core-features/#namespace-as-a-service","title":"Namespace as a Service","text":"<p>Projects and Workloads</p> <p>Namespaces in Kube-DC function as projects, providing isolated environments for deploying and managing diverse workloads.</p> <p>Every project in Kube-DC is allocated its own Kubernetes namespace with extended capabilities for running both containers and virtual machines.</p> <p>Capabilities:</p> <ul> <li>Unified Management: Deploy and manage both VMs and containers from a single interface</li> <li>Project Isolation: Complete network and resource isolation between projects</li> <li>Resource Quotas: Set limits on CPU, memory, storage, and other resources per project</li> <li>Integrated Dashboard: View and manage all workloads through a unified web interface</li> <li>Custom Templates: Create and use templates for quick deployment of common workloads</li> </ul> <p>Real-World Applications</p> <ul> <li>Application Modernization: Run legacy VMs alongside containerized microservices</li> <li>Development Environments: Provide isolated environments for development, testing, and staging</li> <li>Mixed Workloads: Support teams that require both traditional and cloud-native infrastructure</li> </ul>"},{"location":"core-features/#network-management","title":"Network Management","text":"<p>Advanced Connectivity</p> <p>Kube-DC's network capabilities enable sophisticated connectivity options while maintaining isolation between projects.</p> <p>Kube-DC provides advanced networking capabilities that bridge traditional data center networking with cloud-native concepts.</p> <p>Capabilities:</p> <ul> <li>Dedicated VPC per Project: Each project gets its own virtual network environment</li> <li>VLAN Integration: Connect to physical network infrastructure using VLANs</li> <li>Software-Defined Networking: Create overlay networks with software-defined control</li> <li>Network Peering: Connect project networks with each other or with external networks</li> <li>NAT and Internet Gateway: Control outbound and inbound internet access per project</li> <li>External IP Assignment: Assign public IPs directly to VMs or Kubernetes services</li> <li>Load Balancer Integration: Create and manage load balancers for services and VMs</li> <li>Network Policies: Define granular rules for network traffic filtering</li> <li>DNS Management: Automatic DNS for services and VMs with custom domain support</li> </ul> <p>Real-World Applications</p> <ul> <li>Hybrid Cloud Deployments: Extend on-premises networks to containerized workloads</li> <li>Multi-Tier Applications: Create complex network topologies for enterprise applications</li> <li>Secure Isolation: Create zero-trust network environments with fine-grained control</li> </ul>"},{"location":"core-features/#virtualization","title":"Virtualization","text":"<p>KubeVirt Integration</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities fully integrated with Kubernetes.</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities integrated with Kubernetes.</p> <p>Capabilities:</p> <ul> <li>Hardware Vendor Support: Compatible with major hardware vendors' servers and components</li> <li>GPU Passthrough: Support for Nvidia GPU passthrough to virtual machines</li> <li>ARM Support: Run VMs on ARM-based infrastructure</li> <li>Web Console: Access VM consoles directly through the web UI</li> <li>SSH Integration: SSH access management with key authentication</li> <li>Live Migration: Move running VMs between nodes without downtime</li> <li>Snapshots: Create point-in-time snapshots of VM volumes</li> <li>VM Templates: Create and use templates for rapid VM provisioning</li> <li>Custom Boot Options: Configure boot order, firmware settings, and UEFI support</li> <li>VM Import/Export: Import existing VMs from other platforms</li> </ul> <p>Real-World Applications</p> <ul> <li>Legacy Application Support: Run applications that require traditional VMs</li> <li>Windows Workloads: Host Windows servers alongside Linux containers</li> <li>GPU-Accelerated Computing: Provide GPU resources for AI/ML or rendering workloads</li> <li>Specialized Operating Systems: Run operating systems not supported in containers</li> </ul>"},{"location":"core-features/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>API-Driven Architecture</p> <p>Kube-DC's API-driven approach enables automation and integration with popular infrastructure tools.</p> <p>Kube-DC leverages and extends the Kubernetes API to enable comprehensive infrastructure automation.</p> <p>Capabilities:</p> <ul> <li>Native Kubernetes API: Manage all Kube-DC resources using standard Kubernetes tools</li> <li>Custom Resource Definitions (CRDs): Extended Kubernetes objects for managing organizations, projects, VMs, and more</li> <li>GitOps Compatible: Deploy and manage infrastructure using GitOps workflows</li> </ul> <p>Real-World Applications</p> <ul> <li>Automated Infrastructure: Create fully automated infrastructure provisioning workflows</li> <li>Self-Service Portals: Build custom self-service interfaces using the Kube-DC API</li> <li>CI/CD Integration: Include infrastructure provisioning in CI/CD pipelines</li> <li>Multi-Cloud Management: Manage Kube-DC resources alongside other cloud resources</li> </ul>"},{"location":"core-features/#integrated-flexible-billing","title":"Integrated Flexible Billing","text":"<p>Cost Management</p> <p>Track, allocate, and manage costs across all resources with Kube-DC's comprehensive billing capabilities.</p> <p>Kube-DC includes comprehensive resource tracking and billing capabilities suitable for both service providers and internal IT organizations.</p> <p>Capabilities:</p> <ul> <li>Resource Metering: Track usage of CPU, memory, storage, GPU, and network resources</li> <li>Custom Pricing Models: Define pricing tiers for different resource types and customers</li> <li>Project-Based Billing: Track and bill resource usage at the project level</li> <li>Cost Allocation: Assign costs to organizational units, projects, or individual resources</li> <li>Quota Enforcement: Automatically enforce resource limits based on billing status</li> <li>Usage Reporting: Generate detailed usage reports for analysis and billing</li> <li>Billing API: Integrate with external billing systems through a comprehensive API</li> <li>Chargeback Models: Support for various internal chargeback models for enterprise use</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Bill customers for exact resource usage</li> <li>Enterprise IT: Implement internal chargeback or showback for departmental resource usage</li> <li>Resource Optimization: Identify resource usage patterns and optimize costs</li> </ul>"},{"location":"core-features/#management-services","title":"Management Services","text":"<p>Value-Added Services</p> <p>Extend Kube-DC's capabilities by offering managed services on top of the core platform.</p> <p>Kube-DC provides a platform for delivering managed services on top of its infrastructure.</p> <p>Capabilities:</p> <p>Database as a Service: Deploy and manage databases with automated operations</p> <ul> <li>PostgreSQL</li> <li>MySQL/MariaDB</li> <li>Microsoft SQL Server</li> <li>And more</li> </ul> <p>Object Storage: S3-compatible storage with multi-tenancy support</p> <p>NoSQL Databases: Managed NoSQL database offerings</p> <ul> <li>Redis</li> <li>MongoDB</li> <li>Elasticsearch/OpenSearch</li> </ul> <p>AI/ML Platform: Infrastructure for deploying and serving AI/ML models</p> <ul> <li>LLM serving</li> <li>Model training infrastructure</li> <li>GPU resource allocation</li> </ul> <p>Backup Services: Automated backup solutions for VMs and containers Monitoring as a Service: Multi-tenant monitoring solutions Service Catalog: Self-service provisioning of common services</p> <p>Real-World Applications</p> <ul> <li>Internal Platform Team: Provide managed services to development teams</li> <li>Managed Service Providers: Offer value-added services beyond basic infrastructure</li> <li>AI/ML Operations: Provide specialized infrastructure for data science teams</li> </ul>"},{"location":"internal-billing-integration/","title":"Internal Billing Integration Documentation","text":""},{"location":"internal-billing-integration/#overview","title":"Overview","text":"<p>This document describes the internal architecture and implementation details of the billing API integration within the Kube-DC platform. This integration provides organization-level billing management and project-specific cost analysis through the Kube-DC UI.</p>"},{"location":"internal-billing-integration/#architecture-overview","title":"Architecture Overview","text":""},{"location":"internal-billing-integration/#system-components","title":"System Components","text":"<pre><code>graph TB\n    subgraph \"Kube-DC Frontend\"\n        UI[Billing UI Component]\n        Auth[JWT Authentication]\n    end\n\n    subgraph \"Kube-DC Backend\"\n        Proxy[Billing Proxy Controller]\n        TokenSvc[Token Service]\n        AuthZ[Authorization Layer]\n    end\n\n    subgraph \"Billing Service\"\n        API[Billing API]\n        DB[(PostgreSQL)]\n    end\n\n    UI --&gt; Auth\n    Auth --&gt; Proxy\n    Proxy --&gt; TokenSvc\n    TokenSvc --&gt; AuthZ\n    AuthZ --&gt; API\n    API --&gt; DB</code></pre>"},{"location":"internal-billing-integration/#authentication-authorization-flow","title":"Authentication &amp; Authorization Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant BillingAPI\n\n    User-&gt;&gt;Frontend: Access Billing Page\n    Frontend-&gt;&gt;Backend: GET /api/billing/project/{ns}/overview\n    Note over Frontend,Backend: JWT Token in Authorization header\n\n    Backend-&gt;&gt;Backend: Extract &amp; validate JWT token\n    Backend-&gt;&gt;Backend: Check namespace permissions\n\n    alt Valid token &amp; authorized namespace\n        Backend-&gt;&gt;BillingAPI: GET /api/project/{ns}/overview\n        BillingAPI-&gt;&gt;Backend: Billing data response\n        Backend-&gt;&gt;Frontend: Authorized data\n        Frontend-&gt;&gt;User: Display billing information\n    else Invalid token or unauthorized\n        Backend-&gt;&gt;Frontend: 401/403 Error\n        Frontend-&gt;&gt;User: Access denied message\n    end</code></pre>"},{"location":"internal-billing-integration/#implementation-details","title":"Implementation Details","text":""},{"location":"internal-billing-integration/#backend-integration","title":"Backend Integration","text":""},{"location":"internal-billing-integration/#file-structure","title":"File Structure","text":"<pre><code>ui/backend/\n\u251c\u2500\u2500 controllers/billing/\n\u2502   \u2514\u2500\u2500 billingController.js     # Main billing proxy controller\n\u251c\u2500\u2500 routes/\n\u2502   \u2514\u2500\u2500 billing.js              # Billing API routes\n\u251c\u2500\u2500 utils/\n\u2502   \u2514\u2500\u2500 logger.js               # Logging utility\n\u2514\u2500\u2500 app.js                      # Main app with billing routes\n</code></pre>"},{"location":"internal-billing-integration/#key-components","title":"Key Components","text":"<p>Billing Controller (<code>controllers/billing/billingController.js</code>) - Proxies requests to internal billing service - Implements JWT token validation - Enforces namespace-based authorization - Handles error responses and logging</p> <p>Authentication Flow <pre><code>// Token extraction\nconst token = tokenService.getToken(req);\n\n// JWT decoding and validation\nconst decodedToken = decodeJWT(token);\nconst userNamespaces = decodedToken.namespaces || [];\n\n// Authorization check\nif (!userNamespaces.includes(namespace)) {\n  return sendErrorResponse(res, 403, 'Access denied to namespace');\n}\n</code></pre></p> <p>Service Communication - Internal service URL: <code>billing-dashboard-svc.billing.svc.cluster.local:5000</code> - Uses Kubernetes service discovery - No external network access required</p>"},{"location":"internal-billing-integration/#frontend-integration","title":"Frontend Integration","text":""},{"location":"internal-billing-integration/#file-structure_1","title":"File Structure","text":"<pre><code>ui/frontend/src/app/ManageOrganization/\n\u251c\u2500\u2500 Billing/\n\u2502   \u2514\u2500\u2500 Billing.tsx             # Main billing component\n\u251c\u2500\u2500 OrganizationRoutes.tsx      # Route definitions\n\u251c\u2500\u2500 OrganizationSidebar.tsx     # Navigation sidebar\n\u2514\u2500\u2500 OrganizationLayout.tsx      # Layout logic\n</code></pre>"},{"location":"internal-billing-integration/#key-features","title":"Key Features","text":"<p>Billing Component (<code>Billing/Billing.tsx</code>) - Uses PatternFly design system - Displays project billing summaries in table format - Implements loading states and error handling - Follows existing Kube-DC UI patterns</p> <p>Security Implementation <pre><code>// Namespace extraction from JWT\nconst getUserNamespaces = React.useMemo(() =&gt; {\n  if (!token) return [];\n  try {\n    const decodedToken = decodeJWT(token);\n    return decodedToken.namespaces || [];\n  } catch (error) {\n    return [];\n  }\n}, [token]);\n\n// API calls with authentication\nconst response = await fetch(`/api/billing/project/${namespace}/overview`, {\n  headers: {\n    'Authorization': `Bearer ${token}`,\n    'Content-Type': 'application/json'\n  },\n  credentials: 'include'\n});\n</code></pre></p>"},{"location":"internal-billing-integration/#api-endpoints","title":"API Endpoints","text":""},{"location":"internal-billing-integration/#available-endpoints","title":"Available Endpoints","text":"Method Endpoint Description Authentication GET <code>/api/billing/health</code> Service health check Required GET <code>/api/billing/projects</code> List accessible projects Required GET <code>/api/billing/project/:namespace/overview</code> Project billing details Required + Namespace access"},{"location":"internal-billing-integration/#response-format","title":"Response Format","text":"<p>Project Overview Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"namespace\": \"project-name\",\n    \"billing_summary\": {\n      \"current_month_total\": 1672.31,\n      \"last_day_spend\": 109.16,\n      \"billing_period\": \"2025-09\",\n      \"total_cost_per_hour\": 2.32\n    },\n    \"compute_instances\": {\n      \"running_pods\": 10,\n      \"running_vms\": 1,\n      \"cpu_cores\": 2.4,\n      \"memory_gib\": 8.8,\n      \"cost_per_hour\": 0.15\n    },\n    \"cost_breakdown\": {\n      \"cpu_cost\": 89.45,\n      \"memory_cost\": 45.23,\n      \"storage_cost\": 12.67,\n      \"network_cost\": 0.00,\n      \"public_ip_cost\": 0.00\n    }\n  },\n  \"timestamp\": \"2025-10-01T15:25:00.000Z\"\n}\n</code></pre></p>"},{"location":"internal-billing-integration/#security-model","title":"Security Model","text":""},{"location":"internal-billing-integration/#jwt-token-structure","title":"JWT Token Structure","text":"<pre><code>{\n  \"org\": \"organization-name\",\n  \"namespaces\": [\"project-1\", \"project-2\"],\n  \"groups\": [\"org-admin\", \"user\"],\n  \"exp\": 1696176000,\n  \"iat\": 1696089600\n}\n</code></pre>"},{"location":"internal-billing-integration/#authorization-levels","title":"Authorization Levels","text":"Role Access Level Permissions <code>org-admin</code> Organization-wide All projects in organization <code>project-user</code> Project-specific Only assigned namespaces <code>guest</code> No access No billing data access"},{"location":"internal-billing-integration/#security-layers","title":"Security Layers","text":"<ol> <li>OIDC Authentication - External identity provider</li> <li>JWT Token Validation - Backend token verification</li> <li>Namespace Authorization - Per-project access control</li> <li>Network Security - Internal service communication only</li> </ol>"},{"location":"internal-billing-integration/#configuration","title":"Configuration","text":""},{"location":"internal-billing-integration/#environment-variables","title":"Environment Variables","text":"<p>Backend Configuration <pre><code># Billing service endpoint (internal)\nBILLING_API_URL=http://billing-dashboard-svc.billing.svc.cluster.local:5000\n\n# Logging level\nLOG_LEVEL=info\n</code></pre></p> <p>Frontend Configuration Uses existing Kube-DC ConfigMap pattern: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dc-frontend-config\ndata:\n  env.js: |\n    window.backendURL = 'https://backend.stage.kube-dc.com';\n    window.frontendURL = 'https://console.stage.kube-dc.com';\n    window.keycloakURL = 'https://login.stage.kube-dc.com';\n</code></pre></p>"},{"location":"internal-billing-integration/#network-policies","title":"Network Policies","text":"<pre><code># Internal service communication\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: kube-dc-billing-access\nspec:\n  podSelector:\n    matchLabels:\n      app: kube-dc-backend\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: billing\n    ports:\n    - protocol: TCP\n      port: 5000\n</code></pre>"},{"location":"internal-billing-integration/#error-handling","title":"Error Handling","text":""},{"location":"internal-billing-integration/#http-status-codes","title":"HTTP Status Codes","text":"Code Description Cause 200 Success Request completed successfully 401 Unauthorized Missing or invalid JWT token 403 Forbidden Valid token, insufficient permissions 503 Service Unavailable Billing service unreachable 500 Internal Server Error Unexpected server error"},{"location":"internal-billing-integration/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"success\": false,\n  \"error\": \"Access denied to namespace\",\n  \"details\": {\n    \"namespace\": \"requested-project\",\n    \"availableNamespaces\": [\"project-1\", \"project-2\"]\n  },\n  \"timestamp\": \"2025-10-01T15:25:00.000Z\"\n}\n</code></pre>"},{"location":"internal-billing-integration/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"internal-billing-integration/#log-levels","title":"Log Levels","text":"<ul> <li>INFO: Normal operations, API calls</li> <li>WARN: Authentication failures, permission denials  </li> <li>ERROR: Service errors, network issues</li> <li>DEBUG: Detailed request/response data (development only)</li> </ul>"},{"location":"internal-billing-integration/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li>API response times</li> <li>Authentication failure rates</li> <li>Service availability</li> <li>Error rates by endpoint</li> <li>Namespace access patterns</li> </ul>"},{"location":"internal-billing-integration/#development-guidelines","title":"Development Guidelines","text":""},{"location":"internal-billing-integration/#code-standards","title":"Code Standards","text":"<ul> <li>Follow existing Kube-DC patterns</li> <li>Use PatternFly components for UI consistency</li> <li>Implement proper error handling</li> <li>Add comprehensive logging</li> <li>Write JSDoc comments for public methods</li> </ul>"},{"location":"internal-billing-integration/#testing-approach","title":"Testing Approach","text":"<ul> <li>Unit tests for controller logic</li> <li>Integration tests for API endpoints</li> <li>Frontend component tests</li> <li>End-to-end authentication flows</li> </ul>"},{"location":"internal-billing-integration/#deployment-process","title":"Deployment Process","text":"<ol> <li>Backend changes deployed via Helm chart</li> <li>Frontend changes built into container image</li> <li>Configuration updates via ConfigMaps</li> <li>Rolling deployment with health checks</li> </ol>"},{"location":"internal-billing-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"internal-billing-integration/#common-issues","title":"Common Issues","text":"<p>\"Authentication token required\" - Check JWT token presence in request headers - Verify token format (Bearer scheme) - Confirm OIDC authentication is working</p> <p>\"Access denied to namespace\" - Verify user has access to requested project - Check JWT token namespace claims - Confirm RBAC configuration</p> <p>\"Billing service unreachable\" - Check billing service pod status - Verify network connectivity - Confirm service DNS resolution</p>"},{"location":"internal-billing-integration/#debug-commands","title":"Debug Commands","text":"<pre><code># Check service status\nkubectl get pods -n kube-dc\nkubectl get pods -n billing\n\n# Check service connectivity\nkubectl exec -n kube-dc deployment/kube-dc-backend -- \\\n  curl -v http://billing-dashboard-svc.billing.svc.cluster.local:5000/api/health\n\n# Check logs\nkubectl logs -n kube-dc deployment/kube-dc-backend\nkubectl logs -n billing deployment/billing-dashboard\n</code></pre>"},{"location":"internal-billing-integration/#future-enhancements","title":"Future Enhancements","text":""},{"location":"internal-billing-integration/#planned-features","title":"Planned Features","text":"<ul> <li>Cost trend analysis and forecasting</li> <li>Budget alerts and notifications</li> <li>Resource optimization recommendations</li> <li>Detailed cost attribution reports</li> <li>Integration with cloud provider billing APIs</li> </ul>"},{"location":"internal-billing-integration/#technical-improvements","title":"Technical Improvements","text":"<ul> <li>Response caching for performance</li> <li>Real-time cost updates via WebSocket</li> <li>Advanced filtering and search capabilities</li> <li>Export functionality for billing reports</li> <li>Integration with monitoring systems</li> </ul> <p>This document is maintained by the Kube-DC development team. For questions or updates, please contact the platform team.</p>"},{"location":"managing-os-images/","title":"Managing OS Images in Kube-DC","text":"<p>This guide explains how to manage operating system images in the Kube-DC platform, including adding new OS options, modifying existing configurations, and updating the system.</p>"},{"location":"managing-os-images/#overview","title":"Overview","text":"<p>OS images in Kube-DC are configured through a Kubernetes ConfigMap that defines: - Available operating systems in the VM creation UI - Default resource requirements (memory, CPU, storage) - Firmware and virtualization settings - Cloud-init configurations - Image URLs and user credentials</p>"},{"location":"managing-os-images/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Helm Chart    \u2502\u2500\u2500\u2500\u25b6\u2502   ConfigMap      \u2502\u2500\u2500\u2500\u25b6\u2502  Backend API    \u2502\n\u2502   Template      \u2502    \u2502 images-configmap \u2502    \u2502 /os-images      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                       \u2502  Frontend UI    \u2502\n                       \u2502 Create VM Modal \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"managing-os-images/#configmap-structure","title":"ConfigMap Structure","text":"<p>The OS images are defined in <code>/charts/kube-dc/templates/os-images-configmap.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: images-configmap\n  namespace: {{ .Release.Namespace }}\ndata:\n  images.yaml: |\n    images:\n      - OS_NAME: \"Ubuntu 24.04\"\n        CLOUD_USER: ubuntu\n        OS_IMAGE_URL: \"https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\"\n        MIN_MEMORY: \"1G\"\n        MIN_VCPU: \"1\"\n        MIN_STORAGE: \"20G\"\n        FIRMWARE_TYPE: \"bios\"\n        MACHINE_TYPE: \"q35\"\n        FEATURES: \"acpi\"\n        CLOUD_INIT: |\n          #cloud-config\n          package_update: true\n          packages:\n            - qemu-guest-agent\n</code></pre>"},{"location":"managing-os-images/#configuration-fields","title":"Configuration Fields","text":""},{"location":"managing-os-images/#required-fields","title":"Required Fields","text":"Field Description Example <code>OS_NAME</code> Display name in UI dropdown <code>\"Ubuntu 24.04\"</code> <code>CLOUD_USER</code> Default SSH user for the OS <code>ubuntu</code> <code>OS_IMAGE_URL</code> HTTP URL to the disk image <code>https://example.com/image.qcow2</code>"},{"location":"managing-os-images/#resource-requirements","title":"Resource Requirements","text":"Field Description Example Notes <code>MIN_MEMORY</code> Minimum RAM requirement <code>\"8G\"</code>, <code>\"512M\"</code> Supports G/M suffixes <code>MIN_VCPU</code> Minimum CPU cores <code>\"2\"</code> String format <code>MIN_STORAGE</code> Minimum disk size <code>\"60G\"</code> Supports G suffix"},{"location":"managing-os-images/#virtualization-settings","title":"Virtualization Settings","text":"Field Description Options Notes <code>FIRMWARE_TYPE</code> Boot firmware <code>\"bios\"</code>, <code>\"efi\"</code> EFI required for Windows 11 <code>MACHINE_TYPE</code> QEMU machine type <code>\"q35\"</code>, <code>\"pc-q35-rhel8.6.0\"</code> Specific types for compatibility <code>FEATURES</code> Virtualization features <code>\"acpi\"</code>, <code>\"hyperv,acpi,apic,smm,tpm\"</code> Comma-separated list"},{"location":"managing-os-images/#supported-features","title":"Supported Features","text":"<ul> <li><code>acpi</code> - Advanced Configuration and Power Interface</li> <li><code>apic</code> - Advanced Programmable Interrupt Controller  </li> <li><code>hyperv</code> - Microsoft Hyper-V enlightenments</li> <li><code>smm</code> - System Management Mode</li> <li><code>tpm</code> - Trusted Platform Module (required for Windows 11)</li> </ul>"},{"location":"managing-os-images/#os-specific-configurations","title":"OS-Specific Configurations","text":""},{"location":"managing-os-images/#linux-distributions","title":"Linux Distributions","text":"<p>Ubuntu/Debian: <pre><code>- OS_NAME: \"Ubuntu 24.04\"\n  CLOUD_USER: ubuntu\n  MIN_MEMORY: \"1G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"20G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n</code></pre></p> <p>CentOS/RHEL: <pre><code>- OS_NAME: \"CentOS Stream 9\"\n  CLOUD_USER: centos\n  MIN_MEMORY: \"2G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"20G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n</code></pre></p>"},{"location":"managing-os-images/#windows-systems","title":"Windows Systems","text":"<p>Windows 11: <pre><code>- OS_NAME: \"Windows 11 Enterprise\"\n  CLOUD_USER: Administrator\n  MIN_MEMORY: \"8G\"\n  MIN_VCPU: \"4\"\n  MIN_STORAGE: \"60G\"\n  FIRMWARE_TYPE: \"efi\"\n  MACHINE_TYPE: \"pc-q35-rhel8.6.0\"\n  FEATURES: \"hyperv,acpi,apic,smm,tpm\"\n</code></pre></p>"},{"location":"managing-os-images/#adding-a-new-os-image","title":"Adding a New OS Image","text":""},{"location":"managing-os-images/#step-1-prepare-the-image","title":"Step 1: Prepare the Image","text":"<ol> <li>Obtain the disk image (qcow2, raw, or vmdk format)</li> <li>Host the image on an HTTP server accessible to your cluster</li> <li>Test the image to ensure it boots correctly</li> </ol>"},{"location":"managing-os-images/#step-2-update-the-configmap","title":"Step 2: Update the ConfigMap","text":"<p>Edit <code>/charts/kube-dc/templates/os-images-configmap.yaml</code>:</p> <pre><code># Add your new OS entry\n- OS_NAME: \"Fedora 40\"\n  CLOUD_USER: fedora\n  OS_IMAGE_URL: \"https://download.fedoraproject.org/pub/fedora/linux/releases/40/Cloud/x86_64/images/Fedora-Cloud-Base-40-1.14.x86_64.qcow2\"\n  MIN_MEMORY: \"2G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"25G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n  CLOUD_INIT: |\n    #cloud-config\n    package_update: true\n    packages:\n      - qemu-guest-agent\n    runcmd:\n      - systemctl enable --now qemu-guest-agent\n</code></pre>"},{"location":"managing-os-images/#step-3-deploy-the-changes","title":"Step 3: Deploy the Changes","text":"<p>Option A: Helm Upgrade (Recommended) <pre><code># From the project root\nhelm upgrade kube-dc ./charts/kube-dc -n kube-dc\n</code></pre></p> <p>Option B: Direct ConfigMap Update <pre><code># Apply the ConfigMap directly\nkubectl apply -f charts/kube-dc/templates/os-images-configmap.yaml\n</code></pre></p>"},{"location":"managing-os-images/#step-4-reload-the-backend","title":"Step 4: Reload the Backend","text":"<p>The backend caches OS images for performance. After updating the ConfigMap:</p> <pre><code># Restart the backend to reload the cache\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n\n# Or wait for the cache TTL (30 seconds) to expire\n</code></pre>"},{"location":"managing-os-images/#modifying-existing-os-images","title":"Modifying Existing OS Images","text":""},{"location":"managing-os-images/#inline-editing","title":"Inline Editing","text":"<p>You can modify the ConfigMap directly in Kubernetes:</p> <pre><code># Edit the ConfigMap in your cluster\nkubectl edit configmap images-configmap -n kube-dc\n</code></pre> <p>Example: Increase Windows 11 memory requirement: <pre><code># Change from:\nMIN_MEMORY: \"8G\"\n# To:\nMIN_MEMORY: \"16G\"\n</code></pre></p> <p>After saving, restart the backend: <pre><code>kubectl rollout restart deployment/kube-dc-backend -n kube-dc\n</code></pre></p>"},{"location":"managing-os-images/#updating-image-urls","title":"Updating Image URLs","text":"<p>If an image URL changes or becomes unavailable:</p> <ol> <li> <p>Update the ConfigMap: <pre><code># Old URL\nOS_IMAGE_URL: \"https://old-server.com/ubuntu-24.04.qcow2\"\n# New URL  \nOS_IMAGE_URL: \"https://new-server.com/ubuntu-24.04.qcow2\"\n</code></pre></p> </li> <li> <p>Apply changes: <pre><code>kubectl apply -f charts/kube-dc/templates/os-images-configmap.yaml\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n</code></pre></p> </li> </ol>"},{"location":"managing-os-images/#testing-changes","title":"Testing Changes","text":""},{"location":"managing-os-images/#verify-configmap-update","title":"Verify ConfigMap Update","text":"<pre><code># Check the ConfigMap was updated\nkubectl get configmap images-configmap -n kube-dc -o yaml\n\n# Test the API endpoint\ncurl -s \"https://backend.stage.kube-dc.com/api/create-vm/your-namespace/os-images\" | jq '.[].OS_NAME'\n</code></pre>"},{"location":"managing-os-images/#test-in-ui","title":"Test in UI","text":"<ol> <li>Open the Kube-DC web interface</li> <li>Navigate to Create VM</li> <li>Check the Operation System dropdown</li> <li>Verify your new OS appears with correct parameters</li> <li>Select the OS and confirm memory/CPU/storage auto-populate</li> </ol>"},{"location":"managing-os-images/#troubleshooting","title":"Troubleshooting","text":""},{"location":"managing-os-images/#os-not-appearing-in-ui","title":"OS Not Appearing in UI","text":"<p>Check the ConfigMap: <pre><code>kubectl describe configmap images-configmap -n kube-dc\n</code></pre></p> <p>Verify backend logs: <pre><code>kubectl logs -n kube-dc deployment/kube-dc-backend --tail=50\n</code></pre></p> <p>Common issues: - YAML syntax errors in ConfigMap - Backend cache not refreshed - Network connectivity to image URL</p>"},{"location":"managing-os-images/#vm-creation-fails","title":"VM Creation Fails","text":"<p>Check image accessibility: <pre><code># Test if the image URL is reachable\ncurl -I \"https://your-image-url.com/image.qcow2\"\n</code></pre></p> <p>Verify resource requirements: - Ensure cluster has sufficient resources - Check storage class availability - Verify network policies allow image downloads</p>"},{"location":"managing-os-images/#backend-cache-issues","title":"Backend Cache Issues","text":"<p>The backend caches OS images for 30 seconds. To force refresh:</p> <pre><code># Restart backend pods\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n\n# Or wait for cache expiration (30 seconds)\n</code></pre>"},{"location":"managing-os-images/#best-practices","title":"Best Practices","text":""},{"location":"managing-os-images/#image-management","title":"Image Management","text":"<ol> <li>Use stable URLs - Avoid URLs that change frequently</li> <li>Host images reliably - Use CDNs or reliable hosting</li> <li>Test images - Verify images boot before adding to production</li> <li>Document changes - Keep track of image versions and changes</li> </ol>"},{"location":"managing-os-images/#resource-requirements_1","title":"Resource Requirements","text":"<ol> <li>Set realistic minimums - Don't under-provision resources</li> <li>Consider workload - Different use cases need different resources  </li> <li>Test performance - Verify VMs perform well with set resources</li> </ol>"},{"location":"managing-os-images/#security","title":"Security","text":"<ol> <li>Verify image sources - Only use trusted image providers</li> <li>Scan images - Check for vulnerabilities before deployment</li> <li>Use HTTPS - Always use secure URLs for image downloads</li> <li>Regular updates - Keep OS images updated with security patches</li> </ol>"},{"location":"managing-os-images/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"managing-os-images/#custom-cloud-init","title":"Custom Cloud-Init","text":"<p>For complex initialization requirements:</p> <pre><code>CLOUD_INIT: |\n  #cloud-config\n  users:\n    - name: admin\n      groups: sudo\n      shell: /bin/bash\n      sudo: ALL=(ALL) NOPASSWD:ALL\n  packages:\n    - docker.io\n    - nginx\n  runcmd:\n    - systemctl enable docker\n    - systemctl start docker\n    - docker run -d -p 80:80 nginx\n</code></pre>"},{"location":"managing-os-images/#windows-specific-settings","title":"Windows-Specific Settings","text":"<p>For Windows VMs, additional configuration may be needed:</p> <pre><code># Windows Server 2022\n- OS_NAME: \"Windows Server 2022\"\n  CLOUD_USER: Administrator\n  MIN_MEMORY: \"4G\"\n  MIN_VCPU: \"2\"\n  MIN_STORAGE: \"80G\"\n  FIRMWARE_TYPE: \"efi\"\n  MACHINE_TYPE: \"pc-q35-rhel8.6.0\"\n  FEATURES: \"hyperv,acpi,apic,smm,tpm\"\n  BOOT_ORDER: \"cdrom,disk\"\n  ADDITIONAL_DISKS: \"virtio-drivers\"\n</code></pre>"},{"location":"managing-os-images/#api-reference","title":"API Reference","text":"<p>The OS images are served via the backend API:</p> <p>Endpoint: <code>GET /api/create-vm/{namespace}/os-images</code></p> <p>Response format: <pre><code>[\n  {\n    \"OS_NAME\": \"Ubuntu 24.04\",\n    \"CLOUD_USER\": \"ubuntu\",\n    \"OS_IMAGE_URL\": \"https://cloud-images.ubuntu.com/...\",\n    \"MIN_MEMORY\": \"1G\",\n    \"MIN_VCPU\": \"1\",\n    \"MIN_STORAGE\": \"20G\",\n    \"FIRMWARE_TYPE\": \"bios\",\n    \"MACHINE_TYPE\": \"q35\",\n    \"FEATURES\": \"acpi\",\n    \"CLOUD_INIT\": \"#cloud-config\\n...\"\n  }\n]\n</code></pre></p>"},{"location":"managing-os-images/#support","title":"Support","text":"<p>For additional help: - Check the Kube-DC documentation - Review troubleshooting guides - Open an issue in the project repository</p>"},{"location":"product-backlog/","title":"Kube-DC Product Backlog","text":"<p>This document outlines the current product backlog for the Kube-DC project, organized by epics and features.</p>"},{"location":"product-backlog/#active-epics","title":"\ud83d\ude80 Active Epics","text":""},{"location":"product-backlog/#epic-windows-support","title":"[Epic] Windows Support","text":"<p>Status: Done</p>"},{"location":"product-backlog/#epic-vmware-migration","title":"[Epic] VMware Migration","text":"<p>Status: Research Phase</p> <ul> <li>\ud83d\udd0d VMware vSphere migration research (CDI, vjailbreak)</li> <li>Investigate migration tools and methodologies</li> <li>Evaluate CDI (Containerized Data Importer) for VM migration</li> <li>Research vjailbreak and other migration utilities</li> </ul>"},{"location":"product-backlog/#epic-organization-management","title":"[Epic] Organization Management","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udccb UI for project and roles</li> <li>Implement project management interface </li> <li>Role-based access control UI components</li> <li> <p>User and group management interfaces</p> </li> <li> <p>\ud83c\udfa8 Customize Login Page</p> </li> <li>Branding and customization options</li> <li>Organization-specific login themes</li> </ul>"},{"location":"product-backlog/#epic-ui-implementation-on-backend","title":"[Epic] UI Implementation on Backend","text":"<p>Status: Multiple Items in Progress</p> <ul> <li>\u274c UI Clone Disks - Not working</li> <li>Fix disk cloning functionality in UI</li> <li> <p>Ensure proper CDI integration</p> </li> <li> <p>\ud83d\udccb UI Create VM from PVC/DataVolume</p> </li> <li>Interface for VM creation from existing storage</li> <li> <p>DataVolume selection and configuration</p> </li> <li> <p>\ud83c\udf10 UI Add VM Static IP</p> </li> <li>Static IP assignment interface</li> <li> <p>Network configuration management</p> </li> <li> <p>\ud83c\udf10 UI Add VM FIP (Floating IP)</p> </li> <li>Floating IP assignment and management</li> <li> <p>Integration with FIP CRD resources</p> </li> <li> <p>\u2696\ufe0f UI Add Load Balancer Setup</p> </li> <li>Load balancer configuration interface</li> <li> <p>Service exposure management</p> </li> <li> <p>\ud83d\udd04 UI Migrate/Clone VM (rook/ceph)</p> </li> <li>VM migration interface with Rook/Ceph backend</li> <li> <p>Live migration capabilities</p> </li> <li> <p>\ud83d\udc65 UI VM Groups</p> </li> <li>VM grouping and management features</li> <li>Bulk operations on VM groups</li> </ul>"},{"location":"product-backlog/#epic-installer","title":"[Epic] Installer","text":"<p>Status: Enhancement Phase</p> <ul> <li>\ud83d\uddc4\ufe0f Postgres DB for Keycloak and Billing to be dedicated in installer stack</li> <li>Separate PostgreSQL deployment for Keycloak</li> <li> <p>Database isolation and management</p> </li> <li> <p>\u26a1 Simplify Installer</p> </li> <li>Streamline installation process</li> <li> <p>Reduce complexity and dependencies</p> </li> <li> <p>\ud83d\udda5\ufe0f Single host install</p> </li> <li>Support for single-node deployments</li> <li> <p>All-in-one installation option</p> </li> <li> <p>\ud83d\udd27 Test on VMware vSX</p> </li> <li>Validation on VMware infrastructure</li> <li> <p>Compatibility testing and documentation</p> </li> <li> <p>\ud83d\udd10 Fix hardcoded passwords in loki.yaml</p> </li> <li>Security improvement for Loki configuration</li> <li>Dynamic password generation</li> </ul>"},{"location":"product-backlog/#epic-licensing","title":"[Epic] Licensing","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udcc4 License for Node Limits per installation</li> <li>Implement licensing system</li> <li>Node-based licensing model</li> <li>License validation and enforcement</li> </ul>"},{"location":"product-backlog/#epic-billing","title":"[Epic] Billing","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udcb0 Billing system implementation</li> <li>Usage tracking and billing</li> <li>Integration with licensing system</li> <li>Cost management features</li> </ul>"},{"location":"product-backlog/#epic-observability","title":"[Epic] Observability","text":"<p>Status: Enhancement Phase</p> <ul> <li>\ud83d\udcca Logs</li> <li>Centralized logging improvements</li> <li> <p>Log aggregation and analysis</p> </li> <li> <p>\ud83d\udcc8 Metrics</p> </li> <li>Enhanced monitoring and metrics collection</li> <li> <p>Performance dashboards</p> </li> <li> <p>\ud83d\udea8 Alerts</p> </li> <li>Alerting system implementation</li> <li>Notification and escalation policies</li> </ul>"},{"location":"product-backlog/#epic-gpu-support","title":"[Epic] GPU Support","text":"<p>Status: Research Phase</p> <ul> <li>\ud83c\udfae Evaluate Project HAMI</li> <li>GPU sharing and management solution</li> <li>Integration assessment with KubeVirt</li> </ul>"},{"location":"product-backlog/#epic-managed-services","title":"[Epic] Managed Services","text":"<p>Status: Planning</p> <ul> <li>\u2638\ufe0f K8s CAPI (Cluster API)</li> <li>Kubernetes cluster management</li> <li> <p>Multi-cluster operations</p> </li> <li> <p>\u2638\ufe0f K8s Vcluster</p> </li> <li>Virtual cluster implementation</li> <li> <p>Tenant isolation improvements</p> </li> <li> <p>\ud83d\uddc4\ufe0f Rook S3</p> </li> <li>Object storage services</li> <li> <p>S3-compatible storage backend</p> </li> <li> <p>\ud83d\uddc4\ufe0f RDS Percona Operators</p> </li> <li>Database-as-a-Service implementation</li> <li>MySQL/PostgreSQL managed services</li> </ul>"},{"location":"product-backlog/#epic-kubevirt-enhancements","title":"[Epic] KubeVirt Enhancements","text":"<p>Status: High Priority</p> <ul> <li>\ud83d\udd04 CDI Cloning (Priority)</li> <li>VM disk cloning capabilities</li> <li> <p>Efficient storage management</p> </li> <li> <p>\ud83d\udd25 CPU, Memory, GPU Hotplug</p> </li> <li>Dynamic resource allocation</li> <li>Live resource scaling for VMs</li> </ul>"},{"location":"product-backlog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":""},{"location":"product-backlog/#critical-bugs","title":"Critical Bugs","text":"<ul> <li>\ud83d\udd27 UI get_kubeconfig.sh namespace issue</li> <li>Fix namespace handling in kubeconfig generation</li> <li> <p>Ensure proper authentication and authorization</p> </li> <li> <p>\ud83e\uddf9 Fix issue with stale jobs with pshell</p> </li> <li>Clean up orphaned pshell jobs</li> <li>Improve job lifecycle management</li> </ul>"},{"location":"product-backlog/#priority-matrix","title":"\ud83d\udcca Priority Matrix","text":""},{"location":"product-backlog/#high-priority","title":"High Priority","text":"<ol> <li>CDI Cloning functionality</li> <li>Windows metrics fixes</li> <li>UI disk cloning repairs</li> <li>Critical bug fixes (kubeconfig, pshell jobs)</li> </ol>"},{"location":"product-backlog/#medium-priority","title":"Medium Priority","text":"<ol> <li>VMware migration research</li> <li>GPU support evaluation</li> <li>Installer simplification</li> <li>Observability enhancements</li> </ol>"},{"location":"product-backlog/#low-priority","title":"Low Priority","text":"<ol> <li>Licensing system</li> <li>Billing implementation</li> <li>Managed services expansion</li> <li>UI enhancements (VM groups, static IP)</li> </ol> <p>Last Updated: September 2025 Document Owner: Kube-DC Product Team</p>"},{"location":"project_resources/","title":"Project Resources Documentation","text":"<p>This document provides a comprehensive overview of all resources created by the Kube-DC Project controller, their finalizers, ownership patterns, and deletion dependencies.</p>"},{"location":"project_resources/#resource-creation-order","title":"Resource Creation Order","text":"<p>When a Project is created, resources are synchronized in this order:</p> <ol> <li>Namespace - Project namespace (<code>{org}-{project}</code>)</li> <li>VPC - Kube-OVN Virtual Private Cloud</li> <li>EIp (Default Gateway) - External IP for project gateway</li> <li>Subnet - Kube-OVN subnet for project pods</li> <li>NetworkAttachmentDefinition - CNI network configuration</li> <li>OvnSnatRule - SNAT rule for outbound traffic</li> <li>Secrets - SSH keypairs and authorized keys</li> <li>RBAC - Roles and RoleBindings</li> <li>VpcDns - DNS configuration for VPC</li> </ol>"},{"location":"project_resources/#detailed-resource-breakdown","title":"Detailed Resource Breakdown","text":""},{"location":"project_resources/#1-namespace","title":"1. Namespace","text":"<ul> <li>Resource: <code>v1.Namespace</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: None (managed by Kubernetes)</li> <li>Created by: <code>NewProjectNamespace()</code> in <code>internal/project/res_namespace.go</code></li> <li>Dependencies: None (first resource created)</li> </ul>"},{"location":"project_resources/#2-vpc-virtual-private-cloud","title":"2. VPC (Virtual Private Cloud)","text":"<ul> <li>Resource: <code>kubeovn.io/v1.Vpc</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectVpc()</code> in <code>internal/project/res_vpc.go</code></li> <li>Dependencies: Namespace must exist</li> <li> <p>Configuration:</p> </li> <li> <p>Static routes to external subnets</p> </li> <li>Extra external subnets based on <code>egressNetworkType</code></li> </ul>"},{"location":"project_resources/#3-eip-external-ip-default-gateway","title":"3. EIp (External IP - Default Gateway)","text":"<ul> <li>Resource: <code>kube-dc.com/v1.EIp</code></li> <li>Name: <code>default-gw</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>eip.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectEip()</code> in <code>internal/project/res_eip_default.go</code></li> <li>Dependencies: Namespace must exist</li> <li> <p>Ownership States:</p> </li> <li> <p><code>Released</code>: No active owners (initial state)</p> </li> <li><code>Shared</code>: Has SNAT rule and/or LoadBalancer services as owners</li> <li><code>Exclusive</code>: Used by FIp resources</li> </ul>"},{"location":"project_resources/#4-ovneip-underlying-ovn-external-ip","title":"4. OvnEip (Underlying OVN External IP)","text":"<ul> <li>Resource: <code>kubeovn.io/v1.OvnEip</code></li> <li>Name: <code>{organization}-{project}-{external-subnet}</code> (e.g., <code>shalb-envoy-ext-public</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: EIp controller via <code>NewOvEipRes()</code> in <code>internal/eip/ovn_eip_res.go</code></li> <li>Dependencies: EIp must exist, external subnet must be available</li> <li> <p>Labels:</p> </li> <li> <p><code>network.kube-dc.com/eip</code>: <code>{namespace}.{eip-name}</code></p> </li> <li> <p>Annotations:</p> </li> <li> <p><code>kube-dc.com/ovn-eip-created-by-eip</code>: <code>{namespace}.{eip-name}</code></p> </li> </ul>"},{"location":"project_resources/#5-subnet","title":"5. Subnet","text":"<ul> <li>Resource: <code>kubeovn.io/v1.Subnet</code></li> <li>Name: <code>{organization}-{project}-default</code> (e.g., <code>shalb-envoy-default</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectSubnet()</code> in <code>internal/project/res_subnet.go</code></li> <li>Dependencies: VPC must exist</li> <li> <p>Configuration:</p> </li> <li> <p>CIDR block from project spec</p> </li> <li>Associated with project VPC</li> <li>Gateway IP (first IP in CIDR)</li> </ul>"},{"location":"project_resources/#6-networkattachmentdefinition","title":"6. NetworkAttachmentDefinition","text":"<ul> <li>Resource: <code>k8s.cni.cncf.io/v1.NetworkAttachmentDefinition</code></li> <li>Name: <code>default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectNad()</code> in <code>internal/project/res_nad.go</code></li> <li>Dependencies: Subnet must exist</li> <li>Configuration: Kube-OVN CNI configuration pointing to project subnet</li> </ul>"},{"location":"project_resources/#7-ovnsnatrule","title":"7. OvnSnatRule","text":"<ul> <li>Resource: <code>kubeovn.io/v1.OvnSnatRule</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectSnat()</code> in <code>internal/project/res_snat.go</code></li> <li>Dependencies: OvnEip must exist and have IP assigned</li> <li> <p>Configuration:</p> </li> <li> <p>Links project subnet to external IP</p> </li> <li>Enables outbound internet access for pods</li> </ul>"},{"location":"project_resources/#8-secrets","title":"8. Secrets","text":""},{"location":"project_resources/#ssh-key-pair-secret","title":"SSH Key Pair Secret","text":"<ul> <li>Resource: <code>v1.Secret</code></li> <li>Name: <code>ssh-keypair-default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectKeyPairSeret()</code> in <code>internal/project/res_secret.go</code></li> <li>Content: Generated SSH public/private key pair</li> </ul>"},{"location":"project_resources/#authorized-keys-secret","title":"Authorized Keys Secret","text":"<ul> <li>Resource: <code>v1.Secret</code></li> <li>Name: <code>authorized-keys-default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectAuthKeySecret()</code> in <code>internal/project/res_secret.go</code></li> <li>Content: SSH public keys for VM access</li> </ul>"},{"location":"project_resources/#9-rbac-resources","title":"9. RBAC Resources","text":""},{"location":"project_resources/#role","title":"Role","text":"<ul> <li>Resource: <code>rbac.authorization.k8s.io/v1.Role</code></li> <li>Name: <code>admin</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectRole()</code> in <code>internal/project/res_role.go</code></li> <li>Permissions: Full access to project resources (pods, services, VMs, etc.)</li> </ul>"},{"location":"project_resources/#rolebinding","title":"RoleBinding","text":"<ul> <li>Resource: <code>rbac.authorization.k8s.io/v1.RoleBinding</code></li> <li>Name: <code>org-admin</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectRoleBinding()</code> in <code>internal/project/res_role_binding.go</code></li> <li>Subject: <code>{organization}:org-admin</code> group</li> </ul>"},{"location":"project_resources/#10-vpcdns-service","title":"10. VpcDns Service","text":"<ul> <li>Resource: <code>v1.Service</code></li> <li>Name: <code>slr-vpc-dns-{organization}-{project}</code></li> <li>Namespace: <code>kube-system</code></li> <li>Finalizer: None (managed by service controller)</li> <li>Created by: <code>NewProjectVpcDns()</code> in <code>internal/project/res_vpc_dns.go</code></li> <li>Purpose: DNS resolution for VPC</li> </ul>"},{"location":"project_resources/#finalizers-summary","title":"Finalizers Summary","text":"Resource Type Finalizer Controller Project <code>project.kube-dc.com/finalizer</code> kube-dc-manager EIp <code>eip.kube-dc.com/finalizer</code> kube-dc-manager OvnEip <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller Vpc <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller Subnet <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller OvnSnatRule <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller NetworkAttachmentDefinition <code>project.kube-dc.com/finalizer</code> kube-dc-manager Secrets <code>project.kube-dc.com/finalizer</code> kube-dc-manager Role <code>project.kube-dc.com/finalizer</code> kube-dc-manager RoleBinding <code>project.kube-dc.com/finalizer</code> kube-dc-manager"},{"location":"project_resources/#deletion-order-and-dependencies","title":"Deletion Order and Dependencies","text":"<p>When a Project is deleted, resources must be removed in reverse dependency order:</p>"},{"location":"project_resources/#phase-1-application-resources","title":"Phase 1: Application Resources","text":"<ol> <li>Pods, Services, VMs - User workloads (deleted by users/operators)</li> <li>FIp resources - Floating IPs (if any exist)</li> </ol>"},{"location":"project_resources/#phase-2-snat-and-networking","title":"Phase 2: SNAT and Networking","text":"<ol> <li>OvnSnatRule - Must be deleted before OvnEip</li> <li>OvnEip - Must be deleted before EIp and Subnet</li> </ol>"},{"location":"project_resources/#phase-3-project-infrastructure","title":"Phase 3: Project Infrastructure","text":"<ol> <li>EIp - External IP resource</li> <li>NetworkAttachmentDefinition - CNI configuration</li> <li>Secrets - SSH keys and authorized keys</li> <li>RBAC - Roles and RoleBindings</li> <li>Subnet - Must be deleted before VPC</li> <li>VPC - Virtual Private Cloud</li> <li>VpcDns - DNS service</li> <li>Namespace - Project namespace (last)</li> </ol>"},{"location":"project_resources/#common-deletion-issues","title":"Common Deletion Issues","text":""},{"location":"project_resources/#stuck-finalizers","title":"Stuck Finalizers","text":"<ul> <li>OvnEip: May get stuck if SNAT rule deletion fails</li> <li>Subnet: May get stuck if pods are still running</li> <li>EIp: May get stuck if project is deleted before EIp controller processes it</li> </ul>"},{"location":"project_resources/#manual-cleanup-commands","title":"Manual Cleanup Commands","text":"<pre><code># Remove stuck finalizers (use with caution)\nkubectl patch ovn-snat-rule {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch ovn-eip {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch subnet {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch eip {name} -n {namespace} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch project {name} -n {org-namespace} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\n</code></pre>"},{"location":"project_resources/#eip-ownership-patterns","title":"EIp Ownership Patterns","text":""},{"location":"project_resources/#ownership-states","title":"Ownership States","text":"<ul> <li>Released: <code>ownershipType: Released</code>, <code>owners: []</code> - No active users</li> <li>Shared: <code>ownershipType: Shared</code>, <code>owners: [...]</code> - Multiple users (SNAT + Services)</li> <li>Exclusive: <code>ownershipType: Exclusive</code>, <code>owners: [single]</code> - Single FIp owner</li> </ul>"},{"location":"project_resources/#owner-types","title":"Owner Types","text":"<ul> <li><code>Snat</code>: SNAT rule using the EIP for outbound traffic</li> <li><code>ServiceLb</code>: LoadBalancer service using the EIP</li> <li><code>FIp</code>: Floating IP using the EIP exclusively</li> </ul>"},{"location":"project_resources/#ownership-transitions","title":"Ownership Transitions","text":"<pre><code>Released \u2192 Shared (first owner added)\nShared \u2192 Released (last owner removed)\nReleased \u2192 Exclusive (FIp claims EIP)\nExclusive \u2192 Released (FIp releases EIP)\n</code></pre>"},{"location":"project_resources/#organization-limits","title":"Organization Limits","text":"<p>Organizations have a configurable limit on the number of ready projects they can contain (default: 3). This limit is enforced by the Project controller during reconciliation.</p> <ul> <li>Configuration: Set via <code>MasterConfig.OrganizationProjectsLimit</code></li> <li>Enforcement: Projects exceeding the limit will not be reconciled until space becomes available</li> <li>Status: Projects blocked by limits show as not ready but remain in the cluster</li> </ul>"},{"location":"project_resources/#enhanced-limit-enforcement-v0131-dev1","title":"Enhanced Limit Enforcement (v0.1.31-dev1+)","text":"<p>The Project controller now provides comprehensive feedback when organization limits are hit:</p> <p>Detailed Logging: - Organization project status with ready/pending counts and project names - Clear error messages with organization, namespace, and limit context - Debug-level logs showing available slots and project lists</p> <p>Status Conditions: Projects blocked by limits receive a <code>LimitCheck</code> condition: <pre><code>status:\n  ready: false\n  conditions:\n  - type: LimitCheck\n    status: \"False\"\n    reason: LimitExceeded\n    message: \"organization limit (3 projects) reached - ready projects: 3 (limit: 3)\"\n    lastTransitionTime: \"2025-01-19T16:45:00Z\"\n</code></pre></p> <p>Automatic Retry: - Projects are automatically requeued every 30 seconds - Reconciliation proceeds when limit space becomes available - No manual intervention required</p> <p>Projects can use different external network types: - cloud: Uses <code>ext-cloud</code> subnet (default) - public: Uses <code>ext-public</code> subnet (real public IPs)</p> <p>The <code>egressNetworkType</code> in Project spec determines which external subnet is used for the default gateway EIP.</p>"},{"location":"quickstart-hetzner/","title":"Master-Worker Setup on Hetzner Dedicated Servers","text":"<p>This guide provides step-by-step instructions for deploying a Kube-DC cluster with a master and worker node setup on Hetzner Dedicated Servers. This deployment leverages Hetzner's vSwitch and additional subnets to provide enterprise-grade networking capabilities for floating IPs and load balancers.</p>"},{"location":"quickstart-hetzner/#prerequisites","title":"Prerequisites","text":"<ol> <li>At least two Hetzner Dedicated Servers</li> <li>Access to Hetzner Robot interface</li> <li>A Hetzner vSwitch configured for your servers (see Hetzner vSwitch documentation)</li> <li>An additional subnet allocated through Hetzner Robot for external IPs and load balancers</li> <li>Wildcard domain ex: *.dev.kube-dc.com shoud be set to main public ip of master node.</li> </ol>"},{"location":"quickstart-hetzner/#server-configuration","title":"Server Configuration","text":""},{"location":"quickstart-hetzner/#1-prepare-servers","title":"1. Prepare Servers","text":"<p>Ensure your Hetzner Dedicated Servers meet these minimum requirements: - Master Node: 4+ CPU cores, 16+ GB RAM - Worker Node: 4+ CPU cores, 16+ GB RAM</p> <p>Install Ubuntu 24.04 LTS on all servers through the Hetzner Robot interface.</p>"},{"location":"quickstart-hetzner/#2-configure-vswitch","title":"2. Configure vSwitch","text":"<p>In the Hetzner Robot interface:</p> <ol> <li>Create a vSwitch if you don't have one already</li> <li>Add your servers to the vSwitch   </li> <li>Request an additional subnet to be used for external IPs (Floating IPs)</li> <li>Assign the subnet to your vSwitch:   </li> </ol> <p>You will get two vlan ids, one for the local network(in example 4012) and one for the external subnet with public ips(in example 4011).</p>"},{"location":"quickstart-hetzner/#network-configuration","title":"Network Configuration","text":""},{"location":"quickstart-hetzner/#1-configure-network-interfaces","title":"1. Configure Network Interfaces","text":"<p>SSH into each server and configure the networking using Netplan. Backup default netplan config: <pre><code>mkdir /root/tmp/\nmv /etc/netplan/*.yaml /root/tmp/\n</code></pre> Create new config(<code>/etc/netplan/60-kube-dc.yaml</code>) Replace values with <code>example</code> by values from default file(see it in <code>/root/tmp/</code>):</p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s31f6_example:  # Primary network interface name (get it from default netplan config)\n      addresses:\n        - 22.22.22.2_example/24  # Primary IP address and subnet mask (get it from default netplan config)\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 22.22.22.1_example  # Gateway IP address (get it from default netplan config)\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      routing-policy:\n        - from: 22.22.22.2_example  # Source-based routing for traffic from gateway (Primary IP)\n          table: 100  # Custom routing table ID\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    enp0s31f6.4012_example:  # VLAN interface name (format: interface.vlan_id, see your VLAN in https://robot.hetzner.com/vswitch/index)\n      id: 4012_example  # VLAN ID (must match your Hetzner vSwitch ID, same vlan_id)\n      link: enp0s31f6_example  # Parent interface for VLAN (same interface from default netplan config)\n      mtu: 1460  # Maximum Transmission Unit size\n      addresses:\n        - 192.168.100.2/22  # Master node IP on private network (This for master node setup)\n       #- 192.168.100.3/22  # Worker node IP                    (This for master node setup)\n</code></pre> <p>Apply the configuration:</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"quickstart-hetzner/#2-system-optimization","title":"2. System Optimization","text":"<p>Downgrade kernel (due to a bug in kernel https://github.com/k3s-io/k3s/issues/11175):</p> <pre><code>sudo apt -y update\nsudo apt install linux-image-6.8.0-52-generic linux-headers-6.8.0-52-generic\n# Remove previous kernel\nsudo apt-get remove --purge linux-image-6.8.0-58-generic linux-headers-6.8.0-58-generic\n# Reboot\nsudo reboot\n</code></pre> <p>On all nodes, update, upgrade, and install required software:</p> <pre><code>sudo apt -y install unzip iptables linux-headers-$(uname -r)\n</code></pre> <p>Update to the latest kernel version:</p> <pre><code>sudo apt -y install linux-generic\nsudo reboot\n</code></pre> <p>After the server reboots, verify your kernel version:</p> <pre><code>uname -r\n</code></pre> <p>Optimize system settings by adding to <code>/etc/sysctl.conf</code>:</p> <pre><code># Increase inotify limits\nfs.inotify.max_user_watches=1524288\nfs.inotify.max_user_instances=4024\n\n# Enable packet forwarding\nnet.ipv4.ip_forward = 1\n</code></pre> <p>Ensure the nf_conntrack module is loaded:</p> <pre><code># Check if the module is loaded\nlsmod | grep nf_conntrack\n\n# If not loaded, load it manually\nsudo modprobe nf_conntrack\n\n# To ensure it's loaded on boot, add it to /etc/modules\necho \"nf_conntrack\" | sudo tee -a /etc/modules\n</code></pre> <p>Apply the changes:</p> <pre><code>sudo sysctl -p\n</code></pre> <p>Disable systemd-resolved to prevent DNS conflicts:</p> <pre><code>sudo systemctl stop systemd-resolved\nsudo systemctl disable systemd-resolved\nsudo rm /etc/resolv.conf\necho \"nameserver 8.8.8.8\" | sudo tee /etc/resolv.conf\necho \"nameserver 8.8.4.4\" | sudo tee -a /etc/resolv.conf\n</code></pre> <p>Update the hosts file on each server with the private IPs:</p> <pre><code># On Master Node\necho \"192.168.100.2 kube-dc-master-1\" | sudo tee -a /etc/hosts\n# On Worker Node\necho \"192.168.100.3 kube-dc-worker-1\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"quickstart-hetzner/#kubernetes-installation","title":"Kubernetes Installation","text":""},{"location":"quickstart-hetzner/#1-install-clusterdev","title":"1. Install Cluster.dev","text":"<p>On the master node, install Cluster.dev:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\n</code></pre>"},{"location":"quickstart-hetzner/#2-configure-and-install-rke2-on-master-node","title":"2. Configure and Install RKE2 on Master Node","text":"<p>Install kubectl:</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Create RKE2 configuration (replace the external IP with your server's public IP):</p> <pre><code>sudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\nnode-name: kube-dc-master-1\ndisable-cloud-controller: true\ndisable: rke2-ingress-nginx\ncni: none\ncluster-cidr: \"10.100.0.0/16\"\nservice-cidr: \"10.101.0.0/16\"\ncluster-dns: \"10.101.0.11\"\nnode-label:\n  - kube-dc-manager=true\n  - kube-ovn/role=master\nkube-apiserver-arg: \n  - authentication-config=/etc/rancher/auth-conf.yaml\ndebug: true\nnode-external-ip: 22.22.22.2_example # Primary IP address (get it from default netplan config)\ntls-san:\n  - kube-api.yourdomain.com\n  - 192.168.100.2 # Master node IP on private network (This for master node setup)\nadvertise-address: 192.168.100.2 # Master node IP on private network (This for master node setup)\nnode-ip: 192.168.100.2 # Master node IP on private network (This for master node setup)\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/auth-conf.yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt: []\nEOF\nsudo chmod 666 /etc/rancher/auth-conf.yaml\n</code></pre> <p>Install RKE2 server:</p> <pre><code>export INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"server\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\n</code></pre> <p>You can check the installation logs here:</p> <pre><code>sudo journalctl -u rke2-server -f\n</code></pre> <p>Configure kubectl:</p> <pre><code>mkdir -p ~/.kube\nsudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config\nsudo chown $(id -u):$(id -g) ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre> <p>Verify the cluster status:</p> <pre><code>kubectl get nodes\n# If you see this output then you can proceed:\nNAME               STATUS     ROLES\nkube-dc-master-1   NotReady   control-plane,etcd,master\n</code></pre>"},{"location":"quickstart-hetzner/#4-join-worker-node-to-the-cluster","title":"4. Join Worker Node to the Cluster","text":"<p>Get the join token from the master node:</p> <pre><code># on master node\nsudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre> <p>On the worker node, create the RKE2 configuration (replace TOKEN with the token from the master node):</p> <pre><code># on worker node\nsudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\ntoken: &lt;TOKEN&gt;\nserver: https://192.168.100.2:9345 # Master node local IP\nnode-name: kube-dc-worker-1\nnode-ip: 192.168.100.3\nEOF\n</code></pre> <p>Install RKE2 agent:</p> <pre><code># on worker node\nexport INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"agent\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-agent.service\nsudo systemctl start rke2-agent.service\n</code></pre> <p>Monitor the agent service:</p> <pre><code># on worker node\nsudo journalctl -u rke2-agent -f\n</code></pre> <p>Verify on the master node that the worker joined successfully:</p> <pre><code># on master node\nkubectl get nodes\n</code></pre>"},{"location":"quickstart-hetzner/#install-kube-dc-components-on-master-node","title":"Install Kube-DC Components on Master Node","text":""},{"location":"quickstart-hetzner/#1-create-clusterdev-project-configuration","title":"1. Create Cluster.dev Project Configuration","text":"<p>On the master node, create a project configuration file:</p> <pre><code>mkdir -p ~/kube-dc-hetzner\ncat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/project.yaml\nkind: Project\nname: kube-dc-hetzner\nbackend: \"default\"\nvariables:\n  kubeconfig: ~/.kube/config\n  debug: true\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#2-create-clusterdev-stack-configuration","title":"2. Create Cluster.dev Stack Configuration","text":"<p>Create the stack configuration file(replace <code>example</code> by appropriate values):</p> <pre><code>cat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/stack.yaml\nname: cluster\ntemplate: https://github.com/kube-dc/kube-dc-public//installer/kube-dc/templates/kube-dc?ref=main\nkind: Stack\nbackend: default\nvariables:\n  debug: \"true\"\n  kubeconfig: /root/.kube/config # Change for your username path to RKE kubeconfig\n\n  cluster_config:\n    pod_cidr: \"10.100.0.0/16\"\n    svc_cidr: \"10.101.0.0/16\"\n    join_cidr: \"100.64.0.0/16\"\n    cluster_dns: \"10.101.0.11\"\n    default_external_network:\n      nodes_list: # list of nodes, where 4011 vlan (external network) is accessible\n        - kube-dc-master-1\n        - kube-dc-worker-1\n      name: external4011_example # VLAN interface for this name you can find here https://robot.hetzner.com/vswitch/index\n      vlan_id: \"4011_example\" # VLAN interface id, see your VLAN in https://robot.hetzner.com/vswitch/index\n      interface: \"enp0s31f6_example\" # Parent interface for VLAN (same interface from default netplan config)\n      cidr: \"33.33.33.33_example/29\" # External subnet provided by Hetzner (should see during VLAN creation here https://robot.hetzner.com/vswitch/index)\n      gateway: 33.33.33.34_example # Gateway for external subnet (should see during VLAN creation here https://robot.hetzner.com/vswitch/index)\n      mtu: \"1400\"\n\n  node_external_ip: 22.22.22.2_example # Primary IP address (get it from default netplan config). Wildcard *.dev.kube-dc.com shoud be faced on this ip\n\n\n  email: \"noreply@example.com\"\n  domain: \"dev.example-kube-dc.com\"\n  install_terraform: true\n\n  create_default:\n    organization:\n      name: example\n      description: \"My test org my-org 1\"\n      email: \"example@example.com\"\n    project:\n      name: demo\n      cidr_block: \"10.1.0.0/16\"\n\n  monitoring:\n    prom_storage: 20Gi\n    retention_size: 17GiB\n    retention: 365d\n\n  versions:\n    kube_dc: \"v0.1.21\" # release version\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#3-deploy-kube-dc","title":"3. Deploy Kube-DC","text":"<p>Run Cluster.dev to deploy Kube-DC components:</p> <pre><code>cd ~/kube-dc-hetzner\ncdev apply\n</code></pre> <p>This process will take 15-20 minutes to complete. You can monitor the deployment progress in the terminal output.</p>"},{"location":"quickstart-hetzner/#4-verify-installation","title":"4. Verify Installation","text":"<p>After successful deployment, you will receive console and login credentials for deployment admin user. Also if you have created some default organization youll get organization admin credentials. Example:</p> <pre><code>keycloak_user = admin\norganization_admin_username = admin\norganization_name = example\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n example -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n example -o jsonpath='{.data.url}' | base64 -d\nconsole_url = https://console.dev.kube-dc.com\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#post-installation-steps","title":"Post-Installation Steps","text":""},{"location":"quickstart-hetzner/#1-access-kube-dc-ui-using-default-organization-credentials","title":"1. Access Kube-DC UI using default organization credentials","text":"<p>After the installation completes, the Kube-DC UI should be accessible at <code>https://console.yourdomain.com</code>. In cdev output there are output for default organization, project and admin user for default organization(use <code>retrieve_organization_password</code> to login):</p> <pre><code>console_url = https://console.dev.kube-dc.com\norganization_admin_username = admin\norganization_name = example\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n example -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n example -o jsonpath='{.data.url}' | base64 -d\n</code></pre>"},{"location":"quickstart-hetzner/#2-keep-credentials-for-keycloak-master-admin-user","title":"2. Keep credentials for Keycloak master admin user","text":"<p>You can save global Keycloak credentials if you need to manage Keycloak as super-admin.</p> <p>Master admin user credentials:</p> <pre><code>keycloak_user = admin\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during the installation:</p> <ol> <li> <p>Check the RKE2 server/agent logs:    <pre><code>sudo journalctl -u rke2-server -f  # On master\nsudo journalctl -u rke2-agent -f   # On worker\n</code></pre></p> </li> <li> <p>Check the Kube-OVN logs:    <pre><code>kubectl logs -n kube-system -l app=kube-ovn-controller\n</code></pre></p> </li> <li> <p>Verify network connectivity between nodes on the private network:    <pre><code>ping 192.168.100.2  # From worker node\nping 192.168.100.3  # From master node\n</code></pre></p> </li> </ol> <p>For additional help, consult the Kube-DC community support resources.</p>"},{"location":"quickstart-overview/","title":"Kube-DC Installation Overview","text":"<p>This document provides a technical overview of the Kube-DC installation process, with detailed explanations of key configuration files and their parameters.</p>"},{"location":"quickstart-overview/#installation-methods","title":"Installation Methods","text":"<p>Kube-DC can be installed in several ways:</p> <ul> <li>Master-Worker deployment: Recommended starting point for new deployments</li> <li>Multi-node HA cluster: For production environments</li> </ul> <p>Start with actual tested deployment on Hetzner Bare Metal Servers.</p>"},{"location":"quickstart-overview/#prerequisites","title":"Prerequisites","text":"<p>Before installing Kube-DC, ensure your system meets the following requirements:</p> <ul> <li>Hardware: Minimum 4 CPU cores, 8GB RAM per node</li> <li>Operating System: Ubuntu 20.04 LTS or newer (24.04 LTS recommended)</li> <li>Network: Dedicated network interface for VM traffic with VLAN support</li> <li>Storage: Local or network storage with support for dynamic provisioning</li> <li>Kubernetes: Version 1.31+ if installing on existing cluster</li> </ul>"},{"location":"quickstart-overview/#network-configuration","title":"Network Configuration","text":"<p>Kube-DC requires proper network configuration for optimal performance. The key requirement is that your external network must be routed through a VLAN to enable advanced networking features.</p>"},{"location":"quickstart-overview/#external-network-requirements","title":"External Network Requirements","text":"<p>Kube-DC networking is built on top of Kube-OVN and requires the following network configuration:</p> <ul> <li>VLAN-capable network interface: A dedicated network interface with VLAN support</li> <li>External subnet with routing: An external subnet that's properly routed to your infrastructure</li> <li>Static IP configuration: Static IP addressing (no DHCP) to ensure network stability</li> </ul> <p>This configuration allows Kube-DC to implement:</p> <ul> <li>Floating IP allocation: Dynamically assign public IPs to workloads</li> <li>Load balancer with external IPs: Distribute traffic to services with public visibility</li> <li>Default gateway per project: Isolate network traffic between projects</li> </ul> <p>All of these features work as a wrapper on top of Kube-OVN, providing enterprise-grade networking capabilities for your infrastructure.</p>"},{"location":"quickstart-overview/#example-network-configuration","title":"Example Network Configuration","text":"<p>Below is an example Netplan configuration with detailed comments for a VLAN-enabled network:</p> <pre><code>network:\n  version: 2  # Netplan version\n  renderer: networkd  # Network renderer to use\n  ethernets:\n    eth0:  # Primary network interface name (check your actual interface name)\n      addresses:\n        - 192.168.1.2/24  # Primary IP address and subnet mask\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 192.168.1.1  # Gateway IP address\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    eth0.100:  # VLAN interface (format: interface.vlan_id)\n      id: 100  # VLAN ID\n      link: eth0  # Parent interface for VLAN \n      mtu: 1500  # Recommended MTU for your network\n      addresses:\n        - 10.100.0.2/24  # Private IP on the VLAN network\n</code></pre> <p>Important</p> <p>Do not use DHCP for the VLAN interface as it would break the initial Kube-OVN setup. Always use static IP configuration.</p>"},{"location":"quickstart-overview/#networking-components","title":"Networking Components","text":"<p>The Kube-DC network setup consists of several key components that work together:</p> <ol> <li>Kube-OVN: Core CNI providing overlay and underlay networking</li> <li>Multus CNI: Enables multiple network interfaces for pods</li> <li>VLAN Integration: Connects Kubernetes networking to physical infrastructure</li> </ol>"},{"location":"quickstart-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC installer deploys the following core components:</p> <ol> <li>Kube-OVN: Advanced networking solution that provides overlay and underlay networking</li> <li>Multus CNI: CNI that enables attaching multiple network interfaces to pods</li> <li>KubeVirt: Virtualization layer for running VMs on Kubernetes</li> <li>Keycloak: Identity and access management solution</li> <li>Cert-Manager: Certificate management for TLS</li> <li>Ingress-NGINX: Ingress controller for external access</li> <li>Prometheus &amp; Loki: Monitoring and logging stack</li> <li>Kube-DC Core: The core management components for Kube-DC</li> </ol>"},{"location":"quickstart-overview/#installation-process-overview","title":"Installation Process Overview","text":"<p>The installation process follows these high-level steps:</p> <ol> <li>System Preparation: Configure network, optimize system settings, and install prerequisites</li> <li>Kubernetes Installation: Install RKE2 on master and worker nodes</li> <li>Kube-DC Installation: Use cluster.dev to deploy Kube-DC components</li> <li>Post-Installation Setup: Configure authentication, networking, and initial organization</li> </ol> <p>For detailed step-by-step instructions, refer to: - Master-Worker Setup (Dedicated Servers)</p>"},{"location":"roadmap/","title":"Kube-DC Product Roadmap","text":"<p>Build Your Own AI &amp; GPU Cloud on Any Server Transform bare-metal servers into a modern cloud with Kubernetes-native orchestration, GPU sharing, and multi-tenancy.</p> <p>Last Updated: December 9, 2025</p>"},{"location":"roadmap/#executive-summary","title":"Executive Summary","text":"Milestone Target Date Key Deliverable Installer v2 Jan 2026 Single-node &amp; simplified installation Global Admin UI Feb 2026 Platform-wide administration console Database as a Service Mar 2026 PostgreSQL, MySQL, MongoDB, Redis S3 Object Storage Apr 2026 Rook/Ceph multi-tenant buckets GPU/AI Platform May 2026 HAMI sharing, KubeFlow integration Billing System Jun 2026 Metering, pricing, usage reports Licensing Jul 2026 Node-based license management Hybrid Cloud Sep 2026 Multi-cluster federation, DR Advanced Networking Oct 2026 VPN, Security Groups, Service Mesh Edge Computing Q1 2027 Lightweight edge deployments"},{"location":"roadmap/#current-state-v0135","title":"Current State (v0.1.35) \u2705","text":""},{"location":"roadmap/#core-platform-complete","title":"Core Platform \u2014 Complete","text":"<ul> <li>Multi-Tenancy: Organizations, Projects, Keycloak SSO, RBAC</li> <li>Networking: Kube-OVN VPC, EIP/FIP, LoadBalancer, multi-network support</li> <li>Virtualization: KubeVirt VMs, Linux/Windows support, VNC, SSH injection</li> <li>KaaS: Multi-tenant control planes (Kamaji), KubeVirt/CloudSigma workers, Cilium CNI</li> <li>Observability: Prometheus metrics, Loki logging, VM monitoring charts</li> <li>UI: Web console, VM lifecycle, monitoring dashboards</li> </ul>"},{"location":"roadmap/#2026-roadmap","title":"2026 Roadmap","text":""},{"location":"roadmap/#q1-2026-foundation-administration","title":"Q1 2026: Foundation &amp; Administration","text":""},{"location":"roadmap/#installer-v20-january-2026","title":"\ud83d\udce6 Installer v2.0 \u2014 January 2026","text":"Feature Description Single-node Install All-in-one deployment for dev/small production Simplified Setup Reduced dependencies, guided installation Air-gapped Support Offline installation capability Security Hardening Dynamic secrets, no hardcoded passwords"},{"location":"roadmap/#global-admin-view-february-2026","title":"\ud83d\udda5\ufe0f Global Admin View \u2014 February 2026","text":"Feature Description Platform Dashboard Cluster-wide resource overview Organization Management Create/manage all organizations User Administration Global user and group management System Health Infrastructure monitoring and alerts Audit Console Platform-wide audit log viewer"},{"location":"roadmap/#q2-2026-managed-services","title":"Q2 2026: Managed Services","text":""},{"location":"roadmap/#database-as-a-service-march-2026","title":"\ud83d\uddc4\ufe0f Database as a Service \u2014 March 2026","text":"Database Features PostgreSQL CloudNativePG, auto-failover, continuous backups MySQL/MariaDB Percona Operator, clustering, PITR MongoDB Sharding, replica sets, automated backups Redis Clustering, persistence, sentinel <p>Capabilities: One-click provisioning, automated backups, connection pooling, performance dashboards</p>"},{"location":"roadmap/#s3-object-storage-april-2026","title":"\ud83d\udcbe S3 Object Storage \u2014 April 2026","text":"Feature Description Rook/Ceph Backend Production-grade object storage Multi-tenant Buckets Per-project isolation IAM Policies Fine-grained access control Lifecycle Management Automated data retention"},{"location":"roadmap/#gpu-aiml-platform-may-2026","title":"\ud83c\udfae GPU &amp; AI/ML Platform \u2014 May 2026","text":"Feature Description GPU Passthrough Full Nvidia GPU to VMs/pods HAMI Integration Fractional GPU sharing vGPU Support Virtual GPUs for multi-tenant KubeFlow ML pipeline orchestration LLM Serving Model inference infrastructure Vector Databases AI-native data stores"},{"location":"roadmap/#q3-2026-monetization-operations","title":"Q3 2026: Monetization &amp; Operations","text":""},{"location":"roadmap/#billing-system-june-2026","title":"\ud83d\udcb0 Billing System \u2014 June 2026","text":"Feature Description Resource Metering CPU, memory, storage, GPU, network Pricing Models Custom tiers, pay-per-use Usage Reports Detailed analytics, export Billing API External system integration Quota Enforcement Automatic limit enforcement"},{"location":"roadmap/#licensing-july-2026","title":"\ud83d\udd10 Licensing \u2014 July 2026","text":"Feature Description License Manager Node-based licensing Feature Gates License-controlled features Usage Tracking Compliance reporting Trial Mode Time-limited evaluations"},{"location":"roadmap/#ui-enhancements-august-2026","title":"\ud83d\udcca UI Enhancements \u2014 August 2026","text":"Feature Description KaaS Console Cluster creation wizard DBaaS Console Database management UI Storage Console S3 bucket management Billing Dashboard Cost visibility and reports"},{"location":"roadmap/#q4-2026-enterprise-integration","title":"Q4 2026: Enterprise Integration","text":""},{"location":"roadmap/#hybrid-cloud-september-2026","title":"\u2601\ufe0f Hybrid Cloud \u2014 September 2026","text":"Feature Description Multi-Cluster Federation Unified management across sites Cloud Bursting Extend to AWS/Azure/GCP Disaster Recovery Cross-site replication Backup Services Automated VM/container backups VMware Migration CDI import, vjailbreak, wizard"},{"location":"roadmap/#advanced-networking-october-2026","title":"\ud83c\udf10 Advanced Networking \u2014 October 2026","text":"Feature Description Network Peering Cross-project connectivity VPN Gateway Site-to-site VPN Security Groups Stateful firewall rules Service Mesh Istio/Linkerd integration DNS Management Custom domains, auto-DNS"},{"location":"roadmap/#2027-roadmap","title":"2027 Roadmap","text":""},{"location":"roadmap/#q1-2027-edge-advanced-automation","title":"Q1 2027: Edge &amp; Advanced Automation","text":""},{"location":"roadmap/#edge-computing-q1-2027","title":"\ud83d\udcf1 Edge Computing \u2014 Q1 2027","text":"Feature Description Edge Clusters Lightweight K3s deployments Edge-to-Core Sync Data synchronization Offline Mode Disconnected operations ARM Support Raspberry Pi, Jetson devices"},{"location":"roadmap/#advanced-automation-q2-2027","title":"\ud83e\udd16 Advanced Automation \u2014 Q2 2027","text":"Feature Description Self-Healing Automated remediation Predictive Scaling AI-driven autoscaling GitOps Native ArgoCD/Flux integration Policy as Code OPA/Kyverno policies"},{"location":"roadmap/#feature-timeline","title":"Feature Timeline","text":"<pre><code>2025 Dec     2026 Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    2027 Q1\n  \u2502            \u2502         \u2502      \u2502      \u2502      \u2502      \u2502      \u2502      \u2502      \u2502      \u2502        \u2502\n  \u25bc            \u25bc         \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc        \u25bc\nCurrent    Installer  Admin  DBaaS   S3   GPU/AI Billing License  UI   Hybrid Network   Edge\n State        v2      View                                        UX    Cloud\n</code></pre>"},{"location":"roadmap/#success-metrics","title":"Success Metrics","text":"Metric Target Time to First VM &lt; 2 minutes Time to First K8s Cluster &lt; 5 minutes Platform Uptime 99.9% VM Boot Time &lt; 60 seconds API Response Time &lt; 200ms (p95) <p>Document Owner: Kube-DC Product Team Feedback: GitHub Discussions</p>"},{"location":"todo/","title":"Todo List","text":"<ul> <li> Implement default NetworkPolicy creation in the Project controller. The controller should create a 'default-deny-all' NetworkPolicy in the project namespace upon project creation. This requires:</li> <li>Adding a <code>res_network_policy.go</code> file in <code>internal/project</code>.</li> <li>Updating <code>internal/project/project.go</code> to call the new function.</li> <li>Rebuilding and deploying the controller image (<code>make docker-build deploy</code>).</li> <li> <p>Re-enabling the e2e test in <code>tests/e2e/project_test.go</code>.</p> </li> <li> <p> Fix Project Controller Deletion Deadlock</p> </li> <li>Issue: The <code>Project</code> controller gets stuck in a deadlock when deleting a <code>Project</code> that has an empty <code>spec.egressNetworkType</code>. The deletion logic in <code>internal/project/res_vpc.go</code> incorrectly attempts to re-generate the <code>kube-ovn</code> VPC if it's not found, which fails because <code>utils.SelectBestExternalSubnet</code> requires an <code>egressNetworkType</code>.</li> <li>Fix: In <code>internal/project/res_vpc.go</code>, inside the <code>NewProjectVpc</code> function, modify the <code>IsNotFound</code> error handling. Before attempting to regenerate the VPC, add a check to see if the project has a <code>DeletionTimestamp</code>. If it does, the function should return the <code>NotFound</code> error directly, as this is an expected condition during cleanup, and regeneration should not occur.</li> </ul>"},{"location":"tutorial-ip-and-lb/","title":"Managing IPs and Load Balancers","text":"<p>This tutorial walks you through managing External IPs (EIPs), Floating IPs (FIPs), Load Balancers, and deploying namespace-scoped Ingress controllers in Kube-DC using kubectl with YAML manifests.</p>"},{"location":"tutorial-ip-and-lb/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Access to a Kube-DC cluster</li> <li>The <code>kubectl</code> command-line tool installed</li> <li>Helm installed (for Ingress controller deployment)</li> <li>A project with the necessary permissions to create network resources</li> </ul>"},{"location":"tutorial-ip-and-lb/#understanding-network-resources-in-kube-dc","title":"Understanding Network Resources in Kube-DC","text":"<p>Kube-DC's networking is built on Kube-OVN and includes several key components:</p> <ol> <li>External IP (EIP): Public IP addresses that provide connectivity from the internet to resources within Kube-DC</li> <li>Floating IP (FIP): Maps an internal IP address (of a VM or pod) to an External IP</li> <li>Service LoadBalancer: Creates and maps an EIP to a service that routes traffic to pods or VMs</li> <li>Ingress Controller: Provides HTTP/HTTPS routing to services within a namespace</li> </ol>"},{"location":"tutorial-ip-and-lb/#managing-external-ips-eips","title":"Managing External IPs (EIPs)","text":"<p>Each project in Kube-DC automatically receives a default EIP that acts as a NAT gateway for outbound traffic. You can also create additional EIPs for specific services.</p>"},{"location":"tutorial-ip-and-lb/#eip-allocation-algorithm","title":"EIP Allocation Algorithm","text":"<p>When an EIP is created, Kube-DC follows a specific algorithm to allocate the external subnet:</p> <ol> <li>Check Default Subnet Compatibility</li> <li>The system first checks if the EIP's required external network type matches the default external subnet type</li> <li>If they match, it looks for a free OEIP in that subnet</li> <li>If a free OEIP is found, it's connected to the EIP</li> <li> <p>If no free OEIP exists, a new one is created</p> </li> <li> <p>Check Connected Subnets</p> </li> <li>If the required network type is different from the default, the system takes a list of external subnets already connected to the project's VPC</li> <li>It retrieves all free OEIPs from these connected subnets</li> <li> <p>If at least one free OEIP is found, it's connected to the EIP</p> </li> <li> <p>Select Best Available Subnet</p> </li> <li>If no connected subnets have free IPs, the system takes a complete list of available external networks</li> <li>Networks are sorted by the number of available IPs (descending order)</li> <li> <p>The network with the most free addresses is selected</p> </li> <li> <p>Connect New Subnet to VPC</p> </li> <li>The selected subnet is connected to the project's VPC</li> <li>The system waits for the OEIP resource to be created</li> <li> <p>Once created, the OEIP is connected to the EIP</p> </li> <li> <p>Error Handling</p> </li> <li>If no networks with free IPs are available, the operation fails with an error</li> </ol> <p>This algorithm ensures optimal IP address utilization while providing the flexibility to support different external network types.</p>"},{"location":"tutorial-ip-and-lb/#creating-an-eip-using-kubectl","title":"Creating an EIP Using kubectl","text":"<p>For automation or GitOps workflows, you can create EIPs using kubectl and YAML manifests.</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: web-server-eip\n  namespace: shalb-demo\nspec: {}\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f eip.yaml\n</code></pre> <p>Check the status:</p> <pre><code>kubectl get eip -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#managing-floating-ips-fips","title":"Managing Floating IPs (FIPs)","text":"<p>Floating IPs map an internal IP address (of a VM or pod) to an External IP, enabling direct access to specific resources.</p>"},{"location":"tutorial-ip-and-lb/#creating-a-fip-using-kubectl","title":"Creating a FIP Using kubectl","text":"<p>Create a FIP manifest:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: database-vm-fip\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171  # Internal IP of your VM or pod\n  eip: web-server-eip     # Name of an existing EIP\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f fip.yaml\n</code></pre> <p>Check the status:</p> <pre><code>kubectl get fip -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#configuring-load-balancers","title":"Configuring Load Balancers","text":"<p>Load Balancers in Kube-DC are implemented as Kubernetes Services of type LoadBalancer, with specific annotations to control their behavior.</p>"},{"location":"tutorial-ip-and-lb/#creating-a-load-balancer-for-pods-using-kubectl","title":"Creating a Load Balancer for Pods Using kubectl","text":"<p>Create a Service manifest:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"  # Use project's default EIP\n    # service.nlb.kube-dc.com/bind-on-eip: \"web-server-eip\"  # Or use a dedicated EIP\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f service-lb.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#creating-a-load-balancer-for-vm-ssh-access","title":"Creating a Load Balancer for VM SSH Access","text":"<p>You can also create a Load Balancer to expose SSH access to a virtual machine:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: debian  # Target VM name\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222  # External port\n      targetPort: 22  # Internal port (SSH)\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f vm-ssh-lb.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#automatic-external-endpoints","title":"Automatic External Endpoints","text":"<p>Kube-DC automatically creates external endpoints for every LoadBalancer service, providing stable DNS-based access across VPC boundaries.</p> <p>When a LoadBalancer service gets an external IP, the controller creates: - External Service: <code>&lt;service-name&gt;-ext</code> (headless service) - Endpoints: Points to the LoadBalancer's external IP</p> <p>This enables cross-VPC communication using stable DNS names like <code>etcd-lb-ext.shalb-demo.svc.cluster.local:2379</code> instead of hardcoded IP addresses.</p> <p>Verify external endpoints:</p> <pre><code># List all external endpoints\nkubectl get endpoints -A --selector=kube-dc.com/managed-by=service-lb-controller\n\n# Check specific service\nkubectl get svc,endpoints -n shalb-demo nginx-service-lb-ext\n</code></pre>"},{"location":"tutorial-ip-and-lb/#managing-network-resources-with-kubectl","title":"Managing Network Resources with kubectl","text":"<p>Check the status of network resources:</p> <pre><code># List all External IPs\nkubectl get eip -n shalb-demo\n\n# List all Floating IPs\nkubectl get fip -n shalb-demo\n\n# Get details about a specific LoadBalancer service\nkubectl describe service nginx-service-lb -n shalb-demo\n\n# Check if your LoadBalancer has an external IP assigned\nkubectl get service -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#deploying-namespace-scoped-ingress-controllers","title":"Deploying Namespace-Scoped Ingress Controllers","text":"<p>For more advanced HTTP/HTTPS routing capabilities, you can deploy an ingress-nginx controller scoped to your specific namespace. This allows you to have complete control over the Ingress resources in your project.</p>"},{"location":"tutorial-ip-and-lb/#understanding-namespace-scoped-ingress","title":"Understanding Namespace-Scoped Ingress","text":"<p>A namespace-scoped ingress controller: - Only watches for Ingress resources in the specified namespace - Doesn't interfere with other controllers in the cluster - Uses your project's networking resources (like the default EIP) - Provides advanced routing, SSL termination, and load balancing</p>"},{"location":"tutorial-ip-and-lb/#deploying-ingress-nginx-controller-with-helm","title":"Deploying ingress-nginx Controller with Helm","text":""},{"location":"tutorial-ip-and-lb/#step-1-create-a-valuesyaml-file-for-your-configuration","title":"Step 1: Create a values.yaml file for your configuration","text":"<pre><code>controller:\n  ingressClassResource:\n    enabled: false  # Disables the default IngressClass creation\n  ingressClass: \"\"  # No default IngressClass\n  scope:\n    enabled: true  # Enables namespace-scoped mode\n    namespace: shalb-demo  # Restricts the controller to this namespace\n  watchIngressWithoutClass: false\n  admissionWebhooks:\n    enabled: false\n  service:\n    annotations:\n      service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nrbac:\n  create: true\n  scope: true\n\ndefaultBackend:\n  enabled: false  # Disables the default backend\n</code></pre> <p>Save this as <code>ingress-values.yaml</code>.</p>"},{"location":"tutorial-ip-and-lb/#step-2-add-the-ingress-nginx-helm-repository-if-not-already-added","title":"Step 2: Add the ingress-nginx Helm repository (if not already added)","text":"<pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-3-install-the-ingress-nginx-controller","title":"Step 3: Install the ingress-nginx controller","text":"<pre><code>helm upgrade --install ingress ingress-nginx/ingress-nginx \\\n  --namespace shalb-demo \\\n  --values ingress-values.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-4-verify-the-installation","title":"Step 4: Verify the installation","text":"<pre><code>kubectl get pods -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\nkubectl get svc -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\n</code></pre> <p>The controller pod should be running, and the service should have an external IP assigned.</p>"},{"location":"tutorial-ip-and-lb/#creating-an-ingress-resource","title":"Creating an Ingress Resource","text":"<p>Once your ingress controller is running, you can create Ingress resources to route traffic to your services:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: shalb-demo\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: example.kube-dc.com  # Replace with your domain\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f example-ingress.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#configuring-ssltls-with-cert-manager","title":"Configuring SSL/TLS with cert-manager","text":"<p>For secure HTTPS connections, you can deploy cert-manager to automatically obtain and manage certificates:</p>"},{"location":"tutorial-ip-and-lb/#step-1-install-cert-manager-in-your-namespace","title":"Step 1: Install cert-manager in your namespace","text":"<pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm install cert-manager jetstack/cert-manager \\\n  --namespace shalb-demo \\\n  --set installCRDs=true \\\n  --set namespace=shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-2-create-an-issuer-or-clusterissuer","title":"Step 2: Create an Issuer or ClusterIssuer","text":"<p>For Let's Encrypt:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: shalb-demo\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: your-email@example.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f issuer.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-3-update-your-ingress-with-tls-configuration","title":"Step 3: Update your Ingress with TLS configuration","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: shalb-demo\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    cert-manager.io/issuer: \"letsencrypt-prod\"\nspec:\n  tls:\n  - hosts:\n    - example.kube-dc.com\n    secretName: example-tls\n  rules:\n  - host: example.kube-dc.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f example-ingress-tls.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#putting-it-all-together-exposing-a-web-application","title":"Putting It All Together: Exposing a Web Application","text":"<p>Let's walk through a complete example of deploying a web application and exposing it to the internet:</p>"},{"location":"tutorial-ip-and-lb/#step-1-deploy-an-nginx-pod","title":"Step 1: Deploy an Nginx Pod","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: shalb-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-2-create-a-service-for-the-deployment","title":"Step 2: Create a Service for the Deployment","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: shalb-demo\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f nginx-service.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-3-deploy-a-namespace-scoped-ingress-controller","title":"Step 3: Deploy a Namespace-Scoped Ingress Controller","text":"<p>Follow the steps in the \"Deploying Namespace-Scoped Ingress Controllers\" section to deploy your ingress controller.</p>"},{"location":"tutorial-ip-and-lb/#step-4-create-an-ingress-resource","title":"Step 4: Create an Ingress Resource","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-ingress\n  namespace: shalb-demo\nspec:\n  rules:\n  - host: nginx.example.com  # Replace with your domain\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f nginx-ingress.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-5-verify-access","title":"Step 5: Verify Access","text":"<p>Get the external IP of your ingress controller's service:</p> <pre><code>kubectl get svc -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\n</code></pre> <p>Configure your DNS to point your domain (e.g., nginx.example.com) to this IP address. Once DNS propagates, you should be able to access your Nginx service using the domain.</p>"},{"location":"tutorial-ip-and-lb/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Naming: Use descriptive names for your network resources</li> <li>EIP Conservation: When possible, use the project's default EIP with annotations rather than creating dedicated EIPs</li> <li>Security: Limit exposed ports to only what's necessary and use HTTPS with valid certificates</li> <li>Monitoring: Regularly check the status of your network resources</li> <li>Documentation: Document which services are exposed on which domains/paths</li> </ol>"},{"location":"tutorial-ip-and-lb/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-ip-and-lb/#common-issues","title":"Common Issues","text":"<ol> <li>EIP Not Allocated: EIP creation may take a few moments. Check status with <code>kubectl get eip</code></li> <li>LoadBalancer Pending: External IP allocation may take time. Check with <code>kubectl describe service</code></li> <li>Cannot Connect to Service: Verify that the service's selector matches your pod labels</li> <li>Ingress Not Routing Traffic: Check ingress controller logs and ingress resource status</li> </ol>"},{"location":"tutorial-ip-and-lb/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Check EIP status and details\nkubectl describe eip web-server-eip -n shalb-demo\n\n# Check FIP status and details\nkubectl describe fip database-vm-fip -n shalb-demo\n\n# Check LoadBalancer service events\nkubectl describe service nginx-service-lb -n shalb-demo\n\n# Check if pods are selected by the service\nkubectl get pods -l app=nginx -n shalb-demo\n\n# Check ingress controller logs\nkubectl logs -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\n\n# Check ingress status\nkubectl describe ingress nginx-ingress -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to manage External IPs (EIPs), Floating IPs (FIPs), LoadBalancer services, and namespace-scoped Ingress controllers in Kube-DC. These networking resources provide flexible options for exposing your applications and VMs to external traffic, with Ingress controllers offering advanced HTTP/HTTPS routing capabilities for your web applications.</p>"},{"location":"tutorial-kubeconfig/","title":"Obtaining and Using Kubeconfig in Your Local Console","text":"<p>This guide explains how to obtain and configure a kubeconfig file for the kube-dc platform to use in your local development environment.</p>"},{"location":"tutorial-kubeconfig/#overview","title":"Overview","text":"<p>The kubeconfig file is essential for authenticating with the Kubernetes API server. In kube-dc, authentication is handled through Keycloak, which provides secure token-based access.</p> <p>This tutorial covers: - Setting up the authentication script - Generating a kubeconfig file - Using kubeconfig with kubectl - Troubleshooting common issues</p>"},{"location":"tutorial-kubeconfig/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Access to a kube-dc organization and project</li> <li>Your Keycloak username and password</li> <li><code>kubectl</code> installed on your local machine</li> <li><code>curl</code> and <code>jq</code> utilities installed (required for token operations)</li> </ul>"},{"location":"tutorial-kubeconfig/#using-the-authentication-helper-script","title":"Using the Authentication Helper Script","text":"<p>kube-dc provides a helper script that simplifies the kubeconfig generation process.</p>"},{"location":"tutorial-kubeconfig/#step-1-download-and-run-the-authentication-script","title":"Step 1: Download and Run the Authentication Script","text":"<p>You can download the authentication script directly from the public repository:</p> <pre><code># Create a directory for the script\nmkdir -p ~/.kube-dc/bin\n\n# Download the script\ncurl -o ~/.kube-dc/bin/kdc_get_kubeconfig.sh https://raw.githubusercontent.com/kube-dc/kube-dc-public/main/hack/auth/kdc_get_kubeconfig.sh\n\n# Make it executable\nchmod +x ~/.kube-dc/bin/kdc_get_kubeconfig.sh\n\n# Run the authentication script with your organization and project name\n~/.kube-dc/bin/kdc_get_kubeconfig.sh your-org/your-project\n</code></pre> <p>Alternatively, if you have the entire repository cloned:</p> <pre><code># If you have the repository already cloned\ncd kube-dc\n./hack/auth/kdc_get_kubeconfig.sh your-org/your-project\n</code></pre> <p>The script will prompt you for the following information: - Keycloak endpoint URL (e.g., <code>https://login.dev.kube-dc.com</code>) - Organization name (your Keycloak realm) - Kubernetes API server URL (e.g., <code>https://kube-api.dev.kube-dc.com:6443</code>) - Cluster name (usually <code>kube-dc</code>) - User name (your Keycloak username) - Context name (usually <code>kube-dc</code>) - CA certificate (you can provide this as a file, paste it directly, or skip for insecure mode)</p>"},{"location":"tutorial-kubeconfig/#step-2-activate-the-generated-configuration","title":"Step 2: Activate the Generated Configuration","text":"<p>After the script completes, activate the configuration:</p> <pre><code>source ~/.kube-dc/your-org-your-project/activate.sh\n</code></pre> <p>This will:</p> <ol> <li>Set the <code>KUBECONFIG</code> environment variable to point to your new configuration</li> <li>Source the environment variables from the <code>.env</code> file</li> <li>Add the <code>kn</code> alias for namespace switching</li> <li>Display instructions for using kubectl</li> </ol> <p>You can now use <code>kubectl</code> commands as usual:</p> <pre><code>kubectl get pods\n</code></pre> <p>On first use, you'll be prompted to enter your Keycloak username and password. The script will obtain tokens and cache them for subsequent commands.</p>"},{"location":"tutorial-kubeconfig/#step-3-test-your-connection","title":"Step 3: Test Your Connection","text":"<p>Test that your kubeconfig works correctly:</p> <pre><code>kubectl get pods\n</code></pre> <p>On first use, you'll be prompted to enter your Keycloak username and password. The script will obtain tokens and cache them for subsequent commands.</p>"},{"location":"tutorial-kubeconfig/#using-the-namespace-switcher","title":"Using the Namespace Switcher","text":"<p>The <code>kn</code> tool is installed automatically during setup and is configured as an alias when you activate the environment. It allows you to easily view and switch between namespaces that your token has permissions to access.</p>"},{"location":"tutorial-kubeconfig/#features","title":"Features","text":"<ul> <li>Intelligent Operation: When used with a kube-dc context, it reads namespace permissions directly from your JWT token</li> <li>Interactive Selection: Run <code>kn</code> without arguments to see a list of available namespaces</li> <li>Direct Selection: Specify a namespace with <code>kn my-namespace</code></li> <li>Fallback Mode: If not in a kube-dc context, falls back to <code>kubens</code> or basic kubectl namespace commands</li> </ul>"},{"location":"tutorial-kubeconfig/#examples","title":"Examples","text":"<pre><code># List available namespaces and select interactively\nkn\n\n# Switch directly to a specific namespace\nkn shalb-demo\n</code></pre>"},{"location":"tutorial-kubeconfig/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-kubeconfig/#authentication-issues","title":"Authentication Issues","text":"<p>If you're experiencing authentication problems:</p> <ol> <li> <p>Token expiration: The refresh token may have expired. Delete the <code>.refresh_token</code> file in your kubeconfig directory and try again:    <pre><code>rm ~/.kube-dc/*-*/scripts/.refresh_token\n</code></pre></p> </li> <li> <p>Invalid credentials: Ensure you're using the correct username and password for your Keycloak account.</p> </li> <li> <p>Connection issues: Verify your network can reach the Keycloak and API servers.</p> </li> </ol>"},{"location":"tutorial-kubeconfig/#permission-issues","title":"Permission Issues","text":"<p>If you can authenticate but receive permission errors:</p> <ol> <li> <p>Namespace access: Ensure you're using the correct namespace in your context. Your namespace should be in the format <code>organization-project</code>.</p> </li> <li> <p>Role assignment: Contact your organization administrator to verify you have the appropriate roles assigned in Keycloak.</p> </li> <li> <p>Resource-specific permissions: Check that your role has permissions for the specific resources you're trying to access.</p> </li> </ol>"},{"location":"tutorial-kubeconfig/#security-considerations","title":"Security Considerations","text":"<ul> <li>Keep your kubeconfig file secure (600 permissions)</li> <li>Never share your refresh or access tokens</li> <li>Be cautious when using <code>insecure-skip-tls-verify: true</code> in production environments</li> <li>If your credentials may be compromised, contact your administrator to revoke your tokens</li> </ul>"},{"location":"tutorial-kubeconfig/#next-steps","title":"Next Steps","text":"<ul> <li>User and Group Management: Learn about role-based access control</li> <li>Tutorial: Virtual Machines: Deploy your first VM</li> <li>Examples: Explore example manifests for various resources</li> </ul>"},{"location":"tutorial-networking-external/","title":"Additional External Network Configuration","text":"<p>This guide explains how to add additional external networks to Kube-DC alongside the default cloud network.</p>"},{"location":"tutorial-networking-external/#overview","title":"Overview","text":"<p>The configuration demonstrates how to add a second external network (public) to an existing Kube-DC setup that already has a cloud external network, using multiple VLANs on a single physical interface per node.</p>"},{"location":"tutorial-networking-external/#network-types-explained-by-example","title":"Network Types Explained by Example","text":""},{"location":"tutorial-networking-external/#cloud-network-egressnetworktype-cloud","title":"Cloud Network (<code>egressNetworkType: cloud</code>)","text":"<ul> <li>Purpose: Default external network for most workloads</li> <li>Subnet: <code>ext-cloud</code> (100.65.0.0/16) on VLAN 4013</li> <li>Use Cases: </li> <li>General internet access for applications</li> <li>Standard egress traffic from project workloads</li> <li>Cost-effective external connectivity</li> <li>IP Pool: Large address space (65,000+ IPs available)</li> </ul>"},{"location":"tutorial-networking-external/#public-network-egressnetworktype-public","title":"Public Network (<code>egressNetworkType: public</code>)","text":"<ul> <li>Purpose: Premium external network for specialized workloads</li> <li>Subnet: <code>ext-public</code> (168.119.17.48/28) on VLAN 4011</li> <li> <p>Use Cases:</p> </li> <li> <p>Production services requiring dedicated public IPs</p> </li> <li>Load balancers and ingress controllers</li> <li>Services needing specific public IP ranges or routing</li> <li>IP Pool: Limited address space with real ipv4 addresses (16 IPs total)</li> </ul>"},{"location":"tutorial-networking-external/#architecture","title":"Architecture","text":"<pre><code>Physical Interface (enp0s31f6)\n\u251c\u2500\u2500 VLAN 4013 (Cloud Network) - 100.65.0.0/16 (ext-cloud)\n\u2514\u2500\u2500 VLAN 4011 (Public Network) - 168.119.17.48/28 (ext-public)\n</code></pre>"},{"location":"tutorial-networking-external/#example-cluster-usage","title":"Example Cluster Usage","text":"<ul> <li>shalb-demo project: Uses <code>egressNetworkType: cloud</code> \u2192 EIP: 100.65.0.102 (development/testing)</li> <li>shalb-dev project: Uses <code>egressNetworkType: public</code> \u2192 EIP: 168.119.17.51 (development with public access)</li> <li>shalb-envoy project: Uses <code>egressNetworkType: public</code> \u2192 EIPs: 168.119.17.52, 168.119.17.54 (production load balancer)</li> </ul>"},{"location":"tutorial-networking-external/#choosing-the-right-network","title":"Choosing the Right Network","text":"<p>Use Cloud Network when: - Need basic internet connectivity - Don't require specific public IP ranges</p> <p>Use Public Network when: - Need dedicated public IP addresses - Have specific routing or compliance requirements - Running load balancers or ingress controllers</p>"},{"location":"tutorial-networking-external/#ovsovn-modifications-applied","title":"OVS/OVN Modifications Applied","text":""},{"location":"tutorial-networking-external/#1-ovs-bridge-configuration","title":"1. OVS Bridge Configuration","text":"<p>The system automatically creates the necessary OVS infrastructure:</p> <p>Bridge: <code>br-ext-cloud</code> - Physical interface <code>enp0s31f6</code> attached with VLAN trunking - Trunk VLANs: <code>[0, 4011, 4013]</code> - Patch ports for both external networks:   - <code>patch-localnet.ext-cloud-to-br-int</code> \u2194 <code>patch-br-int-to-localnet.ext-cloud</code>   - <code>patch-localnet.ext-public-to-br-int</code> \u2194 <code>patch-br-int-to-localnet.ext-public</code></p>"},{"location":"tutorial-networking-external/#2-ovn-logical-switches","title":"2. OVN Logical Switches","text":"<p>Two logical switches are created automatically: - <code>ext-cloud</code> (for VLAN 4013) - <code>ext-public</code> (for VLAN 4011)</p>"},{"location":"tutorial-networking-external/#3-providernetwork-status","title":"3. ProviderNetwork Status","text":"<p>The existing ProviderNetwork <code>ext-cloud</code> is updated to include both VLANs: <pre><code>status:\n  vlans: [\"vlan4013\", \"vlan4011\"]\n  ready: true\n  readyNodes:\n  - kube-dc-master-1\n  - kube-dc-worker-1\n</code></pre></p>"},{"location":"tutorial-networking-external/#configuration-steps","title":"Configuration Steps","text":""},{"location":"tutorial-networking-external/#1-apply-vlan-configuration","title":"1. Apply VLAN Configuration","text":"<pre><code>kubectl apply -f examples/networking/additional-external-network.yaml\n</code></pre>"},{"location":"tutorial-networking-external/#2-verify-configuration","title":"2. Verify Configuration","text":"<pre><code># Check ProviderNetwork VLANs\nkubectl get provider-network ext-cloud -o jsonpath='{.status.vlans}'\n# Expected output: [\"vlan4013\",\"vlan4011\"]\n\n# Check external subnets\nkubectl get subnets ext-cloud ext-public\n# Expected: ext-cloud (100.65.0.0/16) and ext-public (168.119.17.48/28)\n\n# Check EIP assignments\nkubectl get eips -A\n# Shows which projects are using which external networks\n\n# Check OVS bridge configuration\nkubectl exec -n kube-system [ovs-pod] -- ovs-vsctl show | grep -A 10 \"br-ext-cloud\"\n\n# Check OVN logical switches\nkubectl exec -n kube-system [ovn-central-pod] -- ovn-nbctl ls-list | grep ext\n</code></pre>"},{"location":"tutorial-networking-external/#3-test-with-project","title":"3. Test with Project","text":"<p>Create projects to test both network types:</p> <p>Project using Cloud Network: <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: test-project-cloud\n  namespace: test-org\nspec:\n  cidrBlock: 10.200.0.0/24\n  egressNetworkType: cloud  # Uses ext-cloud subnet (100.65.0.0/16)\n</code></pre></p> <p>Project using Public Network: <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: test-project-public\n  namespace: test-org\nspec:\n  cidrBlock: 10.201.0.0/24\n  egressNetworkType: public  # Uses ext-public subnet (168.119.17.48/28)\n</code></pre></p>"},{"location":"tutorial-networking-external/#key-points","title":"Key Points","text":"<ol> <li>Single ProviderNetwork: Use one ProviderNetwork per physical interface with multiple VLANs attached</li> <li>Automatic Configuration: OVS bridges, patch ports, and OVN logical switches are created automatically</li> <li>VLAN Trunking: The physical interface supports multiple VLANs simultaneously</li> <li>No Manual OVS Changes: All OVS/OVN modifications are handled by Kube-DC controllers</li> </ol>"},{"location":"tutorial-networking-external/#prerequisites","title":"Prerequisites","text":"<ul> <li>Physical network infrastructure supporting VLAN trunking</li> <li>vSwitch configured with appropriate VLAN IDs</li> </ul>"},{"location":"tutorial-networking-external/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-networking-external/#check-vlan-interface-on-nodes","title":"Check VLAN Interface on Nodes","text":"<pre><code># On cluster nodes\nip link show enp0s31f6.4011\nip addr show enp0s31f6.4011\n</code></pre>"},{"location":"tutorial-networking-external/#check-ovn-resources","title":"Check OVN Resources","text":"<pre><code># Check OVN-EIP resources\nkubectl get ovn-eip | grep ext-public\n\n# Check subnet status\nkubectl get subnet ext-public -o yaml\n</code></pre>"},{"location":"tutorial-networking-external/#test-connectivity","title":"Test Connectivity","text":"<pre><code># Test from pod\nkubectl exec -n [namespace] [pod] -- wget -qO- http://httpbin.org/ip\n</code></pre>"},{"location":"tutorial-user-groups/","title":"User and Group Management","text":"<p>This guide explains how to set up and manage users, groups, and roles in Kube-DC using Kubernetes RBAC and Keycloak integration.</p>"},{"location":"tutorial-user-groups/#overview","title":"Overview","text":"<p>Kube-DC implements a multi-tenant access control system that combines:</p> <ul> <li>Kubernetes RBAC: Handles resource-level permissions within namespaces</li> <li>Organization Groups: Manages project-level access across namespaces</li> <li>Keycloak Integration: Provides user authentication and group management</li> </ul>"},{"location":"tutorial-user-groups/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you're working from an Organization Admin perspective. You'll need:</p> <ul> <li>Access to the Kube-DC cluster with organization admin privileges</li> <li><code>kubectl</code> configured to access your cluster with organization admin privileges</li> <li>Access to the Keycloak organization admin console</li> </ul> <p>Before You Begin</p> <p>During organization and project creation you will get a namespace with organization name <code>&lt;orgname&gt;</code> created and project namespace with <code>&lt;orgname&gt;-&lt;projectname&gt;</code> pattern.</p>"},{"location":"tutorial-user-groups/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"tutorial-user-groups/#creating-project-roles","title":"Creating Project Roles","text":"<p>Create a Kubernetes Role to define permissions within a project namespace. These roles dictate what actions users can perform on specific resources.</p> <pre><code>apiVersion: rbac.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: shalb-demo  # Replace with your project namespace\n  name: resource-manager\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\", \"services\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"daemonsets\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n</code></pre> <p>Apply the role to your namespace using:</p> <pre><code>kubectl apply -f role.yaml\n</code></pre> <p>Role Scope</p> <p>Remember that Roles are namespace-scoped. If you need permissions across multiple namespaces, you need to create a separate Role in each Project namespace.</p>"},{"location":"tutorial-user-groups/#creating-organization-groups","title":"Creating Organization Groups","text":"<p>Create an OrganizationGroup Custom Resource (CR) to define group permissions across projects.</p> <p>Key Points</p> <ul> <li>The OrganizationGroup CR automatically creates a corresponding group in Keycloak</li> <li>This CR must be created in the organization namespace, not the project namespace</li> <li>Role bindings would be created by this CR</li> </ul> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb  # namespace of the organization (not the project)\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - resource-manager\n  # Additional projects and roles can be added:\n  # - project: \"prod\"\n  #   roles:\n  #   - resource-manager\n</code></pre> <p>Apply the group configuration:</p> <pre><code>kubectl apply -f organization-group.yaml\n</code></pre>"},{"location":"tutorial-user-groups/#managing-users-in-keycloak","title":"Managing Users in Keycloak","text":""},{"location":"tutorial-user-groups/#access-keycloak-admin-console","title":"Access Keycloak Admin Console","text":"<p>Retrieve Keycloak access credentials from your organization namespace:</p> <pre><code>kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.user}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\n</code></pre> <p>Remember</p> <p>Replace <code>shalb</code> with your own organization namespace in the commands above.</p>"},{"location":"tutorial-user-groups/#create-and-configure-users","title":"Create and Configure Users","text":"<ol> <li>Log in to the Keycloak admin console using the retrieved credentials</li> <li>Navigate to Users \u2192 Add User </li> <li>Fill in the required user information</li> <li>Set up initial password in the Credentials tab</li> <li>Add the user to the appropriate group (e.g., \"app-manager\") via the Groups tab </li> </ol> <p>User Group Mapping</p> <p>Any groups created via OrganizationGroup CRs will appear automatically in Keycloak. Changes to group membership in Keycloak are synchronized with Kubernetes RBAC.</p>"},{"location":"tutorial-user-groups/#accessing-kube-dc-ui","title":"Accessing Kube-DC UI","text":"<ol> <li>Navigate to the Kube-DC UI login page</li> <li>Log in using the credentials created in Keycloak</li> <li>Verify access to assigned project resources</li> </ol> <p>Permissions Troubleshooting</p> <p>If a user cannot access expected resources: - Verify they're assigned to the correct groups in Keycloak - Check that the OrganizationGroup CR includes the correct projects and roles - Ensure the underlying Kubernetes Roles have appropriate permissions - Examine the Keycloak logs for authentication issues</p> <p>Permission changes may take up to 5 minutes to propagate through the system.</p>"},{"location":"tutorial-virtual-machines/","title":"Deploying VMs &amp; Containers","text":"<p>This tutorial walks you through deploying virtual machines and containers in Kube-DC. You'll learn both the UI-based approach and how to use kubectl with YAML manifests.</p>"},{"location":"tutorial-virtual-machines/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Access to a Kube-DC cluster</li> <li>The <code>kubectl</code> command-line tool installed</li> <li>The <code>virtctl</code> plugin installed for KubeVirt (optional, but recommended)</li> <li>A project with the necessary permissions to create VMs and containers</li> </ul>"},{"location":"tutorial-virtual-machines/#understanding-vm-components-in-kube-dc","title":"Understanding VM Components in Kube-DC","text":"<p>Kube-DC's virtualization is powered by KubeVirt and consists of several components:</p> <ol> <li>VirtualMachine (VM): Defines the VM configuration and lifecycle</li> <li>DataVolume: Manages the VM's disk image(s)</li> <li>VirtualMachineInstance (VMI): Represents a running instance of a VM</li> </ol>"},{"location":"tutorial-virtual-machines/#creating-a-vm-using-the-kube-dc-ui","title":"Creating a VM Using the Kube-DC UI","text":""},{"location":"tutorial-virtual-machines/#step-1-navigate-to-vm-creation","title":"Step 1: Navigate to VM Creation","text":"<ol> <li>Log in to the Kube-DC dashboard</li> <li>Select your project from the dropdown menu (e.g., \"demo\")</li> <li>Navigate to \"Virtual Machines\" in the left sidebar</li> <li>Click the \"+\" button to create a new VM</li> </ol>"},{"location":"tutorial-virtual-machines/#step-2-configure-basic-vm-parameters","title":"Step 2: Configure Basic VM Parameters","text":"<p>In the VM creation wizard, specify the basic parameters:</p> <ol> <li>VM Name: Enter a name for your VM (e.g., \"new-vm-name\")</li> <li>Operation System: Select from the dropdown (e.g., \"Ubuntu 24.04\")</li> <li>Advanced Options: Expand this section if you want to customize the image source</li> </ol> <p></p>"},{"location":"tutorial-virtual-machines/#step-3-configure-vm-resources","title":"Step 3: Configure VM Resources","text":"<p>Continue configuring the VM:</p> <ol> <li>Number of vCPUs: Select the number of virtual CPUs</li> <li>RAM (GB): Specify the amount of memory</li> <li>Subnet: Choose the network for your VM</li> <li>Root Storage Size (GB): Set the disk size</li> <li>Root Storage Type: Select the storage class</li> </ol>"},{"location":"tutorial-virtual-machines/#step-4-review-and-create","title":"Step 4: Review and Create","text":"<ol> <li>Click \"Next\" to proceed to the review page</li> <li>Review the generated VM configuration</li> <li>The UI shows the actual YAML that will be applied</li> <li>Click \"Finish\" to create the VM</li> </ol>"},{"location":"tutorial-virtual-machines/#step-5-monitor-vm-creation","title":"Step 5: Monitor VM Creation","text":"<p>After creation:</p> <ol> <li>You'll be redirected to the VM list</li> <li>Wait for the VM to reach \"Running\" state</li> <li>Note the assigned IP address</li> </ol>"},{"location":"tutorial-virtual-machines/#managing-vms-via-the-ui","title":"Managing VMs via the UI","text":""},{"location":"tutorial-virtual-machines/#viewing-vm-details","title":"Viewing VM Details","text":"<p>Click on a VM name to view its details page, which includes:</p> <ol> <li>Guest OS: Information about the operating system</li> <li>VM Details: Status, VPC subnet, and node placement</li> <li>Performance Metrics: Real-time CPU, memory, and storage usage</li> <li>Conditions: Agent connection and other status indicators</li> </ol> <p></p>"},{"location":"tutorial-virtual-machines/#accessing-vm-console","title":"Accessing VM Console","text":"<p>From the VM details page, you have two options:</p> <ol> <li>Launch Remote Console: Opens a graphical console in your browser</li> <li>Launch SSH Terminal: Opens a web-based SSH terminal</li> </ol> <p>These options provide direct access to your VM without requiring SSH client configuration.</p>"},{"location":"tutorial-virtual-machines/#vm-actions","title":"VM Actions","text":"<p>The UI supports common VM management actions:</p> <ul> <li>Start/Stop: Control the VM power state</li> <li>Restart: Reboot the VM</li> <li>Delete: Remove the VM and its resources</li> <li>Configure: Modify VM settings</li> </ul>"},{"location":"tutorial-virtual-machines/#creating-a-vm-using-kubectl-manifests","title":"Creating a VM Using kubectl Manifests","text":"<p>For automation or GitOps workflows, you can create VMs using kubectl and YAML manifests.</p>"},{"location":"tutorial-virtual-machines/#step-1-create-datavolume","title":"Step 1: Create DataVolume","text":"<p>First, create a DataVolume to serve as the VM's disk:</p> <pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: ubuntu-vm-disk\n  namespace: shalb-demo\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f ubuntu-datavolume.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#step-2-create-the-vm-definition","title":"Step 2: Create the VM Definition","text":"<p>Create a VM manifest:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\n  namespace: shalb-demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: shalb-demo/default\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n        cpu:\n          cores: 2\n        memory:\n          guest: 4G\n      volumes:\n      - dataVolume:\n          name: ubuntu-vm-disk\n        name: root-volume\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |-\n            #cloud-config\n            chpasswd: { expire: False }\n            password: temppassword\n            ssh_pwauth: True\n            package_update: true\n            package_upgrade: true\n            packages:\n            - qemu-guest-agent\n            runcmd:\n            - [ systemctl, enable, qemu-guest-agent ]\n            - [ systemctl, start, qemu-guest-agent ]\n</code></pre> <p>Apply the VM manifest:</p> <pre><code>kubectl apply -f ubuntu-vm.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#step-3-monitor-vm-status","title":"Step 3: Monitor VM Status","text":"<p>Check the status of your VM:</p> <pre><code>kubectl get virtualmachines -n shalb-demo\nkubectl get virtualmachineinstances -n shalb-demo\n</code></pre>"},{"location":"tutorial-virtual-machines/#vm-examples-for-different-operating-systems","title":"VM Examples for Different Operating Systems","text":"<p>Kube-DC supports various operating systems. Here are examples for the most common ones:</p>"},{"location":"tutorial-virtual-machines/#debian","title":"Debian","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: debian-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 14G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-generic-amd64.qcow2\n---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: debian-vm\n  namespace: shalb-demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: shalb-demo/default\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n        cpu:\n          cores: 1\n        memory:\n          guest: 2G\n      volumes:\n      - dataVolume:\n          name: debian-base-img\n        name: root-volume\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |-\n            #cloud-config\n            chpasswd: { expire: False }\n            password: temppassword\n            ssh_pwauth: True\n            package_update: true\n            packages:\n            - qemu-guest-agent\n            runcmd:\n            - [ systemctl, start, qemu-guest-agent ]\n</code></pre>"},{"location":"tutorial-virtual-machines/#alpine-linux","title":"Alpine Linux","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: alpine-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 2G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://dl-cdn.alpinelinux.org/alpine/v3.19/releases/cloud/nocloud_alpine-3.19.1-x86_64-bios-cloudinit-r0.qcow2\n</code></pre>"},{"location":"tutorial-virtual-machines/#centos-stream-9","title":"CentOS Stream 9","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: centos-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud.centos.org/centos/9-stream/x86_64/images/CentOS-Stream-GenericCloud-9-latest.x86_64.qcow2\n</code></pre> <p>Note: CentOS Stream 9 requires additional SELinux configuration to enable guest agent SSH key injection. The OS configuration includes proper SELinux booleans and contexts to allow guest agent operations. See the CentOS example in <code>examples/virtual-machine/centos-8.yaml</code> for the complete cloud-init configuration with SELinux setup.</p>"},{"location":"tutorial-virtual-machines/#virtual-machine-health-checks","title":"Virtual Machine Health Checks","text":"<p>Kube-DC supports VM health checks to ensure your VMs are running properly:</p> <pre><code>spec:\n  template:\n    spec:\n      readinessProbe:\n        guestAgentPing: {}\n        failureThreshold: 10\n        initialDelaySeconds: 20\n        periodSeconds: 10\n        timeoutSeconds: 5\n      livenessProbe:\n        failureThreshold: 10\n        initialDelaySeconds: 120\n        periodSeconds: 20\n        timeoutSeconds: 5\n        httpGet:\n          port: 80\n</code></pre>"},{"location":"tutorial-virtual-machines/#exposing-vm-services","title":"Exposing VM Services","text":""},{"location":"tutorial-virtual-machines/#creating-a-service-for-vm","title":"Creating a Service for VM","text":"<p>To expose a service running on your VM:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh-service\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: ubuntu-vm\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222\n      targetPort: 22\n</code></pre> <p>Apply this service:</p> <pre><code>kubectl apply -f vm-service.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#using-floating-ips-for-vms","title":"Using Floating IPs for VMs","text":"<p>You can assign a floating IP to your VM for direct external access:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: ubuntu-vm-fip\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171\n  eip: vm-eip\n</code></pre> <p>First, ensure you have an EIP:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: vm-eip\n  namespace: shalb-demo\nspec: {}\n</code></pre>"},{"location":"tutorial-virtual-machines/#deploying-containers-alongside-vms","title":"Deploying Containers Alongside VMs","text":"<p>Kube-DC allows you to run containers alongside VMs. Here's how to deploy a simple Nginx container:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: shalb-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n</code></pre> <p>Create a service for the Nginx deployment:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-virtual-machines/#best-practices-for-vm-management","title":"Best Practices for VM Management","text":""},{"location":"tutorial-virtual-machines/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Allocate appropriate resources based on the OS and workload requirements</li> <li>Monitor VM performance to adjust resources as needed</li> <li>Use resource quotas to prevent resource exhaustion</li> </ul>"},{"location":"tutorial-virtual-machines/#security","title":"Security","text":"<ul> <li>Change default passwords immediately</li> <li>Use SSH keys instead of passwords when possible</li> <li>Keep guest OS updated with security patches</li> <li>Apply network policies to control VM traffic</li> </ul>"},{"location":"tutorial-virtual-machines/#efficiency","title":"Efficiency","text":"<ul> <li>Use cloud-init for automated VM configuration</li> <li>Create VM templates for standardized deployments</li> <li>Use the smallest OS image that meets your requirements</li> </ul>"},{"location":"tutorial-virtual-machines/#troubleshooting-vms","title":"Troubleshooting VMs","text":""},{"location":"tutorial-virtual-machines/#common-issues","title":"Common Issues","text":"<ol> <li> <p>VM stuck in provisioning: Check DataVolume status and events    <pre><code>kubectl get datavolume -n shalb-demo\nkubectl describe datavolume ubuntu-vm-disk -n shalb-demo\n</code></pre></p> </li> <li> <p>VM not accessible via network: Verify network configuration    <pre><code>kubectl get vmi -n shalb-demo -o jsonpath='{.items[*].status.interfaces[*].ipAddress}'\n</code></pre></p> </li> <li> <p>Cloud-init not running: Check cloud-init logs inside the VM    <pre><code># Inside the VM\nsudo cat /var/log/cloud-init.log\n</code></pre></p> </li> </ol>"},{"location":"tutorial-virtual-machines/#accessing-vm-logs","title":"Accessing VM Logs","text":"<pre><code>kubectl get events -n shalb-demo\nvirtctl console ubuntu-vm -n shalb-demo\nvirtctl logs ubuntu-vm -n shalb-demo\n</code></pre>"},{"location":"tutorial-virtual-machines/#advanced-kubevirt-features","title":"Advanced KubeVirt Features","text":"<p>For more advanced features, refer to the KubeVirt documentation:</p> <ul> <li>VM Snapshots</li> <li>Live Migration</li> <li>GPU Passthrough</li> <li>Storage Management</li> </ul>"},{"location":"tutorial-virtual-machines/#conclusion","title":"Conclusion","text":"<p>You've now learned how to deploy and manage VMs and containers in Kube-DC using both the intuitive UI and kubectl manifests. This hybrid approach allows you to choose the most appropriate method for your workflow, whether you prefer interactive management or automation through GitOps practices.</p>"},{"location":"tutorial-windows-vm/","title":"Windows 11 VM Tutorial - Complete Setup Guide","text":"<p>This comprehensive guide covers the complete process of setting up Windows 11 VMs in KubeVirt, from infrastructure setup to golden image creation and deployment.</p>"},{"location":"tutorial-windows-vm/#overview","title":"Overview","text":"<p>This tutorial provides two deployment methods:</p> <ol> <li>Golden Image Deployment (Recommended) - Deploy pre-configured VMs in 5-10 minutes</li> <li>Fresh Installation - Create custom Windows installations with full control</li> </ol>"},{"location":"tutorial-windows-vm/#prerequisites","title":"Prerequisites","text":"<ul> <li>KubeVirt and CDI installed and running</li> <li>Multus CNI with OVN network configured</li> <li>StorageClass <code>local-path</code> available</li> <li>Ingress controller for HTTP access to ISOs</li> </ul>"},{"location":"tutorial-windows-vm/#step-1-create-iso-hosting-environment","title":"Step 1: Create ISO Hosting Environment","text":""},{"location":"tutorial-windows-vm/#11-create-dedicated-namespace","title":"1.1 Create Dedicated Namespace","text":"<pre><code># hack/windows/iso-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: iso\n</code></pre>"},{"location":"tutorial-windows-vm/#12-create-storage-for-isos","title":"1.2 Create Storage for ISOs","text":"<pre><code># hack/windows/iso-storage-pvc-iso-ns.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: iso-storage\n  namespace: iso\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi  # Increased for Windows ISO + golden images\n  storageClassName: local-path\n</code></pre>"},{"location":"tutorial-windows-vm/#13-http-server-for-isos","title":"1.3 HTTP Server for ISOs","text":"<pre><code># hack/windows/nginx-iso-server-iso-ns.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-iso-server\n  namespace: iso\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-iso-server\n  template:\n    metadata:\n      labels:\n        app: nginx-iso-server\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: iso-storage\n          mountPath: /usr/share/nginx/html\n        - name: nginx-config\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: iso-storage\n        persistentVolumeClaim:\n          claimName: iso-storage\n      - name: nginx-config\n        configMap:\n          name: nginx-iso-config\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-iso-config\n  namespace: iso\ndata:\n  default.conf: |\n    server {\n        listen 80;\n        server_name _;\n        root /usr/share/nginx/html;\n\n        location / {\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n        }\n\n        location ~* \\.(iso|img|qcow2)$ {\n            add_header Content-Type application/octet-stream;\n            add_header Cache-Control \"public, max-age=3600\";\n        }\n    }\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-iso-server\n  namespace: iso\nspec:\n  selector:\n    app: nginx-iso-server\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-windows-vm/#14-ingress-configuration-with-tls","title":"1.4 Ingress Configuration with TLS","text":"<pre><code># hack/windows/iso-ingress-iso-ns.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: iso-server-ingress\n  namespace: iso\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod-http\n    nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"3600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"3600\"\n    nginx.ingress.kubernetes.io/proxy-buffering: \"off\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - iso.stage.kube-dc.com\n    secretName: iso-server-tls\n  rules:\n  - host: iso.stage.kube-dc.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: iso-server\n            port:\n              number: 80\n</code></pre>"},{"location":"tutorial-windows-vm/#15-deploy-infrastructure","title":"1.5 Deploy Infrastructure","text":"<pre><code># Deploy all infrastructure components\nkubectl apply -f hack/windows/iso-namespace.yaml\nkubectl apply -f hack/windows/iso-storage-pvc-iso-ns.yaml\nkubectl apply -f hack/windows/nginx-iso-server-iso-ns.yaml\nkubectl apply -f hack/windows/iso-ingress-iso-ns.yaml\n\n# Verify deployment\nkubectl get pods -n iso\nkubectl get ingress -n iso\n</code></pre>"},{"location":"tutorial-windows-vm/#step-2-download-and-upload-windows-iso","title":"Step 2: Download and Upload Windows ISO","text":""},{"location":"tutorial-windows-vm/#21-download-windows-11-enterprise-iso","title":"2.1 Download Windows 11 Enterprise ISO","text":"<ol> <li>Visit: https://www.microsoft.com/en-us/evalcenter/download-windows-11-enterprise</li> <li>Select: ISO \u2013 Enterprise download 64-bit edition (90-day evaluation)  </li> <li>Download the ISO file (approximately 5.4GB)</li> </ol>"},{"location":"tutorial-windows-vm/#22-download-virtio-drivers","title":"2.2 Download VirtIO Drivers","text":"<pre><code># Download latest VirtIO drivers\nwget https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso\n</code></pre>"},{"location":"tutorial-windows-vm/#23-upload-files-to-cluster","title":"2.3 Upload Files to Cluster","text":"<pre><code># Create temporary upload pod\nkubectl apply -f hack/windows/iso-upload-pod.yaml\n\n# Wait for pod to be ready\nkubectl wait --for=condition=Ready pod/iso-upload-pod -n iso --timeout=60s\n\n# Upload Windows 11 ISO (replace with your actual filename)\nkubectl cp ~/Win11_24H2_EnglishInternational_x64.iso iso/iso-upload-pod:/storage/win11-x64.iso\n\n# Upload VirtIO drivers\nkubectl cp ~/virtio-win.iso iso/iso-upload-pod:/storage/virtio-win.iso\n\n# Upload OpenSSH installation script\nkubectl cp hack/windows/install-openssh-windows.ps1 iso/iso-upload-pod:/storage/install-openssh-windows.ps1\n\n# Clean up upload pod\nkubectl delete pod iso-upload-pod -n iso --wait=true\n\n# Verify files are accessible\ncurl -I https://iso.stage.kube-dc.com/win11-x64.iso\ncurl -I https://iso.stage.kube-dc.com/virtio-win.iso\n</code></pre>"},{"location":"tutorial-windows-vm/#step-3-fresh-windows-installation","title":"Step 3: Fresh Windows Installation","text":""},{"location":"tutorial-windows-vm/#31-deploy-installation-vm","title":"3.1 Deploy Installation VM","text":"<p>Use the complete VM manifest that includes all required DataVolumes:</p> <pre><code># Deploy Windows VM with installation ISOs\nkubectl apply -f hack/windows/windows11-vm.yaml\n\n# Monitor DataVolume download progress\nkubectl get dv -n shalb-dev\n\n# Check VM status\nkubectl get vm,vmi -n shalb-dev | grep windows11\n</code></pre>"},{"location":"tutorial-windows-vm/#32-windows-installation-process","title":"3.2 Windows Installation Process","text":"<pre><code># Access VM console via VNC\nvirtctl vnc windows11-vm -n shalb-dev\n\n# Or use VNC proxy\nvirtctl vnc windows11-vm -n shalb-dev --proxy-only --port 5900\n# Then connect VNC client to localhost:5900\n</code></pre> <p>Installation Steps:</p> <ol> <li> <p>Boot from Windows ISO: VM boots from Windows installer (bootOrder: 1)</p> </li> <li> <p>Load VirtIO Drivers: </p> </li> <li>When prompted for disk drivers, click \"Load driver\"</li> <li>Browse to VirtIO drivers CDROM</li> <li>Navigate to <code>/amd64/w11/</code> folder</li> <li>Install VirtIO SCSI controller drivers (for disk access)</li> <li> <p>DO NOT install network drivers yet (to create local account)</p> </li> <li> <p>Install Windows: </p> </li> <li>Select the 60GB VirtIO disk for installation</li> <li> <p>Complete Windows 11 setup with local account</p> </li> <li> <p>Post-Installation:</p> </li> <li> <p>Install remaining VirtIO drivers from CDROM (network, balloon, RNG)</p> </li> <li>Install QEMU Guest Agent from VirtIO drivers CDROM</li> <li>Run Windows Updates</li> </ol>"},{"location":"tutorial-windows-vm/#33-configure-ssh-and-rdp","title":"3.3 Configure SSH and RDP","text":"<p>After Windows installation, configure SSH and RDP access:</p> <pre><code># Method 1: Download and run script\nInvoke-WebRequest -Uri \"https://iso.stage.kube-dc.com/install-openssh-windows.ps1\" -OutFile \"install-openssh-windows.ps1\"\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n.\\install-openssh-windows.ps1\n\n# Method 2: Direct execution (bypass execution policy)\nPowerShell -ExecutionPolicy Bypass -Command \"Invoke-Expression (Invoke-WebRequest -Uri 'https://iso.stage.kube-dc.com/install-openssh-windows.ps1').Content\"\n</code></pre> <p>Script Features:</p> <ul> <li>\u2705 Installs OpenSSH Server using Windows capabilities</li> <li>\u2705 Configures SSH service for automatic startup  </li> <li>\u2705 Opens SSH port 22 in Windows Firewall (all network profiles)</li> <li>\u2705 Enables Remote Desktop (port 3389)</li> <li>\u2705 Enables ICMP ping (IPv4 and IPv6)</li> <li>\u2705 Provides detailed verification and status reporting</li> </ul>"},{"location":"tutorial-windows-vm/#step-4-create-golden-image","title":"Step 4: Create Golden Image","text":""},{"location":"tutorial-windows-vm/#41-prepare-vm-for-golden-image","title":"4.1 Prepare VM for Golden Image","text":"<pre><code># 1. Inside Windows VM, run Sysprep (optional but recommended)\n# Navigate to: C:\\Windows\\System32\\Sysprep\\sysprep.exe\n# Options: Generalize, Enter System Out-of-Box Experience (OOBE), Shutdown\n\n# 2. Stop the source VM (CRITICAL for export)\nkubectl patch vm windows11-vm -n shalb-dev --type merge -p '{\"spec\":{\"runStrategy\":\"Halted\"}}'\nkubectl wait --for=delete vmi/windows11-vm -n shalb-dev --timeout=300s\n</code></pre>"},{"location":"tutorial-windows-vm/#42-export-to-qcow2-golden-image","title":"4.2 Export to QCOW2 Golden Image","text":"<pre><code># hack/windows/export-golden-image.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: export-golden-image\n  namespace: shalb-dev\nspec:\n  restartPolicy: Never\n  containers:\n    - name: exporter\n      image: ubuntu:22.04\n      command: [\"sh\", \"-c\"]\n      args:\n        - |\n          set -e\n          apt-get update &amp;&amp; apt-get install -y qemu-utils curl\n          cd /pvc\n\n          echo \"=== Disk Information ===\"\n          DISK_SIZE_BYTES=$(qemu-img info --output=json disk.img | grep '\"virtual-size\"' | cut -d: -f2 | tr -d ' ,')\n          DISK_SIZE_GB=$((DISK_SIZE_BYTES / 1024 / 1024 / 1024))\n          echo \"Source disk: ${DISK_SIZE_GB}GB (${DISK_SIZE_BYTES} bytes)\"\n\n          echo \"=== Converting to compressed QCOW2 ===\"\n          qemu-img convert -p -O qcow2 -c disk.img windows11-x64-golden.qcow2\n\n          echo \"=== Final Golden Image ===\"\n          ls -lh windows11-x64-golden.qcow2\n          qemu-img info windows11-x64-golden.qcow2\n\n          echo \"=== Upload to ISO server ===\"\n          # Upload to ISO server storage\n          curl -T windows11-x64-golden.qcow2 http://nginx-iso-server.iso.svc.cluster.local/windows11-x64-golden.qcow2 || echo \"Upload failed, manual copy required\"\n\n          echo \"=== Export completed, sleeping for inspection ===\"\n          sleep 3600\n      volumeMounts:\n        - name: source-disk\n          mountPath: /pvc\n  volumes:\n    - name: source-disk\n      persistentVolumeClaim:\n        claimName: windows11-disk\n</code></pre>"},{"location":"tutorial-windows-vm/#43-export-process","title":"4.3 Export Process","text":"<pre><code># Export golden image\nkubectl apply -f hack/windows/export-golden-image.yaml\nkubectl wait --for=condition=Ready pod/export-golden-image -n shalb-dev --timeout=120s\n\n# Monitor export progress\nkubectl logs -n shalb-dev export-golden-image -f\n\n# Manual copy to ISO server (if curl upload fails)\nkubectl cp shalb-dev/export-golden-image:/pvc/windows11-x64-golden.qcow2 /tmp/\nkubectl cp /tmp/windows11-x64-golden.qcow2 iso/iso-upload-pod:/storage/\n\n# Verify golden image is available\ncurl -I https://iso.stage.kube-dc.com/windows11-x64-golden.qcow2\n\n# Clean up export pod\nkubectl delete pod export-golden-image -n shalb-dev --wait=true\n</code></pre>"},{"location":"tutorial-windows-vm/#step-5-deploy-from-golden-image","title":"Step 5: Deploy from Golden Image","text":""},{"location":"tutorial-windows-vm/#51-golden-image-deployment-recommended","title":"5.1 Golden Image Deployment (Recommended)","text":"<pre><code># Deploy VM from golden image\nkubectl apply -f hack/windows/win11-x64.yaml\n\n# Create SSH key secret for key injection\nkubectl create secret generic authorized-keys-default \\\n  --from-file=key1=~/.ssh/id_rsa.pub \\\n  -n shalb-dev\n\n# Monitor deployment\nkubectl get vm,vmi,dv -n shalb-dev | grep win11-x64\n\n# Get VM IP when ready\nkubectl get vmi win11-x64 -n shalb-dev -o jsonpath='{.status.interfaces[0].ipAddress}'\n\n# SSH to VM (once guest agent is ready)\nssh kube-dc@&lt;vm-ip&gt;\n</code></pre>"},{"location":"tutorial-windows-vm/#52-golden-image-benefits","title":"5.2 Golden Image Benefits","text":"Aspect Golden Image Fresh Install Deployment Time 5-10 minutes 30+ minutes Download Size 21.3GB compressed 5.4GB + drivers Configuration Pre-configured Manual setup required SSH/RDP Ready immediately Requires script execution VirtIO Drivers Pre-installed Manual installation Use Case Production deployment Custom configurations"},{"location":"tutorial-windows-vm/#step-6-troubleshooting","title":"Step 6: Troubleshooting","text":""},{"location":"tutorial-windows-vm/#61-common-issues","title":"6.1 Common Issues","text":"<p>DataVolume stuck in ImportScheduled: <pre><code># Check CDI importer pods\nkubectl get pods -n cdi\nkubectl logs -n cdi &lt;importer-pod&gt;\n\n# Check storage provisioner\nkubectl get pods -n local-path-storage\n</code></pre></p> <p>VM won't start - CPU resources: <pre><code># Check node resources\nkubectl describe nodes | grep -A 10 \"Allocated resources\"\n\n# Reduce VM CPU if needed\nkubectl patch vm &lt;vm-name&gt; -n &lt;namespace&gt; --type merge -p '{\"spec\":{\"template\":{\"spec\":{\"domain\":{\"cpu\":{\"cores\":2}}}}}}'\n</code></pre></p> <p>SSH not working: <pre><code># Check guest agent connection\nkubectl describe vmi &lt;vm-name&gt; -n &lt;namespace&gt; | grep -i agent\n\n# Test network connectivity\nkubectl run test-pod --image=nicolaka/netshoot --rm -it -- ping &lt;vm-ip&gt;\nkubectl run test-pod --image=nicolaka/netshoot --rm -it -- nc -zv &lt;vm-ip&gt; 22\n</code></pre></p> <p>Storage issues with local-path: <pre><code># Check local-path provisioner\nkubectl get pods -n local-path-storage\nkubectl logs -n local-path-storage &lt;provisioner-pod&gt;\n\n# Note: local-path doesn't support Block mode, use Filesystem mode\n</code></pre></p>"},{"location":"tutorial-windows-vm/#62-verification-commands","title":"6.2 Verification Commands","text":"<pre><code># Check all Windows VMs\nkubectl get vm -A | grep -i win\n\n# Check DataVolume progress\nkubectl get dv -n &lt;namespace&gt;\n\n# Access VM console\nvirtctl vnc &lt;vm-name&gt; -n &lt;namespace&gt;\n\n# Check VM resource usage\nkubectl top pods -n &lt;namespace&gt; | grep virt-launcher\n</code></pre>"},{"location":"tutorial-windows-vm/#required-manifests-summary","title":"Required Manifests Summary","text":"<p>Infrastructure (Step 1): - <code>hack/windows/iso-namespace.yaml</code> - ISO namespace - <code>hack/windows/iso-storage-pvc-iso-ns.yaml</code> - Storage for ISOs - <code>hack/windows/nginx-iso-server-iso-ns.yaml</code> - HTTP server - <code>hack/windows/iso-ingress-iso-ns.yaml</code> - Ingress configuration</p> <p>Utilities: - <code>hack/windows/iso-upload-pod.yaml</code> - Upload files to cluster - <code>hack/windows/install-openssh-windows.ps1</code> - SSH/RDP configuration script</p> <p>VM Deployment: - <code>hack/windows/windows11-vm.yaml</code> - Fresh installation VM - <code>hack/windows/win11-x64.yaml</code> - Golden image deployment</p> <p>Golden Image Creation: - <code>hack/windows/export-golden-image.yaml</code> - Export VM to QCOW2 - <code>hack/windows/windows11-enterprise-golden-datasource.yaml</code> - DataSource for cloning</p> <p>Optional: - <code>hack/windows/windows11-from-datasource.yaml</code> - Deploy from DataSource (same namespace) - <code>hack/windows/pvc-init-windows11-disk.yaml</code> - Fix disk size issues if needed</p>"},{"location":"tutorial-windows-vm/#available-resources","title":"Available Resources","text":"<p>Once deployed, the following resources are available:</p> <ul> <li>Windows 11 ISO: <code>https://iso.stage.kube-dc.com/win11-x64.iso</code> (5.4GB)</li> <li>VirtIO Drivers: <code>https://iso.stage.kube-dc.com/virtio-win.iso</code> (700MB)</li> <li>SSH Script: <code>https://iso.stage.kube-dc.com/install-openssh-windows.ps1</code> (5KB)</li> <li>Golden Image: <code>https://iso.stage.kube-dc.com/windows11-x64-golden.qcow2</code> (21.3GB)</li> </ul>"},{"location":"tutorial-windows-vm/#security-considerations","title":"Security Considerations","text":"<ul> <li>SSH Keys: Use KubeVirt accessCredentials for secure key injection {{ ... }}</li> <li>Network Policies: Implement Kubernetes network policies for VM isolation</li> <li>Sysprep: Run before creating golden images to avoid SID conflicts</li> <li>Firewall: Script configures Windows Firewall appropriately for SSH, RDP, and ICMP</li> </ul> <p>\u2705 Complete Windows 11 VM infrastructure with golden image support ready for production use!</p>"},{"location":"user-groups/","title":"User and Group Management","text":"<p>This guide explains how to set up and manage users, groups, and roles in Kube-DC using Kubernetes RBAC and Keycloak integration.</p>"},{"location":"user-groups/#overview","title":"Overview","text":"<p>Kube-DC implements a multi-tenant access control system that combines:</p> <ul> <li>Kubernetes RBAC: Handles resource-level permissions within namespaces</li> <li>Organization Groups: Manages project-level access across namespaces</li> <li>Keycloak Integration: Provides user authentication and group management</li> </ul>"},{"location":"user-groups/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you're working from an Organization Admin perspective. You'll need:</p> <ul> <li>Access to the Kube-DC cluster with organization admin privileges</li> <li><code>kubectl</code> configured to access your cluster with organization admin privileges</li> <li>Access to the Keycloak organization admin console</li> </ul> <p>Before You Begin</p> <p>During organization and project creation you will get a namespace with organization name <code>&lt;orgname&gt;</code> created and project namespace with <code>&lt;orgname&gt;-&lt;projectname&gt;</code> pattern.</p>"},{"location":"user-groups/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"user-groups/#creating-project-roles","title":"Creating Project Roles","text":"<p>Create a Kubernetes Role to define permissions within a project namespace. These roles dictate what actions users can perform on specific resources.</p> <pre><code>apiVersion: rbac.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: shalb-demo  # Replace with your project namespace\n  name: resource-manager\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\", \"services\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"daemonsets\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n</code></pre> <p>Apply the role to your namespace using:</p> <pre><code>kubectl apply -f role.yaml\n</code></pre> <p>Role Scope</p> <p>Remember that Roles are namespace-scoped. If you need permissions across multiple namespaces, you need to create a separate Role in each Project namespace.</p>"},{"location":"user-groups/#creating-organization-groups","title":"Creating Organization Groups","text":"<p>Create an OrganizationGroup Custom Resource (CR) to define group permissions across projects.</p> <p>Key Points</p> <ul> <li>The OrganizationGroup CR automatically creates a corresponding group in Keycloak</li> <li>This CR must be created in the organization namespace, not the project namespace</li> <li>Role bindings would be created by this CR</li> </ul> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb  # namespace of the organization (not the project)\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - resource-manager\n  # Additional projects and roles can be added:\n  # - project: \"prod\"\n  #   roles:\n  #   - resource-manager\n</code></pre> <p>Apply the group configuration:</p> <pre><code>kubectl apply -f organization-group.yaml\n</code></pre>"},{"location":"user-groups/#managing-users-in-keycloak","title":"Managing Users in Keycloak","text":""},{"location":"user-groups/#access-keycloak-admin-console","title":"Access Keycloak Admin Console","text":"<p>Retrieve Keycloak access credentials from your organization namespace:</p> <pre><code>kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.user}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\n</code></pre> <p>Remember</p> <p>Replace <code>shalb</code> with your own organization namespace in the commands above.</p>"},{"location":"user-groups/#create-and-configure-users","title":"Create and Configure Users","text":"<ol> <li>Log in to the Keycloak admin console using the retrieved credentials</li> <li>Navigate to Users \u2192 Add User </li> <li>Fill in the required user information</li> <li>Set up initial password in the Credentials tab</li> <li>Add the user to the appropriate group (e.g., \"app-manager\") via the Groups tab </li> </ol> <p>User Group Mapping</p> <p>Any groups created via OrganizationGroup CRs will appear automatically in Keycloak. Changes to group membership in Keycloak are synchronized with Kubernetes RBAC.</p>"},{"location":"user-groups/#accessing-kube-dc-ui","title":"Accessing Kube-DC UI","text":"<ol> <li>Navigate to the Kube-DC UI login page</li> <li>Log in using the credentials created in Keycloak</li> <li>Verify access to assigned project resources</li> </ol> <p>Permissions Troubleshooting</p> <p>If a user cannot access expected resources: - Verify they're assigned to the correct groups in Keycloak - Check that the OrganizationGroup CR includes the correct projects and roles - Ensure the underlying Kubernetes Roles have appropriate permissions - Examine the Keycloak logs for authentication issues</p> <p>Permission changes may take up to 5 minutes to propagate through the system.</p>"},{"location":"prd/cloud_network_enable_cluster/","title":"Enabling Cloud Network Access for Management Cluster Components","text":""},{"location":"prd/cloud_network_enable_cluster/#problem-statement","title":"Problem Statement","text":"<p>The management cluster has two external networks available: - ext-public (<code>168.119.17.48/28</code>) - Public internet-facing IPs on VLAN 4011 - ext-cloud (<code>100.65.0.0/16</code>) - Private cloud network on VLAN 4013</p> <p>By default, pods running in the <code>ovn-default</code> subnet (e.g., <code>kamaji-system</code>, <code>kube-system</code>) cannot reach services exposed on the <code>ext-cloud</code> network, even though both subnets are within the same <code>ovn-cluster</code> VPC.</p> <p>This causes issues when: - Kamaji controller needs to connect to dedicated etcd datastores exposed via LoadBalancer on ext-cloud - Any management component needs to access services on the cloud network</p>"},{"location":"prd/cloud_network_enable_cluster/#root-cause","title":"Root Cause","text":"<p>The <code>ovn-default</code> subnet has <code>natOutgoing: true</code>, which means outbound traffic is NAT'd through the node's external interface (join network). However:</p> <ol> <li>Traffic to <code>ext-cloud</code> (100.65.0.x) is routed via the physical VLAN 4013</li> <li>Return traffic from ext-cloud doesn't know how to reach the OVN internal network (10.100.0.0/16)</li> <li>No SNAT is configured for traffic from <code>ovn-default</code> to <code>ext-cloud</code></li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#solution","title":"Solution","text":"<p>Three configurations were required on the <code>ovn-cluster</code> VPC:</p>"},{"location":"prd/cloud_network_enable_cluster/#1-static-route-to-ext-cloud-gateway","title":"1. Static Route to ext-cloud Gateway","text":"<pre><code># Added to vpc/ovn-cluster spec.staticRoutes\n- cidr: 100.65.0.0/16\n  nextHopIP: 100.65.0.1  # ext-cloud gateway\n  policy: policyDst\n</code></pre> <p>This tells the OVN router where to send traffic destined for ext-cloud.</p>"},{"location":"prd/cloud_network_enable_cluster/#2-policy-route-to-allow-traffic","title":"2. Policy Route to Allow Traffic","text":"<pre><code># Added to vpc/ovn-cluster spec.policyRoutes  \n- action: allow\n  match: ip4.dst == 100.65.0.0/16\n  priority: 31000\n</code></pre> <p>Without this, traffic to ext-cloud would be dropped by default policies that only allow traffic to known internal subnets.</p>"},{"location":"prd/cloud_network_enable_cluster/#3-snat-rule-for-return-traffic","title":"3. SNAT Rule for Return Traffic","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: OvnSnatRule\nmetadata:\n  name: ovn-cluster-to-ext-cloud\nspec:\n  ovnEip: ovn-cluster-ext-cloud  # Uses 100.65.0.101\n  vpcSubnet: ovn-default\n</code></pre> <p>This NATs traffic from <code>10.100.0.0/16</code> to <code>100.65.0.101</code>, allowing return traffic to find its way back.</p>"},{"location":"prd/cloud_network_enable_cluster/#current-configuration","title":"Current Configuration","text":""},{"location":"prd/cloud_network_enable_cluster/#vpc-ovn-cluster","title":"VPC ovn-cluster","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Vpc\nmetadata:\n  name: ovn-cluster\nspec:\n  enableExternal: true\n  staticRoutes:\n  - cidr: 100.65.0.0/16\n    nextHopIP: 100.65.0.1\n    policy: policyDst\n  policyRoutes:\n  - action: allow\n    match: ip4.dst == 100.65.0.0/16\n    priority: 31000\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#ovnsnatrule","title":"OvnSnatRule","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: OvnSnatRule\nmetadata:\n  name: ovn-cluster-to-ext-cloud\nspec:\n  ovnEip: ovn-cluster-ext-cloud\n  vpcSubnet: ovn-default\nstatus:\n  ready: true\n  v4Eip: 100.65.0.101\n  v4IpCidr: 10.100.0.0/16\n  vpc: ovn-cluster\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#verification","title":"Verification","text":"<p>Test connectivity from kamaji-system to ext-cloud:</p> <pre><code># Create test service on ext-cloud\nkubectl -n shalb-demo run nginx-test --image=nginx:alpine\nkubectl -n shalb-demo expose pod nginx-test --port=8080 --target-port=80 --type=LoadBalancer \\\n  --dry-run=client -o yaml | \\\n  kubectl annotate -f - service.nlb.kube-dc.com/bind-on-eip=default-gw --local -o yaml | \\\n  kubectl apply -f -\n\n# Test from kamaji-system\nkubectl -n kamaji-system run test --image=busybox --rm -it --restart=Never -- \\\n  wget -qO- --timeout=5 http://100.65.0.102:8080\n\n# Cleanup\nkubectl -n shalb-demo delete pod nginx-test svc nginx-test\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#ovn-commands-for-debugging","title":"OVN Commands for Debugging","text":"<pre><code># View routes on ovn-cluster router\nkubectl ko nbctl lr-route-list ovn-cluster\n\n# View policy routes\nkubectl ko nbctl lr-policy-list ovn-cluster\n\n# View NAT rules\nkubectl ko nbctl lr-nat-list ovn-cluster\n\n# Trace packet path\nkubectl ko trace &lt;namespace&gt;/&lt;pod&gt; &lt;dest-ip&gt; tcp &lt;port&gt;\n\n# View router ports\nkubectl ko nbctl show ovn-cluster\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#project-vpcs-and-ext-cloud-access","title":"Project VPCs and ext-cloud Access","text":""},{"location":"prd/cloud_network_enable_cluster/#existing-project-vpcs","title":"Existing Project VPCs","text":"<p>Project VPCs (like <code>shalb-demo</code>, <code>shalb-dev</code>) are isolated from <code>ovn-cluster</code> VPC. They have their own configuration:</p> VPC ext-cloud Access Configuration <code>shalb-dev</code> \u2705 Yes <code>extraExternalSubnets: [\"ext-cloud\", \"ext-public\"]</code> <code>shalb-demo</code> \u2705 Yes Default gateway on ext-cloud (<code>100.65.0.1</code>) <code>shalb-envoy</code> \u274c No (public only) <code>extraExternalSubnets: [\"ext-public\"]</code>"},{"location":"prd/cloud_network_enable_cluster/#creating-new-vpcs-with-ext-cloud-access","title":"Creating New VPCs with ext-cloud Access","text":"<p>When creating a new project VPC that needs ext-cloud access:</p>"},{"location":"prd/cloud_network_enable_cluster/#option-1-set-ext-cloud-as-default-gateway-recommended","title":"Option 1: Set ext-cloud as Default Gateway (Recommended)","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Vpc\nmetadata:\n  name: my-project\n  annotations:\n    network.kube-dc.com/default-gw-subnet-name: ext-cloud\nspec:\n  enableExternal: true\n  namespaces:\n  - my-project\n  staticRoutes:\n  - cidr: 0.0.0.0/0\n    nextHopIP: 100.65.0.1  # ext-cloud gateway\n    policy: policyDst\n</code></pre> <p>This routes all external traffic through ext-cloud.</p>"},{"location":"prd/cloud_network_enable_cluster/#option-2-add-ext-cloud-as-extra-external-subnet","title":"Option 2: Add ext-cloud as Extra External Subnet","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Vpc\nmetadata:\n  name: my-project\nspec:\n  enableExternal: true\n  extraExternalSubnets:\n  - ext-cloud\n  - ext-public  # Optional: include both\n  namespaces:\n  - my-project\n</code></pre> <p>This allows the VPC to use both networks.</p>"},{"location":"prd/cloud_network_enable_cluster/#key-points-for-new-vpcs","title":"Key Points for New VPCs","text":"<ol> <li>No changes needed to ovn-cluster VPC - The configuration above is global and allows <code>ovn-default</code> pods to reach ext-cloud</li> <li>Project VPCs are independent - Each project VPC needs its own routing configuration</li> <li>SNAT is automatic - When using EIP resources, SNAT is configured automatically</li> <li>extraExternalSubnets - This setting allows the VPC to connect to external subnets on physical VLANs</li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#network-architecture-diagram","title":"Network Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Physical Network                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502   VLAN 4011      \u2502                 \u2502   VLAN 4013      \u2502      \u2502\n\u2502  \u2502   ext-public     \u2502                 \u2502   ext-cloud      \u2502      \u2502\n\u2502  \u2502 168.119.17.48/28 \u2502                 \u2502 100.65.0.0/16    \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502                                     \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       OVN Logical Network                        \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                    ovn-cluster VPC                          \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  ovn-default \u2502    \u2502   ext-cloud  \u2502    \u2502  ext-public  \u2502  \u2502 \u2502\n\u2502  \u2502  \u250210.100.0.0/16 \u2502\u2500\u2500\u2500\u25b6\u2502100.65.0.0/16 \u2502    \u2502168.119.17/28 \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502              \u2502    \u2502              \u2502    \u2502              \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502 kamaji-sys   \u2502    \u2502  SNAT via    \u2502    \u2502              \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502 kube-system  \u2502    \u2502 100.65.0.101 \u2502    \u2502              \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  shalb-dev VPC  \u2502  \u2502 shalb-demo VPC  \u2502  \u2502 shalb-envoy VPC \u2502  \u2502\n\u2502  \u2502  10.1.0.0/16    \u2502  \u2502  10.0.10.0/24   \u2502  \u2502  10.0.40.0/24   \u2502  \u2502\n\u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502\n\u2502  \u2502 ext-cloud: \u2705   \u2502  \u2502 ext-cloud: \u2705   \u2502  \u2502 ext-cloud: \u274c   \u2502  \u2502\n\u2502  \u2502 ext-public: \u2705  \u2502  \u2502 ext-public: \u274c  \u2502  \u2502 ext-public: \u2705  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#troubleshooting","title":"Troubleshooting","text":""},{"location":"prd/cloud_network_enable_cluster/#symptom-timeout-when-accessing-ext-cloud-from-kamaji-system","title":"Symptom: Timeout when accessing ext-cloud from kamaji-system","text":"<ol> <li> <p>Check static route exists: <pre><code>kubectl ko nbctl lr-route-list ovn-cluster | grep 100.65\n</code></pre></p> </li> <li> <p>Check policy route allows traffic: <pre><code>kubectl ko nbctl lr-policy-list ovn-cluster | grep 100.65\n</code></pre></p> </li> <li> <p>Check SNAT rule exists: <pre><code>kubectl ko nbctl lr-nat-list ovn-cluster\nkubectl get ovn-snat-rules ovn-cluster-to-ext-cloud\n</code></pre></p> </li> <li> <p>Trace the packet: <pre><code>kubectl ko trace kamaji-system/&lt;pod-name&gt; 100.65.0.102 tcp 8080\n</code></pre></p> </li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#symptom-project-vpc-cannot-reach-ext-cloud","title":"Symptom: Project VPC cannot reach ext-cloud","text":"<ol> <li> <p>Check VPC has extraExternalSubnets or staticRoute: <pre><code>kubectl get vpc &lt;vpc-name&gt; -o yaml\n</code></pre></p> </li> <li> <p>Verify EIP and SNAT are configured: <pre><code>kubectl get ovn-eip,ovn-snat-rules | grep &lt;vpc-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#references","title":"References","text":"<ul> <li>Kube-OVN VPC Documentation</li> <li>Kube-OVN External Gateway</li> </ul>"},{"location":"prd/cloud_network_enable_public_ingress/","title":"Exposing Cloud Network Services via Public Ingress","text":""},{"location":"prd/cloud_network_enable_public_ingress/#problem-statement","title":"Problem Statement","text":"<p>Tenant Kubernetes clusters (Kamaji TenantControlPlanes) have their API servers exposed on the ext-cloud network (<code>100.65.0.0/16</code>), which is a private cloud network not directly accessible from the public internet.</p> <p>Users need to access tenant cluster APIs from the public internet using hostnames like: - <code>demo-cluster-cp.stage.kube-dc.com</code> - <code>cluster-a.stage.kube-dc.com</code></p>"},{"location":"prd/cloud_network_enable_public_ingress/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           Public Internet                                    \u2502\n\u2502                                  \u2502                                           \u2502\n\u2502                                  \u25bc                                           \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502                    \u2502    Nginx Ingress Controller  \u2502                          \u2502\n\u2502                    \u2502      88.99.29.250:443       \u2502                          \u2502\n\u2502                    \u2502      (ext-public VLAN)       \u2502                          \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                                  \u2502 SSL Passthrough (SNI)                     \u2502\n\u2502                                  \u25bc                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                        ovn-cluster VPC                                 \u2502  \u2502\n\u2502  \u2502                                                                        \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      SNAT via       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502   ovn-default    \u2502    100.65.0.101     \u2502    ext-cloud     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  10.100.0.0/16   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6  \u2502  100.65.0.0/16   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502                  \u2502                      \u2502                  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  - ingress-nginx \u2502                      \u2502  - Tenant APIs   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  - kamaji-system \u2502                      \u2502  - etcd LBs      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  - default ns    \u2502                      \u2502                  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_network_enable_public_ingress/#solution","title":"Solution","text":""},{"location":"prd/cloud_network_enable_public_ingress/#prerequisites","title":"Prerequisites","text":"<ol> <li>SNAT rule configured on <code>ovn-cluster</code> VPC (see <code>cloud_network_enable_cluster.md</code>)</li> <li>SSL passthrough enabled on nginx ingress controller: <code>--enable-ssl-passthrough</code></li> <li>TCP services ConfigMap configured: <code>--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</code></li> </ol>"},{"location":"prd/cloud_network_enable_public_ingress/#nginx-ingress-controller-configuration","title":"Nginx Ingress Controller Configuration","text":"<p>The following args must be added to the nginx ingress controller deployment:</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - name: controller\n        args:\n        - /nginx-ingress-controller\n        - --enable-ssl-passthrough           # Required for SNI-based routing\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services  # Optional: for TCP proxy\n        # ... other args\n</code></pre>"},{"location":"prd/cloud_network_enable_public_ingress/#two-exposure-methods","title":"Two Exposure Methods","text":""},{"location":"prd/cloud_network_enable_public_ingress/#method-1-ssl-passthrough-recommended-for-k8s-apis","title":"Method 1: SSL Passthrough (Recommended for K8s APIs)","text":"<p>Use case: Multi-tenant with hostname-based routing on port 443</p> <p>How it works: 1. Nginx intercepts TLS traffic on port 443 2. Reads SNI (Server Name Indication) to determine hostname 3. Forwards entire TLS connection to backend without termination 4. Backend handles TLS (preserves original certificates)</p> <p>Configuration per tenant cluster:</p> <pre><code># 1. ClusterIP Service (MUST NOT be headless)\napiVersion: v1\nkind: Service\nmetadata:\n  name: {cluster-name}-api-proxy\n  namespace: default\n  labels:\n    kube-dc.com/tenant-cluster: \"{cluster-name}\"\nspec:\n  type: ClusterIP  # Required - ssl-passthrough sends traffic to ClusterIP\n  ports:\n  - name: api\n    port: 6443\n    protocol: TCP\n---\n# 2. Endpoints pointing to ext-cloud IP\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: {cluster-name}-api-proxy\n  namespace: default\nsubsets:\n- addresses:\n  - ip: {ext-cloud-ip}  # e.g., 100.65.0.105\n  ports:\n  - name: api\n    port: 6443\n    protocol: TCP\n---\n# 3. Ingress with ssl-passthrough annotation\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {cluster-name}-api\n  namespace: default\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: {cluster-name}.stage.kube-dc.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: {cluster-name}-api-proxy\n            port:\n              number: 6443\n</code></pre> <p>Critical requirement: The service must have a ClusterIP (not headless). From nginx-ingress documentation:</p> <p>\"traffic to Passthrough backends is sent to the clusterIP of the backing Service instead of individual Endpoints\"</p>"},{"location":"prd/cloud_network_enable_public_ingress/#method-2-tcp-configmap-port-based","title":"Method 2: TCP ConfigMap (Port-based)","text":"<p>Use case: Single service per port, no hostname routing</p> <p>Configuration:</p> <pre><code># ConfigMap for TCP services\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\ndata:\n  \"6443\": \"default/demo-cluster-api-proxy:6443\"\n  # Add more ports as needed\n  # \"6444\": \"default/cluster-b-api-proxy:6443\"\n</code></pre> <p>Nginx Service must expose the TCP port:</p> <pre><code>spec:\n  ports:\n  - name: tcp-6443\n    port: 6443\n    targetPort: 6443\n    protocol: TCP\n</code></pre> <p>Limitation: Each service requires a unique port. Not suitable for multi-tenant.</p>"},{"location":"prd/cloud_network_enable_public_ingress/#comparison","title":"Comparison","text":"Feature SSL Passthrough TCP ConfigMap Routing SNI/hostname-based Port-based Port Shared 443 Unique per service TLS Preserved (passthrough) Preserved (passthrough) Multi-tenant \u2705 Yes \u274c No (port conflicts) DNS required Yes No Best for K8s APIs, multi-tenant Single service, testing"},{"location":"prd/cloud_network_enable_public_ingress/#verification","title":"Verification","text":""},{"location":"prd/cloud_network_enable_public_ingress/#test-ssl-passthrough","title":"Test SSL Passthrough","text":"<pre><code># Test via hostname (SSL passthrough)\ncurl -k https://demo-cluster-cp.stage.kube-dc.com/version\n\n# Expected: Kubernetes API version response\n{\n  \"major\": \"1\",\n  \"minor\": \"34\",\n  \"gitVersion\": \"v1.34.0\",\n  ...\n}\n</code></pre>"},{"location":"prd/cloud_network_enable_public_ingress/#test-tcp-proxy","title":"Test TCP Proxy","text":"<pre><code># Test via IP:port (TCP proxy)\ncurl -k https://88.99.29.250:6443/version\n\n# Expected: Same Kubernetes API version response\n</code></pre>"},{"location":"prd/cloud_network_enable_public_ingress/#verify-connectivity-path","title":"Verify Connectivity Path","text":"<pre><code># Test from nginx namespace to ext-cloud\nkubectl run -n ingress-nginx test-conn --rm -it --restart=Never \\\n  --image=busybox -- nc -zv 100.65.0.105 6443\n\n# Expected: 100.65.0.105 (100.65.0.105:6443) open\n</code></pre>"},{"location":"prd/cloud_network_enable_public_ingress/#automation","title":"Automation","text":"<p>The <code>kdc-cluster-controller</code> should automatically create these resources when a TenantControlPlane is provisioned:</p> <ol> <li>Service in <code>default</code> namespace with ClusterIP</li> <li>Endpoints pointing to the tenant API's ext-cloud IP</li> <li>Ingress with ssl-passthrough annotation</li> </ol>"},{"location":"prd/cloud_network_enable_public_ingress/#implementation-notes","title":"Implementation Notes","text":"<ol> <li>Namespace: Resources are created in <code>default</code> namespace (part of <code>ovn-default</code> subnet) to ensure SNAT connectivity to ext-cloud</li> <li>Naming convention: <code>{tenant-namespace}-{cluster-name}-api-proxy</code></li> <li>Cleanup: Resources should be deleted when TenantControlPlane is deleted</li> </ol>"},{"location":"prd/cloud_network_enable_public_ingress/#troubleshooting","title":"Troubleshooting","text":""},{"location":"prd/cloud_network_enable_public_ingress/#ssl-passthrough-not-working","title":"SSL Passthrough not working","text":"<ol> <li> <p>Check ssl-passthrough is enabled:    <pre><code>kubectl get deployment -n ingress-nginx ingress-nginx-controller \\\n  -o jsonpath='{.spec.template.spec.containers[0].args}' | grep passthrough\n</code></pre></p> </li> <li> <p>Check service has ClusterIP (not headless):    <pre><code>kubectl get svc {service-name} -n default\n# ClusterIP should NOT be \"None\"\n</code></pre></p> </li> <li> <p>Check endpoints exist:    <pre><code>kubectl get endpoints {service-name} -n default\n</code></pre></p> </li> <li> <p>Check nginx config includes passthrough server:    <pre><code>kubectl exec -n ingress-nginx {nginx-pod} -- \\\n  grep -i \"passthrough\" /etc/nginx/nginx.conf\n</code></pre></p> </li> </ol>"},{"location":"prd/cloud_network_enable_public_ingress/#connection-timeouts","title":"Connection timeouts","text":"<ol> <li> <p>Verify SNAT rule exists:    <pre><code>kubectl ko nbctl lr-nat-list ovn-cluster\n# Should show: snat 10.100.0.0/16 -&gt; 100.65.0.101\n</code></pre></p> </li> <li> <p>Test connectivity from nginx pod:    <pre><code>kubectl run -n ingress-nginx test --rm -it --restart=Never \\\n  --image=busybox -- nc -zv {ext-cloud-ip} 6443\n</code></pre></p> </li> </ol>"},{"location":"prd/cloud_network_enable_public_ingress/#security-considerations","title":"Security Considerations","text":"<ol> <li>TLS certificates: Tenant API certificates are preserved (not terminated at nginx)</li> <li>Authentication: Kubernetes RBAC and client certificates work as expected</li> <li>Network isolation: Only <code>ovn-default</code> pods can reach ext-cloud via SNAT</li> <li>No credential exposure: SSL passthrough means nginx never sees unencrypted traffic</li> </ol>"},{"location":"prd/cloud_network_enable_public_ingress/#references","title":"References","text":"<ul> <li>Nginx Ingress SSL Passthrough</li> <li>Cloud Network Enable Cluster</li> <li>Kamaji TenantControlPlane</li> </ul>"},{"location":"prd/cloud_network_higress_hostnetwork/","title":"Scalable Public Ingress with Higress on hostNetwork","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#problem-statement","title":"Problem Statement","text":"<p>The current Nginx Ingress Controller has limitations when scaling to thousands of tenant clusters:</p> <ol> <li>Reload Instability: Configuration changes cause temporary connection drops</li> <li>Long Connection Termination: Active connections terminated during config updates (critical for K8s API watch streams)</li> <li>Slow Config Propagation: 2+ minutes for route updates at 10,000+ routes</li> <li>SSL Passthrough Overhead: Performance penalty when bypassing nginx for TLS passthrough</li> <li>Single VIP Bottleneck: All traffic through one LoadBalancer IP</li> </ol>"},{"location":"prd/cloud_network_higress_hostnetwork/#solution-overview","title":"Solution Overview","text":"<p>Deploy Higress (Alibaba's Envoy-based gateway) using hostNetwork on dedicated gateway nodes to achieve:</p> <ul> <li>Wire-speed performance: Direct network access, no NAT/DNAT overhead</li> <li>Multiple public IPs: Horizontal scaling across gateway nodes</li> <li>Hot reload: Millisecond config updates via Envoy xDS protocol</li> <li>20,000+ routes: Tested at scale by Sealos (87,000 users, 2,000+ tenants)</li> </ul>"},{"location":"prd/cloud_network_higress_hostnetwork/#architecture","title":"Architecture","text":"<pre><code>                              Internet\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502    DNS Round-Robin      \u2502\n                    \u2502  *.stage.kube-dc.com    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                        \u2502                        \u2502\n        \u25bc                        \u25bc                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 gw-node-1     \u2502        \u2502 gw-node-2     \u2502        \u2502 gw-node-3     \u2502\n\u2502 168.119.17.49 \u2502        \u2502 168.119.17.50 \u2502        \u2502 168.119.17.51 \u2502\n\u2502               \u2502        \u2502               \u2502        \u2502               \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502        \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502        \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Higress   \u2502 \u2502        \u2502 \u2502 Higress   \u2502 \u2502        \u2502 \u2502 Higress   \u2502 \u2502\n\u2502 \u2502 Gateway   \u2502 \u2502        \u2502 \u2502 Gateway   \u2502 \u2502        \u2502 \u2502 Gateway   \u2502 \u2502\n\u2502 \u2502 (hostNet) \u2502 \u2502        \u2502 \u2502 (hostNet) \u2502 \u2502        \u2502 \u2502 (hostNet) \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502        \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502        \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                        \u2502                        \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502    ovn-cluster VPC      \u2502\n                    \u2502    (SNAT 100.65.0.101)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   ext-cloud Network     \u2502\n                    \u2502   100.65.0.0/16         \u2502\n                    \u2502                         \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n                    \u2502  \u2502 Tenant Clusters \u2502    \u2502\n                    \u2502  \u2502 API Servers     \u2502    \u2502\n                    \u2502  \u2502 100.65.0.x:6443 \u2502    \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#why-higress","title":"Why Higress?","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#comparison-with-nginx-ingress","title":"Comparison with Nginx Ingress","text":"Feature Nginx Ingress Higress Config Update Reload (connection drops) Hot reload (xDS) Update Time (10K routes) ~2 minutes ~3 seconds SSL Passthrough Performance penalty Native Envoy Long Connections Terminated on reload Preserved Memory at Scale High Low Nginx Compatibility Native Annotation support"},{"location":"prd/cloud_network_higress_hostnetwork/#why-not-full-istio","title":"Why Not Full Istio?","text":"<ul> <li>Istio is a full service mesh (overkill for ingress-only)</li> <li>Higher resource consumption</li> <li>More complex operations</li> <li>Higress uses Envoy directly without Istio dependency</li> </ul>"},{"location":"prd/cloud_network_higress_hostnetwork/#implementation","title":"Implementation","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#prerequisites","title":"Prerequisites","text":"<ol> <li>Dedicated Gateway Nodes: 2-3 nodes with public IPs on ext-public network</li> <li>Node Labels: <code>node-role.kubernetes.io/gateway=true</code></li> <li>DNS Configuration: Wildcard DNS pointing to gateway node IPs</li> </ol>"},{"location":"prd/cloud_network_higress_hostnetwork/#step-1-label-gateway-nodes","title":"Step 1: Label Gateway Nodes","text":"<pre><code># Label nodes that have public IPs for gateway duty\nkubectl label node kube-dc-gw-1 node-role.kubernetes.io/gateway=true\nkubectl label node kube-dc-gw-2 node-role.kubernetes.io/gateway=true\nkubectl label node kube-dc-gw-3 node-role.kubernetes.io/gateway=true\n\n# Optionally taint to prevent other workloads\nkubectl taint node kube-dc-gw-1 node-role.kubernetes.io/gateway=true:NoSchedule\nkubectl taint node kube-dc-gw-2 node-role.kubernetes.io/gateway=true:NoSchedule\nkubectl taint node kube-dc-gw-3 node-role.kubernetes.io/gateway=true:NoSchedule\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#step-2-install-higress","title":"Step 2: Install Higress","text":"<pre><code># Add Higress Helm repository\nhelm repo add higress https://higress.io/helm-charts\nhelm repo update\n\n# Install Higress with hostNetwork configuration\nhelm install higress higress/higress \\\n  -n higress-system \\\n  --create-namespace \\\n  --set global.enableIstioAPI=false \\\n  --set higress-core.gateway.hostNetwork=true \\\n  --set higress-core.gateway.dnsPolicy=ClusterFirstWithHostNet \\\n  --set higress-core.gateway.nodeSelector.\"node-role\\.kubernetes\\.io/gateway\"=true \\\n  --set higress-core.gateway.tolerations[0].key=node-role.kubernetes.io/gateway \\\n  --set higress-core.gateway.tolerations[0].operator=Exists \\\n  --set higress-core.gateway.tolerations[0].effect=NoSchedule \\\n  --set higress-core.gateway.replicas=3\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#step-3-configure-tls-passthrough-for-tenant-apis","title":"Step 3: Configure TLS Passthrough for Tenant APIs","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#option-a-using-ingress-with-ssl-passthrough-annotation","title":"Option A: Using Ingress with SSL Passthrough Annotation","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: demo-cluster-api\n  namespace: default\n  annotations:\n    higress.io/ssl-passthrough: \"true\"\nspec:\n  ingressClassName: higress\n  rules:\n  - host: demo-cluster-cp.stage.kube-dc.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: demo-cluster-api-proxy\n            port:\n              number: 6443\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#option-b-using-gateway-api-recommended","title":"Option B: Using Gateway API (Recommended)","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: tenant-gateway\n  namespace: higress-system\nspec:\n  gatewayClassName: higress\n  listeners:\n  - name: tls-passthrough\n    port: 443\n    protocol: TLS\n    hostname: \"*.stage.kube-dc.com\"\n    tls:\n      mode: Passthrough\n    allowedRoutes:\n      namespaces:\n        from: All\n---\napiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TLSRoute\nmetadata:\n  name: demo-cluster-api\n  namespace: default\nspec:\n  parentRefs:\n  - name: tenant-gateway\n    namespace: higress-system\n    sectionName: tls-passthrough\n  hostnames:\n  - \"demo-cluster-cp.stage.kube-dc.com\"\n  rules:\n  - backendRefs:\n    - name: demo-cluster-api-proxy\n      port: 6443\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#step-4-backend-service-configuration","title":"Step 4: Backend Service Configuration","text":"<p>Create ClusterIP service with endpoints pointing to ext-cloud tenant API:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: demo-cluster-api-proxy\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - name: api\n    port: 6443\n    protocol: TCP\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: demo-cluster-api-proxy\n  namespace: default\nsubsets:\n- addresses:\n  - ip: 100.65.0.105  # Tenant API on ext-cloud\n  ports:\n  - name: api\n    port: 6443\n    protocol: TCP\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#step-5-dns-configuration","title":"Step 5: DNS Configuration","text":"<p>Configure wildcard DNS to point to all gateway nodes:</p> <pre><code>; Round-robin DNS for high availability\n*.stage.kube-dc.com.    300    IN    A    168.119.17.49\n*.stage.kube-dc.com.    300    IN    A    168.119.17.50\n*.stage.kube-dc.com.    300    IN    A    168.119.17.51\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#automation-for-tenant-clusters","title":"Automation for Tenant Clusters","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#controller-integration","title":"Controller Integration","text":"<p>The <code>kdc-cluster-controller</code> should automatically create:</p> <ol> <li>ClusterIP Service with ext-cloud endpoint</li> <li>TLSRoute (or Ingress) for the tenant API</li> <li>DNS record (if using external-dns)</li> </ol> <p>Example controller logic:</p> <pre><code>func (r *KdcClusterReconciler) reconcileIngress(ctx context.Context, cluster *kdcv1.KdcCluster) error {\n    // Get tenant API external IP from cloud network\n    externalIP := cluster.Status.ControlPlaneEndpoint.Host\n\n    // Create proxy service\n    proxySvc := &amp;corev1.Service{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      cluster.Name + \"-api-proxy\",\n            Namespace: \"default\",\n        },\n        Spec: corev1.ServiceSpec{\n            Type: corev1.ServiceTypeClusterIP,\n            Ports: []corev1.ServicePort{{\n                Name: \"api\",\n                Port: 6443,\n            }},\n        },\n    }\n\n    // Create endpoints pointing to ext-cloud IP\n    endpoints := &amp;corev1.Endpoints{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      cluster.Name + \"-api-proxy\",\n            Namespace: \"default\",\n        },\n        Subsets: []corev1.EndpointSubset{{\n            Addresses: []corev1.EndpointAddress{{\n                IP: externalIP,\n            }},\n            Ports: []corev1.EndpointPort{{\n                Name: \"api\",\n                Port: 6443,\n            }},\n        }},\n    }\n\n    // Create TLSRoute for SSL passthrough\n    tlsRoute := &amp;gatewayv1alpha2.TLSRoute{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      cluster.Name + \"-api\",\n            Namespace: \"default\",\n        },\n        Spec: gatewayv1alpha2.TLSRouteSpec{\n            Hostnames: []gatewayv1alpha2.Hostname{\n                gatewayv1alpha2.Hostname(cluster.Name + \".stage.kube-dc.com\"),\n            },\n            Rules: []gatewayv1alpha2.TLSRouteRule{{\n                BackendRefs: []gatewayv1alpha2.BackendRef{{\n                    BackendObjectReference: gatewayv1alpha2.BackendObjectReference{\n                        Name: gatewayv1alpha2.ObjectName(cluster.Name + \"-api-proxy\"),\n                        Port: ptr.To(gatewayv1alpha2.PortNumber(6443)),\n                    },\n                }},\n            }},\n        },\n    }\n\n    return r.createOrUpdate(ctx, proxySvc, endpoints, tlsRoute)\n}\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#verification","title":"Verification","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#test-tls-passthrough","title":"Test TLS Passthrough","text":"<pre><code># Test from external network\ncurl -k -v https://demo-cluster-cp.stage.kube-dc.com:443/healthz\n\n# Verify SNI routing\nopenssl s_client -connect 168.119.17.49:443 \\\n  -servername demo-cluster-cp.stage.kube-dc.com &lt;/dev/null 2&gt;/dev/null | \\\n  openssl x509 -noout -subject\n\n# Test kubectl access\nkubectl --kubeconfig=/path/to/tenant-kubeconfig get nodes\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#monitor-gateway-performance","title":"Monitor Gateway Performance","text":"<pre><code># Check Higress controller logs\nkubectl logs -n higress-system -l app=higress-controller -f\n\n# Check gateway pods\nkubectl get pods -n higress-system -l app=higress-gateway -o wide\n\n# Verify hostNetwork binding\nkubectl exec -n higress-system &lt;gateway-pod&gt; -- netstat -tlnp | grep 443\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#horizontal-scaling","title":"Horizontal Scaling","text":"Tenants Gateway Nodes Recommended Setup &lt; 100 2 Basic HA 100-500 3 Standard 500-2000 5 High capacity 2000+ 7+ Enterprise scale"},{"location":"prd/cloud_network_higress_hostnetwork/#resource-requirements-per-gateway-node","title":"Resource Requirements per Gateway Node","text":"Scale CPU Memory Network &lt; 500 routes 2 cores 2 GB 1 Gbps 500-2000 routes 4 cores 4 GB 10 Gbps 2000-10000 routes 8 cores 8 GB 10 Gbps 10000+ routes 16 cores 16 GB 25 Gbps"},{"location":"prd/cloud_network_higress_hostnetwork/#security-considerations","title":"Security Considerations","text":"<ol> <li>Network Isolation: Gateway nodes should only expose ports 80/443</li> <li>Rate Limiting: Configure Higress rate limiting plugins per tenant</li> <li>WAF: Enable Higress WAF plugin for DDoS protection</li> <li>mTLS: Consider mTLS between gateway and backend services</li> <li>Certificate Management: Use cert-manager with Let's Encrypt</li> </ol>"},{"location":"prd/cloud_network_higress_hostnetwork/#migration-from-nginx-ingress","title":"Migration from Nginx Ingress","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#phase-1-parallel-deployment","title":"Phase 1: Parallel Deployment","text":"<ol> <li>Deploy Higress alongside existing nginx</li> <li>Test with non-critical tenants</li> <li>Validate TLS passthrough works</li> </ol>"},{"location":"prd/cloud_network_higress_hostnetwork/#phase-2-gradual-migration","title":"Phase 2: Gradual Migration","text":"<ol> <li>Create Higress routes for new tenants</li> <li>Migrate existing tenants incrementally</li> <li>Monitor for issues</li> </ol>"},{"location":"prd/cloud_network_higress_hostnetwork/#phase-3-cutover","title":"Phase 3: Cutover","text":"<ol> <li>Update DNS to point to Higress nodes</li> <li>Keep nginx running for rollback</li> <li>After validation, decommission nginx</li> </ol>"},{"location":"prd/cloud_network_higress_hostnetwork/#troubleshooting","title":"Troubleshooting","text":""},{"location":"prd/cloud_network_higress_hostnetwork/#route-not-working","title":"Route Not Working","text":"<pre><code># Check Higress controller config sync\nkubectl logs -n higress-system -l app=higress-controller | grep -i error\n\n# Verify route is loaded\nkubectl exec -n higress-system &lt;gateway-pod&gt; -- \\\n  curl -s localhost:15000/config_dump | jq '.configs[] | select(.[\"@type\"] | contains(\"RouteConfiguration\"))'\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#connection-timeouts","title":"Connection Timeouts","text":"<pre><code># Verify gateway can reach ext-cloud network\nkubectl exec -n higress-system &lt;gateway-pod&gt; -- \\\n  nc -zv 100.65.0.105 6443\n\n# Check SNAT is working\nkubectl exec -n higress-system &lt;gateway-pod&gt; -- \\\n  curl -v --connect-timeout 5 https://100.65.0.105:6443/healthz -k\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#ssl-passthrough-not-working","title":"SSL Passthrough Not Working","text":"<pre><code># Verify TLS passthrough is enabled\nkubectl get ingress -A -o yaml | grep -A5 \"ssl-passthrough\"\n\n# Check listener configuration\nkubectl exec -n higress-system &lt;gateway-pod&gt; -- \\\n  curl -s localhost:15000/listeners | jq '.[] | select(.name | contains(\"443\"))'\n</code></pre>"},{"location":"prd/cloud_network_higress_hostnetwork/#references","title":"References","text":"<ul> <li>Higress GitHub</li> <li>Higress Documentation</li> <li>Sealos Migration Story</li> <li>Gateway API TLSRoute</li> <li>Envoy TLS Passthrough</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/","title":"PRD: Public Ingress Exposure for Tenant Clusters","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#overview","title":"Overview","text":"<p>This document defines the requirements for exposing tenant Kubernetes API servers and LoadBalancer services publicly via SSL passthrough ingress. The pattern enables external access to services running within the cloud network (<code>ext-cloud</code>) through public gateway nodes.</p>"},{"location":"prd/cloud_network_ingress_exposure_req/#problem-statement","title":"Problem Statement","text":"<p>Currently, tenant clusters (KdcCluster) have their API servers exposed on the internal cloud network (<code>ext-cloud</code>, e.g., <code>100.65.0.0/16</code>). To enable external access:</p> <ol> <li>SSL Passthrough is required for K8s API servers (clients verify server certificates)</li> <li>Certificate SANs must include public hostnames for TLS validation</li> <li>Proxy services must be created to route traffic from gateway to internal endpoints</li> <li>DNS records must be configured for public domain resolution</li> </ol>"},{"location":"prd/cloud_network_ingress_exposure_req/#architecture","title":"Architecture","text":"<pre><code>Internet\n    \u2502\n    \u2502 HTTPS (port 443)\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Public Gateway (hostNetwork)                       \u2502\n\u2502               nginx-ingress / Higress / Envoy Gateway           \u2502\n\u2502                                                                 \u2502\n\u2502   SNI: demo-cluster-cp.stage.kube-dc.com                       \u2502\n\u2502         \u2514\u2500\u2500\u25ba Route to ClusterIP proxy service                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u2502 TCP Proxy (via ClusterIP Service + Endpoints)\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               OVN Cloud Network (ext-cloud)                      \u2502\n\u2502                                                                 \u2502\n\u2502   LoadBalancer Service: 100.65.0.105:6443                       \u2502\n\u2502         \u2514\u2500\u2500\u25ba Kamaji TenantControlPlane Pod                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#components-affected","title":"Components Affected","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#1-kube-dc-k8-manager-kdccluster-controller","title":"1. kube-dc-k8-manager (KdcCluster Controller)","text":"<p>File: <code>internal/controller/kdccluster_controller.go</code></p>"},{"location":"prd/cloud_network_ingress_exposure_req/#current-implementation","title":"Current Implementation","text":"<pre><code>func (r *KdcClusterReconciler) reconcileTenantControlPlane(...) error {\n    tcp = &amp;kamajiv1alpha1.TenantControlPlane{\n        Spec: kamajiv1alpha1.TenantControlPlaneSpec{\n            NetworkProfile: kamajiv1alpha1.NetworkProfileSpec{\n                Port: 6443,\n                // Missing: CertSANs for public hostname\n            },\n        },\n    }\n}\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#required-changes","title":"Required Changes","text":"<ol> <li>Add <code>PublicExposure</code> field to KdcClusterSpec:</li> </ol> <pre><code>// In api/v1alpha1/kdccluster_types.go\n\n// PublicExposureSpec defines public ingress exposure configuration\ntype PublicExposureSpec struct {\n    // Enable public exposure via SSL passthrough ingress\n    // +kubebuilder:default=false\n    // +optional\n    Enabled bool `json:\"enabled,omitempty\"`\n\n    // Public domain suffix (e.g., \"stage.kube-dc.com\")\n    // Final hostname: {cluster-name}-cp.{domainSuffix}\n    // +optional\n    DomainSuffix string `json:\"domainSuffix,omitempty\"`\n\n    // Additional certificate SANs to add to the API server certificate\n    // Auto-generated public hostname is always included when enabled\n    // +optional\n    AdditionalCertSANs []string `json:\"additionalCertSANs,omitempty\"`\n}\n\ntype KdcClusterSpec struct {\n    // ... existing fields ...\n\n    // Public exposure configuration for API server\n    // +optional\n    PublicExposure PublicExposureSpec `json:\"publicExposure,omitempty\"`\n}\n</code></pre> <ol> <li>Update <code>reconcileTenantControlPlane</code> to include certSANs:</li> </ol> <pre><code>func (r *KdcClusterReconciler) reconcileTenantControlPlane(ctx context.Context, cluster *k8sv1alpha1.KdcCluster, dataStoreName string) error {\n    // Build certSANs list\n    certSANs := []string{}\n\n    if cluster.Spec.PublicExposure.Enabled {\n        // Generate public hostname\n        publicHostname := fmt.Sprintf(\"%s-cp.%s\", \n            cluster.Name, \n            cluster.Spec.PublicExposure.DomainSuffix)\n        certSANs = append(certSANs, publicHostname)\n\n        // Add any additional SANs\n        certSANs = append(certSANs, cluster.Spec.PublicExposure.AdditionalCertSANs...)\n    }\n\n    tcp = &amp;kamajiv1alpha1.TenantControlPlane{\n        Spec: kamajiv1alpha1.TenantControlPlaneSpec{\n            NetworkProfile: kamajiv1alpha1.NetworkProfileSpec{\n                Port:     6443,\n                CertSANs: certSANs,  // Add this field\n            },\n        },\n    }\n    // ...\n}\n</code></pre> <ol> <li>Create proxy service and ingress for public exposure:</li> </ol> <pre><code>func (r *KdcClusterReconciler) reconcilePublicExposure(ctx context.Context, cluster *k8sv1alpha1.KdcCluster) error {\n    if !cluster.Spec.PublicExposure.Enabled {\n        return nil\n    }\n\n    // Get the LoadBalancer service external IP\n    tcpSvc := &amp;corev1.Service{}\n    tcpName := fmt.Sprintf(\"%s-cp\", cluster.Name)\n    if err := r.Get(ctx, types.NamespacedName{Name: tcpName, Namespace: cluster.Namespace}, tcpSvc); err != nil {\n        return err\n    }\n\n    externalIP := \"\"\n    for _, ingress := range tcpSvc.Status.LoadBalancer.Ingress {\n        if ingress.IP != \"\" {\n            externalIP = ingress.IP\n            break\n        }\n    }\n    if externalIP == \"\" {\n        return fmt.Errorf(\"LoadBalancer IP not yet assigned\")\n    }\n\n    // Create proxy service in ingress namespace\n    proxyServiceName := fmt.Sprintf(\"%s-api-proxy\", cluster.Name)\n    publicHostname := fmt.Sprintf(\"%s-cp.%s\", cluster.Name, cluster.Spec.PublicExposure.DomainSuffix)\n\n    // 1. Create ClusterIP Service\n    proxySvc := &amp;corev1.Service{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      proxyServiceName,\n            Namespace: \"default\",  // Or dedicated ingress namespace\n            Labels: map[string]string{\n                \"kube-dc.com/managed-by\":     \"kdccluster-controller\",\n                \"kube-dc.com/cluster\":        cluster.Name,\n                \"kube-dc.com/cluster-ns\":     cluster.Namespace,\n                \"kube-dc.com/public-exposure\": \"true\",\n            },\n        },\n        Spec: corev1.ServiceSpec{\n            Type:      corev1.ServiceTypeClusterIP,\n            ClusterIP: corev1.ClusterIPNone,  // Headless\n            Ports: []corev1.ServicePort{{\n                Name:     \"https\",\n                Port:     6443,\n                Protocol: corev1.ProtocolTCP,\n            }},\n        },\n    }\n\n    // 2. Create Endpoints pointing to ext-cloud IP\n    endpoints := &amp;corev1.Endpoints{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      proxyServiceName,\n            Namespace: \"default\",\n        },\n        Subsets: []corev1.EndpointSubset{{\n            Addresses: []corev1.EndpointAddress{{\n                IP: externalIP,\n            }},\n            Ports: []corev1.EndpointPort{{\n                Name:     \"https\",\n                Port:     6443,\n                Protocol: corev1.ProtocolTCP,\n            }},\n        }},\n    }\n\n    // 3. Create Ingress with SSL passthrough\n    ingress := &amp;networkingv1.Ingress{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      proxyServiceName,\n            Namespace: \"default\",\n            Annotations: map[string]string{\n                \"nginx.ingress.kubernetes.io/ssl-passthrough\": \"true\",\n                \"nginx.ingress.kubernetes.io/backend-protocol\": \"HTTPS\",\n            },\n        },\n        Spec: networkingv1.IngressSpec{\n            IngressClassName: ptr.To(\"nginx\"),\n            Rules: []networkingv1.IngressRule{{\n                Host: publicHostname,\n                IngressRuleValue: networkingv1.IngressRuleValue{\n                    HTTP: &amp;networkingv1.HTTPIngressRuleValue{\n                        Paths: []networkingv1.HTTPIngressPath{{\n                            Path:     \"/\",\n                            PathType: ptr.To(networkingv1.PathTypePrefix),\n                            Backend: networkingv1.IngressBackend{\n                                Service: &amp;networkingv1.IngressServiceBackend{\n                                    Name: proxyServiceName,\n                                    Port: networkingv1.ServiceBackendPort{\n                                        Number: 6443,\n                                    },\n                                },\n                            },\n                        }},\n                    },\n                },\n            }},\n        },\n    }\n\n    // Apply resources...\n    return nil\n}\n</code></pre> <ol> <li>Update KdcCluster status with public endpoint:</li> </ol> <pre><code>type KdcClusterStatus struct {\n    // ... existing fields ...\n\n    // PublicEndpoint is the publicly accessible API server endpoint\n    // +optional\n    PublicEndpoint string `json:\"publicEndpoint,omitempty\"`\n}\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#2-kube-dc-service-controller-optional-enhancement","title":"2. kube-dc Service Controller (Optional Enhancement)","text":"<p>File: <code>internal/service_lb/external_endpoint.go</code></p> <p>The existing <code>ExternalEndpointManager</code> already creates headless services with endpoints pointing to LoadBalancer external IPs. This can be leveraged for the proxy service pattern.</p>"},{"location":"prd/cloud_network_ingress_exposure_req/#current-capability","title":"Current Capability","text":"<ul> <li>Creates <code>{service-name}-ext</code> headless service</li> <li>Populates endpoints with LoadBalancer external IP</li> <li>Labels: <code>kube-dc.com/managed-by</code>, <code>kube-dc.com/source-service</code></li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#optional-enhancement-public-exposure-annotation","title":"Optional Enhancement: Public Exposure Annotation","text":"<p>Add support for <code>expose-to-public</code> annotation on LoadBalancer services:</p> <pre><code>// In api/kube-dc.com/v1/values.go\nconst (\n    // ... existing constants ...\n\n    // ServiceLbExposeToPublicAnnotation triggers public ingress creation\n    ServiceLbExposeToPublicAnnotation = \"service.nlb.kube-dc.com/expose-to-public\"\n\n    // ServiceLbPublicHostnameAnnotation specifies custom public hostname\n    ServiceLbPublicHostnameAnnotation = \"service.nlb.kube-dc.com/public-hostname\"\n)\n</code></pre> <p>When a LoadBalancer service has <code>expose-to-public: \"true\"</code>:</p> <ol> <li>Controller creates proxy service in gateway namespace</li> <li>Controller creates SSL passthrough ingress</li> <li>Public hostname pattern: <code>{svc-name}-{namespace}.{domain-suffix}</code></li> </ol>"},{"location":"prd/cloud_network_ingress_exposure_req/#usage-examples","title":"Usage Examples","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#example-1-kdccluster-with-public-exposure","title":"Example 1: KdcCluster with Public Exposure","text":"<pre><code>apiVersion: k8s.kube-dc.com/v1alpha1\nkind: KdcCluster\nmetadata:\n  name: demo-cluster\n  namespace: shalb-demo\nspec:\n  version: \"v1.34.1\"\n  controlPlane:\n    replicas: 2\n  publicExposure:\n    enabled: true\n    domainSuffix: \"stage.kube-dc.com\"\n    # Additional SANs (optional)\n    additionalCertSANs:\n    - \"api.demo.example.com\"\n</code></pre> <p>Result: - API server certificate includes: <code>demo-cluster-cp.stage.kube-dc.com</code> - Ingress created: <code>demo-cluster-api-proxy</code> with SSL passthrough - Public endpoint: <code>https://demo-cluster-cp.stage.kube-dc.com:443</code></p>"},{"location":"prd/cloud_network_ingress_exposure_req/#example-2-generic-loadbalancer-service-with-public-exposure","title":"Example 2: Generic LoadBalancer Service with Public Exposure","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-tcp-service\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/expose-to-public: \"true\"\n    service.nlb.kube-dc.com/public-hostname: \"myapp.stage.kube-dc.com\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8443\n    targetPort: 8443\n    protocol: TCP\n</code></pre> <p>Result: - LoadBalancer gets ext-cloud IP (e.g., <code>100.65.0.110</code>) - Proxy service + endpoints created in gateway namespace - Ingress with SSL passthrough to <code>myapp.stage.kube-dc.com</code></p>"},{"location":"prd/cloud_network_ingress_exposure_req/#implementation-checklist","title":"Implementation Checklist","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#phase-1-kdccluster-certsans-required","title":"Phase 1: KdcCluster certSANs (Required)","text":"<ul> <li> Add <code>PublicExposureSpec</code> to <code>KdcClusterSpec</code> in <code>api/v1alpha1/kdccluster_types.go</code></li> <li> Update <code>reconcileTenantControlPlane()</code> to pass <code>certSANs</code> to Kamaji</li> <li> Update <code>reconcileKamajiControlPlane()</code> for CAPI mode</li> <li> Add <code>PublicEndpoint</code> to <code>KdcClusterStatus</code></li> <li> Run <code>make generate manifests</code></li> <li> Add unit tests</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#phase-2-proxy-service-ingress-creation-required","title":"Phase 2: Proxy Service &amp; Ingress Creation (Required)","text":"<ul> <li> Add <code>reconcilePublicExposure()</code> function to KdcCluster controller</li> <li> Create ClusterIP headless service in gateway namespace</li> <li> Create Endpoints with ext-cloud LoadBalancer IP</li> <li> Create Ingress with SSL passthrough annotation</li> <li> Handle cleanup on cluster deletion</li> <li> Add integration tests</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#phase-3-generic-service-exposure-optional","title":"Phase 3: Generic Service Exposure (Optional)","text":"<ul> <li> Add <code>ServiceLbExposeToPublicAnnotation</code> constant</li> <li> Update service controller to watch for annotation</li> <li> Create ingress for annotated services</li> <li> Document annotation usage</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#phase-4-dns-integration-future","title":"Phase 4: DNS Integration (Future)","text":"<ul> <li> Integrate with external-dns or custom DNS controller</li> <li> Auto-create DNS records for public hostnames</li> <li> Support wildcard DNS patterns</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#testing","title":"Testing","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#manual-test-kdccluster-public-exposure","title":"Manual Test: KdcCluster Public Exposure","text":"<pre><code># 1. Create cluster with public exposure\nkubectl apply -f - &lt;&lt;EOF\napiVersion: k8s.kube-dc.com/v1alpha1\nkind: KdcCluster\nmetadata:\n  name: test-public\n  namespace: shalb-demo\nspec:\n  version: \"v1.34.1\"\n  publicExposure:\n    enabled: true\n    domainSuffix: \"stage.kube-dc.com\"\nEOF\n\n# 2. Wait for cluster ready\nkubectl wait kdccluster test-public -n shalb-demo --for=condition=Ready --timeout=300s\n\n# 3. Verify certSANs\nkubectl get tenantcontrolplane test-public-cp -n shalb-demo -o jsonpath='{.spec.networkProfile.certSANs}'\n# Expected: [\"test-public-cp.stage.kube-dc.com\"]\n\n# 4. Verify ingress created\nkubectl get ingress test-public-api-proxy -n default\n\n# 5. Test public access\nkubectl --kubeconfig=&lt;(kubectl get secret test-public-cp-admin-kubeconfig -n shalb-demo -o jsonpath='{.data.admin\\.conf}' | base64 -d | sed 's|server:.*|server: https://test-public-cp.stage.kube-dc.com:443|') get nodes\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#security-considerations","title":"Security Considerations","text":"<ol> <li>Certificate Validation: Ensure public hostname is in certSANs before creating ingress</li> <li>Network Policies: Gateway pods need access to ext-cloud network</li> <li>RBAC: Controller needs permissions to create ingress in gateway namespace</li> <li>Rate Limiting: Consider rate limiting on public endpoints</li> <li>Audit Logging: Log public exposure changes</li> </ol>"},{"location":"prd/cloud_network_ingress_exposure_req/#dependencies","title":"Dependencies","text":"<ul> <li>nginx-ingress-controller with <code>--enable-ssl-passthrough</code> flag</li> <li>DNS configured for <code>*.stage.kube-dc.com</code> pointing to gateway nodes</li> <li>Network connectivity: gateway nodes \u2192 ext-cloud network (via SNAT/routes)</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#part-2-customer-self-service-public-exposure","title":"Part 2: Customer Self-Service Public Exposure","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#overview_1","title":"Overview","text":"<p>This section describes how customers can expose their own services (websites, APIs) running in the cloud network to the public internet using Gateway API resources created in their own namespace.</p>"},{"location":"prd/cloud_network_ingress_exposure_req/#two-exposure-patterns","title":"Two Exposure Patterns","text":"Pattern Use Case Certificate Location Gateway Role TLS Passthrough K8s API, databases, custom TLS apps Backend (customer) L4 proxy (SNI routing) TLS Termination Websites, REST APIs Gateway (Let's Encrypt) L7 proxy (HTTP routing)"},{"location":"prd/cloud_network_ingress_exposure_req/#pattern-1-tls-passthrough-backend-owns-certificate","title":"Pattern 1: TLS Passthrough (Backend Owns Certificate)","text":"<pre><code>Customer App (has own cert) \u2190\u2500\u2500 Gateway (SNI routing) \u2190\u2500\u2500 Internet\n                                    \u2502\n                                    \u2514\u2500\u2500 TLSRoute (customer namespace)\n</code></pre> <ul> <li>Customer's service terminates TLS</li> <li>Gateway only routes based on SNI header</li> <li>No certificate management at gateway level</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#pattern-2-tls-termination-gateway-owns-certificate","title":"Pattern 2: TLS Termination (Gateway Owns Certificate)","text":"<pre><code>Customer App (plain HTTP) \u2190\u2500\u2500 Gateway (TLS termination) \u2190\u2500\u2500 Internet\n                                    \u2502                          \u2502\n                                    \u2514\u2500\u2500 HTTPRoute              \u2514\u2500\u2500 Let's Encrypt cert\n                                        (customer namespace)\n</code></pre> <ul> <li>Gateway terminates TLS using Let's Encrypt certificate</li> <li>Traffic to backend can be HTTP or re-encrypted</li> <li>cert-manager automates certificate issuance</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#architecture-customer-self-service","title":"Architecture: Customer Self-Service","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Customer Project Namespace (shalb-demo)                  \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Service (LoadBalancer on ext-cloud)                                \u2502   \u2502\n\u2502  \u2502  name: my-website                                                   \u2502   \u2502\n\u2502  \u2502  EIP: 100.65.0.120:443                                             \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  HTTPRoute (TLS Termination) OR TLSRoute (Passthrough)              \u2502   \u2502\n\u2502  \u2502  hostname: mywebsite.example.com                                    \u2502   \u2502\n\u2502  \u2502  backendRef: points to proxy service in gateway namespace           \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Certificate (cert-manager)                                         \u2502   \u2502\n\u2502  \u2502  issuerRef: letsencrypt-prod                                        \u2502   \u2502\n\u2502  \u2502  dnsNames: [mywebsite.example.com]                                  \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u2502 Cross-namespace reference via ReferenceGrant\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Gateway Namespace (higress-system)                    \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Gateway (shared)                                                   \u2502   \u2502\n\u2502  \u2502  listeners:                                                         \u2502   \u2502\n\u2502  \u2502    - name: https-terminate (port 443, TLS Terminate)               \u2502   \u2502\n\u2502  \u2502    - name: tls-passthrough (port 8443, TLS Passthrough)            \u2502   \u2502\n\u2502  \u2502  allowedRoutes:                                                     \u2502   \u2502\n\u2502  \u2502    namespaces: {from: All}  # Customer routes attach here          \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  ReferenceGrant (auto-created by controller)                        \u2502   \u2502\n\u2502  \u2502  from: [customer namespace] kind: HTTPRoute/TLSRoute               \u2502   \u2502\n\u2502  \u2502  to: Service (proxy services)                                       \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#gateway-api-resources","title":"Gateway API Resources","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#shared-gateway-configuration","title":"Shared Gateway Configuration","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: public-gateway\n  namespace: higress-system\n  annotations:\n    # cert-manager will create certs for TLS Terminate listeners\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  gatewayClassName: higress  # or envoy-gateway\n  listeners:\n  # TLS Termination listener (Let's Encrypt certs)\n  - name: https-terminate\n    protocol: HTTPS\n    port: 443\n    hostname: \"*.stage.kube-dc.com\"\n    tls:\n      mode: Terminate\n      certificateRefs:\n      - name: wildcard-stage-kube-dc-com\n        kind: Secret\n    allowedRoutes:\n      namespaces:\n        from: All  # Allow routes from any namespace\n      kinds:\n      - kind: HTTPRoute\n\n  # TLS Passthrough listener (customer certs)\n  - name: tls-passthrough\n    protocol: TLS\n    port: 8443\n    hostname: \"*.stage.kube-dc.com\"\n    tls:\n      mode: Passthrough\n    allowedRoutes:\n      namespaces:\n        from: All\n      kinds:\n      - kind: TLSRoute\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#customer-httproute-tls-termination","title":"Customer HTTPRoute (TLS Termination)","text":"<p>Customer creates in their namespace:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: my-website\n  namespace: shalb-demo  # Customer's namespace\nspec:\n  parentRefs:\n  - name: public-gateway\n    namespace: higress-system\n    sectionName: https-terminate\n  hostnames:\n  - \"mywebsite.stage.kube-dc.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: my-website-proxy  # Proxy service created by controller\n      namespace: higress-system\n      port: 443\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#customer-tlsroute-ssl-passthrough","title":"Customer TLSRoute (SSL Passthrough)","text":"<p>For services where customer manages their own certificate:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TLSRoute\nmetadata:\n  name: my-secure-api\n  namespace: shalb-demo\nspec:\n  parentRefs:\n  - name: public-gateway\n    namespace: higress-system\n    sectionName: tls-passthrough\n  hostnames:\n  - \"api.stage.kube-dc.com\"\n  rules:\n  - backendRefs:\n    - name: my-secure-api-proxy\n      namespace: higress-system\n      port: 8443\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#referencegrant-auto-created-by-controller","title":"ReferenceGrant (Auto-created by Controller)","text":"<p>The kube-dc controller creates this to allow cross-namespace references:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: allow-shalb-demo-routes\n  namespace: higress-system  # Target namespace (gateway)\nspec:\n  from:\n  - group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    namespace: shalb-demo  # Source namespace (customer)\n  - group: gateway.networking.k8s.io\n    kind: TLSRoute\n    namespace: shalb-demo\n  to:\n  - group: \"\"\n    kind: Service\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#cert-manager-integration","title":"cert-manager Integration","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#clusterissuer-for-lets-encrypt","title":"ClusterIssuer for Let's Encrypt","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@kube-dc.com\n    privateKeySecretRef:\n      name: letsencrypt-prod-account-key\n    solvers:\n    # HTTP-01 challenge via Gateway\n    - http01:\n        gatewayHTTPRoute:\n          parentRefs:\n          - kind: Gateway\n            name: public-gateway\n            namespace: higress-system\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#customer-certificate-request","title":"Customer Certificate Request","text":"<p>Customer can request specific certificate:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: mywebsite-tls\n  namespace: shalb-demo\nspec:\n  secretName: mywebsite-tls-secret\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n  dnsNames:\n  - mywebsite.example.com  # Customer's own domain\n  - www.mywebsite.example.com\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#controller-implementation","title":"Controller Implementation","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#new-crd-publicexposure","title":"New CRD: PublicExposure","text":"<pre><code>apiVersion: kube-dc.com/v1\nkind: PublicExposure\nmetadata:\n  name: my-website\n  namespace: shalb-demo\nspec:\n  # Reference to the LoadBalancer service in cloud network\n  serviceRef:\n    name: my-website\n    port: 443\n\n  # Public hostname(s)\n  hostnames:\n  - \"mywebsite.stage.kube-dc.com\"\n\n  # Exposure mode\n  mode: HTTPRoute  # or TLSRoute for passthrough\n\n  # TLS configuration\n  tls:\n    # For HTTPRoute: use Let's Encrypt\n    issuerRef:\n      name: letsencrypt-prod\n      kind: ClusterIssuer\n    # OR for TLSRoute: passthrough (no cert needed at gateway)\n    mode: Passthrough\n\n  # Optional: customer's own domain (requires DNS validation)\n  customDomain:\n    hostname: \"www.mywebsite.example.com\"\n    # DNS-01 challenge for custom domains\n    dnsChallenge: true\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#controller-logic","title":"Controller Logic","text":"<pre><code>func (r *PublicExposureReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    pe := &amp;kubedcv1.PublicExposure{}\n    if err := r.Get(ctx, req.NamespacedName, pe); err != nil {\n        return ctrl.Result{}, client.IgnoreNotFound(err)\n    }\n\n    // 1. Get the source LoadBalancer service\n    svc := &amp;corev1.Service{}\n    if err := r.Get(ctx, types.NamespacedName{\n        Name:      pe.Spec.ServiceRef.Name,\n        Namespace: pe.Namespace,\n    }, svc); err != nil {\n        return ctrl.Result{}, err\n    }\n\n    // 2. Get external IP from cloud network\n    externalIP := getLoadBalancerIP(svc)\n    if externalIP == \"\" {\n        return ctrl.Result{RequeueAfter: 10 * time.Second}, nil\n    }\n\n    // 3. Create proxy service in gateway namespace\n    proxyServiceName := fmt.Sprintf(\"%s-%s-proxy\", pe.Namespace, pe.Name)\n    if err := r.reconcileProxyService(ctx, pe, proxyServiceName, externalIP); err != nil {\n        return ctrl.Result{}, err\n    }\n\n    // 4. Create ReferenceGrant allowing cross-namespace reference\n    if err := r.reconcileReferenceGrant(ctx, pe); err != nil {\n        return ctrl.Result{}, err\n    }\n\n    // 5. Create route based on mode\n    switch pe.Spec.Mode {\n    case \"HTTPRoute\":\n        // 5a. Create Certificate if using Let's Encrypt\n        if pe.Spec.TLS.IssuerRef != nil {\n            if err := r.reconcileCertificate(ctx, pe); err != nil {\n                return ctrl.Result{}, err\n            }\n        }\n        // 5b. Create HTTPRoute\n        if err := r.reconcileHTTPRoute(ctx, pe, proxyServiceName); err != nil {\n            return ctrl.Result{}, err\n        }\n    case \"TLSRoute\":\n        // 5c. Create TLSRoute for passthrough\n        if err := r.reconcileTLSRoute(ctx, pe, proxyServiceName); err != nil {\n            return ctrl.Result{}, err\n        }\n    }\n\n    return ctrl.Result{}, nil\n}\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#customer-usage-examples","title":"Customer Usage Examples","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#example-1-simple-website-tls-termination-with-lets-encrypt","title":"Example 1: Simple Website (TLS Termination with Let's Encrypt)","text":"<pre><code># Customer creates LoadBalancer service for their app\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-blog\n  namespace: shalb-demo\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: my-blog\n---\n# Customer requests public exposure\napiVersion: kube-dc.com/v1\nkind: PublicExposure\nmetadata:\n  name: my-blog\n  namespace: shalb-demo\nspec:\n  serviceRef:\n    name: my-blog\n    port: 80\n  hostnames:\n  - \"blog.stage.kube-dc.com\"\n  mode: HTTPRoute\n  tls:\n    issuerRef:\n      name: letsencrypt-prod\n      kind: ClusterIssuer\n</code></pre> <p>Result: - Let's Encrypt certificate auto-issued for <code>blog.stage.kube-dc.com</code> - HTTPRoute created, attached to shared Gateway - Website accessible at <code>https://blog.stage.kube-dc.com</code></p>"},{"location":"prd/cloud_network_ingress_exposure_req/#example-2-secure-api-tls-passthrough-with-own-certificate","title":"Example 2: Secure API (TLS Passthrough with Own Certificate)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: secure-api\n  namespace: shalb-demo\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 443\n    targetPort: 8443\n  selector:\n    app: secure-api\n---\napiVersion: kube-dc.com/v1\nkind: PublicExposure\nmetadata:\n  name: secure-api\n  namespace: shalb-demo\nspec:\n  serviceRef:\n    name: secure-api\n    port: 443\n  hostnames:\n  - \"api.stage.kube-dc.com\"\n  mode: TLSRoute\n  tls:\n    mode: Passthrough  # Customer manages their own cert\n</code></pre> <p>Result: - TLSRoute created for SNI-based routing - Customer's app terminates TLS with its own certificate - Gateway passes through encrypted traffic</p>"},{"location":"prd/cloud_network_ingress_exposure_req/#example-3-custom-domain-customers-own-domain","title":"Example 3: Custom Domain (Customer's Own Domain)","text":"<pre><code>apiVersion: kube-dc.com/v1\nkind: PublicExposure\nmetadata:\n  name: my-saas\n  namespace: shalb-demo\nspec:\n  serviceRef:\n    name: my-saas-app\n    port: 443\n  hostnames:\n  - \"app.stage.kube-dc.com\"  # Platform domain\n  mode: HTTPRoute\n  tls:\n    issuerRef:\n      name: letsencrypt-prod\n      kind: ClusterIssuer\n  # Customer's custom domain\n  customDomain:\n    hostname: \"app.customer-corp.com\"\n    # Customer must add CNAME: app.customer-corp.com \u2192 app.stage.kube-dc.com\n    dnsChallenge: false  # Use HTTP-01 after CNAME is set\n</code></pre>"},{"location":"prd/cloud_network_ingress_exposure_req/#implementation-phases","title":"Implementation Phases","text":""},{"location":"prd/cloud_network_ingress_exposure_req/#phase-1-basic-httproutetlsroute-support","title":"Phase 1: Basic HTTPRoute/TLSRoute Support","text":"<ul> <li> Install Gateway API CRDs</li> <li> Configure shared Gateway with multiple listeners</li> <li> Create PublicExposure CRD</li> <li> Implement controller to create proxy services, routes, ReferenceGrants</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#phase-2-cert-manager-integration","title":"Phase 2: cert-manager Integration","text":"<ul> <li> Install cert-manager</li> <li> Configure ClusterIssuer for Let's Encrypt</li> <li> Auto-create Certificates for HTTPRoute exposures</li> <li> Support wildcard certificates for platform domains</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#phase-3-custom-domain-support","title":"Phase 3: Custom Domain Support","text":"<ul> <li> DNS-01 challenge support for custom domains</li> <li> CNAME validation before certificate issuance</li> <li> Documentation for customers on DNS setup</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#phase-4-advanced-features","title":"Phase 4: Advanced Features","text":"<ul> <li> Rate limiting per PublicExposure</li> <li> WAF integration</li> <li> Metrics and monitoring per exposure</li> <li> Cost allocation/billing integration</li> </ul>"},{"location":"prd/cloud_network_ingress_exposure_req/#security-considerations_1","title":"Security Considerations","text":"<ol> <li>ReferenceGrant Scope: Only create ReferenceGrants for projects with valid subscriptions</li> <li>Rate Limiting: Prevent abuse of Let's Encrypt rate limits</li> <li>Domain Validation: Verify customer owns custom domains before issuance</li> <li>Network Policies: Ensure gateway can only reach authorized backends</li> <li>Audit Logging: Log all PublicExposure create/update/delete events</li> </ol>"},{"location":"prd/cloud_network_ingress_exposure_req/#references","title":"References","text":"<ul> <li>Kamaji TenantControlPlane NetworkProfile</li> <li>nginx-ingress SSL Passthrough</li> <li>Kubernetes Gateway API TLSRoute</li> <li>Gateway API ReferenceGrant (GEP-709)</li> <li>cert-manager Gateway Integration</li> <li>Let's Encrypt HTTP-01 Challenge</li> </ul>"},{"location":"prd/endpoint_for_lb/","title":"PRD: Automatic External Endpoints for LoadBalancer Services","text":"<p>Status: \u2705 Implemented Version: v0.1.34-dev1 Date: 2025-11-19  </p>"},{"location":"prd/endpoint_for_lb/#problem-statement","title":"Problem Statement","text":"<p>When using LoadBalancer services in kube-dc multi-tenant VPC environments, external clients (such as Kamaji controllers, CI/CD systems, or other tenants) need stable DNS-based access to these services. Currently, users must:</p> <ol> <li>Manually discover the LoadBalancer's external IP address</li> <li>Hardcode IPs in configurations, certificates, and kubeconfigs</li> <li>Manually create and maintain Service/Endpoints pairs for external access</li> <li>Track and update these endpoints whenever LoadBalancer IPs change</li> </ol> <p>This creates operational burden and increases the risk of configuration drift, especially in multi-tenant scenarios where LoadBalancers are frequently created, updated, or recreated.</p>"},{"location":"prd/endpoint_for_lb/#real-world-impact","title":"Real-World Impact","text":"<p>Scenario: Kamaji Multi-Tenant Setup - Kamaji controller runs in <code>kamaji-system</code> namespace - Tenant control planes run in tenant VPC namespaces (e.g., <code>shalb-envoy</code>) - etcd cluster exposed via LoadBalancer with external IP <code>168.119.17.55</code> - Kamaji cannot reach <code>etcd.shalb-envoy.svc.cluster.local</code> (internal ClusterIP) due to network isolation - Must use external IP <code>168.119.17.55:2379</code> in DataStore configuration - When LoadBalancer is recreated, IP changes, breaking all references</p> <p>Current Workaround: <pre><code># Manual Service + Endpoints creation\napiVersion: v1\nkind: Service\nmetadata:\n  name: etcd-lb-ext\n  namespace: shalb-envoy\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - port: 2379\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: etcd-lb-ext\n  namespace: shalb-envoy\nsubsets:\n  - addresses:\n      - ip: 168.119.17.55  # Must be manually updated!\n    ports:\n      - port: 2379\n</code></pre></p> <p>Then Kamaji can use: <code>etcd-lb-ext.shalb-envoy.svc.cluster.local:2379</code></p>"},{"location":"prd/endpoint_for_lb/#implemented-solution","title":"Implemented Solution","text":"<p>Automatically create and manage external endpoints for every LoadBalancer service managed by kube-dc. The service controller:</p> <ol> <li>Creates a headless Service + Endpoints pair when a LoadBalancer is created</li> <li>Updates the Endpoints IP when the LoadBalancer's external IP changes</li> <li>Deletes the external endpoint pair when the LoadBalancer is deleted</li> <li>Supports multiple endpoints if a LoadBalancer has multiple external IPs</li> </ol>"},{"location":"prd/endpoint_for_lb/#naming-convention","title":"Naming Convention","text":"<p>For a LoadBalancer service named <code>&lt;service-name&gt;</code>, create: - Service: <code>&lt;service-name&gt;-ext</code> (headless ClusterIP: None) - Endpoints: <code>&lt;service-name&gt;-ext</code> (pointing to LoadBalancer external IP)</p> <p>Examples: - <code>etcd-lb</code> \u2192 <code>etcd-lb-ext.shalb-envoy.svc.cluster.local</code> - <code>cluster-a-cp</code> \u2192 <code>cluster-a-cp-ext.shalb-envoy.svc.cluster.local</code> - <code>api-gateway</code> \u2192 <code>api-gateway-ext.tenant-ns.svc.cluster.local</code></p>"},{"location":"prd/endpoint_for_lb/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 kube-dc Service Controller (internal/controller/core/service_controller.go) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u251c\u2500 Reconcile LoadBalancer Service\n                              \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                              \u2502\n                \u25bc                              \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 EIP Management     \u2502        \u2502 External Endpoint Mgmt \u2502\n    \u2502 (existing)         \u2502        \u2502 (NEW)                  \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502 - Create EIP       \u2502        \u2502 - Create Service-ext   \u2502\n    \u2502 - Bind to LB       \u2502        \u2502 - Create Endpoints     \u2502\n    \u2502 - Update status    \u2502        \u2502 - Update on IP change  \u2502\n    \u2502 - Delete on remove \u2502        \u2502 - Delete on LB delete  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                              \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u25bc\n                    LoadBalancer gets external IP\n                               \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u25bc                             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Service: etcd-lb    \u2502      \u2502 Service: etcd-lb-ext\u2502\n    \u2502 Type: LoadBalancer  \u2502      \u2502 Type: ClusterIP     \u2502\n    \u2502 ExternalIP:         \u2502      \u2502 ClusterIP: None     \u2502\n    \u2502   168.119.17.55     \u2502      \u2502                     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                           \u2502\n                                           \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Endpoints: etcd-lb-ext   \u2502\n                              \u2502 IP: 168.119.17.55        \u2502\n                              \u2502 Port: 2379               \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/endpoint_for_lb/#technical-specification","title":"Technical Specification","text":""},{"location":"prd/endpoint_for_lb/#1-service-resource-structure","title":"1. Service Resource Structure","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: &lt;service-name&gt;-ext\n  namespace: &lt;service-namespace&gt;\n  labels:\n    kube-dc.com/managed-by: service-lb-controller\n    kube-dc.com/source-service: &lt;service-name&gt;\n    kube-dc.com/endpoint-type: external\n  ownerReferences:\n  - apiVersion: v1\n    kind: Service\n    name: &lt;service-name&gt;\n    uid: &lt;service-uid&gt;\n    controller: true\n    blockOwnerDeletion: true\nspec:\n  type: ClusterIP\n  clusterIP: None  # Headless service\n  ports:\n  - name: &lt;port-name&gt;\n    port: &lt;port&gt;\n    protocol: &lt;protocol&gt;\n  # Copy all ports from source LoadBalancer service\n</code></pre>"},{"location":"prd/endpoint_for_lb/#2-endpoints-resource-structure","title":"2. Endpoints Resource Structure","text":"<pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  name: &lt;service-name&gt;-ext\n  namespace: &lt;service-namespace&gt;\n  labels:\n    kube-dc.com/managed-by: service-lb-controller\n    kube-dc.com/source-service: &lt;service-name&gt;\n    kube-dc.com/endpoint-type: external\n  ownerReferences:\n  - apiVersion: v1\n    kind: Service\n    name: &lt;service-name&gt;-ext\n    uid: &lt;service-ext-uid&gt;\n    controller: true\n    blockOwnerDeletion: true\nsubsets:\n  - addresses:\n      - ip: &lt;loadbalancer-external-ip-1&gt;\n      - ip: &lt;loadbalancer-external-ip-2&gt;  # If multiple IPs\n    ports:\n      - name: &lt;port-name&gt;\n        port: &lt;port&gt;\n        protocol: &lt;protocol&gt;\n</code></pre>"},{"location":"prd/endpoint_for_lb/#3-controller-logic-implemented","title":"3. Controller Logic (Implemented)","text":""},{"location":"prd/endpoint_for_lb/#file-internalservice_lbexternal_endpointgo","title":"File: <code>internal/service_lb/external_endpoint.go</code>","text":"<pre><code>package servicelb\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    corev1 \"k8s.io/api/core/v1\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n    \"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\"\n)\n\nconst (\n    ExternalEndpointSuffix = \"-ext\"\n    ManagedByLabel         = \"kube-dc.com/managed-by\"\n    SourceServiceLabel     = \"kube-dc.com/source-service\"\n    EndpointTypeLabel      = \"kube-dc.com/endpoint-type\"\n    ControllerName         = \"service-lb-controller\"\n)\n\n// ExternalEndpointManager manages external endpoint resources for LoadBalancer services\ntype ExternalEndpointManager struct {\n    client.Client\n    Service *corev1.Service\n}\n\n// Sync creates or updates external Service and Endpoints\nfunc (m *ExternalEndpointManager) Sync(ctx context.Context) error {\n    if m.Service.Spec.Type != corev1.ServiceTypeLoadBalancer {\n        return nil // Only for LoadBalancer services\n    }\n\n    externalIPs := m.getExternalIPs()\n    if len(externalIPs) == 0 {\n        // LoadBalancer not ready yet, skip\n        return nil\n    }\n\n    // Create or update external Service\n    if err := m.syncExternalService(ctx); err != nil {\n        return fmt.Errorf(\"failed to sync external service: %w\", err)\n    }\n\n    // Create or update Endpoints\n    if err := m.syncEndpoints(ctx, externalIPs); err != nil {\n        return fmt.Errorf(\"failed to sync endpoints: %w\", err)\n    }\n\n    return nil\n}\n\n// Delete removes external Service and Endpoints\nfunc (m *ExternalEndpointManager) Delete(ctx context.Context) error {\n    extSvcName := m.getExternalServiceName()\n\n    // Delete external Service (Endpoints will be cascaded via ownerReference)\n    extSvc := &amp;corev1.Service{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        },\n    }\n\n    if err := m.Client.Delete(ctx, extSvc); client.IgnoreNotFound(err) != nil {\n        return fmt.Errorf(\"failed to delete external service: %w\", err)\n    }\n\n    return nil\n}\n\nfunc (m *ExternalEndpointManager) getExternalServiceName() string {\n    return m.Service.Name + ExternalEndpointSuffix\n}\n\nfunc (m *ExternalEndpointManager) getExternalIPs() []string {\n    ips := []string{}\n    for _, ingress := range m.Service.Status.LoadBalancer.Ingress {\n        if ingress.IP != \"\" {\n            ips = append(ips, ingress.IP)\n        }\n    }\n    return ips\n}\n\nfunc (m *ExternalEndpointManager) syncExternalService(ctx context.Context) error {\n    extSvcName := m.getExternalServiceName()\n    extSvc := &amp;corev1.Service{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        },\n    }\n\n    _, err := controllerutil.CreateOrUpdate(ctx, m.Client, extSvc, func() error {\n        // Set labels\n        if extSvc.Labels == nil {\n            extSvc.Labels = make(map[string]string)\n        }\n        extSvc.Labels[ManagedByLabel] = ControllerName\n        extSvc.Labels[SourceServiceLabel] = m.Service.Name\n        extSvc.Labels[EndpointTypeLabel] = \"external\"\n\n        // Set controller reference (ensures garbage collection)\n        if err := controllerutil.SetControllerReference(m.Service, extSvc, m.Scheme()); err != nil {\n            return err\n        }\n\n        // Configure as headless service\n        extSvc.Spec.Type = corev1.ServiceTypeClusterIP\n        extSvc.Spec.ClusterIP = corev1.ClusterIPNone\n\n        // Copy ports from source service\n        extSvc.Spec.Ports = []corev1.ServicePort{}\n        for _, port := range m.Service.Spec.Ports {\n            extSvc.Spec.Ports = append(extSvc.Spec.Ports, corev1.ServicePort{\n                Name:     port.Name,\n                Port:     port.Port,\n                Protocol: port.Protocol,\n            })\n        }\n\n        return nil\n    })\n\n    return err\n}\n\nfunc (m *ExternalEndpointManager) syncEndpoints(ctx context.Context, externalIPs []string) error {\n    extSvcName := m.getExternalServiceName()\n    endpoints := &amp;corev1.Endpoints{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        },\n    }\n\n    _, err := controllerutil.CreateOrUpdate(ctx, m.Client, endpoints, func() error {\n        // Set labels\n        if endpoints.Labels == nil {\n            endpoints.Labels = make(map[string]string)\n        }\n        endpoints.Labels[ManagedByLabel] = ControllerName\n        endpoints.Labels[SourceServiceLabel] = m.Service.Name\n        endpoints.Labels[EndpointTypeLabel] = \"external\"\n\n        // Get external service for owner reference\n        extSvc := &amp;corev1.Service{}\n        if err := m.Client.Get(ctx, client.ObjectKey{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        }, extSvc); err != nil {\n            return fmt.Errorf(\"failed to get external service: %w\", err)\n        }\n\n        // Set controller reference to external service\n        if err := controllerutil.SetControllerReference(extSvc, endpoints, m.Scheme()); err != nil {\n            return err\n        }\n\n        // Build addresses\n        addresses := []corev1.EndpointAddress{}\n        for _, ip := range externalIPs {\n            addresses = append(addresses, corev1.EndpointAddress{\n                IP: ip,\n            })\n        }\n\n        // Build ports\n        ports := []corev1.EndpointPort{}\n        for _, port := range m.Service.Spec.Ports {\n            ports = append(ports, corev1.EndpointPort{\n                Name:     port.Name,\n                Port:     port.Port,\n                Protocol: port.Protocol,\n            })\n        }\n\n        // Set subsets\n        endpoints.Subsets = []corev1.EndpointSubset{\n            {\n                Addresses: addresses,\n                Ports:     ports,\n            },\n        }\n\n        return nil\n    })\n\n    return err\n}\n</code></pre>"},{"location":"prd/endpoint_for_lb/#integration-in-service_controllergo","title":"Integration in <code>service_controller.go</code>","text":"<pre><code>// In reconcileSync function, after EIP and LoadBalancer sync:\n\nfunc (r *ServiceReconciler) reconcileSync(ctx context.Context, req ctrl.Request, svc *corev1.Service, endpoints *corev1.Endpoints, project *kubedccomv1.Project) (ctrl.Result, error) {\n    log := log.FromContext(ctx).WithName(\"Sync:\").WithValues(\"ServiceLoadBalancer\", req.Name)\n\n    // ... existing EIP sync ...\n\n    // ... existing LoadBalancer sync ...\n\n    // ... existing external IP status update ...\n\n    // NEW: Sync external endpoints for cross-VPC access\n    extEndpointMgr := &amp;serviceLb.ExternalEndpointManager{\n        Client:  r.Client,\n        Service: svc,\n    }\n    if err := extEndpointMgr.Sync(ctx); err != nil {\n        log.Error(err, \"Failed to sync external endpoints\")\n        // Don't fail the reconciliation, just log the error\n    }\n\n    return ctrl.Result{}, nil\n}\n\n// In reconcileDelete function:\n\nfunc (r *ServiceReconciler) reconcileDelete(ctx context.Context, req ctrl.Request, svc *corev1.Service, endpoints *corev1.Endpoints, project *kubedccomv1.Project) (ctrl.Result, error) {\n    log := log.FromContext(ctx).WithName(\"Delete:\").WithValues(\"ServiceLoadBalancer\", req.Name)\n\n    // NEW: Delete external endpoints first\n    extEndpointMgr := &amp;serviceLb.ExternalEndpointManager{\n        Client:  r.Client,\n        Service: svc,\n    }\n    if err := extEndpointMgr.Delete(ctx); err != nil {\n        log.Error(err, \"Failed to delete external endpoints\")\n    }\n\n    // ... existing EIP and LoadBalancer delete logic ...\n\n    return ctrl.Result{}, nil\n}\n</code></pre>"},{"location":"prd/endpoint_for_lb/#4-rbac-permissions","title":"4. RBAC Permissions","text":"<p>Add to <code>config/rbac/role.yaml</code>:</p> <pre><code>- apiGroups: [\"\"]\n  resources: [\"services\", \"endpoints\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"prd/endpoint_for_lb/#benefits","title":"Benefits","text":""},{"location":"prd/endpoint_for_lb/#1-operational-simplicity","title":"1. Operational Simplicity","text":"<ul> <li>Zero manual intervention: Endpoints created/updated automatically</li> <li>Self-healing: Endpoints always reflect current LoadBalancer IPs</li> <li>Consistent naming: Predictable <code>-ext</code> suffix convention</li> </ul>"},{"location":"prd/endpoint_for_lb/#2-multi-tenant-support","title":"2. Multi-Tenant Support","text":"<ul> <li>Cross-VPC access: External endpoints work across network boundaries</li> <li>Stable DNS: Use <code>&lt;service&gt;-ext.namespace.svc.cluster.local</code> in all configs</li> <li>No IP hardcoding: Certificates, kubeconfigs, and DataStores use DNS names</li> </ul>"},{"location":"prd/endpoint_for_lb/#3-resilience","title":"3. Resilience","text":"<ul> <li>Owner references: Automatic cleanup when services are deleted</li> <li>Reconciliation: Controller ensures consistency even after disruptions</li> <li>Multiple IPs: Supports LoadBalancers with multiple external IPs</li> </ul>"},{"location":"prd/endpoint_for_lb/#use-cases","title":"Use Cases","text":""},{"location":"prd/endpoint_for_lb/#1-kamaji-multi-tenant-control-planes-primary-use-case","title":"1. Kamaji Multi-Tenant Control Planes \u2b50 PRIMARY USE CASE","text":"<p>Problem: Kamaji controller in <code>kamaji-system</code> cannot reach etcd in tenant VPC via ClusterIP.</p> <p>Current (manual): <pre><code>apiVersion: kamaji.clastix.io/v1alpha1\nkind: DataStore\nmetadata:\n  name: shalb-envoy-etcd\nspec:\n  driver: etcd\n  endpoints:\n  - 168.119.17.55:2379  # \u274c Hardcoded IP - breaks when service recreated\n</code></pre></p> <p>With auto-managed endpoints: <pre><code>apiVersion: kamaji.clastix.io/v1alpha1\nkind: DataStore\nmetadata:\n  name: shalb-envoy-etcd\nspec:\n  driver: etcd\n  endpoints:\n  - etcd-lb-ext.shalb-envoy.svc.cluster.local:2379  # \u2705 Stable DNS name\n  tlsConfig:\n    # ... certificates with DNS SANs (not IP SANs)\n</code></pre></p> <p>How it works: 1. LoadBalancer <code>etcd-lb</code> gets external IP <code>168.119.17.55</code> 2. Controller auto-creates Service <code>etcd-lb-ext</code> (headless) 3. Controller auto-creates Endpoints <code>etcd-lb-ext</code> pointing to <code>168.119.17.55</code> 4. Kamaji resolves <code>etcd-lb-ext.shalb-envoy.svc.cluster.local</code> \u2192 <code>168.119.17.55</code> 5. When IP changes, controller updates Endpoints automatically 6. No DataStore configuration change needed!</p>"},{"location":"prd/endpoint_for_lb/#2-cross-tenant-api-access","title":"2. Cross-Tenant API Access","text":"<pre><code># Tenant A accessing Tenant B's API\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tenant-b-access\n  namespace: tenant-a\ndata:\n  api-endpoint: https://api-gateway-ext.tenant-b.svc.cluster.local:8443\n</code></pre>"},{"location":"prd/endpoint_for_lb/#3-cicd-integration","title":"3. CI/CD Integration","text":"<pre><code># CI pipeline can use stable DNS names\nkubectl --kubeconfig=/tmp/kubeconfig \\\n  --server=https://cluster-ext.tenant-prod.svc.cluster.local:6443 \\\n  get nodes\n</code></pre>"},{"location":"prd/endpoint_for_lb/#testing-strategy","title":"Testing Strategy","text":""},{"location":"prd/endpoint_for_lb/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test Service creation with correct naming and labels</li> <li>Test Endpoints creation with correct IPs and ports</li> <li>Test update when LoadBalancer IP changes</li> <li>Test deletion and cleanup</li> <li>Test multiple external IPs</li> </ul>"},{"location":"prd/endpoint_for_lb/#integration-tests","title":"Integration Tests","text":"<ol> <li>Create LoadBalancer service</li> <li>Verify <code>-ext</code> Service and Endpoints are created</li> <li>Verify DNS resolves to external IP</li> <li>Update LoadBalancer (trigger IP change)</li> <li>Verify Endpoints updated with new IP</li> <li>Delete LoadBalancer</li> <li>Verify <code>-ext</code> resources cleaned up</li> </ol>"},{"location":"prd/endpoint_for_lb/#e2e-tests","title":"E2E Tests","text":"<ul> <li>Deploy Kamaji with multi-tenant setup</li> <li>Verify etcd DataStore works with <code>-ext</code> endpoint</li> <li>Verify TenantControlPlane can access etcd</li> <li>Simulate IP change and verify automatic reconciliation</li> </ul>"},{"location":"prd/endpoint_for_lb/#implementation-status","title":"Implementation Status","text":""},{"location":"prd/endpoint_for_lb/#phase-1-core-implementation-complete","title":"Phase 1: Core Implementation \u2705 Complete","text":"<ul> <li> Create <code>external_endpoint.go</code> with manager logic</li> <li> Integrate into <code>service_controller.go</code></li> <li> Add RBAC permissions for endpoints</li> <li> Documentation in <code>docs/tutorial-ip-and-lb.md</code></li> <li> Documentation in <code>docs/architecture-networking.md</code></li> </ul>"},{"location":"prd/endpoint_for_lb/#phase-2-deployment-testing-complete","title":"Phase 2: Deployment &amp; Testing \u2705 Complete","text":"<ul> <li> Deploy to staging environment (v0.1.34-dev1)</li> <li> Verified 10+ LoadBalancer services automatically got external endpoints</li> <li> DNS resolution tested and working</li> </ul>"},{"location":"prd/endpoint_for_lb/#test-results-2025-11-19","title":"Test Results (2025-11-19)","text":"<pre><code># All LoadBalancer services automatically got -ext endpoints:\n$ kubectl get endpoints -A --selector=kube-dc.com/managed-by=service-lb-controller\nNAMESPACE     NAME                                     ENDPOINTS\nshalb-dev     etcd-lb-ext                              168.119.17.51:2379\nshalb-dev     kamaji-demo-cp-ext                       168.119.17.59:6443\nshalb-dev     debug-net-lb-ext                         168.119.17.51:80,168.119.17.51:443\nshalb-envoy   cluster-a-cp-ext                         168.119.17.53:6443\nshalb-envoy   etcd-lb-ext                              168.119.17.55:2379,168.119.17.55:6443\n...\n\n# DNS resolution verified:\n$ kubectl run -it --rm debug --image=busybox -- nslookup etcd-lb-ext.shalb-dev.svc.cluster.local\nName:   etcd-lb-ext.shalb-dev.svc.cluster.local\nAddress: 168.119.17.51\n</code></pre>"},{"location":"prd/endpoint_for_lb/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>This feature is fully backwards compatible: - Existing LoadBalancer services continue to work unchanged - External endpoints are additive (new resources only) - No breaking changes to existing APIs or configurations - Users can opt-out by deleting the <code>-ext</code> resources (controller will recreate, but won't affect original service)</p>"},{"location":"prd/endpoint_for_lb/#success-metrics","title":"Success Metrics","text":"<ul> <li>Automation rate: 100% of LoadBalancer services have external endpoints</li> <li>Manual interventions: Reduce IP update operations to zero</li> <li>Reconciliation time: External endpoints updated within 10 seconds of IP change</li> <li>Error rate: &lt; 0.1% endpoint sync failures</li> </ul>"},{"location":"prd/endpoint_for_lb/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Configurable naming: Annotation to customize <code>-ext</code> suffix</li> <li>Selective enablement: Annotation to opt-in/opt-out per service</li> <li>External DNS integration: Automatically create DNS records</li> <li>Metrics: Prometheus metrics for endpoint sync operations</li> <li>Webhook validation: Prevent manual modification of managed resources</li> <li>Kamaji DataStore CRD enhancement: Add <code>externalNetworkType</code> field to Kamaji DataStore CRD to allow users to specify which external network type (<code>cloud</code> or <code>public</code>) to use when connecting via external endpoints. This ensures proper network routing and IP allocation matching the infrastructure requirements</li> </ol>"},{"location":"prd/endpoint_for_lb/#related-files","title":"Related Files","text":"<ul> <li><code>internal/service_lb/external_endpoint.go</code> - External endpoint manager implementation</li> <li><code>internal/controller/core/service_controller.go</code> - Service controller integration</li> <li><code>docs/tutorial-ip-and-lb.md</code> - User documentation</li> <li><code>docs/architecture-networking.md</code> - Architecture documentation</li> </ul>"},{"location":"prd/endpoint_for_lb/#related-documentation","title":"Related Documentation","text":"<ul> <li>Kamaji Multi-Tenant Architecture: <code>/examples/kamaji-capi/mt/README.md</code></li> <li>Service LoadBalancer Architecture: <code>/docs/prd/svc_lb_architecture.md</code></li> <li>EIP Management: <code>/internal/eip/</code></li> </ul>"},{"location":"prd/service_lb_sync_issue/","title":"Service LoadBalancer OVN Sync Issue","text":""},{"location":"prd/service_lb_sync_issue/#summary","title":"Summary","text":"<p>When kube-ovn-controller restarts (due to OVN database timeouts or other failures), the LoadBalancer VIP entries managed by <code>kube-dc-manager</code> are lost from the OVN Northbound database. This causes external services (like tenant cluster API servers) to become unreachable until <code>kube-dc-manager</code> is manually restarted.</p>"},{"location":"prd/service_lb_sync_issue/#issue-details","title":"Issue Details","text":""},{"location":"prd/service_lb_sync_issue/#observed-behavior","title":"Observed Behavior","text":"<p>Date: 2025-12-01</p> <p>Symptoms: - Tenant cluster control planes unreachable: <code>dial tcp 168.119.17.55:6443: connect: no route to host</code> - All EIP resources show <code>READY: true</code> - OVN EIP resources show <code>READY: true</code> - VMs are running correctly</p> <p>Timeline: 1. <code>kube-ovn-controller</code> experienced OVN database connection timeouts:    <pre><code>E1201 10:34:42.771547 controller.go:1021] OVN database echo timeout (4/5) after 60s\nE1201 10:35:57.772950 controller.go:1021] OVN database echo timeout (5/5) after 60s\nE1201 10:35:57.773181 klog.go:10] \"OVN database connection timeout after 5 attempts\"\n</code></pre> 2. <code>kube-ovn-controller</code> pod restarted at 10:35:58 3. After restart, OVN NB database was missing LoadBalancer entries for:    - <code>shalb-envoy-user-a-cp-tcp</code> (168.119.17.55:6443)    - <code>shalb-envoy-user-b-cp-tcp</code> (168.119.17.53:6443)    - <code>shalb-envoy-user-c-cp-tcp</code> (168.119.17.56:6443)    - <code>shalb-envoy-envoy-delta-controller-envoy-tcp</code> (168.119.17.54:443/10000/9901)</p>"},{"location":"prd/service_lb_sync_issue/#root-cause-analysis","title":"Root Cause Analysis","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     writes to      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  kube-dc-manager    \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 \u2502   OVN NB Database   \u2502\n\u2502  (service_lb.go)    \u2502                    \u2502   (LoadBalancers)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    \u2191\n                                                    \u2502 reconciles\n                                                    \u2502\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502 kube-ovn-controller \u2502\n                                           \u2502                     \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The Problem:</p> <ol> <li><code>kube-dc-manager</code> writes LoadBalancer VIPs directly to OVN NB database via <code>ovs.NbClient</code></li> <li><code>kube-ovn-controller</code> manages its own OVN resources but does NOT manage <code>kube-dc-manager</code>'s LBs</li> <li>When <code>kube-ovn-controller</code> restarts, it reconciles resources it knows about</li> <li><code>kube-dc-manager</code>'s LBs are NOT reconciled because:</li> <li>They are not tracked by any Kubernetes resource that <code>kube-ovn-controller</code> watches</li> <li><code>kube-dc-manager</code> only reconciles on Endpoints changes, not on OVN state changes</li> </ol>"},{"location":"prd/service_lb_sync_issue/#affected-code","title":"Affected Code","text":"<p><code>internal/service_lb/service_lb.go</code>:</p> <pre><code>func (r *LBResource) Sync(ctx context.Context) error {\n    // Builds VIP maps\n    vipListTcp := map[string]string{}\n    // ...\n\n    // Updates OVN LB directly\n    err = r.updateLbs(r.tcpLbName(), ovnnb.LoadBalancerProtocolTCP, vipListTcp)\n\n    // Attaches to router and switch\n    err = r.ovsCli.LogicalRouterUpdateLoadBalancers(r.projectRouter.Name, ...)\n    err = r.ovsCli.LogicalSwitchUpdateLoadBalancers(project.SubnetName(r.project), ...)\n}\n</code></pre> <p>The sync is triggered by: - Endpoint creation/update - Service creation/update</p> <p>NOT triggered by: - OVN database reconnection - kube-ovn-controller restart - OVN NB database state changes</p>"},{"location":"prd/service_lb_sync_issue/#solutions-considered","title":"Solutions Considered","text":"Option Description Pros Cons Status Periodic Reconciliation Full resync every N minutes Simple Downtime up to N min, wasteful \u274c Rejected OVN Connection Monitor Track connection state Immediate Requires OVS client mods \u274c Rejected K8s Annotation State Store LB state in annotations Declarative Complex implementation \u274c Rejected OVSDB Event Watch Subscribe to LB deletions Real-time Complex, stability concerns \u274c Rejected LB Watcher + Restart Detection Verify LBs periodically + detect restarts Minimal impact, targeted Slight delay \u2705 Implemented"},{"location":"prd/service_lb_sync_issue/#implemented-solution","title":"Implemented Solution","text":"<p>LB Watcher with Periodic Verification + kube-ovn-controller Restart Detection</p>"},{"location":"prd/service_lb_sync_issue/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            LBWatcher                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Periodic Verification      \u2502    \u2502  Restart Detection          \u2502    \u2502\n\u2502  \u2502  (every 2 min)              \u2502    \u2502  (poll every 30s)           \u2502    \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n\u2502  \u2502 1. List LoadBalancer svcs   \u2502    \u2502 1. Check pod restart count  \u2502    \u2502\n\u2502  \u2502 2. Check OVN LB exists      \u2502    \u2502 2. If changed: wait 30s     \u2502    \u2502\n\u2502  \u2502 3. If missing: trigger      \u2502    \u2502 3. Verify all LBs           \u2502    \u2502\n\u2502  \u2502    reconciliation           \u2502    \u2502                             \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                 \u2502                                \u2502                      \u2502\n\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502                                  \u25bc                                      \u2502\n\u2502                    Update annotation on Service                         \u2502\n\u2502                    kube-dc.com/lb-resync-timestamp                      \u2502\n\u2502                                  \u2502                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Service Controller         \u2502\n                    \u2502  (existing reconcile loop)  \u2502\n                    \u2502  - Detects annotation change\u2502\n                    \u2502  - Recreates OVN LB         \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/service_lb_sync_issue/#files-modifiedcreated","title":"Files Modified/Created","text":"<ol> <li><code>internal/service_lb/lb_watcher.go</code> (New)</li> <li><code>LBWatcher</code> struct implementing <code>manager.Runnable</code></li> <li>Periodic verification every 2 minutes</li> <li>kube-ovn-controller pod restart detection (every 30s)</li> <li> <p>Triggers reconciliation via annotation update</p> </li> <li> <p><code>internal/controller/core/service_controller.go</code> (Modified)</p> </li> <li>Added <code>lbWatcher</code> field to <code>ServiceReconciler</code></li> <li>Added RBAC for pods: <code>+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch</code></li> <li>Added predicate for <code>kube-dc.com/lb-resync-timestamp</code> annotation</li> <li>Integrated <code>LBWatcher</code> startup via <code>mgr.Add()</code></li> </ol>"},{"location":"prd/service_lb_sync_issue/#performance-impact","title":"Performance Impact","text":"Metric Value Notes Periodic check interval 2 min Configurable via <code>LBVerificationInterval</code> Pod restart poll 30s Single API call OVN LB check ~10ms per svc Only for provisioned LB services Reconciliation trigger On-demand Only when LB actually missing"},{"location":"prd/service_lb_sync_issue/#test-results-2025-12-02","title":"Test Results (2025-12-02)","text":"<p>Test: Manual LB Deletion Recovery</p> <pre><code># Deleted OVN LB manually\n$ ovn-nbctl lb-del shalb-dev-etcd-lb-tcp\n\n# LB Watcher detected and recovered (within 2 min):\n12:46:14 [WARN] LB watcher: OVN LB missing for service shalb-dev/etcd-lb, triggering reconciliation\n12:46:14 [INFO] Update service-lb resync annotation changed, run reconciliation\n12:46:15 [INFO] Kind: ServiceLb, Message: Create Lb shalb-dev-etcd-lb-tcp\n12:46:15 [INFO] Kind: ServiceLb, Message: Add Vip Lb shalb-dev-etcd-lb-tcp, Vip 168.119.17.51:2379\n12:46:15 [INFO] LB watcher: triggered reconciliation for 1 services with missing LBs\n</code></pre> <p>Result: \u2705 LB automatically recreated within 2 minutes</p>"},{"location":"prd/service_lb_sync_issue/#verification-steps","title":"Verification Steps","text":"<p>After implementing fix, verify with:</p> <pre><code># 1. Check OVN LBs exist\nkubectl exec -n kube-system deployment/ovn-central -- \\\n  ovn-nbctl --no-leader-only lb-list | grep \"168.119\"\n\n# 2. Simulate failure - restart kube-ovn-controller\nkubectl rollout restart deployment/kube-ovn-controller -n kube-system\n\n# 3. Wait 2 minutes and verify LBs still exist\nsleep 120\nkubectl exec -n kube-system deployment/ovn-central -- \\\n  ovn-nbctl --no-leader-only lb-list | grep \"168.119\"\n\n# 4. Test connectivity\ncurl -k https://168.119.17.55:6443/version\n</code></pre>"},{"location":"prd/service_lb_sync_issue/#related-files","title":"Related Files","text":"<ul> <li><code>internal/service_lb/lb_watcher.go</code> - NEW LB Watcher implementation</li> <li><code>internal/service_lb/service_lb.go</code> - ServiceLB OVN sync logic</li> <li><code>internal/controller/core/service_controller.go</code> - Service controller (modified)</li> </ul>"},{"location":"prd/service_lb_sync_issue/#version","title":"Version","text":"<ul> <li>Implemented in: v0.1.34-dev4</li> <li>Date: 2025-12-02</li> </ul>"},{"location":"prd/svc_lb_architecture/","title":"Service LoadBalancer Architecture in Kube-DC","text":"<p>Status: \u2705 Documented &amp; Misconceptions Resolved Last Updated: 2025-12-02  </p>"},{"location":"prd/svc_lb_architecture/#summary","title":"Summary","text":"<p>Service LoadBalancers in kube-dc work without requiring VPC policy routes or NAT rules. The load balancer VIPs are directly accessible through the OVN router's external interface.</p>"},{"location":"prd/svc_lb_architecture/#how-it-works","title":"How It Works","text":""},{"location":"prd/svc_lb_architecture/#1-network-topology","title":"1. Network Topology","text":"<pre><code>External Network (168.119.17.48/28)\n         \u2193\nOVN Router Port: shalb-dev-ext-public\n  MAC: ae:d6:b9:e1:76:78\n  Network: 168.119.17.51/28\n         \u2193\nOVN Load Balancer VIPs (168.119.17.51-.63)\n         \u2193\nBackend Pods/VMs in Project VPC\n</code></pre>"},{"location":"prd/svc_lb_architecture/#2-eip-allocation-for-service-loadbalancers","title":"2. EIP Allocation for Service LoadBalancers","text":"<p>When a Service of type <code>LoadBalancer</code> is created:</p> <ol> <li>kube-dc Service Controller (<code>internal/controller/core/service_controller.go</code>):</li> <li>Detects the LoadBalancer service</li> <li> <p>Creates an <code>EIp</code> resource</p> </li> <li> <p>kube-dc EIP Controller (<code>internal/eip/ovn_eip_res.go</code>):</p> </li> <li>Creates an <code>OvnEip</code> resource</li> <li>Allocates an IP from the external subnet (e.g., <code>ext-public</code>)</li> <li> <p>Type: <code>lrp</code> (Logical Router Port)</p> </li> <li> <p>Kube-OVN (upstream controller):</p> </li> <li>Does NOT create NAT rules for <code>lrp</code> type EIPs when used with load balancers</li> <li>The IP becomes part of the router's external interface subnet</li> </ol>"},{"location":"prd/svc_lb_architecture/#3-load-balancer-vip-configuration","title":"3. Load Balancer VIP Configuration","text":"<p>kube-dc Service LB Controller (<code>internal/service_lb/service_lb.go</code>):</p> <pre><code>// Creates OVN load balancer\nvipKey := fmt.Sprintf(\"%s:%d\", r.ipAddress, port.Port)  // e.g., \"168.119.17.55:80\"\nbackends := \"10.1.0.40:31416,10.1.0.41:31416\"  // Pod/VM IPs:ports\n\n// Attaches to BOTH router and logical switch\novsCli.LogicalRouterUpdateLoadBalancers(r.projectRouter.Name, ...)  // shalb-dev\novsCli.LogicalSwitchUpdateLoadBalancers(project.SubnetName(r.project), ...)  // shalb-dev-default\n</code></pre>"},{"location":"prd/svc_lb_architecture/#4-traffic-flow","title":"4. Traffic Flow","text":""},{"location":"prd/svc_lb_architecture/#external-client-service","title":"External Client \u2192 Service","text":"<pre><code>1. Client sends packet to 168.119.17.55:80\n2. ARP resolution: Who has 168.119.17.55?\n3. OVN router responds: ae:d6:b9:e1:76:78 (my MAC)\n4. Packet arrives at router's external port\n5. OVN load balancer intercepts (VIP match)\n6. Packet DNAT'd to backend: 10.1.0.40:31416\n7. Response SNAT'd back through VIP\n8. Client receives response from 168.119.17.55:80\n</code></pre>"},{"location":"prd/svc_lb_architecture/#internal-pod-service-clusterip","title":"Internal Pod \u2192 Service (ClusterIP)","text":"<p>Normal kube-proxy/Cilium routing within the cluster.</p>"},{"location":"prd/svc_lb_architecture/#key-differences-from-fip","title":"Key Differences from FIP","text":"Feature Service LoadBalancer (lrp EIP) Floating IP (FIP) OvnEip Type <code>lrp</code> <code>lrp</code> NAT Rules None (uses LB VIP) <code>dnat_and_snat</code> Policy Routes Not needed Required for outbound Annotation <code>ovn.kubernetes.io/vpc_nat</code> NOT needed <code>ovn.kubernetes.io/vpc_nat: {vpc}-{fip-name}</code> required Use Case Inbound load balancing Bidirectional NAT to VM/Pod Status.nat <code>\"\"</code> (empty) <code>fip</code>"},{"location":"prd/svc_lb_architecture/#verification-commands","title":"Verification Commands","text":""},{"location":"prd/svc_lb_architecture/#check-service-loadbalancer","title":"Check Service LoadBalancer","text":"<pre><code># 1. Verify Service has external IP\nkubectl get svc -n {namespace} {service-name}\n\n# 2. Check EIP resource\nkubectl get eip -n {namespace} | grep slb-\n\n# 3. Verify OvnEip (no vpc_nat needed)\nEIP_NAME=$(kubectl get eip -n {namespace} {eip-name} -o jsonpath='{.status.ovnEIpRef}')\nkubectl get ovn-eip $EIP_NAME -o yaml\n\n# 4. Check OVN load balancer\nkubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lb-list | grep {namespace}\n\n# 5. Verify router attachment\nkubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lr-lb-list {namespace}\n\n# 6. Check ARP resolution (from gateway/bastion)\narp -n | grep {external-ip}\n# Should show router MAC: ae:d6:b9:e1:76:78 (for shalb-dev)\n\n# 7. Test connectivity\ncurl http://{external-ip}:{port}\n</code></pre>"},{"location":"prd/svc_lb_architecture/#check-vpc-router-configuration","title":"Check VPC Router Configuration","text":"<pre><code># Show router ports and subnet\nkubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl show {vpc-name}\n\n# Example output:\nrouter 9cae37d3-65ae-46e9-ad95-a0ebf58108d9 (shalb-dev)\n    port shalb-dev-ext-public\n        mac: \"ae:d6:b9:e1:76:78\"\n        networks: [\"168.119.17.51/28\"]  # \u2190 All EIPs in this range\n        gateway chassis: [...]\n</code></pre>"},{"location":"prd/svc_lb_architecture/#common-misconceptions-resolved","title":"Common Misconceptions (Resolved)","text":"<p>Note: As of 2025-12-02, the codebase has been cleaned up to remove sources of these misconceptions.</p>"},{"location":"prd/svc_lb_architecture/#misconception-1-policy-routes-required","title":"\u274c Misconception 1: Policy Routes Required","text":"<p>FALSE: Service LoadBalancers do NOT require VPC policy routes.</p> <p>Policy routes (like <code>ip4.src==168.119.17.X reroute 168.119.17.49</code>) are only needed for: - Outbound traffic from VMs/Pods with dedicated EIPs - FIP resources that need bidirectional NAT</p> <p>Service LoadBalancers use the router's external interface directly.</p> <p>Status: \u2705 No incorrect references found in codebase.</p>"},{"location":"prd/svc_lb_architecture/#misconception-2-vpc_nat-annotation-required","title":"\u274c Misconception 2: vpc_nat Annotation Required","text":"<p>FALSE: The <code>ovn.kubernetes.io/vpc_nat</code> annotation is NOT required for Service LoadBalancer OvnEips.</p> <p>This annotation is only needed for: - FIP resources (creates DNAT/SNAT rules) - Resources that need VPC-level NAT management</p> <p>Service LoadBalancers work through the OVN load balancer VIP mechanism.</p> <p>Status: \u2705 kube-dc controllers verified - no <code>vpc_nat</code> usage for Service LBs.</p>"},{"location":"prd/svc_lb_architecture/#misconception-3-kyverno-policy-is-mandatory","title":"\u274c Misconception 3: Kyverno Policy is Mandatory","text":"<p>FALSE: The Kyverno policies for Service LoadBalancers were NOT required.</p> <p>Status: \u2705 REMOVED on 2025-12-02: - Deleted <code>installer/kube-dc/templates/kube-dc/kyverno/mutate-tenant-svc-lb.yaml</code> - Deleted <code>installer/kube-dc/templates/kube-dc/kyverno/mutate-svc-lb-dep.yaml</code> - Removed unnecessary OVN annotations from <code>examples/capi-cluster/addons.yaml</code></p> <p>These policies set annotations (<code>ovn.kubernetes.io/vpc</code>, etc.) that were NOT read by kube-dc controllers.</p>"},{"location":"prd/svc_lb_architecture/#troubleshooting","title":"Troubleshooting","text":""},{"location":"prd/svc_lb_architecture/#service-not-accessible-externally","title":"Service Not Accessible Externally","text":"<ol> <li> <p>Check if EIP is allocated: <pre><code>kubectl get svc -n {namespace} {service-name}\n# STATUS should show EXTERNAL-IP\n</code></pre></p> </li> <li> <p>Verify OVN load balancer exists: <pre><code>kubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lb-list | grep {svc-name}\n</code></pre></p> </li> <li> <p>Check backend endpoints: <pre><code>kubectl get endpoints -n {namespace} {service-name}\n# Should list pod/VM IPs\n</code></pre></p> </li> <li> <p>Verify router attachment: <pre><code>kubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lr-lb-list {vpc-name}\n# Should list the load balancer\n</code></pre></p> </li> <li> <p>Test from within cluster first: <pre><code>kubectl run test --image=curlimages/curl --rm -i -n {namespace} \\\n  -- curl http://{external-ip}:{port}\n</code></pre></p> </li> <li> <p>Check ARP resolution (from external host): <pre><code># On gateway/bastion\narp -n | grep {external-ip}\n# Should show router MAC\n</code></pre></p> </li> </ol>"},{"location":"prd/svc_lb_architecture/#service-works-internally-but-not-externally","title":"Service Works Internally but Not Externally","text":"<p>Possible causes:</p> <ol> <li>Firewall rules on external gateway/firewall</li> <li>Network routing - external network may not route to EIP subnet</li> <li>Testing from wrong location - if testing from bastion/gateway that has the subnet assigned locally, connections will be routed locally</li> </ol> <p>Solution: - Test from a truly external client (your laptop, different server) - Check firewall rules on the physical network infrastructure - Verify routing table on the internet gateway</p>"},{"location":"prd/svc_lb_architecture/#slow-initial-connection","title":"Slow Initial Connection","text":"<p>If first connection fails but subsequent ones work: - ARP cache warming - first packet triggers ARP resolution - Wait 5-10 seconds and try again - Check ARP cache: <code>arp -n | grep {external-ip}</code></p>"},{"location":"prd/svc_lb_architecture/#code-references","title":"Code References","text":""},{"location":"prd/svc_lb_architecture/#service-lb-controller","title":"Service LB Controller","text":"<ul> <li>Main controller: <code>/home/voa/projects/kube-dc/internal/controller/core/service_controller.go</code></li> <li>Load balancer logic: <code>/home/voa/projects/kube-dc/internal/service_lb/service_lb.go</code></li> <li>EIP management: <code>/home/voa/projects/kube-dc/internal/service_lb/eip_res.go</code></li> </ul>"},{"location":"prd/svc_lb_architecture/#eip-controller","title":"EIP Controller","text":"<ul> <li>OvnEip creation: <code>/home/voa/projects/kube-dc/internal/eip/ovn_eip_res.go</code></li> <li>Lines 134-150: OvnEip resource generation (NO vpc_nat annotation needed)</li> </ul>"},{"location":"prd/svc_lb_architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Service LB Tutorial: <code>/home/voa/projects/kube-dc/docs/tutorial-ip-and-lb.md</code></li> <li>Networking Architecture: <code>/home/voa/projects/kube-dc/docs/architecture-networking.md</code></li> <li>FIP Resources: <code>/home/voa/projects/kube-dc/internal/fip/res_fip.go</code></li> </ul>"}]}