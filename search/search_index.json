{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What Is Kube-DC?","text":"<p>Kube-DC is an advanced, enterprise-grade platform that transforms Kubernetes into a comprehensive Data Center solution supporting both virtual machines and containerized workloads. It provides organizations with a unified management interface for all their infrastructure needs, from multi-tenancy and virtualization to networking and billing.</p>"},{"location":"#overview","title":"Overview","text":"<p>Kube-DC bridges the gap between traditional virtualization and modern container orchestration, allowing teams to run both legacy workloads and cloud-native applications on the same platform. By leveraging Kubernetes as the foundation, Kube-DC inherits its robust ecosystem while extending functionality to support enterprise requirements.</p> <p></p>"},{"location":"#key-features-at-a-glance","title":"Key Features at a Glance","text":"<p>Kube-DC offers a comprehensive set of features designed for modern data center operations:</p> <ul> <li>Multi-Tenancy - Host multiple organizations with isolated environments and custom SSO integration</li> <li>Unified Workload Management - Run both VMs and containers on the same platform</li> <li>Advanced Networking - VPC per project, VLAN support, and software-defined networking</li> <li>Enterprise Virtualization - KubeVirt integration with GPU passthrough and live migration</li> <li>Infrastructure as Code - Kubernetes-native APIs with support for Terraform, Ansible, and more</li> <li>Integrated Billing - Track and allocate costs for all resources</li> <li>Managed Services Platform - Deploy databases, storage, and AI/ML infrastructure</li> </ul> <p>For detailed information about each feature, including capabilities and use cases, visit the Core Features page.</p>"},{"location":"#why-choose-kube-dc","title":"Why Choose Kube-DC?","text":""},{"location":"#for-enterprise-it","title":"For Enterprise IT","text":"<ul> <li>Run legacy VMs alongside modern containers</li> <li>Implement chargeback models for departmental resource usage</li> <li>Provide self-service infrastructure while maintaining governance</li> <li>Reduce operational costs by consolidating virtualization and container platforms</li> </ul>"},{"location":"#for-service-providers","title":"For Service Providers","text":"<ul> <li>Offer multi-tenant infrastructure with complete isolation</li> <li>Provide value-added services beyond basic IaaS</li> <li>Implement flexible billing based on actual resource usage</li> <li>Support diverse customer workloads on a single platform</li> </ul>"},{"location":"#for-devops-teams","title":"For DevOps Teams","text":"<ul> <li>Unify VM and container management workflows</li> <li>Implement infrastructure as code for all resources</li> <li>Integrate with existing CI/CD pipelines</li> <li>Enable developer self-service while maintaining control</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to explore Kube-DC? Check out our Quick Start guides to begin your journey.</p> <p>For a deeper understanding of the underlying architecture and concepts, visit the Architecture &amp; Concepts section.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Kube-DC is built with a focus on community collaboration. Visit our Community &amp; Support page to learn how to get involved, report issues, or seek assistance.</p>"},{"location":"CODEBASE_CONTEXT/","title":"Codebase Context for Kube-DC","text":"<p>This document summarizes the codebase structure of the Kube-DC project for future reference. It serves as a single source of truth to avoid repeating exploratory analysis.</p>"},{"location":"CODEBASE_CONTEXT/#top-level-organization","title":"Top-level Organization","text":"<pre><code>.github/             # GitHub workflows and issue templates\n.vscode/             # Editor settings\napi/                 # Kubernetes CRD API definitions for kube-dc.com\ncharts/              # Helm charts for deploying Kube-DC\ncmd/                 # Controller manager entry point (main.go)\ndocs/                # User-facing documentation and architecture guides\nexamples/            # Sample manifests and usage scenarios\nhack/                # Development and automation scripts\ninternal/            # Core libraries, controllers, and utilities\ninstaller/           # Installation manifests and scripts\nservices/            # Auxiliary Kubernetes services (DB, storage, etc.)\nui/                  # Web UI (frontend and backend)\n\nDockerfile           # Container image for controller manager\nDockerfile_manager   # Alternate Dockerfile for the manager image\n.dockerignore        # Files to ignore in Docker builds\n.gitignore           # Git ignore rules\n.golangci.yml        # GolangCI-Lint configuration\nMakefile             # Build and automation targets\nPROJECT              # Project metadata\nREADME.md            # Project overview and key features\ngo.mod, go.sum       # Go module definitions\npackage-lock.json    # Node/NPM dependency lock file for UI backend\nmkdocs.yml           # MkDocs configuration for documentation site\n</code></pre>"},{"location":"CODEBASE_CONTEXT/#detailed-directory-breakdown","title":"Detailed Directory Breakdown","text":""},{"location":"CODEBASE_CONTEXT/#cmd","title":"cmd/","text":"<p>Contains the <code>main.go</code> entry point for the Kube-DC controller manager that initializes and runs Kubernetes controllers.</p>"},{"location":"CODEBASE_CONTEXT/#internal","title":"internal/","text":"<p>Modular Go packages implementing business logic and controller patterns: - service_lb/: Load balancer and external IP management - organization/: Organization CRD and Keycloak integration - eip/, fip/: External/ floating IP resource controllers - project/, organizationgroup/: CRD controllers for multi-tenancy - client/, objmgr/, controller/, utils/: Core abstractions for resource management</p>"},{"location":"CODEBASE_CONTEXT/#ui-web-ui","title":"ui/ (Web UI)","text":"<p>The <code>ui</code> directory contains the Kube\u2011DC user interface, split into two subprojects:</p>"},{"location":"CODEBASE_CONTEXT/#frontend","title":"frontend/","text":"<p>React/TypeScript single\u2011page application scaffolded from PatternFly Seed: - Entry: <code>src/index.tsx</code> and <code>src/app/</code> for layout, routing, and components - Build: Webpack configs (<code>webpack.common.js</code>, <code>webpack.dev.js</code>, <code>webpack.prod.js</code>) and scripts in <code>package.json</code> - Assets &amp; manifests: <code>kubernetes/</code> holds deployment, service, and ingress YAML for UI - Dev tools: Jest tests, Storybook (<code>stories/</code>), ESLint/Prettier, bundle analyzer, and Surge deployment (<code>dr-surge.js</code>)</p> <p><pre><code>ui/frontend/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 index.tsx\n\u2502   \u2514\u2500\u2500 app/\n\u251c\u2500\u2500 kubernetes/\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 webpack.common.js\n\u2514\u2500\u2500 README.md\n</code></pre> \u3010F:ui/frontend/README.md\u2020L1-L6\u3011\u3010F:ui/frontend/package.json\u2020L9-L16\u3011\u3010F:ui/frontend/webpack.common.js\u2020L1-L7\u3011</p>"},{"location":"CODEBASE_CONTEXT/#backend","title":"backend/","text":"<p>Node.js/Express API server that provides UI endpoints and in\u2011cluster proxies: - Server: <code>app.js</code> sets up routes, CORS, body parsing, and WebSocket proxy for VNC - Controllers: <code>controllers/</code> contains modules for cloud-shell, VMs, volumes, network, projects, metrics, system functions, etc. - Proxy: HTTP and WebSocket proxy middleware to route VNC and other traffic via Kubernetes services - Kubernetes manifests: <code>kubernetes/</code> and <code>kubernetes_service/</code> directories for deployment YAML</p> <p><pre><code>ui/backend/\n\u251c\u2500\u2500 app.js\n\u251c\u2500\u2500 controllers/\n\u2502   \u251c\u2500\u2500 cloudShellModule.js\n\u2502   \u2514\u2500\u2500 volumeModule.js\n\u251c\u2500\u2500 kubernetes/\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 README.md\n</code></pre> \u3010F:ui/backend/app.js\u2020L1-L20\u3011\u3010F:ui/backend/controllers/cloudShellModule.js\u2020L1-L10\u3011</p>"},{"location":"CODEBASE_CONTEXT/#hack","title":"hack/","text":"<p>Utility scripts for: - cluster setup and bootstrap - UI code updates and build automation - integration tests and version management</p>"},{"location":"CODEBASE_CONTEXT/#charts","title":"charts/","text":"<p>Helm chart definitions to deploy Kube-DC components onto a Kubernetes cluster.</p>"},{"location":"CODEBASE_CONTEXT/#docs","title":"docs/","text":"<p>Markdown files for: - Tutorials (quickstart, kubeconfig, IP &amp; LB, VMs, user groups) - Architecture (networking, virtualization, multi-tenancy, overview) - Community and support guidelines</p>"},{"location":"CODEBASE_CONTEXT/#examples","title":"examples/","text":"<p>Sample manifests demonstrating cluster API integration, VM workloads, and organization/user configurations.</p>"},{"location":"CODEBASE_CONTEXT/#installer","title":"installer/","text":"<p>Installation scripts and YAML manifests for bootstrapping the control plane and CRDs.</p>"},{"location":"CODEBASE_CONTEXT/#services","title":"services/","text":"<p>Predefined Kubernetes objects for ancillary services such as database and storage provisioning.</p>"},{"location":"CODEBASE_CONTEXT/#go-controller-manager-architecture","title":"Go Controller Manager Architecture","text":""},{"location":"CODEBASE_CONTEXT/#cmdmaingo","title":"cmd/main.go","text":"<ul> <li>Registers schemes for Kubernetes core, kube-dc CRDs, OVN, and CNI types.</li> <li>Parses flags (metrics address, leader election, Keycloak debug, HTTP/2, config secret).</li> <li>Initializes controller-runtime Manager with metrics server, health/readiness probes, and webhook server.</li> <li>Sets global configuration (ConfigSecretName, KubeDcNamespace).</li> <li>Registers Reconcilers: OrganizationReconciler, ProjectReconciler, OrganizationGroupReconciler, EIpReconciler, FIpReconciler, ServiceReconciler.</li> </ul> <p><pre><code>// Add CRD schemes and plugins; initialize Manager and register controllers\nutilruntime.Must(clientgoscheme.AddToScheme(scheme))\nutilruntime.Must(kubedccomv1.AddToScheme(scheme))\n// ...\nmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{...})\n// ...\n(&amp;controller.OrganizationReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme(), Debug: debug}).SetupWithManager(mgr)\n(&amp;controller.ProjectReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme(), Debug: debug}).SetupWithManager(mgr)\n// ...\n(&amp;corecontroller.ServiceReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme()}).SetupWithManager(mgr)\n</code></pre> \u3010F:cmd/main.go\u2020L54-L60\u3011\u3010F:cmd/main.go\u2020L169-L212\u3011</p>"},{"location":"CODEBASE_CONTEXT/#crd-types-and-schemas-apikube-dccomv1","title":"CRD Types and Schemas (api/kube-dc.com/v1)","text":"<ul> <li>Defines custom resources: Organization, Project, OrganizationGroup, EIp, FIp.</li> <li><code>*_types.go</code> files describe Spec and Status fields; <code>*_extend.go</code> adds loader and helper methods.</li> </ul> <p><pre><code>api/kube-dc.com/v1/\n\u251c\u2500\u2500 organization_types.go\n\u251c\u2500\u2500 project_types.go\n\u251c\u2500\u2500 organizationgroup_types.go\n\u251c\u2500\u2500 eip_types.go\n\u251c\u2500\u2500 fip_types.go\n\u251c\u2500\u2500 organization_extend.go\n\u2514\u2500\u2500 project_extend.go\n</code></pre> \u3010F:api/kube-dc.com/v1/organization_types.go\u2020L1-L80\u3011\u3010F:api/kube-dc.com/v1/project_types.go\u2020L1-L80\u3011</p>"},{"location":"CODEBASE_CONTEXT/#controllers-internalcontroller","title":"Controllers (internal/controller)","text":"<ul> <li>OrganizationReconciler: Manages Organization CR, delegates to internal/organization for Keycloak realm, auth config, roles, and secrets.</li> <li>ProjectReconciler: Manages Project CR, orchestrates namespace, VPC, subnet, SNAT, EIP, keypairs, roles, and DNS via internal/project.</li> <li>OrganizationGroupReconciler: Syncs OrganizationGroup CR, handling Keycloak groups and Kubernetes rolebindings.</li> <li>EIpReconciler / FIpReconciler: Reconcile external/Floating IP CRs.</li> <li>ServiceReconciler: Reconciles <code>ServiceTypeLoadBalancer</code> Services and their Endpoints; loads Project context; manages external IPs via <code>NewSvcLbEIpRes</code> and OVN-based load balancers via <code>NewLoadBalancerRes</code> in <code>service_controller.go</code>.   \u3010F:internal/controller/core/service_controller.go\u2020L52-L83\u3011\u3010F:internal/controller/core/service_controller.go\u2020L116-L140\u3011</li> </ul> <p><pre><code>internal/controller/\n\u251c\u2500\u2500 kube-dc.com/\n\u2502   \u251c\u2500\u2500 organization_controller.go\n\u2502   \u251c\u2500\u2500 project_controller.go\n\u2502   \u251c\u2500\u2500 organizatongroup_controller.go\n\u2502   \u251c\u2500\u2500 eip_controller.go\n\u2502   \u2514\u2500\u2500 fip_controller.go\n\u2514\u2500\u2500 core/\n    \u2514\u2500\u2500 service_controller.go\n</code></pre> \u3010F:internal/controller/kube-dc.com/organization_controller.go\u2020L1-L30\u3011\u3010F:internal/controller/core/service_controller.go\u2020L1-L20\u3011</p>"},{"location":"CODEBASE_CONTEXT/#business-logic-internal-packages","title":"Business Logic (internal packages)","text":"<ul> <li>internal/organization: Orchestrates Organization CR synchronization by invoking resource controllers:</li> <li><code>organization.go</code>: Sync/Delete pipeline calling NewKeycloakRealm, NewKubeAuthConfig, NewRealmRole, NewRealmAccessSeret to manage Keycloak realms, Kubernetes auth secrets, realm roles, and access secrets.     \u3010F:internal/organization/organization.go\u2020L12-L58\u3011\u3010F:internal/organization/organization.go\u2020L61-L102\u3011</li> <li>internal/project: Orchestrates Project CR lifecycle, provisioning namespaces, networking, and identities:</li> <li><code>project.go</code>: Sync/Delete pipeline calling NewProjectNamespace, NewProjectVpc, NewProjectEip, NewProjectSubnet, NewProjectNad, NewProjectSnat, NewProjectKeyPairSeret, NewProjectAuthKeySecret, NewProjectKeycloakRole, NewProjectRole, NewProjectRoleBinding, NewProjectVpcDns.     \u3010F:internal/project/project.go\u2020L13-L58\u3011\u3010F:internal/project/project.go\u2020L59-L137\u3011</li> <li>internal/organizationgroup: Manages Keycloak group and Kubernetes bindings per project.</li> <li>internal/service_lb: Implements Service LoadBalancer logic using OVN and EIp CRD:</li> <li><code>service_lb.go</code>: Defines <code>LBResource</code> which configures OVN logical router/switch load balancers (VIPs\u2192backends) via <code>NewLoadBalancerRes</code>, with <code>Sync</code>/<code>Delete</code> methods to mutate OVN NB DB.</li> <li><code>eip_res.go</code>: Defines <code>NewSvcLbEIpRes</code> to reconcile external IP addresses (EIp CRD) for services, based on annotations or project gateway, with functions to Get/Create/Delete and update status.   \u3010F:internal/service_lb/service_lb.go\u2020L30-L41\u3011\u3010F:internal/service_lb/eip_res.go\u2020L18-L27\u3011</li> <li>internal/objmgr: Generic resource manager abstractions for creating/updating Kubernetes objects.</li> <li>internal/utils: Common utilities (random names, JSON copy, resource processor).</li> </ul>"},{"location":"CODEBASE_CONTEXT/#external-integrations","title":"External Integrations","text":"<ul> <li>Keycloak via gocloak for identity management.</li> <li>OVN via kube-ovn client for software\u2011defined networking.</li> <li>NetworkAttachmentDefinitions via CNI client for custom network attachments.</li> </ul>"},{"location":"CODEBASE_CONTEXT/#metrics-healthchecks-leader-election","title":"Metrics, Healthchecks &amp; Leader Election","text":"<ul> <li>Exposes secure metrics endpoint with authentication filters.</li> <li>Readiness and liveness probes via <code>/healthz</code> and <code>/readyz</code>.</li> <li>Optional leader election for HA controller managers.</li> </ul>"},{"location":"CODEBASE_CONTEXT/#installation-via-clusterdev-infrastructure-as-code","title":"Installation via cluster.dev Infrastructure as Code","text":"<p>Kube-DC leverages the cluster.dev IaC framework (v0.9.7) to provision and deploy its control plane and dependencies.</p> <p>Under <code>installer/kube-dc</code>: - stack.yaml: Defines a <code>Stack</code> using the <code>templates/kube-dc/</code> StackTemplate to orchestrate installation units.   <pre><code>name: cluster\ntemplate: \"./templates/kube-dc/\"\nkind: Stack\n</code></pre>   \u3010F:installer/kube-dc/stack.yaml\u2020L1-L4\u3011\u3010F:go.mod\u2020L16\u3011 - project.yaml: Defines a <code>Project</code> for cluster.dev, setting owner organization and project defaults.   <pre><code>name: dev\nkind: Project\n</code></pre>   \u3010F:installer/kube-dc/project.yaml\u2020L1-L3\u3011 - templates/kube-dc/template.yaml: StackTemplate with sequential units: Terraform install, password generators, CRDs, Helm charts (kube-ovn, multus-cni, kubevirt, Keycloak, cert-manager, ingress-nginx, monitoring stack, kube-dc core), and custom shell hooks.   \u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L17-L23\u3011</p> <p>The installer docs demonstrate bootstrapping cluster.dev CLI and deploying the stack: - Bootstrapping cluster.dev: <code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh</code>   \u3010F:docs/quickstart-hetzner.md\u2020L175-L176\u3011 - High-level install step: \u201cKube-DC Installation: Use cluster.dev to deploy Kube-DC components\u201d   \u3010F:docs/quickstart-overview.md\u2020L104-L105\u3011</p>"},{"location":"CODEBASE_CONTEXT/#cicd-testing","title":"CI/CD &amp; Testing","text":""},{"location":"CODEBASE_CONTEXT/#github-actions-workflows","title":"GitHub Actions workflows","text":"<ul> <li>release.yaml: on tag pushes, builds and pushes Helm charts via <code>hack/build.sh</code> inside Alpine/helm container.   \u3010F:.github/workflows/release.yaml\u2020L1-L20\u3011</li> <li>sync_to_public_repo.yaml: on <code>main</code> changes to charts/examples/docs/installer/hack, syncs to the public kube-dc-public repo.   \u3010F:.github/workflows/sync_to_public_repo.yaml\u2020L1-L55\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#local-ci-via-makefile-go-code","title":"Local CI via Makefile (Go code)","text":"<ul> <li><code>make test</code>: run unit tests with envtest and coverage.</li> <li><code>make test-e2e</code>: run end-to-end tests via Kind.</li> <li><code>make lint</code>, <code>make fmt</code>, <code>make vet</code>: lint, format, and vet Go code.</li> <li><code>make build</code>: compile the controller manager binary.   \u3010F:Makefile\u2020L14-L23\u3011\u3010F:Makefile\u2020L27-L38\u3011\u3010F:Makefile\u2020L95-L114\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#frontend-ci-reacttypescript","title":"Frontend CI (React/TypeScript)","text":"<ul> <li><code>npm run ci-checks</code>: type-check, ESLint lint, and Jest coverage tests.</li> <li>Additional scripts: <code>start:dev</code>, <code>build</code>, <code>test</code>, <code>storybook</code>, <code>bundle-profile:analyze</code>.   \u3010F:ui/frontend/package.json\u2020L21-L26\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#backend-ci-nodejsexpress","title":"Backend CI (Node.js/Express)","text":"<ul> <li><code>npm run lint</code>: run ESLint for backend controllers.   \u3010F:ui/backend/package.json\u2020L28-L33\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#usage","title":"Usage","text":"<p>Refer to this file for project structure insights to avoid redundant codebase exploration.</p>"},{"location":"architecture-multi-tenancy/","title":"Multi-Tenancy &amp; RBAC","text":"<p>Kube-DC implements a comprehensive multi-tenant architecture that leverages Kubernetes namespaces and Keycloak for identity and access management. This document explains how organizations, projects, and groups in Kube-DC are mapped to Kubernetes and Keycloak objects.</p>"},{"location":"architecture-multi-tenancy/#core-components-and-mapping-structure","title":"Core Components and Mapping Structure","text":"<p>The following diagram illustrates the mapping between Kube-DC structures and the underlying Kubernetes and Keycloak components:</p> <pre><code>graph TD\n    User[User 1] --&gt;|Authenticates| KC[Keycloak Realm]\n    User --&gt;|Obtains Group and Role| KCG[Keycloak Group]\n    User --&gt;|Obtains Group and Role| KCR[Keycloak Role]\n\n    ORG[Organization] --&gt;|Maps to| ORGNS[Organization Namespace]\n\n    ORGNS --&gt;|Contains| ORGGRP[Organization Group]\n\n    PROJ[Project A] --&gt;|Maps to| PNS[Project A NS]\n    PROJ2[Project B] --&gt;|Maps to| PNS2[Project B NS]\n\n    ORGGRP --&gt;|Maps to| K8GRPCRD[Group CRD]\n    ORGGRP --&gt;|Maps to| KCGRP[Keycloak Group]\n\n    KCGRP --&gt;|Maps to| K8SROLE[K8s RoleBinding]\n    KCR --&gt;|Maps to| K8SROLE\n\n    K8GRPCRD --&gt;|Defines permissions for| PNS\n    K8GRPCRD --&gt;|Defines permissions for| PNS2\n\n    KK[Keycloak Client Role] --&gt;|KK to K8s Role Mapping| K8R[K8s Role]</code></pre>"},{"location":"architecture-multi-tenancy/#organization-structure","title":"Organization Structure","text":""},{"location":"architecture-multi-tenancy/#organization","title":"Organization","text":"<p>An Organization is the top-level entity in Kube-DC that represents a company, department, or team.</p> <p>Example Organization YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Organization\nmetadata:\n  name: shalb\n  namespace: shalb\nspec: \n  description: \"Shalb organization\"\n  email: \"arti@shalb.com\"\n</code></pre> <p>Mapping:</p> <ul> <li>Each Organization maps to a dedicated Kubernetes namespace with the same name</li> <li>A corresponding Keycloak Client is created for the organization</li> <li>The Organization serves as a logical grouping for Projects and OrganizationGroups</li> </ul>"},{"location":"architecture-multi-tenancy/#project","title":"Project","text":"<p>A Project represents a logical grouping of resources within an Organization. Projects help segregate workloads and manage access control.</p> <p>Example Project YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre> <p>Mapping:</p> <ul> <li>Each Project maps to a dedicated Kubernetes namespace in the format: <code>{organization}-{project}</code> (e.g., <code>shalb-demo</code>)</li> <li>Projects receive their own network CIDR block for resource isolation</li> <li>Kubernetes namespaces provide the boundary for resource quotas and access control</li> </ul>"},{"location":"architecture-multi-tenancy/#organizationgroup","title":"OrganizationGroup","text":"<p>An OrganizationGroup maps users to roles within specific projects, defining what actions they can perform.</p> <p>Example OrganizationGroup YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - admin\n  - project: \"prod\"\n    roles:\n    - resource-manager\n</code></pre> <p>Mapping:</p> <ul> <li>OrganizationGroups are implemented as Kubernetes Custom Resource Definitions (CRDs)</li> <li>Each OrganizationGroup maps to a Keycloak Group</li> <li>The permissions defined in OrganizationGroups determine the Kubernetes RoleBindings that grant access to resources</li> <li>Different roles can be assigned for different projects</li> </ul>"},{"location":"architecture-multi-tenancy/#authentication-and-authorization-flow","title":"Authentication and Authorization Flow","text":"<p>User Authentication:</p> <ul> <li>Users authenticate through Keycloak</li> <li>Upon successful authentication, users receive JSON Web Tokens (JWTs)</li> </ul> <p>Group and Role Assignment:</p> <ul> <li>Users are assigned to Keycloak Groups based on their OrganizationGroup membership</li> <li>Keycloak maps these groups to corresponding roles</li> </ul> <p>Kubernetes Authorization:</p> <ul> <li>The Kubernetes API server validates the user's JWT</li> <li>RoleBindings determine what actions the user can perform within each namespace</li> <li>Resource access is controlled at the Project (namespace) level</li> </ul> <p>Resource Access:</p> <ul> <li>Users can only access resources in projects where they have appropriate role assignments</li> <li>Actions are restricted based on the permissions defined in their roles</li> </ul>"},{"location":"architecture-multi-tenancy/#role-based-access-control","title":"Role-Based Access Control","text":"<p>Kube-DC provides several built-in roles that can be assigned to users via OrganizationGroups:</p> <ul> <li>Admin: Full access to all resources within a project</li> <li>Resource Manager: Can create and manage resources, but cannot modify project settings</li> <li>Viewer: Read-only access to project resources</li> </ul> <p>Example Role YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Role\nmetadata:\n  name: resource-manager\n  namespace: shalb\nspec:\n  rules:\n  - apiGroups: [\"*\"]\n    resources: [\"pods\", \"services\", \"deployments\", \"statefulsets\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"kubevirt.io\"]\n    resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"architecture-multi-tenancy/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture-multi-tenancy/#kubernetes-components","title":"Kubernetes Components","text":"<ul> <li>Namespaces: Used to isolate Organizations and Projects</li> <li>RBAC: Role-Based Access Control for managing permissions</li> <li>CRDs: Custom Resource Definitions for Kube-DC specific resources</li> <li>NetworkPolicies: Ensure network isolation between Projects</li> </ul>"},{"location":"architecture-multi-tenancy/#keycloak-integration","title":"Keycloak Integration","text":"<ul> <li>Realm: Represents the authentication domain</li> <li>Clients: Each Organization has a dedicated client</li> <li>Groups: Map to OrganizationGroups in Kube-DC</li> <li>Roles: Define permissions that can be assigned to users</li> <li>Role Mappings: Connect Keycloak roles to Kubernetes RBAC</li> </ul>"},{"location":"architecture-multi-tenancy/#practical-application","title":"Practical Application","text":"<p>When a user is added to an organization group in Kube-DC:</p> <ol> <li>The corresponding Keycloak group membership is created</li> <li>The user inherits roles based on the group's permissions</li> <li>When the user accesses the Kubernetes API, their JWT contains the group and role information</li> <li>Kubernetes RBAC evaluates the JWT against RoleBindings to determine access</li> <li>The user can operate only within the boundaries of their assigned permissions</li> </ol> <p>This multi-layered approach ensures secure isolation between tenants while providing fine-grained access control within each project.</p>"},{"location":"architecture-networking/","title":"Networking Architecture","text":"<p>Kube-DC provides enterprise-grade networking through Kube-OVN and Envoy Gateway, enabling multi-tenant isolation, flexible service exposure, and automatic TLS management.</p>"},{"location":"architecture-networking/#quick-navigation","title":"Quick Navigation","text":"Section Description Network Types Cloud vs Public networks Physical Layer VLANs and provider bridges OVN Architecture VPCs, subnets, routers Service Exposure LoadBalancers, Gateway Routes Envoy Gateway HTTP/HTTPS/gRPC routing"},{"location":"architecture-networking/#network-types","title":"Network Types","text":"<p>Kube-DC supports two external network types:</p> Type Subnet IP Range Internet Routable Use Case Cloud <code>ext-cloud</code> 100.65.0.0/16 \u274c No (NAT pool) Web apps, APIs, cost-effective Public <code>ext-public</code> 168.119.17.48/28 \u2705 Yes VMs, game servers, direct access"},{"location":"architecture-networking/#physical-network-layer","title":"Physical Network Layer","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         PHYSICAL NETWORK                                    \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502      VLAN 4011          \u2502              \u2502      VLAN 4013          \u2502       \u2502\n\u2502  \u2502      ext-public         \u2502              \u2502      ext-cloud          \u2502       \u2502\n\u2502  \u2502   168.119.17.48/28      \u2502              \u2502   100.65.0.0/16         \u2502       \u2502\n\u2502  \u2502                         \u2502              \u2502                         \u2502       \u2502\n\u2502  \u2502   Gateway: 168.119.17.49\u2502              \u2502   Gateway: 100.65.0.1   \u2502       \u2502\n\u2502  \u2502   Internet-routable     \u2502              \u2502   Internal-only         \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502              \u2502                                        \u2502                     \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502                               \u2502                                             \u2502\n\u2502                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n\u2502                     \u2502   Provider Bridge \u2502                                   \u2502\n\u2502                     \u2502   br-ext-cloud    \u2502                                   \u2502\n\u2502                     \u2502   (on each node)  \u2502                                   \u2502\n\u2502                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              OVN NETWORK                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture-networking/#ovn-logical-network","title":"OVN Logical Network","text":""},{"location":"architecture-networking/#management-vpc-ovn-cluster","title":"Management VPC (ovn-cluster)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          ovn-cluster VPC (Management)                          \u2502\n\u2502                                                                                \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502   \u2502   ovn-default   \u2502  \u2502    ext-cloud    \u2502  \u2502   ext-public    \u2502  \u2502   join    \u2502 \u2502\n\u2502   \u2502  10.100.0.0/16  \u2502  \u2502  100.65.0.0/16  \u2502  \u2502168.119.17.48/28 \u2502  \u2502172.30.0.0 \u2502 \u2502\n\u2502   \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502   /22     \u2502 \u2502\n\u2502   \u2502 \u2022 kube-system   \u2502  \u2502 \u2022 Cloud LB VIPs \u2502  \u2502 \u2022 Public LB VIPs\u2502  \u2502 \u2022 Node IPs\u2502 \u2502\n\u2502   \u2502 \u2022 envoy-gateway \u2502  \u2502 \u2022 Cloud EIPs    \u2502  \u2502 \u2022 Public EIPs   \u2502  \u2502           \u2502 \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502            \u2502                    \u2502                    \u2502                 \u2502       \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                 \u2502                    \u2502                         \u2502\n\u2502                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n\u2502                       \u2502         ovn-cluster Router             \u2502               \u2502\n\u2502                       \u2502                                        \u2502               \u2502\n\u2502                       \u2502  Ports:                                \u2502               \u2502\n\u2502                       \u2502  \u2022 ovn-default: 10.100.0.1             \u2502               \u2502\n\u2502                       \u2502  \u2022 ext-cloud: 100.65.0.101             \u2502               \u2502\n\u2502                       \u2502  \u2022 join: 172.30.0.1                    \u2502               \u2502\n\u2502                       \u2502                                        \u2502               \u2502\n\u2502                       \u2502  SNAT: 10.100.0.0/16 \u2192 100.65.0.101    \u2502               \u2502\n\u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture-networking/#project-vpcs","title":"Project VPCs","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Cloud Project VPC             \u2502  \u2502   Public Project VPC            \u2502\n\u2502   (egressNetworkType: cloud)    \u2502  \u2502   (egressNetworkType: public)   \u2502\n\u2502                                 \u2502  \u2502                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  project-a-default      \u2502   \u2502  \u2502   \u2502  project-b-default      \u2502   \u2502\n\u2502   \u2502     10.0.10.0/24        \u2502   \u2502  \u2502   \u2502     10.0.20.0/24        \u2502   \u2502\n\u2502   \u2502                         \u2502   \u2502  \u2502   \u2502                         \u2502   \u2502\n\u2502   \u2502  \u2022 Customer pods        \u2502   \u2502  \u2502   \u2502  \u2022 Customer pods        \u2502   \u2502\n\u2502   \u2502  \u2022 Customer VMs         \u2502   \u2502  \u2502   \u2502  \u2022 Customer VMs         \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502               \u2502                 \u2502  \u2502               \u2502                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  project-a Router       \u2502   \u2502  \u2502   \u2502  project-b Router       \u2502   \u2502\n\u2502   \u2502                         \u2502   \u2502  \u2502   \u2502                         \u2502   \u2502\n\u2502   \u2502  EIP: 100.65.0.102      \u2502   \u2502  \u2502   \u2502  EIP: 168.119.17.51     \u2502   \u2502\n\u2502   \u2502  (ext-cloud)            \u2502   \u2502  \u2502   \u2502  (ext-public)           \u2502   \u2502\n\u2502   \u2502                         \u2502   \u2502  \u2502   \u2502                         \u2502   \u2502\n\u2502   \u2502  SNAT: 10.0.10.0/24     \u2502   \u2502  \u2502   \u2502  SNAT: 10.0.20.0/24     \u2502   \u2502\n\u2502   \u2502       \u2192 100.65.0.102    \u2502   \u2502  \u2502   \u2502       \u2192 168.119.17.51   \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture-networking/#subnet-summary","title":"Subnet Summary","text":"Subnet VPC CIDR Purpose <code>ovn-default</code> ovn-cluster 10.100.0.0/16 Management pods <code>ext-cloud</code> ovn-cluster 100.65.0.0/16 Cloud LB VIPs, EIPs <code>ext-public</code> ovn-cluster 168.119.17.48/28 Public LB VIPs, EIPs <code>join</code> ovn-cluster 172.30.0.0/22 Node-to-OVN connectivity <code>{project}-default</code> {project} 10.x.x.x/24 Customer pods/VMs"},{"location":"architecture-networking/#policy-routing-for-secondary-external-networks","title":"Policy Routing for Secondary External Networks","text":"<p>When an EIP is allocated from a secondary external network (different from the project's default), kube-dc automatically creates policy routes to ensure return traffic uses the correct gateway.</p> <p>Example: A cloud project (<code>egressNetworkType: cloud</code>) with a public EIP:</p> <pre><code>Project: my-project (default: ext-cloud)\n\u251c\u2500\u2500 Default Route: 0.0.0.0/0 \u2192 100.65.0.1 (cloud gateway)\n\u251c\u2500\u2500 Public FIP: 91.224.11.10 \u2192 10.0.0.7 (pod IP)\n\u2514\u2500\u2500 Policy Route: ip4.src == 10.0.0.7 \u2192 91.224.11.1 (public gateway)\n</code></pre> <p>Without the policy route, return traffic from the pod would use the default cloud gateway, causing SNAT to fail and packets to be dropped.</p> <p>Priority levels: | Priority | Purpose | |----------|---------| | 31000 | Allow internal subnet traffic | | 30010 | SvcLB source-based reroute | | 30000 | FIP source-based reroute |</p> <p>See Secondary External Network PRD for implementation details.</p>"},{"location":"architecture-networking/#service-exposure","title":"Service Exposure","text":"<p>Kube-DC provides multiple ways to expose services:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         SERVICE EXPOSURE OPTIONS                                \u2502\n\u2502                                                                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                      1. Gateway Routes (Recommended)                    \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   Internet \u2192 Envoy Gateway (88.99.29.250:443) \u2192 HTTPRoute \u2192 Service     \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   -  Automatic TLS certificates                                         \u2502   \u2502\n\u2502   \u2502   -  Auto-generated hostnames                                           \u2502   \u2502\n\u2502   \u2502   -  Shared infrastructure (cost-effective)                             \u2502   \u2502\n\u2502   \u2502   -  HTTP/HTTPS/gRPC support                                            \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   Annotations:                                                          \u2502   \u2502\n\u2502   \u2502   \u2022 expose-route: https                                                 \u2502   \u2502\n\u2502   \u2502   \u2022 route-hostname: custom.domain.com (optional)                        \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                      2. EIP + LoadBalancer                              \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   Internet \u2192 EIP (dedicated IP) \u2192 OVN LB \u2192 Service \u2192 Pods/VMs           \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   -  Dedicated IP address                                               \u2502   \u2502\n\u2502   \u2502   -  Any TCP/UDP protocol                                               \u2502   \u2502\n\u2502   \u2502   -  Direct VM access                                                   \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   Annotations:                                                          \u2502   \u2502\n\u2502   \u2502   \u2022 bind-on-default-gw-eip: \"true\"                                      \u2502   \u2502\n\u2502   \u2502   \u2022 bind-on-eip: \"my-eip\"                                               \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                      3. Floating IP (FIP)                               \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   Internet \u2192 EIP \u2192 1:1 NAT \u2192 Internal IP (VM/Pod)                       \u2502   \u2502\n\u2502   \u2502                                                                         \u2502   \u2502\n\u2502   \u2502   -  Direct IP mapping                                                  \u2502   \u2502\n\u2502   \u2502   -  VM sees public IP                                                  \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture-networking/#network-elements","title":"Network Elements","text":""},{"location":"architecture-networking/#user-visible-network-resources","title":"User-Visible Network Resources","text":""},{"location":"architecture-networking/#external-ip-eip","title":"External IP (EIP)","text":"<p>External IPs provide connectivity from the public internet to resources within Kube-DC. Each EIP is allocated from the provider network.</p> <p>Example EIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: ssh-arti\n  namespace: shalb-demo\nspec: {}  \n</code></pre>"},{"location":"architecture-networking/#floating-ip-fip","title":"Floating IP (FIP)","text":"<p>Floating IPs map an internal IP address (of a VM or pod) to an External IP, enabling direct access to specific resources.</p> <p>Example FIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: fedora-arti\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171\n  eip: ssh-arti\n</code></pre>"},{"location":"architecture-networking/#kubernetes-service","title":"Kubernetes Service","text":"<p>Standard Kubernetes Services for in-cluster service discovery and load balancing.</p>"},{"location":"architecture-networking/#service-type-loadbalancer","title":"Service Type LoadBalancer","text":"<p>Creates and maps an EIP to a service that routes traffic to pods or VMs. Can use either a dedicated EIP or the project's default EIP.</p> <p>Example Service LoadBalancer YAML with default gateway EIP:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n</code></pre> <p>Example Service LoadBalancer for VM SSH access:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: debian\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222\n      targetPort: 22\n</code></pre>"},{"location":"architecture-networking/#internal-network-resources","title":"Internal Network Resources","text":""},{"location":"architecture-networking/#dnat-rule","title":"DNAT Rule","text":"<p>Destination Network Address Translation rules proxy requests from the internet through an EIP to resources within the VPC network. These are created automatically when an EIP is associated with a resource.</p>"},{"location":"architecture-networking/#snat","title":"SNAT","text":"<p>Source Network Address Translation is used for outbound connections from VPC subnets through EIPs, allowing resources within the VPC to communicate with the internet.</p>"},{"location":"architecture-networking/#project-network-provisioning","title":"Project Network Provisioning","text":"<p>When a new project is created in Kube-DC:</p> <ol> <li>The project is allocated a dedicated subnet from the VPC CIDR range</li> <li>Each project connected to the internet receives an EIP</li> <li>All project outbound traffic is routed through its assigned EIP</li> <li>Project-specific network policies are applied for isolation</li> </ol> <p>Example project creation with CIDR allocation:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre>"},{"location":"architecture-networking/#load-balancer-implementation","title":"Load Balancer Implementation","text":"<p>Kube-DC uses a specialized implementation for Service LoadBalancers:</p> <ul> <li>When a Service with type <code>LoadBalancer</code> is created, an OVS-based LoadBalancer routes traffic to service endpoints</li> <li>Endpoints can be Kubernetes pods or KubeVirt VMs</li> <li>The LoadBalancer can use either:</li> <li>The project's default gateway EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"</code>)</li> <li>A dedicated EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-eip: \"eip-name\"</code>)</li> </ul>"},{"location":"architecture-networking/#automatic-external-endpoints-v0134","title":"Automatic External Endpoints (v0.1.34+)","text":"<p>Kube-DC automatically creates external endpoints for LoadBalancer services to enable cross-VPC communication.</p> <p>When a LoadBalancer receives an external IP, the controller creates: - External Service (<code>&lt;service-name&gt;-ext</code>): Headless service - Endpoints (<code>&lt;service-name&gt;-ext</code>): Points to the LoadBalancer's external IP</p> <p>This solves cross-VPC access by providing stable DNS names (e.g., <code>etcd-lb-ext.shalb-envoy.svc.cluster.local</code>) instead of hardcoded IPs. Endpoints are automatically updated when IPs change and deleted with the LoadBalancer service.</p> <p>External endpoints are labeled with <code>kube-dc.com/managed-by: service-lb-controller</code>.</p>"},{"location":"architecture-networking/#kube-ovn-for-vpc-management","title":"Kube-OVN for VPC Management","text":"<p>Kube-OVN is a key component of Kube-DC's networking architecture, providing the foundation for multi-tenant network isolation through VPC networks.</p>"},{"location":"architecture-networking/#vpc-isolation","title":"VPC Isolation","text":"<p>Different VPC networks are independent of each other and can be separately configured with: - Subnet CIDRs - Routing policies - Security policies - Outbound gateways - EIP allocations</p>"},{"location":"architecture-networking/#overlay-vs-underlay-networks","title":"Overlay vs. Underlay Networks","text":"<p>Kube-DC supports both networking approaches:</p>"},{"location":"architecture-networking/#overlay-networks","title":"Overlay Networks","text":"<ul> <li>Software-defined networks that encapsulate packets</li> <li>Provide maximum flexibility for network segmentation</li> <li>Independent of physical network topology</li> <li>Managed entirely by Kube-OVN</li> <li>Ideal for multi-tenant environments</li> </ul>"},{"location":"architecture-networking/#underlay-networks","title":"Underlay Networks","text":"<ul> <li>Direct mapping to physical network infrastructure</li> <li>Better performance with reduced encapsulation overhead</li> <li>Requires coordination with physical network infrastructure</li> <li>Physical switches handle data-plane forwarding</li> <li>Cannot be isolated by VPCs as they are managed by physical switches</li> </ul>"},{"location":"architecture-networking/#network-security","title":"Network Security","text":"<p>Kube-DC implements multiple layers of network security:</p> <p>Project Isolation</p> <ul> <li>Each project receives its own subnet</li> <li>Traffic between projects is controlled by network policies</li> </ul> <p>VPC Segmentation</p> <ul> <li>Projects can be placed in different VPCs for stricter isolation</li> <li>Each VPC has its own network stack and routing tables</li> </ul> <p>Kubernetes Network Policies</p> <ul> <li>Fine-grained control over ingress and egress traffic</li> <li>Can be applied at the namespace, pod, or service level</li> </ul> <p>Subnet ACLs</p> <ul> <li>Control traffic at the subnet level</li> <li>Provide an additional layer of security beyond network policies</li> </ul>"},{"location":"architecture-networking/#envoy-gateway","title":"Envoy Gateway","text":"<p>Envoy Gateway provides HTTP/HTTPS/gRPC routing with automatic TLS management.</p>"},{"location":"architecture-networking/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         ENVOY GATEWAY ARCHITECTURE                           \u2502\n\u2502                                                                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                    envoy-gateway-system namespace                     \u2502  \u2502\n\u2502   \u2502                    (in ovn-default subnet: 10.100.0.0/16)             \u2502  \u2502\n\u2502   \u2502                                                                       \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502\n\u2502   \u2502   \u2502  Envoy Gateway         \u2502     \u2502  Envoy Proxy Pod               \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  Controller            \u2502     \u2502                                \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502                        \u2502     \u2502  Listens on:                   \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 Watches Gateway,    \u2502     \u2502  \u2022 :443 (HTTPS)                \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502    HTTPRoute, TLSRoute \u2502     \u2502  \u2022 :80 (HTTP)                  \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 Manages certs       \u2502     \u2502                                \u2502   \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502\n\u2502   \u2502                                                   \u2502                   \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502\n\u2502   \u2502   \u2502  Gateway: eg                                                  \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502                                                               \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  Listeners:                                                   \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 http (80)    - Redirect to HTTPS / ACME challenges         \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 https (443)  - Dynamic per-service listeners               \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 tls (443)    - TLS passthrough for Kubernetes API          \u2502   \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                              \u2502\n\u2502                                        \u2502                                     \u2502\n\u2502                                        \u2502 externalIPs: 88.99.29.250           \u2502\n\u2502                                        \u25bc                                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                         TRAFFIC FLOW                                  \u2502  \u2502\n\u2502   \u2502                                                                       \u2502  \u2502\n\u2502   \u2502   Client (https://my-app.stage.kube-dc.com)                           \u2502  \u2502\n\u2502   \u2502         \u2502                                                             \u2502  \u2502\n\u2502   \u2502         \u25bc                                                             \u2502  \u2502\n\u2502   \u2502   DNS \u2192 88.99.29.250 (Gateway external IP)                            \u2502  \u2502\n\u2502   \u2502         \u2502                                                             \u2502  \u2502\n\u2502   \u2502         \u25bc                                                             \u2502  \u2502\n\u2502   \u2502   Envoy Gateway (TLS termination with auto-cert)                      \u2502  \u2502\n\u2502   \u2502         \u2502                                                             \u2502  \u2502\n\u2502   \u2502         \u25bc HTTPRoute matches hostname                                  \u2502  \u2502\n\u2502   \u2502   Backend Service (in customer namespace)                             \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture-networking/#gateway-route-flow","title":"Gateway Route Flow","text":"<p>When a service with <code>expose-route: https</code> annotation is created:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     GATEWAY ROUTE CREATION FLOW                             \u2502\n\u2502                                                                             \u2502\n\u2502   1. Service Created                                                        \u2502\n\u2502      \u2514\u2500\u25ba annotations:                                                       \u2502\n\u2502            service.nlb.kube-dc.com/expose-route: \"https\"                    \u2502\n\u2502                                                                             \u2502\n\u2502   2. Controller Creates:                                                    \u2502\n\u2502      \u251c\u2500\u25ba Certificate (cert-manager)                                         \u2502\n\u2502      \u2502     name: my-app-tls                                                 \u2502\n\u2502      \u2502     issuer: letsencrypt                                              \u2502\n\u2502      \u2502                                                                      \u2502\n\u2502      \u251c\u2500\u25ba Gateway Listener (patched into Gateway)                            \u2502\n\u2502      \u2502     name: https-my-app-namespace                                     \u2502\n\u2502      \u2502     port: 443                                                        \u2502\n\u2502      \u2502     hostname: my-app-namespace.stage.kube-dc.com                     \u2502\n\u2502      \u2502     certificateRef: my-app-tls                                       \u2502\n\u2502      \u2502                                                                      \u2502\n\u2502      \u251c\u2500\u25ba HTTPRoute                                                          \u2502\n\u2502      \u2502     parentRef: Gateway/eg (listener: https-my-app-namespace)         \u2502\n\u2502      \u2502     backendRef: Service/my-app                                       \u2502\n\u2502      \u2502                                                                      \u2502\n\u2502      \u2514\u2500\u25ba ReferenceGrant (if cross-namespace)                                \u2502\n\u2502                                                                             \u2502\n\u2502   3. Status Updated:                                                        \u2502\n\u2502      \u2514\u2500\u25ba route-hostname-status: my-app-namespace.stage.kube-dc.com          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture-networking/#route-types","title":"Route Types","text":"Type Port TLS Handling Use Case <code>http</code> 80 None Plain HTTP traffic <code>https</code> 443 Gateway terminates (auto-cert) Web apps, APIs <code>tls-passthrough</code> 443 App terminates Kubernetes API, end-to-end encryption"},{"location":"architecture-networking/#related-documentation","title":"Related Documentation","text":"<ul> <li>Service Exposure Guide - How to expose services</li> <li>Virtual Machines - VM networking</li> <li>User &amp; Group Management - RBAC for network resources</li> </ul>"},{"location":"architecture-overview/","title":"Overall Architecture","text":"<p>Kube-DC provides a comprehensive multi-tenant cloud infrastructure platform built on Kubernetes and enhanced with enterprise-grade features like virtualization, networking, and identity management.</p>"},{"location":"architecture-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC architecture consists of several key components that work together to deliver a complete cloud platform:</p> <p></p>"},{"location":"architecture-overview/#architectural-layers","title":"Architectural Layers","text":"<p>Kube-DC is organized into main architectural layers:</p> <pre><code>graph TD\n    K8s[Kubernetes] --&gt; KubeVirt[KubeVirt]    \n    K8s --&gt; KubeOVN[Kube-OVN]    \n    K8s --&gt; Keycloak[Keycloak]    \n    K8s --&gt; LBController[Kube-DC LB Controller]    \n    K8s --&gt; MultiTenant[Multi-Tenant Controller]\n\n    KubeVirt --&gt;|Provides| VMs[Virtual Machines]\n    KubeOVN --&gt;|Manages| Networking[Network VLANs/VPCs]\n    Keycloak --&gt;|Controls| IAM[Identity &amp; Access]\n    LBController --&gt;|Enables| LoadBalancing[Load Balancing, Floating IPs]\n    MultiTenant --&gt;|Organizes| Resources[Organization and Projects]</code></pre> <p>Infrastructure Layer</p> <ul> <li>Bare metal servers or cloud infrastructure</li> <li>Kubernetes core services</li> <li>Storage subsystems</li> </ul> <p>Virtualization Layer</p> <ul> <li>KubeVirt for VM provisioning and management</li> <li>Container workloads</li> <li>Hybrid application support</li> </ul> <p>Networking Layer</p> <ul> <li>Kube-OVN for software-defined networking</li> <li>Multi-tenant network isolation</li> <li>External IP addressing and service exposure</li> </ul> <p>Management Layer</p> <ul> <li>Multi-tenancy resource organization</li> <li>Identity and access management via Keycloak</li> <li>User interface and API access</li> </ul>"},{"location":"architecture-overview/#multi-tenant-organization","title":"Multi-Tenant Organization","text":"<p>Kube-DC introduces a hierarchical resource organization model:</p> <ul> <li>Organizations - Top-level entities representing companies or teams</li> <li>Projects - Logical groupings of resources within an organization</li> <li>Groups - Collections of users with defined roles and permissions</li> </ul> <p>This multi-tenant structure maps to Kubernetes and Keycloak components to provide isolation and access control. For detailed information on the multi-tenancy architecture, see the Multi-Tenancy &amp; RBAC documentation.</p>"},{"location":"architecture-overview/#network-architecture","title":"Network Architecture","text":"<p>Kube-DC leverages Kube-OVN to provide advanced networking capabilities:</p> <ul> <li>Virtual Private Clouds (VPCs) for network isolation</li> <li>External and Floating IPs for service exposure</li> <li>Load balancing and service routing</li> </ul> <p>For detailed information on the networking architecture, see the Networking (Kube-OVN, VLANs) documentation.</p>"},{"location":"architecture-overview/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC integrates KubeVirt to enable VM workloads alongside containers:</p> <ul> <li>VM lifecycle management through Kubernetes APIs</li> <li>Hardware passthrough capabilities</li> <li>Mixed container and VM environments</li> </ul> <p>For detailed information on the virtualization architecture, see the Virtualization (KubeVirt) documentation.</p>"},{"location":"architecture-overview/#key-benefits","title":"Key Benefits","text":"<ul> <li>Multi-tenant isolation: Secure separation between organizations and projects</li> <li>Unified management: Single platform for VMs and containers</li> <li>Network flexibility: Advanced SDN capabilities with Kube-OVN</li> <li>Enterprise security: Integrated identity management with Keycloak</li> <li>API-driven architecture: Consistent interfaces for automation and integration</li> </ul>"},{"location":"architecture-virtualization/","title":"Virtualization (KubeVirt)","text":"<p>Kube-DC leverages KubeVirt to provide powerful virtual machine capabilities alongside traditional container workloads. This document covers the virtualization architecture, features, and how VMs are managed within the platform.</p>"},{"location":"architecture-virtualization/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC's virtualization layer is built on KubeVirt, which extends Kubernetes to support virtual machine workloads. This architecture enables consistent management of both containers and VMs through the same API and tooling.</p> <pre><code>graph TD\n    K8s[Kubernetes API] --&gt; KV[KubeVirt Controller]\n    K8s --&gt; CDI[Containerized Data Importer]\n\n    KV --&gt; VMI[VM Instances]\n    CDI --&gt; DV[Data Volumes]\n\n    VMI --&gt; POD[VM Pods]\n    DV --&gt; PVC[Persistent Volumes]\n\n    subgraph \"VM Management\"\n        KV\n        VMI\n        POD\n    end\n\n    subgraph \"Storage Management\"\n        CDI\n        DV\n        PVC\n    end\n\n    UI[Kube-DC Dashboard] --&gt; K8s\n    CLI[kubectl/virtctl] --&gt; K8s</code></pre>"},{"location":"architecture-virtualization/#core-components","title":"Core Components","text":""},{"location":"architecture-virtualization/#kubevirt-controller","title":"KubeVirt Controller","text":"<p>The KubeVirt controller manages the lifecycle of virtual machines by:</p> <ul> <li>Translating VM specifications into Kubernetes resources</li> <li>Scheduling VMs on appropriate nodes</li> <li>Managing VM state (start, stop, pause, resume)</li> <li>Providing VM migration capabilities</li> <li>Handling VM monitoring and health checks</li> </ul>"},{"location":"architecture-virtualization/#containerized-data-importer-cdi","title":"Containerized Data Importer (CDI)","text":"<p>CDI handles storage provisioning for VMs by:</p> <ul> <li>Creating and managing Data Volumes</li> <li>Importing disk images from HTTP/S3 sources</li> <li>Converting disk formats as needed</li> <li>Cloning existing volumes</li> </ul>"},{"location":"architecture-virtualization/#data-volumes","title":"Data Volumes","text":"<p>Data Volumes serve as the storage backbone for VMs, providing:</p> <ul> <li>Storage allocation for VM disks</li> <li>Integration with Kubernetes storage classes</li> <li>Automated provisioning and cleanup</li> </ul>"},{"location":"architecture-virtualization/#vm-management-in-kube-dc","title":"VM Management in Kube-DC","text":""},{"location":"architecture-virtualization/#vm-creation-and-configuration","title":"VM Creation and Configuration","text":"<p>Kube-DC allows users to create VMs through YAML definitions or the web UI. VM configurations include:</p> <p>Example VM Definition:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\n  namespace: demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: default/ovn-demo\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n        cpu:\n          cores: 2\n        memory:\n          guest: 4G\n      volumes:\n      - dataVolume:\n          name: ubuntu-base-img\n        name: root-volume\n</code></pre>"},{"location":"architecture-virtualization/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Kube-DC provides templates for a variety of operating systems:</p> <ul> <li>Ubuntu (20.04, 22.04, 24.04)</li> <li>Debian</li> <li>CentOS/RHEL</li> <li>Fedora</li> <li>Alpine Linux</li> <li>FreeBSD</li> <li>openSUSE</li> <li>Minimal images (cirros)</li> </ul>"},{"location":"architecture-virtualization/#network-integration","title":"Network Integration","text":"<p>VMs in Kube-DC are integrated with the same network architecture as containers:</p> <ul> <li>Each VM can connect to VPC networks via Multus CNI</li> <li>VMs receive IP addresses from the project's CIDR block</li> <li>Network policies apply to VMs just like containers</li> <li>VMs can use floating IPs and load balancer services</li> </ul>"},{"location":"architecture-virtualization/#storage-management","title":"Storage Management","text":"<p>Kube-DC provides flexible storage options for VMs:</p> <ul> <li>Support for multiple storage classes</li> <li>Persistent storage using Kubernetes PVCs</li> <li>Live volume resizing</li> <li>Volume snapshots and cloning</li> </ul>"},{"location":"architecture-virtualization/#vm-customization","title":"VM Customization","text":"<p>VMs can be customized through cloud-init configurations:</p> <pre><code>cloudInitNoCloud:\n  userData: |-\n    #cloud-config\n    chpasswd: { expire: False }\n    password: securepassword\n    ssh_pwauth: True\n    package_update: true\n    package_upgrade: true\n    packages:\n    - qemu-guest-agent\n    runcmd:\n    - [ systemctl, start, qemu-guest-agent ]\n</code></pre> <p>This allows for: - Setting initial passwords - SSH key distribution - Software installation - Custom scripts execution - Network configuration</p>"},{"location":"architecture-virtualization/#health-monitoring","title":"Health Monitoring","text":"<p>VMs in Kube-DC support health checks through:</p> <pre><code>readinessProbe:\n  guestAgentPing: {}\n  failureThreshold: 10\n  initialDelaySeconds: 20\n  periodSeconds: 10\n</code></pre> <p>Health checks ensure: - VM is properly booted - Guest agent is responsive - Cloud-init has completed - Custom health check scripts pass</p>"},{"location":"architecture-virtualization/#web-ui-management","title":"Web UI Management","text":"<p>Kube-DC provides an intuitive web interface for VM management:</p> <p></p>"},{"location":"architecture-virtualization/#vm-dashboard-features","title":"VM Dashboard Features","text":"<p>The VM dashboard provides:</p> <ul> <li>VM Status Monitoring: Running status, uptime, and conditions</li> <li>Performance Metrics: Real-time CPU, memory, and storage usage</li> <li>VM Details: OS version, network configuration, and node placement</li> <li>Console Access: Direct web-based console access to VMs</li> <li>SSH Terminal: Direct SSH access from the browser</li> <li>Network Information: IP addresses and VPC subnet details</li> </ul>"},{"location":"architecture-virtualization/#vm-lifecycle-management","title":"VM Lifecycle Management","text":"<p>Through the UI, administrators and users can:</p> <ul> <li>Create VMs from templates or custom images</li> <li>Start, stop, pause, and restart VMs</li> <li>Adjust resource allocations (CPU, memory)</li> <li>Take snapshots for backup purposes</li> <li>Clone VMs to create new instances</li> <li>Migrate VMs between nodes</li> </ul>"},{"location":"architecture-virtualization/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture-virtualization/#gpu-passthrough","title":"GPU Passthrough","text":"<p>Kube-DC supports GPU passthrough for high-performance computing and AI workloads:</p> <pre><code>domain:\n  devices:\n    gpus:\n    - deviceName: nvidia.com/GP102GL_Tesla_P40\n      name: gpu1\n</code></pre>"},{"location":"architecture-virtualization/#live-migration","title":"Live Migration","text":"<p>VMs can be migrated between nodes without downtime:</p> <pre><code>spec:\n  strategy:\n    type: LiveMigrate\n</code></pre>"},{"location":"architecture-virtualization/#vm-snapshots","title":"VM Snapshots","text":"<p>Kube-DC supports VM snapshots for point-in-time recovery:</p> <pre><code>apiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: my-vm-snapshot\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: my-vm\n</code></pre>"},{"location":"architecture-virtualization/#vm-templates","title":"VM Templates","text":"<p>Organization administrators can create standardized VM templates for their users, ensuring consistent deployments and reducing configuration errors.</p>"},{"location":"architecture-virtualization/#integration-with-multi-tenancy","title":"Integration with Multi-Tenancy","text":"<p>VMs in Kube-DC operate within the same multi-tenant architecture as containers:</p> <ul> <li>VMs are created within specific projects</li> <li>Organization and project permissions control VM access</li> <li>Network isolation is enforced between projects</li> <li>VM metrics are included in project billing and quotas</li> </ul>"},{"location":"architecture-virtualization/#best-practices","title":"Best Practices","text":""},{"location":"architecture-virtualization/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Allocate sufficient memory for the guest OS (minimum 1GB for most Linux distributions)</li> <li>Consider CPU overcommit ratios when planning node capacity</li> <li>Use appropriate storage classes for VM performance requirements</li> </ul>"},{"location":"architecture-virtualization/#vm-optimization","title":"VM Optimization","text":"<ul> <li>Install guest agents for improved integration</li> <li>Use cloud-init for automated VM configuration</li> <li>Configure readiness probes for proper health monitoring</li> <li>Use virtio drivers for improved performance</li> </ul>"},{"location":"architecture-virtualization/#conclusion","title":"Conclusion","text":"<p>Kube-DC's integration of KubeVirt provides a seamless experience for managing both VMs and containers in a single platform. This unified approach simplifies infrastructure management, improves resource utilization, and enables hybrid application architectures that combine the benefits of both virtualization and containerization.</p>"},{"location":"billing-plans-configuration/","title":"Billing Plans &amp; Resource Quota Configuration","text":"<p>This guide explains how to configure billing plans, resource quotas, and EIP limits for Kube-DC organizations using the <code>billing-plans</code> ConfigMap.</p>"},{"location":"billing-plans-configuration/#overview","title":"Overview","text":"<p>Kube-DC enforces organization-level resource limits using four mechanisms:</p> <ol> <li>HierarchicalResourceQuota (HRQ) \u2014 Aggregates resource usage across all project namespaces within an organization. Enforced at pod scheduling time.</li> <li>LimitRange \u2014 Provides default CPU/memory requests and limits for containers that don't specify them. Required for HRQ to work correctly.</li> <li>EIP Quota \u2014 Limits the number of public Elastic IPs an organization can allocate.</li> <li>Object Storage Quota \u2014 Manages S3 storage limits via Rook-Ceph <code>CephObjectStoreUser</code> quotas.</li> </ol> <p>All four are driven by a single ConfigMap: <code>billing-plans</code> in the <code>kube-dc</code> namespace.</p>"},{"location":"billing-plans-configuration/#billing-provider-feature-flag","title":"Billing Provider Feature Flag","text":"<p>The quota system is decoupled from any specific payment provider. A payment provider (Stripe, WHMCS, etc.) is optional and controlled by the <code>BILLING_PROVIDER</code> environment variable on the UI backend:</p> Value Behavior <code>none</code> (default) Quota-only mode. Plans load from ConfigMap, HRQ/LimitRange/EIP quotas enforced. No payment flow. Plan assignment via <code>kubectl</code> annotations. <code>stripe</code> Full Stripe integration: checkout sessions, webhooks, customer portal, subscription CRUD. <code>whmcs</code> (Future) WHMCS webhook integration. <p>When <code>BILLING_PROVIDER=none</code>: - <code>GET /api/billing/config</code> returns <code>{ provider: \"none\", features: { quotas: true, checkout: false, portal: false, ... } }</code> - <code>GET /api/billing/plans</code>, <code>/addons</code>, <code>/quota-usage</code>, <code>/quota-status</code>, <code>/organization-subscription</code> all work normally - Subscription management endpoints (<code>POST/PUT/DELETE /organization-subscription</code>, <code>/verify-checkout</code>, <code>/webhook</code>, <code>/customer-portal</code>) are not mounted - The frontend hides Subscribe/Cancel/Manage Payment buttons automatically</p> <p>To assign a plan manually without a payment provider: <pre><code>kubectl annotate organization/&lt;org-name&gt; -n &lt;org-namespace&gt; \\\n  billing.kube-dc.com/plan-id=dev-pool \\\n  billing.kube-dc.com/subscription=active \\\n  --overwrite\n</code></pre></p>"},{"location":"billing-plans-configuration/#how-it-works","title":"How It Works","text":"<pre><code>billing-plans ConfigMap (kube-dc namespace)\n        \u2502\n        \u25bc\nOrganization Controller (watches ConfigMap for changes)\n        \u2502\n        \u251c\u2500\u25ba HierarchicalResourceQuota (org namespace)\n        \u2502       \u2514\u2500\u25ba Enforced across all child project namespaces\n        \u2502\n        \u251c\u2500\u25ba LimitRange (org namespace)\n        \u2502       \u2514\u2500\u25ba Propagated by HNC to all project namespaces\n        \u2502\n        \u251c\u2500\u25ba EIP Quota (checked on EIP creation)\n        \u2502\n        \u2514\u2500\u25ba CephObjectStoreUser (rook-ceph namespace)\n                \u2514\u2500\u25ba S3 storage quota enforced server-side by Ceph RGW\n</code></pre> <p>When a billing plan is assigned to an organization (via annotations), the controller:</p> <ol> <li>Reads the plan definition from the ConfigMap</li> <li>Computes resource limits (base plan + addons + system overhead + burst ratio)</li> <li>Creates/updates the <code>plan-quota</code> HRQ and <code>default-resource-limits</code> LimitRange in the organization namespace</li> <li>Creates/updates the <code>CephObjectStoreUser</code> in the <code>rook-ceph</code> namespace with the plan's <code>objectStorage</code> quota</li> </ol> <p>Live updates: Editing the ConfigMap automatically triggers reconciliation of all organizations within seconds \u2014 no restart required.</p>"},{"location":"billing-plans-configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Hierarchical Namespace Controller (HNC) installed with HRQ support</li> <li>HNC configured to propagate <code>LimitRange</code> resources (<code>mode: Propagate</code>)</li> <li>Project namespaces configured as children of the organization namespace via HNC hierarchy</li> <li>(Optional) Rook-Ceph installed for Object Storage (S3) quota enforcement</li> </ul>"},{"location":"billing-plans-configuration/#configmap-reference","title":"ConfigMap Reference","text":"<p>Create the ConfigMap in the <code>kube-dc</code> namespace:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: billing-plans\n  namespace: kube-dc\ndata:\n  plans.yaml: |\n    plans:\n      &lt;plan-id&gt;:\n        requests:\n          cpu: \"&lt;cpu&gt;\"\n          memory: \"&lt;memory&gt;\"\n          storage: \"&lt;storage&gt;\"\n        pods: &lt;number&gt;\n        servicesLB: &lt;number&gt;\n        burstRatio: &lt;float&gt;\n        limitRange:\n          defaultCPU: \"&lt;cpu&gt;\"\n          defaultMemory: \"&lt;memory&gt;\"\n          defaultRequestCPU: \"&lt;cpu&gt;\"\n          defaultRequestMem: \"&lt;memory&gt;\"\n          maxCPU: \"&lt;cpu&gt;\"\n          maxMemory: \"&lt;memory&gt;\"\n          minCPU: \"&lt;cpu&gt;\"\n          minMemory: \"&lt;memory&gt;\"\n          maxPodCPU: \"&lt;cpu&gt;\"\n          maxPodMemory: \"&lt;memory&gt;\"\n          maxPVCStorage: \"&lt;storage&gt;\"\n          minPVCStorage: \"&lt;storage&gt;\"\n    suspendedPlan:\n      cpu: \"&lt;cpu&gt;\"\n      memory: \"&lt;memory&gt;\"\n      pods: &lt;number&gt;\n      servicesLB: &lt;number&gt;\n    systemOverhead:\n      cpuPerProject: &lt;millicores&gt;\n      memPerProject: &lt;MiB&gt;\n    addons:\n      &lt;addon-id&gt;:\n        cpu: \"&lt;cpu&gt;\"\n        memory: \"&lt;memory&gt;\"\n        storage: \"&lt;storage&gt;\"\n    eipQuota:\n      &lt;plan-id&gt;: &lt;number&gt;\n</code></pre>"},{"location":"billing-plans-configuration/#field-reference","title":"Field Reference","text":""},{"location":"billing-plans-configuration/#plansplan-id","title":"<code>plans.&lt;plan-id&gt;</code>","text":"<p>Each plan defines the base resource allocation for an organization.</p> Field Description Example <code>requests.cpu</code> Base CPU request quota <code>\"8\"</code> <code>requests.memory</code> Base memory request quota <code>\"24Gi\"</code> <code>requests.storage</code> Storage request quota <code>\"160Gi\"</code> <code>pods</code> Maximum number of pods across all projects <code>200</code> <code>servicesLB</code> Maximum LoadBalancer services <code>100</code> <code>burstRatio</code> Multiplier for limits over requests (e.g., 2.0 = limits are 2\u00d7 requests) <code>2.0</code>"},{"location":"billing-plans-configuration/#plansplan-idlimitrange","title":"<code>plans.&lt;plan-id&gt;.limitRange</code>","text":"<p>Default resource values applied to containers that don't specify their own. Without this, pods missing resource requests will be rejected by the quota system.</p> Field Description Example <code>defaultCPU</code> Default CPU limit per container <code>\"500m\"</code> <code>defaultMemory</code> Default memory limit per container <code>\"512Mi\"</code> <code>defaultRequestCPU</code> Default CPU request per container <code>\"250m\"</code> <code>defaultRequestMem</code> Default memory request per container <code>\"256Mi\"</code> <code>maxCPU</code> Maximum CPU per container <code>\"4\"</code> <code>maxMemory</code> Maximum memory per container <code>\"12Gi\"</code> <code>minCPU</code> Minimum CPU per container <code>\"10m\"</code> <code>minMemory</code> Minimum memory per container <code>\"16Mi\"</code> <code>maxPodCPU</code> Maximum CPU per pod (all containers) <code>\"8\"</code> <code>maxPodMemory</code> Maximum memory per pod (all containers) <code>\"24Gi\"</code> <code>maxPVCStorage</code> Maximum PVC size <code>\"160Gi\"</code> <code>minPVCStorage</code> Minimum PVC size <code>\"1Gi\"</code>"},{"location":"billing-plans-configuration/#suspendedplan","title":"<code>suspendedPlan</code>","text":"<p>Minimal resources allowed when an organization's subscription is suspended.</p> Field Description Example <code>cpu</code> CPU request and limit <code>\"500m\"</code> <code>memory</code> Memory request and limit <code>\"1Gi\"</code> <code>pods</code> Maximum pods <code>10</code> <code>servicesLB</code> Maximum LoadBalancer services <code>0</code>"},{"location":"billing-plans-configuration/#systemoverhead","title":"<code>systemOverhead</code>","text":"<p>Per-project overhead added to the organization's quota to account for system pods (VPC DNS, network agents, etc.).</p> Field Description Example <code>cpuPerProject</code> Millicores added per project <code>100</code> <code>memPerProject</code> MiB added per project <code>128</code> <p>The total overhead is <code>cpuPerProject \u00d7 organizationProjectsLimit</code> (default: 3 projects).</p>"},{"location":"billing-plans-configuration/#addons","title":"<code>addons</code>","text":"<p>Resource add-ons that can be attached to an organization via the <code>billing.kube-dc.com/addons</code> annotation.</p> Field Description Example <code>cpu</code> Additional CPU per addon unit <code>\"4\"</code> <code>memory</code> Additional memory per addon unit <code>\"8Gi\"</code> <code>storage</code> Additional storage per addon unit <code>\"40Gi\"</code>"},{"location":"billing-plans-configuration/#eipquota","title":"<code>eipQuota</code>","text":"<p>Maximum number of Elastic IPs (EIPs) per plan.</p> <pre><code>eipQuota:\n  dev-pool: 1\n  pro-pool: 1\n  scale-pool: 3\n</code></pre>"},{"location":"billing-plans-configuration/#example-configmap","title":"Example ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: billing-plans\n  namespace: kube-dc\ndata:\n  plans.yaml: |\n    plans:\n      dev-pool:\n        displayName: \"Dev Pool\"\n        description: \"Best for: Sandbox / Dev\"\n        price: 19\n        currency: \"EUR\"\n        recommended: false\n        objectStorage: 20\n        ipv4: 1\n        features:\n          - \"4 vCPU\"\n          - \"8 GB RAM\"\n          - \"60 GB NVMe Storage\"\n          - \"20 GB Object Storage included\"\n          - \"1 Dedicated IPv4\"\n          - \"Nested Clusters (KubeVirt)\"\n          - \"Unlimited 1Gbit/s Bandwidth\"\n        requests:\n          cpu: \"4\"\n          memory: \"8Gi\"\n          storage: \"60Gi\"\n        pods: 100\n        servicesLB: 100\n        burstRatio: 3.0\n        limitRange:\n          defaultCPU: \"500m\"\n          defaultMemory: \"512Mi\"\n          defaultRequestCPU: \"100m\"\n          defaultRequestMem: \"128Mi\"\n          maxCPU: \"2\"\n          maxMemory: \"4Gi\"\n          minCPU: \"10m\"\n          minMemory: \"16Mi\"\n          maxPodCPU: \"4\"\n          maxPodMemory: \"8Gi\"\n          maxPVCStorage: \"60Gi\"\n          minPVCStorage: \"1Gi\"\n      pro-pool:\n        displayName: \"Pro Pool\"\n        description: \"Best for: Production / Teams\"\n        price: 49\n        currency: \"EUR\"\n        recommended: true\n        objectStorage: 100\n        ipv4: 1\n        features:\n          - \"8 vCPU\"\n          - \"24 GB RAM\"\n          - \"160 GB NVMe Storage\"\n          - \"100 GB Object Storage included\"\n          - \"1 Dedicated IPv4\"\n          - \"Nested Clusters (KubeVirt)\"\n          - \"Unlimited 1Gbit/s Bandwidth\"\n        requests:\n          cpu: \"8\"\n          memory: \"24Gi\"\n          storage: \"160Gi\"\n        pods: 200\n        servicesLB: 100\n        burstRatio: 2.0\n        limitRange:\n          defaultCPU: \"500m\"\n          defaultMemory: \"512Mi\"\n          defaultRequestCPU: \"250m\"\n          defaultRequestMem: \"256Mi\"\n          maxCPU: \"4\"\n          maxMemory: \"12Gi\"\n          minCPU: \"10m\"\n          minMemory: \"1Mi\"\n          maxPodCPU: \"8\"\n          maxPodMemory: \"24Gi\"\n          maxPVCStorage: \"160Gi\"\n          minPVCStorage: \"1Gi\"\n      scale-pool:\n        displayName: \"Scale Pool\"\n        description: \"Best for: High Load / VDC\"\n        price: 99\n        currency: \"EUR\"\n        recommended: false\n        objectStorage: 500\n        ipv4: 3\n        features:\n          - \"16 vCPU\"\n          - \"56 GB RAM\"\n          - \"320 GB NVMe Storage\"\n          - \"500 GB Object Storage included\"\n          - \"3 Dedicated IPv4\"\n          - \"Nested Clusters (KubeVirt)\"\n          - \"Unlimited 1Gbit/s Bandwidth\"\n        requests:\n          cpu: \"16\"\n          memory: \"56Gi\"\n          storage: \"320Gi\"\n        pods: 500\n        servicesLB: 100\n        burstRatio: 1.5\n        limitRange:\n          defaultCPU: \"1\"\n          defaultMemory: \"1Gi\"\n          defaultRequestCPU: \"500m\"\n          defaultRequestMem: \"512Mi\"\n          maxCPU: \"8\"\n          maxMemory: \"32Gi\"\n          minCPU: \"10m\"\n          minMemory: \"16Mi\"\n          maxPodCPU: \"16\"\n          maxPodMemory: \"56Gi\"\n          maxPVCStorage: \"320Gi\"\n          minPVCStorage: \"1Gi\"\n    suspendedPlan:\n      cpu: \"500m\"\n      memory: \"1Gi\"\n      pods: 10\n      servicesLB: 0\n    systemOverhead:\n      cpuPerProject: 100\n      memPerProject: 128\n    addons:\n      turbo-x1:\n        displayName: \"Turbo x1\"\n        description: \"+4 GB RAM \u2022 +2 vCPU (Burst)\"\n        price: 9\n        currency: \"EUR\"\n        cpu: \"2\"\n        memory: \"4Gi\"\n        storage: \"20Gi\"\n      turbo-x2:\n        displayName: \"Turbo x2\"\n        description: \"+8 GB RAM \u2022 +4 vCPU (Burst)\"\n        price: 16\n        currency: \"EUR\"\n        cpu: \"4\"\n        memory: \"8Gi\"\n        storage: \"40Gi\"\n    eipQuota:\n      dev-pool: 1\n      pro-pool: 1\n      scale-pool: 3\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f billing-plans-configmap.yaml\n</code></pre>"},{"location":"billing-plans-configuration/#how-quotas-are-computed","title":"How Quotas Are Computed","text":""},{"location":"billing-plans-configuration/#hrq-computation","title":"HRQ Computation","text":"<p>For an organization with plan <code>pro-pool</code>, 1\u00d7 <code>turbo-x1</code> addon, and 3 projects:</p> <pre><code>Base CPU requests:     8    (from plan)\n+ Addon CPU:          +2    (turbo-x1 \u00d7 1)\n+ System overhead:    +0.3  (100m \u00d7 3 projects)\n= Total requests.cpu:  10.3\n\nBurst ratio:           2.0  (from plan)\nlimits.cpu = 10.3 \u00d7 2.0 = 20.6\n</code></pre> <p>The resulting HRQ <code>plan-quota</code>:</p> <pre><code>spec:\n  hard:\n    requests.cpu:            \"10300m\"\n    requests.memory:         \"29056Mi\"   # 24Gi + 4Gi addon + 384Mi overhead\n    limits.cpu:              \"20600m\"\n    limits.memory:           \"58112Mi\"\n    requests.storage:        \"180Gi\"     # 160Gi + 20Gi addon\n    pods:                    \"200\"\n    services.loadbalancers:  \"100\"\n</code></pre>"},{"location":"billing-plans-configuration/#burst-ratio","title":"Burst Ratio","text":"<p>The burst ratio determines how much <code>limits</code> exceed <code>requests</code>:</p> Plan Burst Ratio Reasoning Dev Pool 3\u00d7 Dev workloads are bursty, low overcommit risk Pro Pool 2\u00d7 Balanced burst for general workloads Scale Pool 1.5\u00d7 Production workloads need predictability <p>Burst applies only to CPU and memory limits. Storage, pods, and LB quotas are not burst-multiplied.</p>"},{"location":"billing-plans-configuration/#limitrange-behavior","title":"LimitRange Behavior","text":"<p>The LimitRange ensures every container has resource requests set, which is required by Kubernetes when a ResourceQuota is active:</p> <ol> <li>Pod created without <code>resources.requests</code> \u2192 LimitRange applies defaults automatically</li> <li>HRQ admission controller checks aggregated usage against the organization quota</li> <li>Pod admitted if within quota; rejected if quota exceeded</li> </ol> <p>The LimitRange is created in the organization namespace and automatically propagated to all project namespaces by HNC.</p>"},{"location":"billing-plans-configuration/#organization-annotations","title":"Organization Annotations","text":"<p>Plans are assigned to organizations via annotations:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Organization\nmetadata:\n  name: acme-corp\n  namespace: acme-corp\n  annotations:\n    billing.kube-dc.com/plan-id: \"pro-pool\"\n    billing.kube-dc.com/subscription: \"active\"\n    billing.kube-dc.com/addons: '[{\"addonId\":\"turbo-x1\",\"quantity\":1}]'\n</code></pre>"},{"location":"billing-plans-configuration/#subscription-states","title":"Subscription States","text":"Status HRQ Behavior <code>active</code> Full plan limits applied <code>trialing</code> Full plan limits applied <code>canceling</code> Full plan limits applied (until period ends) <code>suspended</code> Minimal quota from <code>suspendedPlan</code> (e.g., 500m CPU, 1Gi memory) No annotation No HRQ created \u2014 no quota enforcement"},{"location":"billing-plans-configuration/#per-project-sub-quotas","title":"Per-Project Sub-Quotas","text":"<p>The HRQ enforces the aggregate limit across all projects. Organization admins can additionally limit individual projects using standard Kubernetes <code>ResourceQuota</code>:</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: project-quota\n  namespace: acme-corp-dev\nspec:\n  hard:\n    requests.cpu: \"2\"\n    requests.memory: \"4Gi\"\n    limits.cpu: \"4\"\n    limits.memory: \"8Gi\"\n</code></pre> <p>The effective limit per resource is <code>min(project ResourceQuota, org HRQ remaining)</code>.</p>"},{"location":"billing-plans-configuration/#updating-plans","title":"Updating Plans","text":"<p>Edit the ConfigMap and apply:</p> <pre><code>kubectl edit configmap billing-plans -n kube-dc\n# or\nkubectl apply -f billing-plans-configmap.yaml\n</code></pre> <p>The controller automatically detects ConfigMap changes and re-reconciles all organizations. HRQs and LimitRanges are updated within seconds.</p>"},{"location":"billing-plans-configuration/#adding-a-new-plan","title":"Adding a New Plan","text":"<p>Add a new entry under <code>plans:</code> with all required fields and a corresponding <code>eipQuota</code> entry:</p> <pre><code>plans:\n  enterprise-pool:\n    requests:\n      cpu: \"32\"\n      memory: \"128Gi\"\n      storage: \"1Ti\"\n    pods: 1000\n    servicesLB: 100\n    burstRatio: 1.2\n    limitRange:\n      defaultCPU: \"1\"\n      defaultMemory: \"2Gi\"\n      defaultRequestCPU: \"500m\"\n      defaultRequestMem: \"1Gi\"\n      maxCPU: \"16\"\n      maxMemory: \"64Gi\"\n      minCPU: \"10m\"\n      minMemory: \"16Mi\"\n      maxPodCPU: \"32\"\n      maxPodMemory: \"128Gi\"\n      maxPVCStorage: \"1Ti\"\n      minPVCStorage: \"1Gi\"\neipQuota:\n  enterprise-pool: 10\n</code></pre>"},{"location":"billing-plans-configuration/#modifying-an-existing-plan","title":"Modifying an Existing Plan","text":"<p>Change the values in the ConfigMap. All organizations on that plan will be updated automatically.</p>"},{"location":"billing-plans-configuration/#monitoring-quota-usage","title":"Monitoring Quota Usage","text":""},{"location":"billing-plans-configuration/#view-hrq-status","title":"View HRQ status","text":"<pre><code>kubectl describe hrq plan-quota -n &lt;org-namespace&gt;\n</code></pre> <p>Output shows <code>spec.hard</code> (limits) and <code>status.used</code> (current usage aggregated across all projects):</p> <pre><code>Spec:\n  Hard:\n    limits.cpu:              16600m\n    limits.memory:           49920Mi\n    pods:                    200\n    requests.cpu:            8300m\n    requests.memory:         24960Mi\n    requests.storage:        160Gi\n    services.loadbalancers:  100\nStatus:\n  Used:\n    limits.cpu:              8560m\n    limits.memory:           15874Mi\n    requests.cpu:            4280m\n    requests.memory:         7937Mi\n    requests.storage:        40Gi\n    pods:                    12\n    services.loadbalancers:  3\n</code></pre>"},{"location":"billing-plans-configuration/#view-limitrange","title":"View LimitRange","text":"<pre><code>kubectl describe limitrange default-resource-limits -n &lt;org-namespace&gt;\n</code></pre>"},{"location":"billing-plans-configuration/#validation","title":"Validation","text":"<p>The ConfigMap is validated on load. The controller will log an error and skip HRQ sync if validation fails. All of the following are required:</p> <ul> <li>At least one plan defined under <code>plans:</code></li> <li>Each plan must have <code>requests.cpu</code>, <code>requests.memory</code>, <code>requests.storage</code></li> <li>Each plan must have <code>burstRatio &gt; 0</code></li> <li>Each plan must have complete <code>limitRange</code> settings</li> <li><code>suspendedPlan.cpu</code> is required</li> <li><code>systemOverhead.cpuPerProject &gt; 0</code> and <code>memPerProject &gt; 0</code></li> <li><code>eipQuota</code> must be defined</li> </ul> <p>Check controller logs for errors:</p> <pre><code>kubectl logs deployment/kube-dc-manager -n kube-dc | grep -i \"billing-plans\\|plan\"\n</code></pre>"},{"location":"billing-plans-configuration/#subscription-lifecycle","title":"Subscription Lifecycle","text":"<p>Organizations transition through the following subscription states:</p> <pre><code>checkout.session.completed\n        \u2502\n        \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   cancel at period end   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  active   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 canceling  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                                       \u2502\n        \u2502 subscription.deleted                  \u2502 period ends \u2192 subscription.deleted\n        \u2502 (payment failure, manual cancel)       \u2502\n        \u25bc                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    7-day grace period    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 suspended  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502  canceled  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                                       \u2502\n        \u2502 re-subscribe                          \u2502 re-subscribe\n        \u25bc                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  active   \u2502                          \u2502  active   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"billing-plans-configuration/#state-details","title":"State Details","text":"Status HRQ Quota Workloads New Deployments S3 Quota <code>active</code> Full plan limits Running Allowed Plan's <code>objectStorage</code> <code>trialing</code> Full plan limits Running Allowed Plan's <code>objectStorage</code> <code>canceling</code> Full plan limits Running Allowed Plan's <code>objectStorage</code> <code>suspended</code> Minimal (100m CPU, 128Mi) Running (grace period) Blocked <code>maxSize=0</code> <code>canceled</code> Minimal (100m CPU, 128Mi) Scaled to zero Blocked <code>maxSize=0</code> <code>past_due</code> Full plan limits Running Allowed Plan's <code>objectStorage</code>"},{"location":"billing-plans-configuration/#grace-period","title":"Grace Period","text":"<p>When a subscription is deleted (via Stripe webhook), the organization enters the <code>suspended</code> state:</p> <ul> <li>7-day grace period \u2014 existing workloads continue running, but new deployments are blocked</li> <li>After 7 days, the controller transitions the org to <code>canceled</code> and suspends all workloads</li> <li>Workload suspension: Deployments/StatefulSets scaled to 0, CronJobs suspended</li> <li>Original replica counts stored in annotations for restoration on re-subscribe</li> </ul>"},{"location":"billing-plans-configuration/#key-annotations","title":"Key Annotations","text":"Annotation Description <code>billing.kube-dc.com/subscription</code> Current status (<code>active</code>, <code>suspended</code>, <code>canceled</code>, etc.) <code>billing.kube-dc.com/plan-id</code> Active plan ID <code>billing.kube-dc.com/plan-name</code> Display name <code>billing.kube-dc.com/suspended-at</code> ISO timestamp when suspension started <code>billing.kube-dc.com/stripe-subscription-id</code> Stripe subscription ID <code>billing.kube-dc.com/stripe-customer-id</code> Stripe customer ID <code>billing.kube-dc.com/addons</code> JSON array of active add-ons"},{"location":"billing-plans-configuration/#api-endpoints","title":"API Endpoints","text":"<p>The billing backend exposes the following REST endpoints under <code>/api/billing/</code>:</p>"},{"location":"billing-plans-configuration/#subscription-management","title":"Subscription Management","text":"Method Endpoint Description <code>GET</code> <code>/organization-subscription</code> Get organization subscription data with quota usage <code>POST</code> <code>/organization-subscription</code> Create new subscription (redirects to Stripe Checkout) <code>PUT</code> <code>/organization-subscription</code> Change plan on existing subscription <code>DELETE</code> <code>/organization-subscription</code> Cancel subscription at period end"},{"location":"billing-plans-configuration/#quota-usage","title":"Quota &amp; Usage","text":"Method Endpoint Description <code>GET</code> <code>/quota-usage</code> Real-time HRQ usage + public EIP count <code>GET</code> <code>/quota-status</code> HRQ existence and enforcement status <code>POST</code> <code>/simulate-downgrade</code> Check if current usage fits target plan"},{"location":"billing-plans-configuration/#plans-add-ons","title":"Plans &amp; Add-ons","text":"Method Endpoint Description <code>GET</code> <code>/plans</code> List available subscription plans <code>GET</code> <code>/addons</code> List available turbo add-ons <code>POST</code> <code>/organization-subscription/addons</code> Add turbo add-on <code>DELETE</code> <code>/organization-subscription/addons/:id</code> Remove turbo add-on"},{"location":"billing-plans-configuration/#per-project-quota-under-apimanage-organization","title":"Per-Project Quota (under <code>/api/manage-organization/</code>)","text":"Method Endpoint Description <code>GET</code> <code>/projects/:id/quota</code> Get project quota details (HRQ, per-project, LimitRange) <code>PUT</code> <code>/projects/:id/quota</code> Set per-project ResourceQuota (org-admin only) <code>DELETE</code> <code>/projects/:id/quota</code> Remove per-project ResourceQuota (org-admin only) <p>Per-project quotas use standard Kubernetes <code>ResourceQuota</code> objects. They coexist with the HRQ \u2014 the most restrictive limit wins. HRQ-managed quotas (prefixed <code>hrq-*</code>) are read-only; only the <code>project-quota</code> ResourceQuota can be managed via the API.</p>"},{"location":"billing-plans-configuration/#stripe-integration","title":"Stripe Integration","text":"Method Endpoint Description <code>POST</code> <code>/verify-checkout</code> Verify Stripe checkout session <code>POST</code> <code>/customer-portal</code> Open Stripe customer portal <code>POST</code> <code>/webhook</code> Stripe webhook handler (raw body)"},{"location":"billing-plans-configuration/#troubleshooting","title":"Troubleshooting","text":"Symptom Cause Fix Pods rejected with \"exceeded quota\" Organization usage exceeds HRQ limits Upgrade plan, remove addons, or delete unused workloads Pods rejected with \"must specify limits\" LimitRange missing or not propagated Verify <code>default-resource-limits</code> LimitRange exists in project namespace HRQ not created ConfigMap missing or invalid Check controller logs, verify ConfigMap exists in <code>kube-dc</code> namespace HRQ not updating after ConfigMap change Controller not watching ConfigMap Check controller logs for \"billing-plans ConfigMap changed\" message EIP creation blocked EIP quota exceeded Check <code>eipQuota</code> setting for the plan Workloads scaled to zero Organization in <code>canceled</code> state Re-subscribe to restore workloads S3 uploads rejected (403) Object storage quota exceeded or org suspended Upgrade plan or re-subscribe Subscription stuck in <code>suspended</code> Grace period not expired yet (7 days) Wait for grace period or re-subscribe"},{"location":"community-support/","title":"Community &amp; Support","text":"<p>Kube-DC has multiple channels for support, community engagement, and professional services. Choose the option that best fits your needs.</p>"},{"location":"community-support/#community-support_1","title":"Community Support","text":""},{"location":"community-support/#github-discussions","title":"GitHub Discussions","text":"<ul> <li>Ask questions and engage with the community</li> <li>Share your experiences and solutions</li> <li>Report bugs and request features</li> <li>Access to public roadmap and project updates</li> <li>Visit GitHub Discussions</li> </ul>"},{"location":"community-support/#slack-community","title":"Slack Community","text":"<p>Join our active Slack community to:</p> <ul> <li>Get real-time help from community members</li> <li>Connect with other Kube-DC users</li> <li>Share your use cases and solutions</li> <li>Stay updated on latest developments</li> <li>Join Kube-DC Slack</li> </ul>"},{"location":"community-support/#documentation","title":"Documentation","text":"<ul> <li>Comprehensive guides and tutorials</li> <li>API reference documentation</li> <li>Best practices and examples</li> <li>Browse Documentation</li> </ul>"},{"location":"community-support/#professional-services","title":"Professional Services","text":""},{"location":"community-support/#commercial-support","title":"Commercial Support","text":"<p>We offer various tiers of commercial support:</p>"},{"location":"community-support/#basic-support","title":"Basic Support","text":"<ul> <li>Business hours support (9/5)</li> <li>Email support</li> <li>24-hour response time</li> <li>Bug fixes and security updates</li> <li>Access to knowledge base</li> </ul>"},{"location":"community-support/#enterprise-support","title":"Enterprise Support","text":"<ul> <li>24/7 support coverage</li> <li>Priority response (2-hour SLA for critical issues)</li> <li>Direct access to engineering team</li> <li>Custom feature development</li> <li>Dedicated support engineer</li> <li>Regular health checks and reviews</li> </ul>"},{"location":"community-support/#professional-services_1","title":"Professional Services","text":""},{"location":"community-support/#implementation-services","title":"Implementation Services","text":"<ul> <li>Architecture design and review</li> <li>Production deployment assistance</li> <li>Migration planning and execution</li> <li>Performance optimization</li> <li>Security hardening</li> </ul>"},{"location":"community-support/#training","title":"Training","text":"<ul> <li>Admin and operator training</li> <li>Developer workshops</li> <li>Custom training programs</li> <li>Certification programs</li> </ul>"},{"location":"community-support/#consulting","title":"Consulting","text":"<ul> <li>Technical architecture consulting</li> <li>Scalability planning</li> <li>High availability design</li> <li>Security assessment</li> <li>Performance optimization</li> <li>Custom integration development</li> </ul>"},{"location":"community-support/#getting-support","title":"Getting Support","text":""},{"location":"community-support/#for-community-support","title":"For Community Support","text":"<ol> <li>Check the documentation</li> <li>Search existing GitHub Issues</li> <li>Join our Slack community</li> <li>Post on GitHub Discussions</li> </ol>"},{"location":"community-support/#for-commercial-support","title":"For Commercial Support","text":"<p>Contact our sales team:</p> <ul> <li>Email: support@kube-dc.com</li> <li>Website: https://kube-dc.com/</li> <li>Phone: +380632441621</li> </ul>"},{"location":"community-support/#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Check our Contributing Guide to learn how you can:</p> <ul> <li>Submit bug reports and feature requests</li> <li>Contribute code</li> <li>Improve documentation</li> <li>Share use cases and examples</li> </ul>"},{"location":"controller_diagram/","title":"Controller Architecture Diagram","text":"<p>A high-level view of Kube-DC controller components (excluding UI) and external dependencies.</p> <pre><code>flowchart TB\n  subgraph Installer\n    CD[cluster.dev IaC]\n  end\n\n  subgraph K8sCluster[\"Kubernetes Cluster &amp; CRDs\"]\n    CRDs[[\"Org, Project, OrgGroup, EIp, FIp CRDs\"]]\n  end\n\n  subgraph Manager[\"Controller Manager\"]\n    OR(OrganizationReconciler)\n    PR(ProjectReconciler)\n    OGR(OrganizationGroupReconciler)\n    EIP(EIpReconciler)\n    FIP(FIpReconciler)\n    SR(ServiceReconciler)\n  end\n\n  subgraph Logic[\"Business Logic Packages\"]\n    OGi[\"internal/organization\"]\n    PI[\"internal/project\"]\n    OGG[\"internal/organizationgroup\"]\n    SLP[\"internal/service_lb\"]\n    OBJ[\"internal/objmgr\"]\n    UTL[\"internal/utils\"]\n  end\n\n  subgraph Ext[\"External Dependencies\"]\n    KC[Keycloak]\n    KO[Kube-OVN]\n    KV[KubeVirt]\n    ML[Multus CNI]\n    CM[Cert-Manager]\n    PM[Prometheus &amp; Loki]\n  end\n\n  CD --&gt; CRDs\n  CRDs --&gt; OR &amp; PR &amp; OGR &amp; EIP &amp; FIP &amp; SR\n\n  OR --&gt; OGi\n  PR --&gt; PI\n  OGR --&gt; OGG\n  EIP --&gt; SLP\n  FIP --&gt; SLP\n  SR --&gt; SLP\n\n  SLP --&gt; KO\n  OGi --&gt; KC\n  PI --&gt; KO &amp; KV &amp; ML\n  PI --&gt; PM &amp; CM\n  PI --&gt; KC\n\n  style CRDs fill:#f9f,stroke:#333,stroke-width:2px\n  style Manager fill:#bbf,stroke:#333,stroke-width:2px\n  style Logic fill:#bfb,stroke:#333,stroke-width:2px\n  style Ext fill:#ffb,stroke:#333,stroke-width:2px\n  style Installer fill:#fbb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"controller_diagram/#networking-integration-kube-ovn-multus","title":"Networking Integration (Kube-OVN &amp; Multus)","text":"<p>Below is a focused diagram showing how Kube-OVN and Multus CNI are installed and integrated via the Project NetworkAttachmentDefinition.</p> <pre><code>flowchart LR\n  subgraph Installer\n    KOV[\"Kube-OVN Helm Chart\"]\n    MULT[\"Multus CNI Helm Chart\"]\n  end\n\n  KOV --&gt; MULT\n\n  subgraph CNIInfra[\"CNI Infrastructure\"]\n    OVN[\"ovn-daemon (kube-ovn)\"]\n    MPods[\"Multus Pods\"]\n  end\n\n  MULT --&gt; MPods\n  KOV --&gt; OVN\n  OVN &amp; MPods --&gt; CNIInfra\n\n  NewNAD[\"NewProjectNad Controller\"]\n  NADCRD[\"NetworkAttachmentDefinition CR\"]\n  CNIConfig[\"Spec.Config: {type:'kube-ovn', server_socket:'/run/openvswitch/kube-ovn-daemon.sock', provider:&lt;proj&gt;} \"]\n  PodAttach[\"Pod annotation 'k8s.v1.cni.cncf.io/networks' = NAD\"]\n\n  NewNAD --&gt; NADCRD\n  NADCRD --&gt; CNIConfig\n  CNIConfig --&gt; PodAttach\n  PodAttach --&gt; CNIInfra\n\n  style Installer fill:#fbb,stroke:#333,stroke-width:1px\n  style CNIInfra fill:#ffb,stroke:#333,stroke-width:1px\n  style NewNAD fill:#bfb,stroke:#333,stroke-width:1px\n  style NADCRD fill:#f9f,stroke:#333,stroke-width:1px</code></pre> <p>Referenced code: - Scheme registration: \u3010F:cmd/main.go\u2020L57-L60\u3011 - NAD controller: \u3010F:internal/project/res_nad.go\u2020L12-L27\u3011   - Installer sequence: \u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L94-L102\u3011\u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L119-L127\u3011</p>"},{"location":"controller_diagram/#eip-fip-serviceloadbalancer-networking-flows","title":"EIP, FIP &amp; ServiceLoadBalancer Networking Flows","text":"<pre><code>flowchart TD\n  subgraph ProjectNet[\"Project Networking Controllers\"]\n    EIPdef[\"NewProjectEip (Default Gateway EIP)\"]\n    EIPcr[NewProjectEip CR]\n    EIPsync[EIpReconciler]\n    FIPsync[FIpReconciler]\n    LBsync[ServiceReconciler]\n  end\n\n  subgraph OVNNB[\"OVN Northbound DB &amp; OVS\"]\n    OVNNBdb[ovn-nb.db]\n    OVSOCK[ovs-db socket]\n  end\n\n  EIPdef --&gt; EIPcr\n  EIPcr --&gt; EIPsync\n  EIPsync --&gt; OVNNBdb\n\n  FIPsync --&gt;|Sync EIP + Floating IP| OVNNBdb\n\n  LBsync --&gt;|Ensure external IP via EIp CR| OVNNBdb\n  LBsync --&gt;|Configure Virtual IPs in LB| OVNNBdb\n\n  OVNNBdb --&gt; OVSOCK\n\n  classDef flow fill:#eef,stroke:#666,stroke-width:1px;\n  class EIPdef,EIPcr,EIPsync,FIPsync,LBsync flow;</code></pre>"},{"location":"controller_diagram/#detailed-network-stack-implementation","title":"Detailed Network Stack Implementation","text":"<ol> <li>Project VPC &amp; Subnet provisioning (<code>internal/project/res_vpc.go</code>)</li> <li>Creates an OVN Virtual Private Cloud via <code>OvnVpc</code> CR and logical switch.</li> <li>NetworkAttachmentDefinition (<code>internal/project/res_nad.go</code>)</li> <li>Defines a Multus NAD with CNI config for <code>kube-ovn</code>, pointing at the OVS socket and project provider.</li> <li>SNAT Rule (<code>internal/project/res_snat.go</code>)</li> <li>Installs an <code>OvnSnatRule</code> to translate pod-source IPs to the project gateway EIP for outbound internet.</li> <li>Default Gateway EIP (<code>internal/project/res_eip_default.go</code>)</li> <li>Ensures a project-scoped <code>EIp</code> CR representing the default gateway external IP, created via <code>NewEipDefault</code>.</li> <li>Floating IP (FIp) (<code>internal/fip/res_eip.go</code> &amp; <code>FIpReconciler</code>)</li> <li>Syncs or creates EIp owned by FIp, then updates <code>FIp.Status.ExternalIP</code> after attaching the EIp to pods via OVN.</li> <li>Service LoadBalancer (<code>internal/service_lb/service_lb.go</code>, <code>internal/service_lb/eip_res.go</code>, <code>ServiceReconciler</code>)</li> <li><code>NewSvcLbEIpRes</code> allocates or binds an external IP for the Service.</li> <li><code>NewLoadBalancerRes</code> uses OVN NB client to define load balancer VIP\u2192backend mappings and injects rules into logical router/switch.</li> <li>Extra External Subnets (<code>internal/project/res_vpc.go</code>)</li> <li>Adds <code>ExtraExternalSubnets</code> field to <code>Vpc.Spec</code> when <code>project.Spec.EgressNetworkType</code> differs from the default external subnet, enabling multi-network external connectivity.    <pre><code>if externalNetwork.Name != defaultExternalSubnet.Name {\n    vpc.Spec.ExtraExternalSubnets = []string{externalNetwork.Name}\n}\n</code></pre>    \u3010F:internal/project/res_vpc.go\u2020L45-L52\u3011</li> </ol> <p>-Refer to code for detailed behavior: - Preamble and flag parsing: \u3010F:cmd/main.go\u2020L117-L131\u3011 - NAD CNI config: \u3010F:internal/project/res_nad.go\u2020L14-L31\u3011 - SNAT via OVN: \u3010F:internal/project/res_snat.go\u2020L14-L45\u3011 - Default EIP creation: \u3010F:internal/project/res_eip_default.go\u2020L15-L42\u3011 - FIp EIP sync: \u3010F:internal/fip/res_eip.go\u2020L25-L50\u3011 - Service LB orchestration: \u3010F:internal/service_lb/service_lb.go\u2020L30-L58\u3011\u3010F:internal/service_lb/eip_res.go\u2020L18-L40\u3011</p>"},{"location":"controller_diagram/#public-vs-cloud-external-networking","title":"Public vs Cloud External Networking","text":"<p>Kube-DC supports two external network types: public (direct public IPs) and cloud (cloud-provider-backed). The type influences EIP/FIP provisioning and SNAT rules:</p> <p><pre><code>// ExternalNetworkType defines how external networks are treated:\ntype ExternalNetworkType string\nconst (\n  ExternalNetworkTypePublic ExternalNetworkType = \"public\"\n  ExternalNetworkTypeCloud  ExternalNetworkType = \"cloud\"\n)\n\n// MasterConfig defaults per resource if not overridden:\nDefaultGwNetworkType, DefaultEipNetworkType,\nDefaultFipNetworkType, DefaultSvcLbNetworkType\n</code></pre> \u3010F:api/kube-dc.com/v1/types.go\u2020L1-L18\u3011</p>"},{"location":"controller_diagram/#project-egress-network-selection","title":"Project Egress Network Selection","text":"<p>The project spec may set <code>egressNetworkType</code> to choose the external subnet for VPC/SNAT/EIP.</p> <p><code>go // GenerateProjectVpc picks externalSubnet based on project.Spec.EgressNetworkType: externalNetwork, _ := utils.SelectBestExternalSubnet(ctx, cli, project.Spec.EgressNetworkType)</code>\u3010F:internal/project/res_vpc.go\u2020L55-L61\u3011</p>"},{"location":"controller_diagram/#snat-rules-for-outbound-traffic","title":"SNAT Rules for Outbound Traffic","text":"<p>SNAT rules ensure pod egress to internet through the gateway EIP:</p> <p><code>go // NewProjectSnat creates OvnSnatRule linking project namespace to gateway EIP base.GeneratedObject = &amp;kubeovn.OvnSnatRule{   Spec: OvnSnatRuleSpec{     OvnEip: DefaultOvnEipName(project, externalSubnet.Name),     Vpc:    projectNamespace,     VpcSubnet: SubnetName(project),   }, }</code>\u3010F:internal/project/res_snat.go\u2020L14-L45\u3011</p>"},{"location":"controller_diagram/#default-gateway-eip-vs-floating-ip","title":"Default Gateway EIP vs Floating IP","text":"<ul> <li>Default Gateway EIP: A single EIp CR per project created by <code>NewProjectEip</code> when no explicit EIP exists. Used for SNAT and default outbound.</li> <li>Floating IP (FIp): EIp allocated per FIp CR to attach public IPs to specific workloads.</li> </ul> <p><code>go // NewProjectEip ensures default project gateway EIp exists WithGetFunction(func(...) {   eip, err := resourcesProcessor.GetProjectGwEip()   if IsNotFound(err) {     newEip, _ := NewEipDefault(...)     base.GeneratedObject = newEip   } })</code>\u3010F:internal/project/res_eip_default.go\u2020L15-L37\u3011</p> <p><code>go // SyncEip for FIp: derives EIp name from FIp and creates/gets it // then FIpReconciler attaches exclusive ownership in OVN</code>\u3010F:internal/fip/res_eip.go\u2020L25-L40\u3011</p>"},{"location":"controller_diagram/#service-loadbalancer-external-ip-binding","title":"Service LoadBalancer External IP Binding","text":"<p>ServiceReconciler uses annotations or defaults to bind EIp to Services:</p> <p>```go // Get or create EIp for Service LB via NewSvcLbEIpRes eipSyncer := NewSvcLbEIpRes(ctx, cli, svc, project) eipSyncer.Sync(ctx)</p> <p>// Configure OVN LB VRRP rules via NewLoadBalancerRes lbRes := NewLoadBalancerRes(ctx, cli, svc, endpoints, eipSyncer.Found(), project) lbRes.Sync(ctx) ```\u3010F:internal/service_lb/eip_res.go\u2020L18-L40\u3011\u3010F:internal/service_lb/service_lb.go\u2020L75-L98\u3011</p>"},{"location":"core-features/","title":"Core Features","text":"<p>Kube-DC extends Kubernetes with a robust set of features designed for enterprise data center operations. This page provides detailed technical specifications and use cases for each of Kube-DC's core capabilities.</p> <p>Looking for a Architectural details? Visit our architectural overview.</p>"},{"location":"core-features/#organization-management","title":"Organization Management","text":"<p>Foundation for Multi-Tenancy</p> <p>Organization Management provides the foundation for Kube-DC's multi-tenant capabilities, enabling complete isolation between different users and groups.</p> <p>Kube-DC's multi-tenant architecture allows service providers to host multiple organizations with complete isolation and customization.</p> <p>Capabilities:</p> <ul> <li>Multi-Organization Support: Host multiple organizations on a single Kube-DC installation with complete logical separation</li> <li>Custom SSO Integration: Each organization can configure its own identity provider:<ul> <li>Google Workspace / Gmail</li> <li>Microsoft Active Directory / Azure AD</li> <li>GitHub</li> <li>GitLab</li> <li>LDAP</li> <li>SAML 2.0 providers</li> <li>OpenID Connect providers</li> </ul> </li> <li>Hierarchical Group Management: Create and manage groups within organizations with inheritance of permissions</li> <li>Flexible RBAC: Assign fine-grained permissions to groups for specific projects or resources</li> <li>Organizational Quotas: Set resource limits at the organization level to ensure fair resource allocation</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Host multiple client organizations with separate authentication systems</li> <li>Enterprise IT: Separate departments with different authentication requirements</li> <li>Educational Institutions: Provide isolated environments for different departments or research groups</li> </ul>"},{"location":"core-features/#namespace-as-a-service","title":"Namespace as a Service","text":"<p>Projects and Workloads</p> <p>Namespaces in Kube-DC function as projects, providing isolated environments for deploying and managing diverse workloads.</p> <p>Every project in Kube-DC is allocated its own Kubernetes namespace with extended capabilities for running both containers and virtual machines.</p> <p>Capabilities:</p> <ul> <li>Unified Management: Deploy and manage both VMs and containers from a single interface</li> <li>Project Isolation: Complete network and resource isolation between projects</li> <li>Resource Quotas: Set limits on CPU, memory, storage, and other resources per project</li> <li>Integrated Dashboard: View and manage all workloads through a unified web interface</li> <li>Custom Templates: Create and use templates for quick deployment of common workloads</li> </ul> <p>Real-World Applications</p> <ul> <li>Application Modernization: Run legacy VMs alongside containerized microservices</li> <li>Development Environments: Provide isolated environments for development, testing, and staging</li> <li>Mixed Workloads: Support teams that require both traditional and cloud-native infrastructure</li> </ul>"},{"location":"core-features/#network-management","title":"Network Management","text":"<p>Advanced Connectivity</p> <p>Kube-DC's network capabilities enable sophisticated connectivity options while maintaining isolation between projects.</p> <p>Kube-DC provides advanced networking capabilities that bridge traditional data center networking with cloud-native concepts.</p> <p>Capabilities:</p> <ul> <li>Dedicated VPC per Project: Each project gets its own virtual network environment</li> <li>VLAN Integration: Connect to physical network infrastructure using VLANs</li> <li>Software-Defined Networking: Create overlay networks with software-defined control</li> <li>Network Peering: Connect project networks with each other or with external networks</li> <li>NAT and Internet Gateway: Control outbound and inbound internet access per project</li> <li>External IP Assignment: Assign public IPs directly to VMs or Kubernetes services</li> <li>Load Balancer Integration: Create and manage load balancers for services and VMs</li> <li>Network Policies: Define granular rules for network traffic filtering</li> <li>DNS Management: Automatic DNS for services and VMs with custom domain support</li> </ul> <p>Real-World Applications</p> <ul> <li>Hybrid Cloud Deployments: Extend on-premises networks to containerized workloads</li> <li>Multi-Tier Applications: Create complex network topologies for enterprise applications</li> <li>Secure Isolation: Create zero-trust network environments with fine-grained control</li> </ul>"},{"location":"core-features/#virtualization","title":"Virtualization","text":"<p>KubeVirt Integration</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities fully integrated with Kubernetes.</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities integrated with Kubernetes.</p> <p>Capabilities:</p> <ul> <li>Hardware Vendor Support: Compatible with major hardware vendors' servers and components</li> <li>GPU Passthrough: Support for Nvidia GPU passthrough to virtual machines</li> <li>ARM Support: Run VMs on ARM-based infrastructure</li> <li>Web Console: Access VM consoles directly through the web UI</li> <li>SSH Integration: SSH access management with key authentication</li> <li>Live Migration: Move running VMs between nodes without downtime</li> <li>Snapshots: Create point-in-time snapshots of VM volumes</li> <li>VM Templates: Create and use templates for rapid VM provisioning</li> <li>Custom Boot Options: Configure boot order, firmware settings, and UEFI support</li> <li>VM Import/Export: Import existing VMs from other platforms</li> </ul> <p>Real-World Applications</p> <ul> <li>Legacy Application Support: Run applications that require traditional VMs</li> <li>Windows Workloads: Host Windows servers alongside Linux containers</li> <li>GPU-Accelerated Computing: Provide GPU resources for AI/ML or rendering workloads</li> <li>Specialized Operating Systems: Run operating systems not supported in containers</li> </ul>"},{"location":"core-features/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>API-Driven Architecture</p> <p>Kube-DC's API-driven approach enables automation and integration with popular infrastructure tools.</p> <p>Kube-DC leverages and extends the Kubernetes API to enable comprehensive infrastructure automation.</p> <p>Capabilities:</p> <ul> <li>Native Kubernetes API: Manage all Kube-DC resources using standard Kubernetes tools</li> <li>Custom Resource Definitions (CRDs): Extended Kubernetes objects for managing organizations, projects, VMs, and more</li> <li>GitOps Compatible: Deploy and manage infrastructure using GitOps workflows</li> </ul> <p>Real-World Applications</p> <ul> <li>Automated Infrastructure: Create fully automated infrastructure provisioning workflows</li> <li>Self-Service Portals: Build custom self-service interfaces using the Kube-DC API</li> <li>CI/CD Integration: Include infrastructure provisioning in CI/CD pipelines</li> <li>Multi-Cloud Management: Manage Kube-DC resources alongside other cloud resources</li> </ul>"},{"location":"core-features/#integrated-flexible-billing","title":"Integrated Flexible Billing","text":"<p>Cost Management</p> <p>Track, allocate, and manage costs across all resources with Kube-DC's comprehensive billing capabilities.</p> <p>Kube-DC includes comprehensive resource tracking and billing capabilities suitable for both service providers and internal IT organizations.</p> <p>Capabilities:</p> <ul> <li>Resource Metering: Track usage of CPU, memory, storage, GPU, and network resources</li> <li>Custom Pricing Models: Define pricing tiers for different resource types and customers</li> <li>Project-Based Billing: Track and bill resource usage at the project level</li> <li>Cost Allocation: Assign costs to organizational units, projects, or individual resources</li> <li>Quota Enforcement: Automatically enforce resource limits based on billing status</li> <li>Usage Reporting: Generate detailed usage reports for analysis and billing</li> <li>Billing API: Integrate with external billing systems through a comprehensive API</li> <li>Chargeback Models: Support for various internal chargeback models for enterprise use</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Bill customers for exact resource usage</li> <li>Enterprise IT: Implement internal chargeback or showback for departmental resource usage</li> <li>Resource Optimization: Identify resource usage patterns and optimize costs</li> </ul>"},{"location":"core-features/#management-services","title":"Management Services","text":"<p>Value-Added Services</p> <p>Extend Kube-DC's capabilities by offering managed services on top of the core platform.</p> <p>Kube-DC provides a platform for delivering managed services on top of its infrastructure.</p> <p>Capabilities:</p> <p>Database as a Service: Deploy and manage databases with automated operations</p> <ul> <li>PostgreSQL</li> <li>MySQL/MariaDB</li> <li>Microsoft SQL Server</li> <li>And more</li> </ul> <p>Object Storage: S3-compatible storage with multi-tenancy support</p> <p>NoSQL Databases: Managed NoSQL database offerings</p> <ul> <li>Redis</li> <li>MongoDB</li> <li>Elasticsearch/OpenSearch</li> </ul> <p>AI/ML Platform: Infrastructure for deploying and serving AI/ML models</p> <ul> <li>LLM serving</li> <li>Model training infrastructure</li> <li>GPU resource allocation</li> </ul> <p>Backup Services: Automated backup solutions for VMs and containers Monitoring as a Service: Multi-tenant monitoring solutions Service Catalog: Self-service provisioning of common services</p> <p>Real-World Applications</p> <ul> <li>Internal Platform Team: Provide managed services to development teams</li> <li>Managed Service Providers: Offer value-added services beyond basic infrastructure</li> <li>AI/ML Operations: Provide specialized infrastructure for data science teams</li> </ul>"},{"location":"deploy-rook-ceph-object-storage/","title":"Deploying Rook Ceph Object Storage (S3) for Kube-DC","text":"<p>S3-compatible object storage backed by Rook Ceph RGW, integrated with Kube-DC billing, quotas, and the UI.</p>"},{"location":"deploy-rook-ceph-object-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster with Kube-DC installed</li> <li>A worker node with available block storage (dedicated disk or loop device)</li> <li>Envoy Gateway for external S3 endpoint (optional)</li> <li>cert-manager with <code>--enable-gateway-api</code> for TLS (optional)</li> <li>DNS record for your S3 endpoint (e.g., <code>s3.example.com</code>)</li> </ul>"},{"location":"deploy-rook-ceph-object-storage/#architecture","title":"Architecture","text":"<pre><code>s3.example.com (HTTPS)\n  \u2192 Gateway (TLS termination)\n    \u2192 rook-ceph-rgw-&lt;store-name&gt;:80 (RGW)\n      \u2192 Ceph OSD (block device on worker)\n</code></pre> Component Purpose Resources Rook Operator Manages Ceph lifecycle 200m CPU / 256Mi MON Cluster monitor 100m CPU / 384Mi MGR Cluster manager 200m CPU / 512Mi OSD (per disk) Object storage daemon 300m CPU / 1Gi RGW S3 gateway 100m CPU / 256Mi"},{"location":"deploy-rook-ceph-object-storage/#step-1-install-rook-operator","title":"Step 1: Install Rook Operator","text":"<pre><code>kubectl create namespace rook-ceph\n\nhelm repo add rook-release https://charts.rook.io/release\nhelm repo update rook-release\n\nhelm install rook-ceph rook-release/rook-ceph \\\n  --namespace rook-ceph \\\n  --version v1.19.1 \\\n  --set crds.enabled=true \\\n  --set allowLoopDevices=true \\\n  --set resources.requests.cpu=200m \\\n  --set resources.requests.memory=256Mi \\\n  --set resources.limits.memory=512Mi\n\n# Verify operator is running\nkubectl wait --for=condition=Ready pod -l app=rook-ceph-operator -n rook-ceph --timeout=120s\n</code></pre> <p>If using loop devices (dev/test), enable them:</p> <pre><code>kubectl set env deployment/rook-ceph-operator -n rook-ceph ROOK_ALLOW_LOOP_DEVICES=true\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#step-2-prepare-storage","title":"Step 2: Prepare Storage","text":""},{"location":"deploy-rook-ceph-object-storage/#option-a-dedicated-disk-production","title":"Option A: Dedicated Disk (Production)","text":"<p>If your worker node has a dedicated disk (e.g., <code>/dev/sdb</code>), skip to Step 3 and reference it directly in the <code>CephCluster</code> spec.</p>"},{"location":"deploy-rook-ceph-object-storage/#option-b-loop-device-devtest","title":"Option B: Loop Device (Dev/Test)","text":"<p>For testing on nodes without a spare disk, create a sparse file-backed loop device:</p> <pre><code># 01-loop-device-setup.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: setup-loop-device\n  namespace: rook-ceph\nspec:\n  nodeName: &lt;YOUR_WORKER_NODE&gt;     # Replace with your worker node name\n  restartPolicy: Never\n  hostPID: true\n  containers:\n  - name: setup\n    image: ubuntu:24.04\n    securityContext:\n      privileged: true\n    command:\n    - /bin/bash\n    - -c\n    - |\n      set -ex\n      LOOP_FILE=/host/var/lib/ceph-osd-block.img\n      LOOP_SIZE_GB=500                # Adjust size as needed\n\n      if losetup -a | grep -q ceph-osd-block; then\n        echo \"Loop device already set up:\"\n        losetup -a | grep ceph-osd-block\n        exit 0\n      fi\n\n      if [ ! -f \"$LOOP_FILE\" ]; then\n        echo \"Creating ${LOOP_SIZE_GB}GB sparse file...\"\n        truncate -s ${LOOP_SIZE_GB}G \"$LOOP_FILE\"\n      fi\n\n      LOOP_DEV=$(losetup --find --show \"$LOOP_FILE\")\n      echo \"Loop device created: $LOOP_DEV\"\n\n      # Persist across reboots via systemd\n      cat &gt; /host/etc/systemd/system/ceph-loop-device.service &lt;&lt; 'SVC'\n      [Unit]\n      Description=Setup Ceph OSD loop device\n      Before=kubelet.service\n      After=local-fs.target\n\n      [Service]\n      Type=oneshot\n      RemainAfterExit=yes\n      ExecStart=/bin/bash -c 'losetup /dev/loop0 /var/lib/ceph-osd-block.img || true'\n      ExecStop=/bin/bash -c 'losetup -d /dev/loop0 || true'\n\n      [Install]\n      WantedBy=multi-user.target\n      SVC\n\n      nsenter -t 1 -m -u -i -n -p -- systemctl daemon-reload\n      nsenter -t 1 -m -u -i -n -p -- systemctl enable ceph-loop-device.service\n      echo \"Done.\"\n    volumeMounts:\n    - name: host-root\n      mountPath: /host\n      mountPropagation: Bidirectional\n    - name: dev\n      mountPath: /dev\n  volumes:\n  - name: host-root\n    hostPath:\n      path: /\n  - name: dev\n    hostPath:\n      path: /dev\n</code></pre> <pre><code>kubectl apply -f 01-loop-device-setup.yaml\nkubectl wait --for=jsonpath='{.status.phase}'=Succeeded pod/setup-loop-device -n rook-ceph --timeout=60s\nkubectl delete pod setup-loop-device -n rook-ceph\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#step-3-deploy-cephcluster","title":"Step 3: Deploy CephCluster","text":"<p>Adjust <code>nodeName</code>, device name, and placement to match your environment.</p> <pre><code># 02-ceph-cluster.yaml\napiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v19.2.1\n    allowUnsupported: false\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 1                        # Use 3 for production HA\n    allowMultiplePerNode: true\n  mgr:\n    count: 1\n    modules:\n      - name: rook\n        enabled: true\n  dashboard:\n    enabled: false\n  storage:\n    useAllNodes: false\n    useAllDevices: false\n    nodes:\n      - name: &lt;YOUR_WORKER_NODE&gt;    # Replace with your worker node name\n        devices:\n          - name: loop0             # Or sdb, nvme0n1, etc.\n  placement:\n    mon:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: kubernetes.io/hostname\n                  operator: In\n                  values:\n                    - &lt;YOUR_WORKER_NODE&gt;\n    mgr:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: kubernetes.io/hostname\n                  operator: In\n                  values:\n                    - &lt;YOUR_WORKER_NODE&gt;\n  resources:\n    mon:\n      requests:\n        cpu: 100m\n        memory: 384Mi\n      limits:\n        memory: 1Gi\n    mgr:\n      requests:\n        cpu: 200m\n        memory: 512Mi\n      limits:\n        memory: 2Gi\n    osd:\n      requests:\n        cpu: 300m\n        memory: 1Gi\n      limits:\n        memory: 4Gi\n</code></pre> <pre><code>kubectl apply -f 02-ceph-cluster.yaml\n\n# Wait for MON, MGR, OSD (~2-3 min)\nwatch kubectl get pods -n rook-ceph | grep -E 'mon|mgr|osd'\n\n# Verify Ceph health\nkubectl -n rook-ceph exec deploy/rook-ceph-operator -- \\\n  ceph -c /var/lib/rook/rook-ceph/rook-ceph.config status\n</code></pre> <p>Note: <code>HEALTH_WARN</code> with \"OSD count &lt; osd_pool_default_size\" is expected for single-node deployments with replication size 1.</p>"},{"location":"deploy-rook-ceph-object-storage/#step-4-deploy-cephobjectstore-rgw","title":"Step 4: Deploy CephObjectStore (RGW)","text":"<pre><code># 03-object-store.yaml\napiVersion: ceph.rook.io/v1\nkind: CephObjectStore\nmetadata:\n  name: my-store\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    replicated:\n      size: 1                       # Use 3 for production HA\n  dataPool:\n    replicated:\n      size: 1                       # Use 3 for production HA\n  preservePoolsOnDelete: true\n  gateway:\n    port: 80\n    instances: 1                    # Use 2+ for production HA\n    resources:\n      requests:\n        cpu: 100m\n        memory: 256Mi\n      limits:\n        memory: 1Gi\n    placement:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: kubernetes.io/hostname\n                  operator: In\n                  values:\n                    - &lt;YOUR_WORKER_NODE&gt;\n</code></pre> <pre><code>kubectl apply -f 03-object-store.yaml\n\n# Wait for RGW pod\nkubectl wait --for=condition=Ready pod -l app=rook-ceph-rgw -n rook-ceph --timeout=120s\n\n# Verify service\nkubectl get svc -n rook-ceph | grep rgw\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#step-5-create-storageclass-for-bucket-provisioning","title":"Step 5: Create StorageClass for Bucket Provisioning","text":"<pre><code># 04-storage-class.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ceph-bucket\nprovisioner: rook-ceph.ceph.rook.io/bucket\nreclaimPolicy: Delete\nparameters:\n  objectStoreName: my-store\n  objectStoreNamespace: rook-ceph\n</code></pre> <pre><code>kubectl apply -f 04-storage-class.yaml\nkubectl get sc ceph-bucket\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#step-6-expose-s3-endpoint-optional-external-access","title":"Step 6: Expose S3 Endpoint (Optional \u2014 External Access)","text":"<p>Skip this step if you only need internal S3 access via the in-cluster service <code>rook-ceph-rgw-my-store.rook-ceph.svc:80</code>.</p>"},{"location":"deploy-rook-ceph-object-storage/#61-dns","title":"6.1 DNS","text":"<p>Create an A record pointing your S3 domain to your gateway's load balancer IP:</p> <pre><code>s3.example.com \u2192 &lt;GATEWAY_LB_IP&gt;\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#62-gateway-listener","title":"6.2 Gateway Listener","text":"<p>Add an HTTPS listener for the S3 hostname to your gateway:</p> <pre><code>kubectl patch gateway eg -n envoy-gateway-system --type=json \\\n  -p='[{\"op\":\"add\",\"path\":\"/spec/listeners/-\",\"value\":{\n    \"name\":\"https-s3\",\n    \"hostname\":\"s3.example.com\",\n    \"port\":443,\n    \"protocol\":\"HTTPS\",\n    \"allowedRoutes\":{\"namespaces\":{\"from\":\"All\"}},\n    \"tls\":{\"mode\":\"Terminate\",\"certificateRefs\":[{\n      \"group\":\"\",\"kind\":\"Secret\",\"name\":\"s3-server-tls\",\"namespace\":\"rook-ceph\"\n    }]}\n  }}]'\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#63-tls-certificate-referencegrant-httproute-and-timeouts","title":"6.3 TLS Certificate, ReferenceGrant, HTTPRoute, and Timeouts","text":"<pre><code># 05-s3-endpoint.yaml\n---\n# TLS Certificate via cert-manager\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: s3-tls\n  namespace: rook-ceph\nspec:\n  secretName: s3-server-tls\n  dnsNames:\n    - s3.example.com\n  issuerRef:\n    kind: ClusterIssuer\n    name: letsencrypt-prod-http    # Your ClusterIssuer name\n---\n# Allow gateway to reference secrets/services in rook-ceph namespace\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: s3-tls-ref\n  namespace: rook-ceph\nspec:\n  from:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n      namespace: envoy-gateway-system\n  to:\n    - group: \"\"\n      kind: Secret\n    - group: \"\"\n      kind: Service\n---\n# Route: s3.example.com \u2192 rook-ceph-rgw-my-store:80\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: s3-endpoint\n  namespace: rook-ceph\nspec:\n  hostnames:\n    - s3.example.com\n  parentRefs:\n    - name: eg\n      namespace: envoy-gateway-system\n      sectionName: https-s3\n  rules:\n    - backendRefs:\n        - name: rook-ceph-rgw-my-store\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n---\n# Disable request timeout for large uploads\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: BackendTrafficPolicy\nmetadata:\n  name: s3-timeouts\n  namespace: rook-ceph\nspec:\n  targetRefs:\n    - group: gateway.networking.k8s.io\n      kind: HTTPRoute\n      name: s3-endpoint\n  timeout:\n    http:\n      requestTimeout: \"0s\"\n      connectionIdleTimeout: 3600s\n</code></pre> <pre><code>kubectl apply -f 05-s3-endpoint.yaml\n\n# Wait for TLS cert\nkubectl get certificate s3-tls -n rook-ceph -w\n\n# Test\ncurl -s -o /dev/null -w \"%{http_code}\" https://s3.example.com/\n# Expected: 200\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#step-7-configure-kube-dc","title":"Step 7: Configure Kube-DC","text":""},{"location":"deploy-rook-ceph-object-storage/#71-helm-values","title":"7.1 Helm Values","text":"<p>Configure the object storage store name and namespace in your Kube-DC Helm values. The defaults match the manifests above:</p> <pre><code># values.yaml (kube-dc chart)\nobjectStorageConfig:\n  namespace: rook-ceph       # default\n  storeName: my-store        # default\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#72-backend-environment-variables","title":"7.2 Backend Environment Variables","text":"<p>The UI backend reads these from environment (all have sensible defaults):</p> Variable Default Description <code>ROOK_NAMESPACE</code> <code>rook-ceph</code> Namespace where Rook-Ceph is deployed <code>CEPH_STORE_NAME</code> <code>my-store</code> Name of the <code>CephObjectStore</code> resource <code>CEPH_STORAGE_CLASS</code> <code>ceph-bucket</code> StorageClass for <code>ObjectBucketClaim</code> provisioning <code>S3_ENDPOINT</code> <code>https://s3.example.com</code> External S3 endpoint URL used by the UI and presigned URLs <p>Set <code>S3_ENDPOINT</code> to your actual S3 domain in the backend deployment.</p>"},{"location":"deploy-rook-ceph-object-storage/#73-billing-plans","title":"7.3 Billing Plans","text":"<p>Object storage quotas are defined per billing plan in <code>values.yaml</code>:</p> <pre><code>plans:\n  dev-pool:\n    objectStorage: 20      # 20 GB, auto: 5 buckets\n  pro-pool:\n    objectStorage: 100     # 100 GB, auto: 20 buckets\n  scale-pool:\n    objectStorage: 500     # 500 GB, auto: 50 buckets\n</code></pre> <p>The Go controller automatically creates a <code>CephObjectStoreUser</code> per organization with these quotas when the org is reconciled.</p>"},{"location":"deploy-rook-ceph-object-storage/#74-backend-service-account-rbac","title":"7.4 Backend Service Account RBAC","text":"<p>The backend service account needs read access to rook-ceph resources. The Kube-DC Helm chart already includes these rules in <code>backend-sa.yaml</code>:</p> <pre><code># Object Storage: CephObjectStoreUser quota + S3 keys from rook-ceph\n- apiGroups: [\"ceph.rook.io\"]\n  resources: [\"cephobjectstoreusers\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"objectbucket.io\"]\n  resources: [\"objectbucketclaims\"]\n  verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#75-project-role-permissions","title":"7.5 Project Role Permissions","text":"<p>Users need <code>objectbucketclaims</code> permissions in their project namespace. The default project roles already include:</p> <pre><code># In default-project-admin-role (and developer role)\n- apiGroups: [objectbucket.io]\n  resources: [objectbucketclaims]\n  verbs: [create, get, list, watch, delete]\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#kube-dc-integration-summary","title":"Kube-DC Integration Summary","text":"<p>Once deployed, the following features are automatically available:</p>"},{"location":"deploy-rook-ceph-object-storage/#go-controller-automatic","title":"Go Controller (automatic)","text":"<ul> <li>Creates <code>CephObjectStoreUser</code> per organization with quotas from billing plan</li> <li>Updates quotas on plan change, blocks uploads on suspension (<code>maxSize=0</code>)</li> <li>Deletes user on organization removal</li> <li>User capabilities: <code>user=*</code>, <code>bucket=*</code> (enables key management via RGW Admin API)</li> </ul>"},{"location":"deploy-rook-ceph-object-storage/#ui-backend-api","title":"UI Backend API","text":"<ul> <li>Bucket Management: Create/delete buckets via <code>ObjectBucketClaim</code>, list with usage stats</li> <li>File Browser: Upload, download (presigned URLs), delete, create folders</li> <li>S3 Access Keys: View org-level credentials, generate additional keys, revoke keys</li> <li>Quota &amp; Usage: Real-time storage usage via RGW Admin API, quota limits from <code>CephObjectStoreUser</code></li> <li>Bucket Policies: Toggle public-read / private access per bucket</li> </ul>"},{"location":"deploy-rook-ceph-object-storage/#ui-frontend","title":"UI Frontend","text":"<ul> <li>Object Storage sidebar tab with tree view (Overview, Buckets, Access Keys)</li> <li>Overview: Quota usage bars, endpoint info, bucket count</li> <li>Buckets: Table with expandable details, access toggle, file browser</li> <li>File Browser: Breadcrumb navigation, drag-and-drop upload, download, folder creation</li> <li>Access Keys: Primary credentials, key management (generate/revoke), CLI examples</li> <li>Billing Integration: Object storage usage bars on Billing Overview and Project pages</li> </ul>"},{"location":"deploy-rook-ceph-object-storage/#verification","title":"Verification","text":""},{"location":"deploy-rook-ceph-object-storage/#test-s3-access","title":"Test S3 Access","text":"<pre><code>import boto3\nfrom botocore.config import Config\n\ns3 = boto3.client('s3',\n    endpoint_url='https://s3.example.com',\n    aws_access_key_id='&lt;ACCESS_KEY&gt;',\n    aws_secret_access_key='&lt;SECRET_KEY&gt;',\n    region_name='us-east-1',\n    config=Config(signature_version='s3v4')\n)\n\ns3.create_bucket(Bucket='test-bucket')\ns3.put_object(Bucket='test-bucket', Key='hello.txt', Body=b'Hello!')\nresp = s3.get_object(Bucket='test-bucket', Key='hello.txt')\nprint(resp['Body'].read().decode())\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#check-ceph-health","title":"Check Ceph Health","text":"<pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-operator -- \\\n  ceph -c /var/lib/rook/rook-ceph/rook-ceph.config status\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#verify-org-user-created","title":"Verify Org User Created","text":"<pre><code>kubectl get cephobjectstoreuser -n rook-ceph\n# Should show one user per organization with objectStorage in their plan\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#cleanup","title":"Cleanup","text":"<pre><code># Delete object store\nkubectl delete cephobjectstore my-store -n rook-ceph\n\n# Delete cluster\nkubectl delete cephcluster rook-ceph -n rook-ceph\n\n# Wait for cleanup, then uninstall operator\nhelm uninstall rook-ceph -n rook-ceph\n\n# Clean host data (on worker node)\nrm -rf /var/lib/rook/*\n</code></pre>"},{"location":"deploy-rook-ceph-object-storage/#references","title":"References","text":"<ul> <li>Rook Ceph Documentation</li> <li>Rook CephObjectStore</li> <li>Rook ObjectBucketClaim</li> <li>Ceph RGW Admin API</li> <li>Rook External Cluster (for remote Ceph)</li> </ul>"},{"location":"internal-billing-integration/","title":"Internal Billing Integration Documentation","text":""},{"location":"internal-billing-integration/#overview","title":"Overview","text":"<p>This document describes the internal architecture and implementation details of the billing and quota system within the Kube-DC platform. The system is designed with a decoupled architecture: resource quotas work independently, and payment providers (Stripe, WHMCS, etc.) are optional plug-ins controlled by a feature flag.</p>"},{"location":"internal-billing-integration/#architecture-overview","title":"Architecture Overview","text":""},{"location":"internal-billing-integration/#design-principles","title":"Design Principles","text":"<ol> <li>Quotas are a core feature \u2014 HRQ, LimitRange, and EIP enforcement work without any payment provider</li> <li>Payment providers are optional \u2014 Controlled by <code>BILLING_PROVIDER</code> environment variable (<code>none</code> | <code>stripe</code> | <code>whmcs</code>)</li> <li>Plans from ConfigMap \u2014 All plan definitions live in a single <code>billing-plans</code> ConfigMap in the <code>kube-dc</code> namespace</li> <li>Frontend adapts dynamically \u2014 UI fetches <code>/api/billing/config</code> and hides payment buttons when no provider is active</li> </ol>"},{"location":"internal-billing-integration/#system-components","title":"System Components","text":"<pre><code>graph TB\n    subgraph \"Kube-DC Frontend\"\n        UI[Billing UI Component]\n        Config[Billing Config Fetch]\n        Auth[JWT Authentication]\n    end\n\n    subgraph \"Kube-DC Backend\"\n        QC[quotaController.js&lt;br/&gt;Always loaded]\n        BC[billingController.js&lt;br/&gt;Always loaded]\n        SP[providers/stripe.js&lt;br/&gt;Loaded when BILLING_PROVIDER=stripe]\n    end\n\n    subgraph \"Kubernetes\"\n        CM[billing-plans ConfigMap]\n        HRQ[HierarchicalResourceQuota]\n        LR[LimitRange]\n        ORG[Organization Annotations]\n    end\n\n    subgraph \"External - Optional\"\n        Stripe[Stripe API]\n    end\n\n    UI --&gt; Auth\n    Config --&gt; QC\n    Auth --&gt; QC\n    Auth --&gt; BC\n    Auth --&gt; SP\n    QC --&gt; CM\n    QC --&gt; ORG\n    QC --&gt; HRQ\n    SP -.-&gt; Stripe</code></pre>"},{"location":"internal-billing-integration/#authentication-authorization-flow","title":"Authentication &amp; Authorization Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant K8s as Kubernetes API\n    participant Stripe as Stripe (optional)\n\n    User-&gt;&gt;Frontend: Access Billing Page\n    Frontend-&gt;&gt;Backend: GET /api/billing/config\n    Backend-&gt;&gt;Frontend: { provider, features }\n    Frontend-&gt;&gt;Frontend: Gate UI based on features\n\n    Frontend-&gt;&gt;Backend: GET /api/billing/organization-subscription\n    Backend-&gt;&gt;K8s: Read Organization annotations\n    K8s-&gt;&gt;Backend: Plan ID, status, addons\n    Backend-&gt;&gt;K8s: Read billing-plans ConfigMap\n    K8s-&gt;&gt;Backend: Plan definitions\n    Backend-&gt;&gt;Frontend: Subscription + quota data\n\n    alt BILLING_PROVIDER=stripe\n        User-&gt;&gt;Frontend: Click Subscribe\n        Frontend-&gt;&gt;Backend: POST /api/billing/organization-subscription\n        Backend-&gt;&gt;Stripe: Create Checkout Session\n        Stripe-&gt;&gt;Backend: Checkout URL\n        Backend-&gt;&gt;Frontend: Redirect to Stripe\n    else BILLING_PROVIDER=none\n        Note over Frontend: Subscribe button hidden\n        Note over Frontend: Plans assigned via kubectl\n    end</code></pre>"},{"location":"internal-billing-integration/#implementation-details","title":"Implementation Details","text":""},{"location":"internal-billing-integration/#backend-architecture","title":"Backend Architecture","text":""},{"location":"internal-billing-integration/#file-structure","title":"File Structure","text":"<pre><code>ui/backend/\n\u251c\u2500\u2500 controllers/billing/\n\u2502   \u251c\u2500\u2500 quotaController.js       # Provider-agnostic: plans, addons, HRQ usage, quota endpoints\n\u2502   \u251c\u2500\u2500 billingController.js     # Health check, project billing proxy, pricing\n\u2502   \u2514\u2500\u2500 providers/\n\u2502       \u2514\u2500\u2500 stripe.js            # Stripe-specific: checkout, webhooks, portal, subscription CRUD\n\u251c\u2500\u2500 routes/\n\u2502   \u2514\u2500\u2500 billing.js               # Route mounting (conditional provider loading)\n\u2514\u2500\u2500 app.js                       # Express app (conditional raw body skip for webhooks)\n</code></pre>"},{"location":"internal-billing-integration/#key-components","title":"Key Components","text":"<p>Quota Controller (<code>controllers/billing/quotaController.js</code>) \u2014 Always loaded - Reads plans and addons from <code>billing-plans</code> ConfigMap via Kubernetes API - Caches plan data with TTL-based invalidation - Reads Organization annotations for subscription state - Provides HRQ and EIP usage data - Exposes <code>/api/billing/config</code> with active provider and feature flags - Exports shared functions used by provider modules:   - <code>getServiceAccountToken</code>, <code>isOrgAdmin</code>, <code>getSubscriptionPlans</code>   - <code>getTurboAddons</code>, <code>getHRQUsage</code>, <code>getPublicEIPUsage</code>   - <code>getOrganizationSubscriptionData</code>, <code>updateOrganizationSubscription</code></p> <p>Billing Controller (<code>controllers/billing/billingController.js</code>) \u2014 Always loaded - Health check endpoint - Project-level billing data proxy - Pricing information</p> <p>Stripe Provider (<code>controllers/billing/providers/stripe.js</code>) \u2014 Loaded only when <code>BILLING_PROVIDER=stripe</code> - Stripe SDK initialization and customer management - Checkout session creation with plan-to-price ID mapping - Webhook handling for subscription lifecycle events - Customer portal session creation - Subscription update and cancellation</p>"},{"location":"internal-billing-integration/#feature-flag-billing_provider","title":"Feature Flag: <code>BILLING_PROVIDER</code>","text":"Value Routes Mounted Behavior <code>none</code> (default) quotaController + billingController Quota-only mode. Plans via ConfigMap, enforcement via HRQ. <code>stripe</code> + providers/stripe.js Full Stripe: checkout, webhooks, portal. <code>whmcs</code> (future) WHMCS webhook integration. <p>Route Mounting (<code>routes/billing.js</code>): <pre><code>const BILLING_PROVIDER = process.env.BILLING_PROVIDER || 'none';\n\n// Always mount provider-agnostic quota routes\nrouter.use('/', quotaRouter);\n\n// Conditionally mount payment provider routes\nif (BILLING_PROVIDER === 'stripe') {\n    const stripeRouter = require('../controllers/billing/providers/stripe');\n    router.use('/', stripeRouter);\n}\n</code></pre></p> <p>Conditional Webhook Body Parsing (<code>app.js</code>): <pre><code>// Only skip JSON parsing for webhook when a provider is active\nif (BILLING_PROVIDER !== 'none' &amp;&amp; req.originalUrl === '/api/billing/webhook') {\n    next(); // Raw body for signature verification\n} else {\n    bodyParser.json()(req, res, next);\n}\n</code></pre></p>"},{"location":"internal-billing-integration/#frontend-architecture","title":"Frontend Architecture","text":""},{"location":"internal-billing-integration/#file-structure_1","title":"File Structure","text":"<pre><code>ui/frontend/src/app/ManageOrganization/\n\u251c\u2500\u2500 Billing/\n\u2502   \u251c\u2500\u2500 Billing.tsx              # Main billing page\n\u2502   \u251c\u2500\u2500 SubscribePlanModal.tsx   # Plan selection and checkout\n\u2502   \u251c\u2500\u2500 api.ts                   # Billing API client (includes getBillingConfig)\n\u2502   \u2514\u2500\u2500 types.ts                 # Provider-agnostic type definitions\n\u251c\u2500\u2500 OrganizationRoutes.tsx       # Route definitions\n\u251c\u2500\u2500 OrganizationSidebar.tsx      # Navigation sidebar\n\u2514\u2500\u2500 OrganizationLayout.tsx       # Layout logic\n</code></pre>"},{"location":"internal-billing-integration/#key-features","title":"Key Features","text":"<p>Provider-Agnostic UI (<code>Billing/Billing.tsx</code>) - Fetches <code>BillingConfig</code> on mount via <code>GET /api/billing/config</code> - Gates all payment buttons behind <code>billingConfig.features.*</code> flags:   - <code>checkout</code> \u2192 Subscribe, Change Plan, Re-subscribe, Cancel buttons   - <code>portal</code> \u2192 Manage Payment, Update Payment Method buttons   - <code>addons</code> \u2192 Add Resources button - Uses PatternFly design system - Displays quota usage with progress bars and alerts</p> <p>Type Definitions (<code>Billing/types.ts</code>) <pre><code>export interface BillingConfig {\n    provider: 'none' | 'stripe' | 'whmcs';\n    features: {\n        quotas: boolean;\n        plans: boolean;\n        checkout: boolean;\n        portal: boolean;\n        webhooks: boolean;\n        addons: boolean;\n    };\n}\n\nexport interface OrganizationSubscription {\n    providerSubscriptionId?: string | null;  // was stripeSubscriptionId\n    providerCustomerId?: string | null;      // was stripeCustomerId\n    // ... other fields\n}\n</code></pre></p>"},{"location":"internal-billing-integration/#api-endpoints","title":"API Endpoints","text":""},{"location":"internal-billing-integration/#always-available-any-billing_provider-value","title":"Always Available (any <code>BILLING_PROVIDER</code> value)","text":"Method Endpoint Description Authentication GET <code>/api/billing/config</code> Active provider and feature flags Required GET <code>/api/billing/health</code> Service health check Required GET <code>/api/billing/plans</code> List available subscription plans Required GET <code>/api/billing/addons</code> List available resource add-ons Required GET <code>/api/billing/organization-subscription</code> Current org subscription and quota data Required (org-admin) GET <code>/api/billing/quota-usage</code> HRQ usage across all projects Required (org-admin) GET <code>/api/billing/quota-status</code> Quota enforcement status Required (org-admin) GET <code>/api/billing/projects</code> List accessible projects with billing data Required GET <code>/api/billing/project/:namespace/overview</code> Project billing details Required + Namespace access"},{"location":"internal-billing-integration/#stripe-provider-only-billing_providerstripe","title":"Stripe Provider Only (<code>BILLING_PROVIDER=stripe</code>)","text":"Method Endpoint Description Authentication POST <code>/api/billing/organization-subscription</code> Create subscription (Stripe Checkout) Required (org-admin) PUT <code>/api/billing/organization-subscription</code> Update subscription (plan change) Required (org-admin) DELETE <code>/api/billing/organization-subscription</code> Cancel subscription Required (org-admin) GET <code>/api/billing/verify-checkout</code> Verify Stripe checkout session Required POST <code>/api/billing/customer-portal</code> Open Stripe customer portal Required (org-admin) POST <code>/api/billing/webhook</code> Stripe webhook handler Stripe signature"},{"location":"internal-billing-integration/#response-format","title":"Response Format","text":"<p>Billing Config Response (<code>GET /api/billing/config</code>) <pre><code>{\n  \"success\": true,\n  \"config\": {\n    \"provider\": \"none\",\n    \"features\": {\n      \"quotas\": true,\n      \"plans\": true,\n      \"checkout\": false,\n      \"portal\": false,\n      \"webhooks\": false,\n      \"addons\": false\n    }\n  }\n}\n</code></pre></p> <p>Organization Subscription Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"organization\": \"my-org\",\n    \"planId\": \"pro-pool\",\n    \"planName\": \"Pro Pool\",\n    \"status\": \"active\",\n    \"providerSubscriptionId\": \"sub_xxx\",\n    \"providerCustomerId\": \"cus_xxx\",\n    \"addons\": [{\"addonId\": \"turbo-x1\", \"quantity\": 1}],\n    \"resources\": { \"cpu\": 10, \"memory\": 28, \"storage\": 180 },\n    \"usage\": {\n      \"cpu\": { \"used\": 4.2, \"limit\": 10 },\n      \"memory\": { \"used\": 12.5, \"limit\": 28 },\n      \"storage\": { \"used\": 45, \"limit\": 180 },\n      \"pods\": { \"used\": 23, \"limit\": 200 }\n    }\n  }\n}\n</code></pre></p>"},{"location":"internal-billing-integration/#security-model","title":"Security Model","text":""},{"location":"internal-billing-integration/#jwt-token-structure","title":"JWT Token Structure","text":"<pre><code>{\n  \"org\": \"organization-name\",\n  \"namespaces\": [\"project-1\", \"project-2\"],\n  \"groups\": [\"org-admin\", \"user\"],\n  \"exp\": 1696176000,\n  \"iat\": 1696089600\n}\n</code></pre>"},{"location":"internal-billing-integration/#authorization-levels","title":"Authorization Levels","text":"Role Access Level Permissions <code>org-admin</code> Organization-wide All projects in organization <code>project-user</code> Project-specific Only assigned namespaces <code>guest</code> No access No billing data access"},{"location":"internal-billing-integration/#security-layers","title":"Security Layers","text":"<ol> <li>OIDC Authentication - External identity provider</li> <li>JWT Token Validation - Backend token verification</li> <li>Namespace Authorization - Per-project access control</li> <li>Network Security - Internal service communication only</li> </ol>"},{"location":"internal-billing-integration/#configuration","title":"Configuration","text":""},{"location":"internal-billing-integration/#environment-variables","title":"Environment Variables","text":"<p>Backend Configuration <pre><code># Billing provider feature flag (required)\nBILLING_PROVIDER=none              # none | stripe | whmcs\n\n# Stripe-specific (only when BILLING_PROVIDER=stripe)\nSTRIPE_SECRET_KEY=sk_xxx           # Stripe API secret key\nSTRIPE_WEBHOOK_SECRET=whsec_xxx    # Stripe webhook signing secret\nSTRIPE_PRICE_DEV_POOL=price_xxx    # Stripe Price ID for Dev Pool plan\nSTRIPE_PRICE_PRO_POOL=price_xxx    # Stripe Price ID for Pro Pool plan\nSTRIPE_PRICE_SCALE_POOL=price_xxx  # Stripe Price ID for Scale Pool plan\nSTRIPE_PRICE_TURBO_X1=price_xxx    # Stripe Price ID for Turbo x1 addon\nSTRIPE_PRICE_TURBO_X2=price_xxx    # Stripe Price ID for Turbo x2 addon\n\n# General\nLOG_LEVEL=info\n</code></pre></p> <p>Frontend Configuration Uses existing Kube-DC ConfigMap pattern. No billing-specific frontend config needed \u2014 the frontend dynamically fetches <code>/api/billing/config</code> at runtime. <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dc-frontend-config\ndata:\n  env.js: |\n    window.backendURL = 'https://backend.stage.kube-dc.com';\n    window.frontendURL = 'https://console.stage.kube-dc.com';\n    window.keycloakURL = 'https://login.stage.kube-dc.com';\n</code></pre></p>"},{"location":"internal-billing-integration/#kubernetes-resources","title":"Kubernetes Resources","text":"<p>The billing system reads from and writes to:</p> Resource Namespace Purpose ConfigMap <code>billing-plans</code> <code>kube-dc</code> Plan definitions, addons, quotas Organization annotations org namespace Subscription state, plan assignment HierarchicalResourceQuota <code>plan-quota</code> org namespace Aggregate resource limits LimitRange <code>default-resource-limits</code> org namespace Default container resources"},{"location":"internal-billing-integration/#error-handling","title":"Error Handling","text":""},{"location":"internal-billing-integration/#http-status-codes","title":"HTTP Status Codes","text":"Code Description Cause 200 Success Request completed successfully 401 Unauthorized Missing or invalid JWT token 403 Forbidden Valid token, insufficient permissions 503 Service Unavailable Billing service unreachable 500 Internal Server Error Unexpected server error"},{"location":"internal-billing-integration/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"success\": false,\n  \"error\": \"Access denied to namespace\",\n  \"details\": {\n    \"namespace\": \"requested-project\",\n    \"availableNamespaces\": [\"project-1\", \"project-2\"]\n  },\n  \"timestamp\": \"2025-10-01T15:25:00.000Z\"\n}\n</code></pre>"},{"location":"internal-billing-integration/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"internal-billing-integration/#log-levels","title":"Log Levels","text":"<ul> <li>INFO: Normal operations, API calls</li> <li>WARN: Authentication failures, permission denials  </li> <li>ERROR: Service errors, network issues</li> <li>DEBUG: Detailed request/response data (development only)</li> </ul>"},{"location":"internal-billing-integration/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li>API response times</li> <li>Authentication failure rates</li> <li>Service availability</li> <li>Error rates by endpoint</li> <li>Namespace access patterns</li> </ul>"},{"location":"internal-billing-integration/#development-guidelines","title":"Development Guidelines","text":""},{"location":"internal-billing-integration/#code-standards","title":"Code Standards","text":"<ul> <li>Follow existing Kube-DC patterns</li> <li>Use PatternFly components for UI consistency</li> <li>Implement proper error handling</li> <li>Add comprehensive logging</li> <li>Write JSDoc comments for public methods</li> </ul>"},{"location":"internal-billing-integration/#testing-approach","title":"Testing Approach","text":"<ul> <li>Unit tests for controller logic</li> <li>Integration tests for API endpoints</li> <li>Frontend component tests</li> <li>End-to-end authentication flows</li> </ul>"},{"location":"internal-billing-integration/#deployment-process","title":"Deployment Process","text":"<ol> <li>Backend changes deployed via Helm chart</li> <li>Frontend changes built into container image</li> <li>Configuration updates via ConfigMaps</li> <li>Rolling deployment with health checks</li> </ol>"},{"location":"internal-billing-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"internal-billing-integration/#common-issues","title":"Common Issues","text":"<p>\"Authentication token required\" - Check JWT token presence in request headers - Verify token format (Bearer scheme) - Confirm OIDC authentication is working</p> <p>\"Access denied to namespace\" - Verify user has access to requested project - Check JWT token namespace claims - Confirm RBAC configuration</p> <p>\"Billing service unreachable\" - Check billing service pod status - Verify network connectivity - Confirm service DNS resolution</p>"},{"location":"internal-billing-integration/#debug-commands","title":"Debug Commands","text":"<pre><code># Check service status\nkubectl get pods -n kube-dc\nkubectl get pods -n billing\n\n# Check service connectivity\nkubectl exec -n kube-dc deployment/kube-dc-backend -- \\\n  curl -v http://billing-dashboard-svc.billing.svc.cluster.local:5000/api/health\n\n# Check logs\nkubectl logs -n kube-dc deployment/kube-dc-backend\nkubectl logs -n billing deployment/billing-dashboard\n</code></pre>"},{"location":"internal-billing-integration/#testing","title":"Testing","text":""},{"location":"internal-billing-integration/#e2e-tests","title":"E2E Tests","text":"<p>Organization quota reconciliation tests are in <code>tests/e2e/organization_quota_test.go</code>:</p> <pre><code># Run all quota tests\ngo test -v ./tests/e2e -ginkgo.focus=\"Organization Quota Reconciliation\" -timeout=15m\n</code></pre> Test What it verifies Org with active plan \u2192 HRQ + LimitRange Controller creates quota resources with correct values Plan change \u2192 HRQ update Changing plan annotation triggers resource update Suspended org \u2192 reduced HRQ Suspended orgs get minimal quota Org deletion \u2192 cleanup HRQ and LimitRange removed on deletion Addons \u2192 HRQ includes addon resources Addon CPU/memory/storage added to base plan No plan \u2192 no HRQ Orgs without billing annotations get no quota"},{"location":"internal-billing-integration/#manual-testing-quota-only-mode","title":"Manual Testing (quota-only mode)","text":"<pre><code># 1. Apply billing-plans ConfigMap\nkubectl apply -f examples/organization/04-billing-plans-configmap.yaml\n\n# 2. Assign a plan to an organization\nkubectl annotate organization/shalb -n shalb \\\n  billing.kube-dc.com/plan-id=dev-pool \\\n  billing.kube-dc.com/subscription=active --overwrite\n\n# 3. Verify HRQ was created\nkubectl get hrq -n shalb\n\n# 4. Verify LimitRange was created\nkubectl get limitrange -n shalb\n\n# 5. Check quota usage\nkubectl describe hrq plan-quota -n shalb\n</code></pre>"},{"location":"internal-billing-integration/#future-enhancements","title":"Future Enhancements","text":""},{"location":"internal-billing-integration/#planned-features","title":"Planned Features","text":"<ul> <li>WHMCS billing provider integration</li> <li>Cost trend analysis and forecasting</li> <li>Budget alerts and notifications</li> <li>Per-project quota management UI</li> <li>Resource optimization recommendations</li> </ul>"},{"location":"internal-billing-integration/#technical-improvements","title":"Technical Improvements","text":"<ul> <li>Plan data caching with ConfigMap watch (replace polling)</li> <li>Real-time quota updates via WebSocket</li> <li>Automated trial-to-paid conversion</li> <li>Multi-currency support</li> </ul> <p>Last updated: February 2026 This document is maintained by the Kube-DC development team.</p>"},{"location":"managing-os-images/","title":"Managing OS Images in Kube-DC","text":"<p>This guide explains how to manage operating system images in the Kube-DC platform, including adding new OS options, modifying existing configurations, and updating the system.</p>"},{"location":"managing-os-images/#overview","title":"Overview","text":"<p>OS images in Kube-DC are configured through a Kubernetes ConfigMap that defines: - Available operating systems in the VM creation UI - Default resource requirements (memory, CPU, storage) - Firmware and virtualization settings - Cloud-init configurations - Image URLs and user credentials</p>"},{"location":"managing-os-images/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Helm Chart    \u2502\u2500\u2500\u2500\u25b6\u2502   ConfigMap      \u2502\u2500\u2500\u2500\u25b6\u2502  Backend API    \u2502\n\u2502   Template      \u2502    \u2502 images-configmap \u2502    \u2502 /os-images      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                       \u2502  Frontend UI    \u2502\n                       \u2502 Create VM Modal \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"managing-os-images/#configmap-structure","title":"ConfigMap Structure","text":"<p>The OS images are defined in <code>/charts/kube-dc/templates/os-images-configmap.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: images-configmap\n  namespace: {{ .Release.Namespace }}\ndata:\n  images.yaml: |\n    images:\n      - OS_NAME: \"Ubuntu 24.04\"\n        CLOUD_USER: ubuntu\n        OS_IMAGE_URL: \"https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\"\n        MIN_MEMORY: \"1G\"\n        MIN_VCPU: \"1\"\n        MIN_STORAGE: \"20G\"\n        FIRMWARE_TYPE: \"bios\"\n        MACHINE_TYPE: \"q35\"\n        FEATURES: \"acpi\"\n        CLOUD_INIT: |\n          #cloud-config\n          package_update: true\n          packages:\n            - qemu-guest-agent\n</code></pre>"},{"location":"managing-os-images/#configuration-fields","title":"Configuration Fields","text":""},{"location":"managing-os-images/#required-fields","title":"Required Fields","text":"Field Description Example <code>OS_NAME</code> Display name in UI dropdown <code>\"Ubuntu 24.04\"</code> <code>CLOUD_USER</code> Default SSH user for the OS <code>ubuntu</code> <code>OS_IMAGE_URL</code> HTTP URL to the disk image <code>https://example.com/image.qcow2</code>"},{"location":"managing-os-images/#resource-requirements","title":"Resource Requirements","text":"Field Description Example Notes <code>MIN_MEMORY</code> Minimum RAM requirement <code>\"8G\"</code>, <code>\"512M\"</code> Supports G/M suffixes <code>MIN_VCPU</code> Minimum CPU cores <code>\"2\"</code> String format <code>MIN_STORAGE</code> Minimum disk size <code>\"60G\"</code> Supports G suffix"},{"location":"managing-os-images/#virtualization-settings","title":"Virtualization Settings","text":"Field Description Options Notes <code>FIRMWARE_TYPE</code> Boot firmware <code>\"bios\"</code>, <code>\"efi\"</code> EFI required for Windows 11 <code>MACHINE_TYPE</code> QEMU machine type <code>\"q35\"</code> Generic q35 recommended for all OS types <code>FEATURES</code> Virtualization features <code>\"acpi\"</code>, <code>\"hyperv,acpi,apic,smm,tpm\"</code> Comma-separated list"},{"location":"managing-os-images/#supported-features","title":"Supported Features","text":"<ul> <li><code>acpi</code> - Advanced Configuration and Power Interface</li> <li><code>apic</code> - Advanced Programmable Interrupt Controller  </li> <li><code>hyperv</code> - Microsoft Hyper-V enlightenments</li> <li><code>smm</code> - System Management Mode</li> <li><code>tpm</code> - Trusted Platform Module (required for Windows 11)</li> </ul>"},{"location":"managing-os-images/#os-specific-configurations","title":"OS-Specific Configurations","text":""},{"location":"managing-os-images/#linux-distributions","title":"Linux Distributions","text":"<p>Ubuntu/Debian: <pre><code>- OS_NAME: \"Ubuntu 24.04\"\n  CLOUD_USER: ubuntu\n  MIN_MEMORY: \"1G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"20G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n</code></pre></p> <p>CentOS/RHEL: <pre><code>- OS_NAME: \"CentOS Stream 9\"\n  CLOUD_USER: centos\n  MIN_MEMORY: \"2G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"20G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n</code></pre></p>"},{"location":"managing-os-images/#windows-systems","title":"Windows Systems","text":"<p>Windows 11: <pre><code>- OS_NAME: \"Windows 11 Enterprise\"\n  CLOUD_USER: Administrator\n  MIN_MEMORY: \"8G\"\n  MIN_VCPU: \"4\"\n  MIN_STORAGE: \"60G\"\n  FIRMWARE_TYPE: \"efi\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"hyperv,acpi,apic,smm,tpm\"\n</code></pre></p>"},{"location":"managing-os-images/#adding-a-new-os-image","title":"Adding a New OS Image","text":""},{"location":"managing-os-images/#step-1-prepare-the-image","title":"Step 1: Prepare the Image","text":"<ol> <li>Obtain the disk image (qcow2, raw, or vmdk format)</li> <li>Host the image on an HTTP server accessible to your cluster</li> <li>Test the image to ensure it boots correctly</li> </ol>"},{"location":"managing-os-images/#step-2-update-the-configmap","title":"Step 2: Update the ConfigMap","text":"<p>Edit <code>/charts/kube-dc/templates/os-images-configmap.yaml</code>:</p> <pre><code># Add your new OS entry\n- OS_NAME: \"Fedora 40\"\n  CLOUD_USER: fedora\n  OS_IMAGE_URL: \"https://download.fedoraproject.org/pub/fedora/linux/releases/40/Cloud/x86_64/images/Fedora-Cloud-Base-40-1.14.x86_64.qcow2\"\n  MIN_MEMORY: \"2G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"25G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n  CLOUD_INIT: |\n    #cloud-config\n    package_update: true\n    packages:\n      - qemu-guest-agent\n    runcmd:\n      - systemctl enable --now qemu-guest-agent\n</code></pre>"},{"location":"managing-os-images/#step-3-deploy-the-changes","title":"Step 3: Deploy the Changes","text":"<p>Option A: Helm Upgrade (Recommended) <pre><code># From the project root\nhelm upgrade kube-dc ./charts/kube-dc -n kube-dc\n</code></pre></p> <p>Option B: Direct ConfigMap Update <pre><code># Apply the ConfigMap directly\nkubectl apply -f charts/kube-dc/templates/os-images-configmap.yaml\n</code></pre></p>"},{"location":"managing-os-images/#step-4-reload-the-backend","title":"Step 4: Reload the Backend","text":"<p>The backend caches OS images for performance. After updating the ConfigMap:</p> <pre><code># Restart the backend to reload the cache\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n\n# Or wait for the cache TTL (30 seconds) to expire\n</code></pre>"},{"location":"managing-os-images/#modifying-existing-os-images","title":"Modifying Existing OS Images","text":""},{"location":"managing-os-images/#inline-editing","title":"Inline Editing","text":"<p>You can modify the ConfigMap directly in Kubernetes:</p> <pre><code># Edit the ConfigMap in your cluster\nkubectl edit configmap images-configmap -n kube-dc\n</code></pre> <p>Example: Increase Windows 11 memory requirement: <pre><code># Change from:\nMIN_MEMORY: \"8G\"\n# To:\nMIN_MEMORY: \"16G\"\n</code></pre></p> <p>After saving, restart the backend: <pre><code>kubectl rollout restart deployment/kube-dc-backend -n kube-dc\n</code></pre></p>"},{"location":"managing-os-images/#updating-image-urls","title":"Updating Image URLs","text":"<p>If an image URL changes or becomes unavailable:</p> <ol> <li> <p>Update the ConfigMap: <pre><code># Old URL\nOS_IMAGE_URL: \"https://old-server.com/ubuntu-24.04.qcow2\"\n# New URL  \nOS_IMAGE_URL: \"https://new-server.com/ubuntu-24.04.qcow2\"\n</code></pre></p> </li> <li> <p>Apply changes: <pre><code>kubectl apply -f charts/kube-dc/templates/os-images-configmap.yaml\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n</code></pre></p> </li> </ol>"},{"location":"managing-os-images/#testing-changes","title":"Testing Changes","text":""},{"location":"managing-os-images/#verify-configmap-update","title":"Verify ConfigMap Update","text":"<pre><code># Check the ConfigMap was updated\nkubectl get configmap images-configmap -n kube-dc -o yaml\n\n# Test the API endpoint\ncurl -s \"https://backend.stage.kube-dc.com/api/create-vm/your-namespace/os-images\" | jq '.[].OS_NAME'\n</code></pre>"},{"location":"managing-os-images/#test-in-ui","title":"Test in UI","text":"<ol> <li>Open the Kube-DC web interface</li> <li>Navigate to Create VM</li> <li>Check the Operation System dropdown</li> <li>Verify your new OS appears with correct parameters</li> <li>Select the OS and confirm memory/CPU/storage auto-populate</li> </ol>"},{"location":"managing-os-images/#troubleshooting","title":"Troubleshooting","text":""},{"location":"managing-os-images/#os-not-appearing-in-ui","title":"OS Not Appearing in UI","text":"<p>Check the ConfigMap: <pre><code>kubectl describe configmap images-configmap -n kube-dc\n</code></pre></p> <p>Verify backend logs: <pre><code>kubectl logs -n kube-dc deployment/kube-dc-backend --tail=50\n</code></pre></p> <p>Common issues: - YAML syntax errors in ConfigMap - Backend cache not refreshed - Network connectivity to image URL</p>"},{"location":"managing-os-images/#vm-creation-fails","title":"VM Creation Fails","text":"<p>Check image accessibility: <pre><code># Test if the image URL is reachable\ncurl -I \"https://your-image-url.com/image.qcow2\"\n</code></pre></p> <p>Verify resource requirements: - Ensure cluster has sufficient resources - Check storage class availability - Verify network policies allow image downloads</p>"},{"location":"managing-os-images/#backend-cache-issues","title":"Backend Cache Issues","text":"<p>The backend caches OS images for 30 seconds. To force refresh:</p> <pre><code># Restart backend pods\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n\n# Or wait for cache expiration (30 seconds)\n</code></pre>"},{"location":"managing-os-images/#best-practices","title":"Best Practices","text":""},{"location":"managing-os-images/#image-management","title":"Image Management","text":"<ol> <li>Use stable URLs - Avoid URLs that change frequently</li> <li>Host images reliably - Use CDNs or reliable hosting</li> <li>Test images - Verify images boot before adding to production</li> <li>Document changes - Keep track of image versions and changes</li> </ol>"},{"location":"managing-os-images/#resource-requirements_1","title":"Resource Requirements","text":"<ol> <li>Set realistic minimums - Don't under-provision resources</li> <li>Consider workload - Different use cases need different resources  </li> <li>Test performance - Verify VMs perform well with set resources</li> </ol>"},{"location":"managing-os-images/#security","title":"Security","text":"<ol> <li>Verify image sources - Only use trusted image providers</li> <li>Scan images - Check for vulnerabilities before deployment</li> <li>Use HTTPS - Always use secure URLs for image downloads</li> <li>Regular updates - Keep OS images updated with security patches</li> </ol>"},{"location":"managing-os-images/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"managing-os-images/#custom-cloud-init","title":"Custom Cloud-Init","text":"<p>For complex initialization requirements:</p> <pre><code>CLOUD_INIT: |\n  #cloud-config\n  users:\n    - name: admin\n      groups: sudo\n      shell: /bin/bash\n      sudo: ALL=(ALL) NOPASSWD:ALL\n  packages:\n    - docker.io\n    - nginx\n  runcmd:\n    - systemctl enable docker\n    - systemctl start docker\n    - docker run -d -p 80:80 nginx\n</code></pre>"},{"location":"managing-os-images/#windows-specific-settings","title":"Windows-Specific Settings","text":"<p>For Windows VMs, additional configuration may be needed:</p> <pre><code># Windows Server 2022\n- OS_NAME: \"Windows Server 2022\"\n  CLOUD_USER: Administrator\n  MIN_MEMORY: \"4G\"\n  MIN_VCPU: \"2\"\n  MIN_STORAGE: \"80G\"\n  FIRMWARE_TYPE: \"efi\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"hyperv,acpi,apic,smm,tpm\"\n  BOOT_ORDER: \"cdrom,disk\"\n  ADDITIONAL_DISKS: \"virtio-drivers\"\n</code></pre>"},{"location":"managing-os-images/#api-reference","title":"API Reference","text":"<p>The OS images are served via the backend API:</p> <p>Endpoint: <code>GET /api/create-vm/{namespace}/os-images</code></p> <p>Response format: <pre><code>[\n  {\n    \"OS_NAME\": \"Ubuntu 24.04\",\n    \"CLOUD_USER\": \"ubuntu\",\n    \"OS_IMAGE_URL\": \"https://cloud-images.ubuntu.com/...\",\n    \"MIN_MEMORY\": \"1G\",\n    \"MIN_VCPU\": \"1\",\n    \"MIN_STORAGE\": \"20G\",\n    \"FIRMWARE_TYPE\": \"bios\",\n    \"MACHINE_TYPE\": \"q35\",\n    \"FEATURES\": \"acpi\",\n    \"CLOUD_INIT\": \"#cloud-config\\n...\"\n  }\n]\n</code></pre></p>"},{"location":"managing-os-images/#support","title":"Support","text":"<p>For additional help: - Check the Kube-DC documentation - Review troubleshooting guides - Open an issue in the project repository</p>"},{"location":"product-backlog/","title":"Kube-DC Product Backlog","text":"<p>This document outlines the current product backlog for the Kube-DC project, organized by epics and features.</p>"},{"location":"product-backlog/#active-epics","title":"\ud83d\ude80 Active Epics","text":""},{"location":"product-backlog/#epic-windows-support","title":"[Epic] Windows Support","text":"<p>Status: Done</p>"},{"location":"product-backlog/#epic-vmware-migration","title":"[Epic] VMware Migration","text":"<p>Status: Research Phase</p> <ul> <li>\ud83d\udd0d VMware vSphere migration research (CDI, vjailbreak)</li> <li>Investigate migration tools and methodologies</li> <li>Evaluate CDI (Containerized Data Importer) for VM migration</li> <li>Research vjailbreak and other migration utilities</li> </ul>"},{"location":"product-backlog/#epic-organization-management","title":"[Epic] Organization Management","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udccb UI for project and roles</li> <li>Implement project management interface </li> <li>Role-based access control UI components</li> <li> <p>User and group management interfaces</p> </li> <li> <p>\ud83c\udfa8 Customize Login Page</p> </li> <li>Branding and customization options</li> <li>Organization-specific login themes</li> </ul>"},{"location":"product-backlog/#epic-ui-implementation-on-backend","title":"[Epic] UI Implementation on Backend","text":"<p>Status: Multiple Items in Progress</p> <ul> <li>\u274c UI Clone Disks - Not working</li> <li>Fix disk cloning functionality in UI</li> <li> <p>Ensure proper CDI integration</p> </li> <li> <p>\ud83d\udccb UI Create VM from PVC/DataVolume</p> </li> <li>Interface for VM creation from existing storage</li> <li> <p>DataVolume selection and configuration</p> </li> <li> <p>\ud83c\udf10 UI Add VM Static IP</p> </li> <li>Static IP assignment interface</li> <li> <p>Network configuration management</p> </li> <li> <p>\ud83c\udf10 UI Add VM FIP (Floating IP)</p> </li> <li>Floating IP assignment and management</li> <li> <p>Integration with FIP CRD resources</p> </li> <li> <p>\u2696\ufe0f UI Add Load Balancer Setup</p> </li> <li>Load balancer configuration interface</li> <li> <p>Service exposure management</p> </li> <li> <p>\ud83d\udd04 UI Migrate/Clone VM (rook/ceph)</p> </li> <li>VM migration interface with Rook/Ceph backend</li> <li> <p>Live migration capabilities</p> </li> <li> <p>\ud83d\udc65 UI VM Groups</p> </li> <li>VM grouping and management features</li> <li>Bulk operations on VM groups</li> </ul>"},{"location":"product-backlog/#epic-installer","title":"[Epic] Installer","text":"<p>Status: Enhancement Phase</p> <ul> <li>\ud83d\uddc4\ufe0f Postgres DB for Keycloak and Billing to be dedicated in installer stack</li> <li>Separate PostgreSQL deployment for Keycloak</li> <li> <p>Database isolation and management</p> </li> <li> <p>\u26a1 Simplify Installer</p> </li> <li>Streamline installation process</li> <li> <p>Reduce complexity and dependencies</p> </li> <li> <p>\ud83d\udda5\ufe0f Single host install</p> </li> <li>Support for single-node deployments</li> <li> <p>All-in-one installation option</p> </li> <li> <p>\ud83d\udd27 Test on VMware vSX</p> </li> <li>Validation on VMware infrastructure</li> <li> <p>Compatibility testing and documentation</p> </li> <li> <p>\ud83d\udd10 Fix hardcoded passwords in loki.yaml</p> </li> <li>Security improvement for Loki configuration</li> <li>Dynamic password generation</li> </ul>"},{"location":"product-backlog/#epic-licensing","title":"[Epic] Licensing","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udcc4 License for Node Limits per installation</li> <li>Implement licensing system</li> <li>Node-based licensing model</li> <li>License validation and enforcement</li> </ul>"},{"location":"product-backlog/#epic-billing-quota-management","title":"[Epic] Billing &amp; Quota Management","text":"<p>Status: In Progress</p> <ul> <li>\u2705 Organization quota enforcement (HRQ + LimitRange + EIP)</li> <li>Plans defined in <code>billing-plans</code> ConfigMap</li> <li>HierarchicalResourceQuota auto-created per org</li> <li>LimitRange propagated via HNC to project namespaces</li> <li>EIP quota per plan, S3 object storage quota via Rook-Ceph</li> <li>Subscription lifecycle: active \u2192 suspended \u2192 canceled with grace period</li> <li>Addons (turbo resource packs) support</li> <li> <p>E2E tests for all quota scenarios</p> </li> <li> <p>\u2705 Billing provider decoupling (<code>BILLING_PROVIDER</code> feature flag)</p> </li> <li><code>none</code> (default): quota-only, plans assigned via kubectl annotations</li> <li><code>stripe</code>: full Stripe integration (checkout, webhooks, portal)</li> <li> <p>Frontend dynamically adapts UI based on <code>/api/billing/config</code></p> </li> <li> <p>\u2705 Stripe integration</p> </li> <li>Checkout sessions, subscription CRUD, customer portal</li> <li>Webhook handling for payment events</li> <li> <p>Isolated in <code>providers/stripe.js</code> (not loaded when <code>BILLING_PROVIDER=none</code>)</p> </li> <li> <p>\ud83d\udd32 WHMCS integration (planned)</p> </li> <li>\ud83d\udd32 Per-project quota management UI</li> <li>\ud83d\udd32 Cost trend analysis and budget alerts</li> </ul>"},{"location":"product-backlog/#epic-observability","title":"[Epic] Observability","text":"<p>Status: Enhancement Phase</p> <ul> <li>\ud83d\udcca Logs</li> <li>Centralized logging improvements</li> <li> <p>Log aggregation and analysis</p> </li> <li> <p>\ud83d\udcc8 Metrics</p> </li> <li>Enhanced monitoring and metrics collection</li> <li> <p>Performance dashboards</p> </li> <li> <p>\ud83d\udea8 Alerts</p> </li> <li>Alerting system implementation</li> <li>Notification and escalation policies</li> </ul>"},{"location":"product-backlog/#epic-gpu-support","title":"[Epic] GPU Support","text":"<p>Status: Research Phase</p> <ul> <li>\ud83c\udfae Evaluate Project HAMI</li> <li>GPU sharing and management solution</li> <li>Integration assessment with KubeVirt</li> </ul>"},{"location":"product-backlog/#epic-managed-services","title":"[Epic] Managed Services","text":"<p>Status: Planning</p> <ul> <li>\u2638\ufe0f K8s CAPI (Cluster API)</li> <li>Kubernetes cluster management</li> <li> <p>Multi-cluster operations</p> </li> <li> <p>\u2638\ufe0f K8s Vcluster</p> </li> <li>Virtual cluster implementation</li> <li> <p>Tenant isolation improvements</p> </li> <li> <p>\ud83d\uddc4\ufe0f Rook S3</p> </li> <li>Object storage services</li> <li> <p>S3-compatible storage backend</p> </li> <li> <p>\ud83d\uddc4\ufe0f RDS Percona Operators</p> </li> <li>Database-as-a-Service implementation</li> <li>MySQL/PostgreSQL managed services</li> </ul>"},{"location":"product-backlog/#epic-kubevirt-enhancements","title":"[Epic] KubeVirt Enhancements","text":"<p>Status: High Priority</p> <ul> <li>\ud83d\udd04 CDI Cloning (Priority)</li> <li>VM disk cloning capabilities</li> <li> <p>Efficient storage management</p> </li> <li> <p>\ud83d\udd25 CPU, Memory, GPU Hotplug</p> </li> <li>Dynamic resource allocation</li> <li>Live resource scaling for VMs</li> </ul>"},{"location":"product-backlog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":""},{"location":"product-backlog/#critical-bugs","title":"Critical Bugs","text":"<ul> <li>\ud83d\udd27 UI get_kubeconfig.sh namespace issue</li> <li>Fix namespace handling in kubeconfig generation</li> <li> <p>Ensure proper authentication and authorization</p> </li> <li> <p>\ud83e\uddf9 Fix issue with stale jobs with pshell</p> </li> <li>Clean up orphaned pshell jobs</li> <li>Improve job lifecycle management</li> </ul>"},{"location":"product-backlog/#priority-matrix","title":"\ud83d\udcca Priority Matrix","text":""},{"location":"product-backlog/#high-priority","title":"High Priority","text":"<ol> <li>CDI Cloning functionality</li> <li>Windows metrics fixes</li> <li>UI disk cloning repairs</li> <li>Critical bug fixes (kubeconfig, pshell jobs)</li> </ol>"},{"location":"product-backlog/#medium-priority","title":"Medium Priority","text":"<ol> <li>VMware migration research</li> <li>GPU support evaluation</li> <li>Installer simplification</li> <li>Observability enhancements</li> </ol>"},{"location":"product-backlog/#low-priority","title":"Low Priority","text":"<ol> <li>Licensing system</li> <li>Billing metering and usage reports</li> <li>Managed services expansion</li> <li>UI enhancements (VM groups, static IP)</li> </ol> <p>Last Updated: February 2026 Document Owner: Kube-DC Product Team</p>"},{"location":"project_resources/","title":"Project Resources Documentation","text":"<p>This document provides a comprehensive overview of all resources created by the Kube-DC Project controller, their finalizers, ownership patterns, and deletion dependencies.</p>"},{"location":"project_resources/#resource-creation-order","title":"Resource Creation Order","text":"<p>When a Project is created, resources are synchronized in this order:</p> <ol> <li>Namespace - Project namespace (<code>{org}-{project}</code>)</li> <li>VPC - Kube-OVN Virtual Private Cloud</li> <li>EIp (Default Gateway) - External IP for project gateway</li> <li>Subnet - Kube-OVN subnet for project pods</li> <li>NetworkAttachmentDefinition - CNI network configuration</li> <li>OvnSnatRule - SNAT rule for outbound traffic</li> <li>Secrets - SSH keypairs and authorized keys</li> <li>RBAC - Roles and RoleBindings</li> <li>VpcDns - DNS configuration for VPC</li> </ol>"},{"location":"project_resources/#detailed-resource-breakdown","title":"Detailed Resource Breakdown","text":""},{"location":"project_resources/#1-namespace","title":"1. Namespace","text":"<ul> <li>Resource: <code>v1.Namespace</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: None (managed by Kubernetes)</li> <li>Created by: <code>NewProjectNamespace()</code> in <code>internal/project/res_namespace.go</code></li> <li>Dependencies: None (first resource created)</li> </ul>"},{"location":"project_resources/#2-vpc-virtual-private-cloud","title":"2. VPC (Virtual Private Cloud)","text":"<ul> <li>Resource: <code>kubeovn.io/v1.Vpc</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectVpc()</code> in <code>internal/project/res_vpc.go</code></li> <li>Dependencies: Namespace must exist</li> <li> <p>Configuration:</p> </li> <li> <p>Static routes to external subnets</p> </li> <li>Extra external subnets based on <code>egressNetworkType</code></li> </ul>"},{"location":"project_resources/#3-eip-external-ip-default-gateway","title":"3. EIp (External IP - Default Gateway)","text":"<ul> <li>Resource: <code>kube-dc.com/v1.EIp</code></li> <li>Name: <code>default-gw</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>eip.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectEip()</code> in <code>internal/project/res_eip_default.go</code></li> <li>Dependencies: Namespace must exist</li> <li> <p>Ownership States:</p> </li> <li> <p><code>Released</code>: No active owners (initial state)</p> </li> <li><code>Shared</code>: Has SNAT rule and/or LoadBalancer services as owners</li> <li><code>Exclusive</code>: Used by FIp resources</li> </ul>"},{"location":"project_resources/#4-ovneip-underlying-ovn-external-ip","title":"4. OvnEip (Underlying OVN External IP)","text":"<ul> <li>Resource: <code>kubeovn.io/v1.OvnEip</code></li> <li>Name: <code>{organization}-{project}-{external-subnet}</code> (e.g., <code>shalb-envoy-ext-public</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: EIp controller via <code>NewOvEipRes()</code> in <code>internal/eip/ovn_eip_res.go</code></li> <li>Dependencies: EIp must exist, external subnet must be available</li> <li> <p>Labels:</p> </li> <li> <p><code>network.kube-dc.com/eip</code>: <code>{namespace}.{eip-name}</code></p> </li> <li> <p>Annotations:</p> </li> <li> <p><code>kube-dc.com/ovn-eip-created-by-eip</code>: <code>{namespace}.{eip-name}</code></p> </li> </ul>"},{"location":"project_resources/#5-subnet","title":"5. Subnet","text":"<ul> <li>Resource: <code>kubeovn.io/v1.Subnet</code></li> <li>Name: <code>{organization}-{project}-default</code> (e.g., <code>shalb-envoy-default</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectSubnet()</code> in <code>internal/project/res_subnet.go</code></li> <li>Dependencies: VPC must exist</li> <li> <p>Configuration:</p> </li> <li> <p>CIDR block from project spec</p> </li> <li>Associated with project VPC</li> <li>Gateway IP (first IP in CIDR)</li> </ul>"},{"location":"project_resources/#6-networkattachmentdefinition","title":"6. NetworkAttachmentDefinition","text":"<ul> <li>Resource: <code>k8s.cni.cncf.io/v1.NetworkAttachmentDefinition</code></li> <li>Name: <code>default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectNad()</code> in <code>internal/project/res_nad.go</code></li> <li>Dependencies: Subnet must exist</li> <li>Configuration: Kube-OVN CNI configuration pointing to project subnet</li> </ul>"},{"location":"project_resources/#7-ovnsnatrule","title":"7. OvnSnatRule","text":"<ul> <li>Resource: <code>kubeovn.io/v1.OvnSnatRule</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectSnat()</code> in <code>internal/project/res_snat.go</code></li> <li>Dependencies: OvnEip must exist and have IP assigned</li> <li> <p>Configuration:</p> </li> <li> <p>Links project subnet to external IP</p> </li> <li>Enables outbound internet access for pods</li> </ul>"},{"location":"project_resources/#8-secrets","title":"8. Secrets","text":""},{"location":"project_resources/#ssh-key-pair-secret","title":"SSH Key Pair Secret","text":"<ul> <li>Resource: <code>v1.Secret</code></li> <li>Name: <code>ssh-keypair-default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectKeyPairSeret()</code> in <code>internal/project/res_secret.go</code></li> <li>Content: Generated SSH public/private key pair</li> </ul>"},{"location":"project_resources/#authorized-keys-secret","title":"Authorized Keys Secret","text":"<ul> <li>Resource: <code>v1.Secret</code></li> <li>Name: <code>authorized-keys-default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectAuthKeySecret()</code> in <code>internal/project/res_secret.go</code></li> <li>Content: SSH public keys for VM access</li> </ul>"},{"location":"project_resources/#9-rbac-resources","title":"9. RBAC Resources","text":""},{"location":"project_resources/#role","title":"Role","text":"<ul> <li>Resource: <code>rbac.authorization.k8s.io/v1.Role</code></li> <li>Name: <code>admin</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectRole()</code> in <code>internal/project/res_role.go</code></li> <li>Permissions: Full access to project resources (pods, services, VMs, etc.)</li> </ul>"},{"location":"project_resources/#rolebinding","title":"RoleBinding","text":"<ul> <li>Resource: <code>rbac.authorization.k8s.io/v1.RoleBinding</code></li> <li>Name: <code>org-admin</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectRoleBinding()</code> in <code>internal/project/res_role_binding.go</code></li> <li>Subject: <code>{organization}:org-admin</code> group</li> </ul>"},{"location":"project_resources/#10-vpcdns-service","title":"10. VpcDns Service","text":"<ul> <li>Resource: <code>v1.Service</code></li> <li>Name: <code>slr-vpc-dns-{organization}-{project}</code></li> <li>Namespace: <code>kube-system</code></li> <li>Finalizer: None (managed by service controller)</li> <li>Created by: <code>NewProjectVpcDns()</code> in <code>internal/project/res_vpc_dns.go</code></li> <li>Purpose: DNS resolution for VPC</li> </ul>"},{"location":"project_resources/#finalizers-summary","title":"Finalizers Summary","text":"Resource Type Finalizer Controller Project <code>project.kube-dc.com/finalizer</code> kube-dc-manager EIp <code>eip.kube-dc.com/finalizer</code> kube-dc-manager OvnEip <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller Vpc <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller Subnet <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller OvnSnatRule <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller NetworkAttachmentDefinition <code>project.kube-dc.com/finalizer</code> kube-dc-manager Secrets <code>project.kube-dc.com/finalizer</code> kube-dc-manager Role <code>project.kube-dc.com/finalizer</code> kube-dc-manager RoleBinding <code>project.kube-dc.com/finalizer</code> kube-dc-manager"},{"location":"project_resources/#deletion-order-and-dependencies","title":"Deletion Order and Dependencies","text":"<p>When a Project is deleted, resources must be removed in reverse dependency order:</p>"},{"location":"project_resources/#phase-1-application-resources","title":"Phase 1: Application Resources","text":"<ol> <li>Pods, Services, VMs - User workloads (deleted by users/operators)</li> <li>FIp resources - Floating IPs (if any exist)</li> </ol>"},{"location":"project_resources/#phase-2-snat-and-networking","title":"Phase 2: SNAT and Networking","text":"<ol> <li>OvnSnatRule - Must be deleted before OvnEip</li> <li>OvnEip - Must be deleted before EIp and Subnet</li> </ol>"},{"location":"project_resources/#phase-3-project-infrastructure","title":"Phase 3: Project Infrastructure","text":"<ol> <li>EIp - External IP resource</li> <li>NetworkAttachmentDefinition - CNI configuration</li> <li>Secrets - SSH keys and authorized keys</li> <li>RBAC - Roles and RoleBindings</li> <li>Subnet - Must be deleted before VPC</li> <li>VPC - Virtual Private Cloud</li> <li>VpcDns - DNS service</li> <li>Namespace - Project namespace (last)</li> </ol>"},{"location":"project_resources/#common-deletion-issues","title":"Common Deletion Issues","text":""},{"location":"project_resources/#stuck-finalizers","title":"Stuck Finalizers","text":"<ul> <li>OvnEip: May get stuck if SNAT rule deletion fails</li> <li>Subnet: May get stuck if pods are still running</li> <li>EIp: May get stuck if project is deleted before EIp controller processes it</li> </ul>"},{"location":"project_resources/#manual-cleanup-commands","title":"Manual Cleanup Commands","text":"<pre><code># Remove stuck finalizers (use with caution)\nkubectl patch ovn-snat-rule {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch ovn-eip {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch subnet {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch eip {name} -n {namespace} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch project {name} -n {org-namespace} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\n</code></pre>"},{"location":"project_resources/#eip-ownership-patterns","title":"EIp Ownership Patterns","text":""},{"location":"project_resources/#ownership-states","title":"Ownership States","text":"<ul> <li>Released: <code>ownershipType: Released</code>, <code>owners: []</code> - No active users</li> <li>Shared: <code>ownershipType: Shared</code>, <code>owners: [...]</code> - Multiple users (SNAT + Services)</li> <li>Exclusive: <code>ownershipType: Exclusive</code>, <code>owners: [single]</code> - Single FIp owner</li> </ul>"},{"location":"project_resources/#owner-types","title":"Owner Types","text":"<ul> <li><code>Snat</code>: SNAT rule using the EIP for outbound traffic</li> <li><code>ServiceLb</code>: LoadBalancer service using the EIP</li> <li><code>FIp</code>: Floating IP using the EIP exclusively</li> </ul>"},{"location":"project_resources/#ownership-transitions","title":"Ownership Transitions","text":"<pre><code>Released \u2192 Shared (first owner added)\nShared \u2192 Released (last owner removed)\nReleased \u2192 Exclusive (FIp claims EIP)\nExclusive \u2192 Released (FIp releases EIP)\n</code></pre>"},{"location":"project_resources/#organization-limits","title":"Organization Limits","text":"<p>Organizations have a configurable limit on the number of ready projects they can contain (default: 3). This limit is enforced by the Project controller during reconciliation.</p> <ul> <li>Configuration: Set via <code>MasterConfig.OrganizationProjectsLimit</code></li> <li>Enforcement: Projects exceeding the limit will not be reconciled until space becomes available</li> <li>Status: Projects blocked by limits show as not ready but remain in the cluster</li> </ul>"},{"location":"project_resources/#enhanced-limit-enforcement-v0131-dev1","title":"Enhanced Limit Enforcement (v0.1.31-dev1+)","text":"<p>The Project controller now provides comprehensive feedback when organization limits are hit:</p> <p>Detailed Logging: - Organization project status with ready/pending counts and project names - Clear error messages with organization, namespace, and limit context - Debug-level logs showing available slots and project lists</p> <p>Status Conditions: Projects blocked by limits receive a <code>LimitCheck</code> condition: <pre><code>status:\n  ready: false\n  conditions:\n  - type: LimitCheck\n    status: \"False\"\n    reason: LimitExceeded\n    message: \"organization limit (3 projects) reached - ready projects: 3 (limit: 3)\"\n    lastTransitionTime: \"2025-01-19T16:45:00Z\"\n</code></pre></p> <p>Automatic Retry: - Projects are automatically requeued every 30 seconds - Reconciliation proceeds when limit space becomes available - No manual intervention required</p> <p>Projects can use different external network types: - cloud: Uses <code>ext-cloud</code> subnet (default) - public: Uses <code>ext-public</code> subnet (real public IPs)</p> <p>The <code>egressNetworkType</code> in Project spec determines which external subnet is used for the default gateway EIP.</p>"},{"location":"quickstart-hetzner/","title":"Master-Worker Setup on Hetzner Dedicated Servers","text":"<p>This guide provides step-by-step instructions for deploying a Kube-DC cluster with a master and worker node setup on Hetzner Dedicated Servers. This deployment leverages Hetzner's vSwitch and additional subnets to provide enterprise-grade networking capabilities for floating IPs and load balancers.</p>"},{"location":"quickstart-hetzner/#prerequisites","title":"Prerequisites","text":"<ol> <li>At least two Hetzner Dedicated Servers</li> <li>Access to Hetzner Robot interface</li> <li>A Hetzner vSwitch configured for your servers (see Hetzner vSwitch documentation)</li> <li>An additional subnet allocated through Hetzner Robot for external IPs and load balancers</li> <li>Wildcard domain ex: *.dev.kube-dc.com shoud be set to main public ip of master node.</li> </ol>"},{"location":"quickstart-hetzner/#server-configuration","title":"Server Configuration","text":""},{"location":"quickstart-hetzner/#1-prepare-servers","title":"1. Prepare Servers","text":"<p>Ensure your Hetzner Dedicated Servers meet these minimum requirements: - Master Node: 4+ CPU cores, 16+ GB RAM - Worker Node: 4+ CPU cores, 16+ GB RAM</p> <p>Install Ubuntu 24.04 LTS on all servers through the Hetzner Robot interface.</p>"},{"location":"quickstart-hetzner/#2-configure-vswitch","title":"2. Configure vSwitch","text":"<p>In the Hetzner Robot interface:</p> <ol> <li>Create a vSwitch if you don't have one already</li> <li>Add your servers to the vSwitch   </li> <li>Request an additional subnet to be used for external IPs (Floating IPs)</li> <li>Assign the subnet to your vSwitch:   </li> </ol> <p>You will get two vlan ids, one for the local network(in example 4012) and one for the external subnet with public ips(in example 4011).</p>"},{"location":"quickstart-hetzner/#network-configuration","title":"Network Configuration","text":""},{"location":"quickstart-hetzner/#1-configure-network-interfaces","title":"1. Configure Network Interfaces","text":"<p>SSH into each server and configure the networking using Netplan. Backup default netplan config: <pre><code>mkdir /root/tmp/\nmv /etc/netplan/*.yaml /root/tmp/\n</code></pre> Create new config(<code>/etc/netplan/60-kube-dc.yaml</code>) Replace values with <code>example</code> by values from default file(see it in <code>/root/tmp/</code>):</p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s31f6_example:  # Primary network interface name (get it from default netplan config)\n      addresses:\n        - 22.22.22.2_example/24  # Primary IP address and subnet mask (get it from default netplan config)\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 22.22.22.1_example  # Gateway IP address (get it from default netplan config)\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      routing-policy:\n        - from: 22.22.22.2_example  # Source-based routing for traffic from gateway (Primary IP)\n          table: 100  # Custom routing table ID\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    enp0s31f6.4012_example:  # VLAN interface name (format: interface.vlan_id, see your VLAN in https://robot.hetzner.com/vswitch/index)\n      id: 4012_example  # VLAN ID (must match your Hetzner vSwitch ID, same vlan_id)\n      link: enp0s31f6_example  # Parent interface for VLAN (same interface from default netplan config)\n      mtu: 1460  # Maximum Transmission Unit size\n      addresses:\n        - 192.168.100.2/22  # Master node IP on private network (This for master node setup)\n       #- 192.168.100.3/22  # Worker node IP                    (This for master node setup)\n</code></pre> <p>Apply the configuration:</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"quickstart-hetzner/#2-system-optimization","title":"2. System Optimization","text":"<p>Downgrade kernel (due to a bug in kernel https://github.com/k3s-io/k3s/issues/11175):</p> <pre><code>sudo apt -y update\nsudo apt install linux-image-6.8.0-52-generic linux-headers-6.8.0-52-generic\n# Remove previous kernel\nsudo apt-get remove --purge linux-image-6.8.0-58-generic linux-headers-6.8.0-58-generic\n# Reboot\nsudo reboot\n</code></pre> <p>On all nodes, update, upgrade, and install required software:</p> <pre><code>sudo apt -y install unzip iptables linux-headers-$(uname -r)\n</code></pre> <p>Update to the latest kernel version:</p> <pre><code>sudo apt -y install linux-generic\nsudo reboot\n</code></pre> <p>After the server reboots, verify your kernel version:</p> <pre><code>uname -r\n</code></pre> <p>Optimize system settings by adding to <code>/etc/sysctl.conf</code>:</p> <pre><code># Increase inotify limits\nfs.inotify.max_user_watches=1524288\nfs.inotify.max_user_instances=4024\n\n# Enable packet forwarding\nnet.ipv4.ip_forward = 1\n</code></pre> <p>Ensure the nf_conntrack module is loaded:</p> <pre><code># Check if the module is loaded\nlsmod | grep nf_conntrack\n\n# If not loaded, load it manually\nsudo modprobe nf_conntrack\n\n# To ensure it's loaded on boot, add it to /etc/modules\necho \"nf_conntrack\" | sudo tee -a /etc/modules\n</code></pre> <p>Apply the changes:</p> <pre><code>sudo sysctl -p\n</code></pre> <p>Disable systemd-resolved to prevent DNS conflicts:</p> <pre><code>sudo systemctl stop systemd-resolved\nsudo systemctl disable systemd-resolved\nsudo rm /etc/resolv.conf\necho \"nameserver 8.8.8.8\" | sudo tee /etc/resolv.conf\necho \"nameserver 8.8.4.4\" | sudo tee -a /etc/resolv.conf\n</code></pre> <p>Update the hosts file on each server with the private IPs:</p> <pre><code># On Master Node\necho \"192.168.100.2 kube-dc-master-1\" | sudo tee -a /etc/hosts\n# On Worker Node\necho \"192.168.100.3 kube-dc-worker-1\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"quickstart-hetzner/#kubernetes-installation","title":"Kubernetes Installation","text":""},{"location":"quickstart-hetzner/#1-install-clusterdev","title":"1. Install Cluster.dev","text":"<p>On the master node, install Cluster.dev:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\n</code></pre>"},{"location":"quickstart-hetzner/#2-configure-and-install-rke2-on-master-node","title":"2. Configure and Install RKE2 on Master Node","text":"<p>Install kubectl:</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Create RKE2 configuration (replace the external IP with your server's public IP):</p> <pre><code>sudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\nnode-name: kube-dc-master-1\ndisable-cloud-controller: true\ndisable: rke2-ingress-nginx\ncni: none\ncluster-cidr: \"10.100.0.0/16\"\nservice-cidr: \"10.101.0.0/16\"\ncluster-dns: \"10.101.0.11\"\nnode-label:\n  - kube-dc-manager=true\n  - kube-ovn/role=master\nkube-apiserver-arg: \n  - authentication-config=/etc/rancher/auth-conf.yaml\ndebug: true\nnode-external-ip: 22.22.22.2_example # Primary IP address (get it from default netplan config)\ntls-san:\n  - kube-api.yourdomain.com\n  - 192.168.100.2 # Master node IP on private network (This for master node setup)\nadvertise-address: 192.168.100.2 # Master node IP on private network (This for master node setup)\nnode-ip: 192.168.100.2 # Master node IP on private network (This for master node setup)\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/auth-conf.yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt: []\nEOF\nsudo chmod 666 /etc/rancher/auth-conf.yaml\n</code></pre> <p>Install RKE2 server:</p> <pre><code>export INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"server\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\n</code></pre> <p>You can check the installation logs here:</p> <pre><code>sudo journalctl -u rke2-server -f\n</code></pre> <p>Configure kubectl:</p> <pre><code>mkdir -p ~/.kube\nsudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config\nsudo chown $(id -u):$(id -g) ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre> <p>Verify the cluster status:</p> <pre><code>kubectl get nodes\n# If you see this output then you can proceed:\nNAME               STATUS     ROLES\nkube-dc-master-1   NotReady   control-plane,etcd,master\n</code></pre>"},{"location":"quickstart-hetzner/#4-join-worker-node-to-the-cluster","title":"4. Join Worker Node to the Cluster","text":"<p>Get the join token from the master node:</p> <pre><code># on master node\nsudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre> <p>On the worker node, create the RKE2 configuration (replace TOKEN with the token from the master node):</p> <pre><code># on worker node\nsudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\ntoken: &lt;TOKEN&gt;\nserver: https://192.168.100.2:9345 # Master node local IP\nnode-name: kube-dc-worker-1\nnode-ip: 192.168.100.3\nEOF\n</code></pre> <p>Install RKE2 agent:</p> <pre><code># on worker node\nexport INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"agent\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-agent.service\nsudo systemctl start rke2-agent.service\n</code></pre> <p>Monitor the agent service:</p> <pre><code># on worker node\nsudo journalctl -u rke2-agent -f\n</code></pre> <p>Verify on the master node that the worker joined successfully:</p> <pre><code># on master node\nkubectl get nodes\n</code></pre>"},{"location":"quickstart-hetzner/#install-kube-dc-components-on-master-node","title":"Install Kube-DC Components on Master Node","text":""},{"location":"quickstart-hetzner/#1-create-clusterdev-project-configuration","title":"1. Create Cluster.dev Project Configuration","text":"<p>On the master node, create a project configuration file:</p> <pre><code>mkdir -p ~/kube-dc-hetzner\ncat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/project.yaml\nkind: Project\nname: kube-dc-hetzner\nbackend: \"default\"\nvariables:\n  kubeconfig: ~/.kube/config\n  debug: true\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#2-create-clusterdev-stack-configuration","title":"2. Create Cluster.dev Stack Configuration","text":"<p>Create the stack configuration file(replace <code>example</code> by appropriate values):</p> <pre><code>cat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/stack.yaml\nname: cluster\ntemplate: https://github.com/kube-dc/kube-dc-public//installer/kube-dc/templates/kube-dc?ref=main\nkind: Stack\nbackend: default\nvariables:\n  debug: \"true\"\n  kubeconfig: /root/.kube/config # Change for your username path to RKE kubeconfig\n\n  cluster_config:\n    pod_cidr: \"10.100.0.0/16\"\n    svc_cidr: \"10.101.0.0/16\"\n    join_cidr: \"100.64.0.0/16\"\n    cluster_dns: \"10.101.0.11\"\n    default_external_network:\n      nodes_list: # list of nodes, where 4011 vlan (external network) is accessible\n        - kube-dc-master-1\n        - kube-dc-worker-1\n      name: external4011_example # VLAN interface for this name you can find here https://robot.hetzner.com/vswitch/index\n      vlan_id: \"4011_example\" # VLAN interface id, see your VLAN in https://robot.hetzner.com/vswitch/index\n      interface: \"enp0s31f6_example\" # Parent interface for VLAN (same interface from default netplan config)\n      cidr: \"33.33.33.33_example/29\" # External subnet provided by Hetzner (should see during VLAN creation here https://robot.hetzner.com/vswitch/index)\n      gateway: 33.33.33.34_example # Gateway for external subnet (should see during VLAN creation here https://robot.hetzner.com/vswitch/index)\n      mtu: \"1400\"\n\n  node_external_ip: 22.22.22.2_example # Primary IP address (get it from default netplan config). Wildcard *.dev.kube-dc.com shoud be faced on this ip\n\n\n  email: \"noreply@example.com\"\n  domain: \"dev.example-kube-dc.com\"\n  install_terraform: true\n\n  create_default:\n    organization:\n      name: example\n      description: \"My test org my-org 1\"\n      email: \"example@example.com\"\n    project:\n      name: demo\n      cidr_block: \"10.1.0.0/16\"\n\n  monitoring:\n    prom_storage: 20Gi\n    retention_size: 17GiB\n    retention: 365d\n\n  versions:\n    kube_dc: \"v0.1.21\" # release version\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#3-deploy-kube-dc","title":"3. Deploy Kube-DC","text":"<p>Run Cluster.dev to deploy Kube-DC components:</p> <pre><code>cd ~/kube-dc-hetzner\ncdev apply\n</code></pre> <p>This process will take 15-20 minutes to complete. You can monitor the deployment progress in the terminal output.</p>"},{"location":"quickstart-hetzner/#4-verify-installation","title":"4. Verify Installation","text":"<p>After successful deployment, you will receive console and login credentials for deployment admin user. Also if you have created some default organization youll get organization admin credentials. Example:</p> <pre><code>keycloak_user = admin\norganization_admin_username = admin\norganization_name = example\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n example -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n example -o jsonpath='{.data.url}' | base64 -d\nconsole_url = https://console.dev.kube-dc.com\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#post-installation-steps","title":"Post-Installation Steps","text":""},{"location":"quickstart-hetzner/#1-access-kube-dc-ui-using-default-organization-credentials","title":"1. Access Kube-DC UI using default organization credentials","text":"<p>After the installation completes, the Kube-DC UI should be accessible at <code>https://console.yourdomain.com</code>. In cdev output there are output for default organization, project and admin user for default organization(use <code>retrieve_organization_password</code> to login):</p> <pre><code>console_url = https://console.dev.kube-dc.com\norganization_admin_username = admin\norganization_name = example\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n example -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n example -o jsonpath='{.data.url}' | base64 -d\n</code></pre>"},{"location":"quickstart-hetzner/#2-keep-credentials-for-keycloak-master-admin-user","title":"2. Keep credentials for Keycloak master admin user","text":"<p>You can save global Keycloak credentials if you need to manage Keycloak as super-admin.</p> <p>Master admin user credentials:</p> <pre><code>keycloak_user = admin\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during the installation:</p> <ol> <li> <p>Check the RKE2 server/agent logs:    <pre><code>sudo journalctl -u rke2-server -f  # On master\nsudo journalctl -u rke2-agent -f   # On worker\n</code></pre></p> </li> <li> <p>Check the Kube-OVN logs:    <pre><code>kubectl logs -n kube-system -l app=kube-ovn-controller\n</code></pre></p> </li> <li> <p>Verify network connectivity between nodes on the private network:    <pre><code>ping 192.168.100.2  # From worker node\nping 192.168.100.3  # From master node\n</code></pre></p> </li> </ol> <p>For additional help, consult the Kube-DC community support resources.</p>"},{"location":"quickstart-overview/","title":"Kube-DC Installation Overview","text":"<p>This document provides a technical overview of the Kube-DC installation process, with detailed explanations of key configuration files and their parameters.</p>"},{"location":"quickstart-overview/#installation-methods","title":"Installation Methods","text":"<p>Kube-DC can be installed in several ways:</p> <ul> <li>Master-Worker deployment: Recommended starting point for new deployments</li> <li>Multi-node HA cluster: For production environments</li> </ul> <p>Start with actual tested deployment on Hetzner Bare Metal Servers.</p>"},{"location":"quickstart-overview/#prerequisites","title":"Prerequisites","text":"<p>Before installing Kube-DC, ensure your system meets the following requirements:</p> <ul> <li>Hardware: Minimum 4 CPU cores, 8GB RAM per node</li> <li>Operating System: Ubuntu 20.04 LTS or newer (24.04 LTS recommended)</li> <li>Network: Dedicated network interface for VM traffic with VLAN support</li> <li>Storage: Local or network storage with support for dynamic provisioning</li> <li>Kubernetes: Version 1.31+ if installing on existing cluster</li> </ul>"},{"location":"quickstart-overview/#network-configuration","title":"Network Configuration","text":"<p>Kube-DC requires proper network configuration for optimal performance. The key requirement is that your external network must be routed through a VLAN to enable advanced networking features.</p>"},{"location":"quickstart-overview/#external-network-requirements","title":"External Network Requirements","text":"<p>Kube-DC networking is built on top of Kube-OVN and requires the following network configuration:</p> <ul> <li>VLAN-capable network interface: A dedicated network interface with VLAN support</li> <li>External subnet with routing: An external subnet that's properly routed to your infrastructure</li> <li>Static IP configuration: Static IP addressing (no DHCP) to ensure network stability</li> </ul> <p>This configuration allows Kube-DC to implement:</p> <ul> <li>Floating IP allocation: Dynamically assign public IPs to workloads</li> <li>Load balancer with external IPs: Distribute traffic to services with public visibility</li> <li>Default gateway per project: Isolate network traffic between projects</li> </ul> <p>All of these features work as a wrapper on top of Kube-OVN, providing enterprise-grade networking capabilities for your infrastructure.</p>"},{"location":"quickstart-overview/#example-network-configuration","title":"Example Network Configuration","text":"<p>Below is an example Netplan configuration with detailed comments for a VLAN-enabled network:</p> <pre><code>network:\n  version: 2  # Netplan version\n  renderer: networkd  # Network renderer to use\n  ethernets:\n    eth0:  # Primary network interface name (check your actual interface name)\n      addresses:\n        - 192.168.1.2/24  # Primary IP address and subnet mask\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 192.168.1.1  # Gateway IP address\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    eth0.100:  # VLAN interface (format: interface.vlan_id)\n      id: 100  # VLAN ID\n      link: eth0  # Parent interface for VLAN \n      mtu: 1500  # Recommended MTU for your network\n      addresses:\n        - 10.100.0.2/24  # Private IP on the VLAN network\n</code></pre> <p>Important</p> <p>Do not use DHCP for the VLAN interface as it would break the initial Kube-OVN setup. Always use static IP configuration.</p>"},{"location":"quickstart-overview/#networking-components","title":"Networking Components","text":"<p>The Kube-DC network setup consists of several key components that work together:</p> <ol> <li>Kube-OVN: Core CNI providing overlay and underlay networking</li> <li>Multus CNI: Enables multiple network interfaces for pods</li> <li>VLAN Integration: Connects Kubernetes networking to physical infrastructure</li> </ol>"},{"location":"quickstart-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC installer deploys the following core components:</p> <ol> <li>Kube-OVN: Advanced networking solution that provides overlay and underlay networking</li> <li>Multus CNI: CNI that enables attaching multiple network interfaces to pods</li> <li>KubeVirt: Virtualization layer for running VMs on Kubernetes</li> <li>Keycloak: Identity and access management solution</li> <li>Cert-Manager: Certificate management for TLS</li> <li>Ingress-NGINX: Ingress controller for external access</li> <li>Prometheus &amp; Loki: Monitoring and logging stack</li> <li>Kube-DC Core: The core management components for Kube-DC</li> </ol>"},{"location":"quickstart-overview/#installation-process-overview","title":"Installation Process Overview","text":"<p>The installation process follows these high-level steps:</p> <ol> <li>System Preparation: Configure network, optimize system settings, and install prerequisites</li> <li>Kubernetes Installation: Install RKE2 on master and worker nodes</li> <li>Kube-DC Installation: Use cluster.dev to deploy Kube-DC components</li> <li>Post-Installation Setup: Configure authentication, networking, and initial organization</li> </ol> <p>For detailed step-by-step instructions, refer to: - Master-Worker Setup (Dedicated Servers)</p>"},{"location":"roadmap/","title":"Kube-DC Product Roadmap","text":"<p>Build Your Own AI &amp; GPU Cloud on Any Server Transform bare-metal servers into a modern cloud with Kubernetes-native orchestration, GPU sharing, and multi-tenancy.</p> <p>Last Updated: December 9, 2025</p>"},{"location":"roadmap/#executive-summary","title":"Executive Summary","text":"Milestone Target Date Key Deliverable Installer v2 Jan 2026 Single-node &amp; simplified installation Global Admin UI Feb 2026 Platform-wide administration console Database as a Service Mar 2026 PostgreSQL, MySQL, MongoDB, Redis S3 Object Storage Apr 2026 Rook/Ceph multi-tenant buckets GPU/AI Platform May 2026 HAMI sharing, KubeFlow integration Billing System Jun 2026 Metering, pricing, usage reports Licensing Jul 2026 Node-based license management Hybrid Cloud Sep 2026 Multi-cluster federation, DR Advanced Networking Oct 2026 VPN, Security Groups, Service Mesh Edge Computing Q1 2027 Lightweight edge deployments"},{"location":"roadmap/#current-state-v0135","title":"Current State (v0.1.35) \u2705","text":""},{"location":"roadmap/#core-platform-complete","title":"Core Platform \u2014 Complete","text":"<ul> <li>Multi-Tenancy: Organizations, Projects, Keycloak SSO, RBAC</li> <li>Networking: Kube-OVN VPC, EIP/FIP, LoadBalancer, multi-network support</li> <li>Virtualization: KubeVirt VMs, Linux/Windows support, VNC, SSH injection</li> <li>KaaS: Multi-tenant control planes (Kamaji), KubeVirt/CloudSigma workers, Cilium CNI</li> <li>Observability: Prometheus metrics, Loki logging, VM monitoring charts</li> <li>UI: Web console, VM lifecycle, monitoring dashboards</li> </ul>"},{"location":"roadmap/#2026-roadmap","title":"2026 Roadmap","text":""},{"location":"roadmap/#q1-2026-foundation-administration","title":"Q1 2026: Foundation &amp; Administration","text":""},{"location":"roadmap/#installer-v20-january-2026","title":"\ud83d\udce6 Installer v2.0 \u2014 January 2026","text":"Feature Description Single-node Install All-in-one deployment for dev/small production Simplified Setup Reduced dependencies, guided installation Air-gapped Support Offline installation capability Security Hardening Dynamic secrets, no hardcoded passwords"},{"location":"roadmap/#global-admin-view-february-2026","title":"\ud83d\udda5\ufe0f Global Admin View \u2014 February 2026","text":"Feature Description Platform Dashboard Cluster-wide resource overview Organization Management Create/manage all organizations User Administration Global user and group management System Health Infrastructure monitoring and alerts Audit Console Platform-wide audit log viewer"},{"location":"roadmap/#q2-2026-managed-services","title":"Q2 2026: Managed Services","text":""},{"location":"roadmap/#database-as-a-service-march-2026","title":"\ud83d\uddc4\ufe0f Database as a Service \u2014 March 2026","text":"Database Features PostgreSQL CloudNativePG, auto-failover, continuous backups MySQL/MariaDB Percona Operator, clustering, PITR MongoDB Sharding, replica sets, automated backups Redis Clustering, persistence, sentinel <p>Capabilities: One-click provisioning, automated backups, connection pooling, performance dashboards</p>"},{"location":"roadmap/#s3-object-storage-april-2026","title":"\ud83d\udcbe S3 Object Storage \u2014 April 2026","text":"Feature Description Rook/Ceph Backend Production-grade object storage Multi-tenant Buckets Per-project isolation IAM Policies Fine-grained access control Lifecycle Management Automated data retention"},{"location":"roadmap/#gpu-aiml-platform-may-2026","title":"\ud83c\udfae GPU &amp; AI/ML Platform \u2014 May 2026","text":"Feature Description GPU Passthrough Full Nvidia GPU to VMs/pods HAMI Integration Fractional GPU sharing vGPU Support Virtual GPUs for multi-tenant KubeFlow ML pipeline orchestration LLM Serving Model inference infrastructure Vector Databases AI-native data stores"},{"location":"roadmap/#q3-2026-monetization-operations","title":"Q3 2026: Monetization &amp; Operations","text":""},{"location":"roadmap/#billing-system-quota-done-metering-june-2026","title":"\ud83d\udcb0 Billing System \u2014 Quota: Done \u2705 | Metering: June 2026","text":"Feature Status Description Quota Enforcement \u2705 Done HRQ + LimitRange + EIP + S3 per plan, addons, suspension lifecycle Billing Provider Decoupling \u2705 Done <code>BILLING_PROVIDER</code> feature flag (none/stripe/whmcs) Stripe Integration \u2705 Done Checkout, webhooks, portal, subscription CRUD Plans from ConfigMap \u2705 Done Live-reloadable plan definitions, no restart needed E2E Quota Tests \u2705 Done 6 tests: create, update, suspend, delete, addons, no-plan Resource Metering \ud83d\udd32 Planned CPU, memory, storage, GPU, network usage tracking Usage Reports \ud83d\udd32 Planned Detailed analytics, export, cost attribution WHMCS Integration \ud83d\udd32 Planned Alternative billing provider support"},{"location":"roadmap/#licensing-july-2026","title":"\ud83d\udd10 Licensing \u2014 July 2026","text":"Feature Description License Manager Node-based licensing Feature Gates License-controlled features Usage Tracking Compliance reporting Trial Mode Time-limited evaluations"},{"location":"roadmap/#ui-enhancements-august-2026","title":"\ud83d\udcca UI Enhancements \u2014 August 2026","text":"Feature Description KaaS Console Cluster creation wizard DBaaS Console Database management UI Storage Console S3 bucket management Billing Dashboard Cost visibility and reports"},{"location":"roadmap/#q4-2026-enterprise-integration","title":"Q4 2026: Enterprise Integration","text":""},{"location":"roadmap/#hybrid-cloud-september-2026","title":"\u2601\ufe0f Hybrid Cloud \u2014 September 2026","text":"Feature Description Multi-Cluster Federation Unified management across sites Cloud Bursting Extend to AWS/Azure/GCP Disaster Recovery Cross-site replication Backup Services Automated VM/container backups VMware Migration CDI import, vjailbreak, wizard"},{"location":"roadmap/#advanced-networking-october-2026","title":"\ud83c\udf10 Advanced Networking \u2014 October 2026","text":"Feature Description Network Peering Cross-project connectivity VPN Gateway Site-to-site VPN Security Groups Stateful firewall rules Service Mesh Istio/Linkerd integration DNS Management Custom domains, auto-DNS"},{"location":"roadmap/#2027-roadmap","title":"2027 Roadmap","text":""},{"location":"roadmap/#q1-2027-edge-advanced-automation","title":"Q1 2027: Edge &amp; Advanced Automation","text":""},{"location":"roadmap/#edge-computing-q1-2027","title":"\ud83d\udcf1 Edge Computing \u2014 Q1 2027","text":"Feature Description Edge Clusters Lightweight K3s deployments Edge-to-Core Sync Data synchronization Offline Mode Disconnected operations ARM Support Raspberry Pi, Jetson devices"},{"location":"roadmap/#advanced-automation-q2-2027","title":"\ud83e\udd16 Advanced Automation \u2014 Q2 2027","text":"Feature Description Self-Healing Automated remediation Predictive Scaling AI-driven autoscaling GitOps Native ArgoCD/Flux integration Policy as Code OPA/Kyverno policies"},{"location":"roadmap/#feature-timeline","title":"Feature Timeline","text":"<pre><code>2025 Dec     2026 Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    2027 Q1\n  \u2502            \u2502         \u2502      \u2502      \u2502      \u2502      \u2502      \u2502      \u2502      \u2502      \u2502        \u2502\n  \u25bc            \u25bc         \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc      \u25bc        \u25bc\nCurrent    Installer  Admin  DBaaS   S3   GPU/AI Billing License  UI   Hybrid Network   Edge\n State        v2      View                                        UX    Cloud\n</code></pre>"},{"location":"roadmap/#success-metrics","title":"Success Metrics","text":"Metric Target Time to First VM &lt; 2 minutes Time to First K8s Cluster &lt; 5 minutes Platform Uptime 99.9% VM Boot Time &lt; 60 seconds API Response Time &lt; 200ms (p95) <p>Document Owner: Kube-DC Product Team Feedback: GitHub Discussions</p>"},{"location":"todo/","title":"Todo List","text":"<ul> <li> Implement default NetworkPolicy creation in the Project controller. The controller should create a 'default-deny-all' NetworkPolicy in the project namespace upon project creation. This requires:</li> <li>Adding a <code>res_network_policy.go</code> file in <code>internal/project</code>.</li> <li>Updating <code>internal/project/project.go</code> to call the new function.</li> <li>Rebuilding and deploying the controller image (<code>make docker-build deploy</code>).</li> <li> <p>Re-enabling the e2e test in <code>tests/e2e/project_test.go</code>.</p> </li> <li> <p> Fix Project Controller Deletion Deadlock</p> </li> <li>Issue: The <code>Project</code> controller gets stuck in a deadlock when deleting a <code>Project</code> that has an empty <code>spec.egressNetworkType</code>. The deletion logic in <code>internal/project/res_vpc.go</code> incorrectly attempts to re-generate the <code>kube-ovn</code> VPC if it's not found, which fails because <code>utils.SelectBestExternalSubnet</code> requires an <code>egressNetworkType</code>.</li> <li>Fix: In <code>internal/project/res_vpc.go</code>, inside the <code>NewProjectVpc</code> function, modify the <code>IsNotFound</code> error handling. Before attempting to regenerate the VPC, add a check to see if the project has a <code>DeletionTimestamp</code>. If it does, the function should return the <code>NotFound</code> error directly, as this is an expected condition during cleanup, and regeneration should not occur.</li> </ul>"},{"location":"tutorial-kubeconfig/","title":"Kube-DC CLI - Kubernetes Access","text":"<p>This guide explains how to install and use the <code>kube-dc</code> CLI tool to access Kubernetes clusters managed by the kube-dc platform.</p>"},{"location":"tutorial-kubeconfig/#overview","title":"Overview","text":"<p>The <code>kube-dc</code> CLI provides secure, browser-based authentication for Kubernetes access. It handles:</p> <ul> <li>Browser-based login - No passwords in terminal</li> <li>Automatic token refresh - 30-day session with seamless refresh</li> <li>Multi-cluster support - Manage multiple organizations and projects</li> <li>Namespace switching - Easy namespace management based on your permissions</li> </ul>"},{"location":"tutorial-kubeconfig/#installation","title":"Installation","text":""},{"location":"tutorial-kubeconfig/#macos","title":"macOS","text":"<pre><code>curl -sL https://github.com/kube-dc/kube-dc-public/releases/latest/download/kube-dc_darwin_amd64 -o /usr/local/bin/kube-dc\nchmod +x /usr/local/bin/kube-dc\n</code></pre>"},{"location":"tutorial-kubeconfig/#linux","title":"Linux","text":"<pre><code>curl -sL https://github.com/kube-dc/kube-dc-public/releases/latest/download/kube-dc_linux_amd64 -o /usr/local/bin/kube-dc\nchmod +x /usr/local/bin/kube-dc\n</code></pre>"},{"location":"tutorial-kubeconfig/#windows","title":"Windows","text":"<pre><code># Download from releases page\nInvoke-WebRequest -Uri \"https://github.com/kube-dc/kube-dc-public/releases/latest/download/kube-dc_windows_amd64.exe\" -OutFile \"$env:USERPROFILE\\bin\\kube-dc.exe\"\n</code></pre>"},{"location":"tutorial-kubeconfig/#quick-start","title":"Quick Start","text":""},{"location":"tutorial-kubeconfig/#1-login-to-your-organization","title":"1. Login to your organization","text":"<pre><code>kube-dc login --domain kube-dc.cloud --org shalb\n</code></pre> <p>This opens your browser for secure authentication. After login: - Your kubeconfig is automatically configured - Contexts are created for each project you have access to - Tokens are securely cached (~/.kube-dc/credentials/)</p>"},{"location":"tutorial-kubeconfig/#2-switch-namespaceproject","title":"2. Switch namespace/project","text":"<pre><code># List available namespaces\nkube-dc ns\n\n# Switch to a project\nkube-dc ns shalb-demo\n</code></pre>"},{"location":"tutorial-kubeconfig/#3-use-kubectl-normally","title":"3. Use kubectl normally","text":"<pre><code>kubectl get pods\nkubectl top pods\nkubectl logs -f my-pod\n</code></pre>"},{"location":"tutorial-kubeconfig/#commands-reference","title":"Commands Reference","text":""},{"location":"tutorial-kubeconfig/#kube-dc-login","title":"<code>kube-dc login</code>","text":"<p>Authenticate with a kube-dc platform.</p> <pre><code>kube-dc login --domain &lt;domain&gt; --org &lt;organization&gt;\n\n# Examples\nkube-dc login --domain kube-dc.cloud --org shalb\nkube-dc login --domain stage.kube-dc.com --org mycompany\n</code></pre> <p>Options: - <code>--domain</code> - Platform domain (e.g., kube-dc.cloud) - <code>--org</code> - Organization/realm name - <code>--insecure</code> - Skip TLS verification (not recommended for production)</p>"},{"location":"tutorial-kubeconfig/#kube-dc-ns","title":"<code>kube-dc ns</code>","text":"<p>Switch between namespaces you have access to.</p> <pre><code># List available namespaces (shows * for current)\nkube-dc ns\n\n# Switch to a namespace\nkube-dc ns shalb-demo\n</code></pre>"},{"location":"tutorial-kubeconfig/#kube-dc-use","title":"<code>kube-dc use</code>","text":"<p>Switch between kube-dc contexts.</p> <pre><code># List all kube-dc contexts\nkube-dc use\n\n# Switch to a specific context\nkube-dc use shalb/demo\n</code></pre>"},{"location":"tutorial-kubeconfig/#kube-dc-logout","title":"<code>kube-dc logout</code>","text":"<p>Remove cached credentials.</p> <pre><code># Logout from current server\nkube-dc logout\n\n# Logout from all servers\nkube-dc logout --all\n</code></pre>"},{"location":"tutorial-kubeconfig/#kube-dc-config","title":"<code>kube-dc config</code>","text":"<p>View configuration and token status.</p> <pre><code># Show current configuration\nkube-dc config show\n\n# List all kube-dc contexts\nkube-dc config get-contexts\n</code></pre>"},{"location":"tutorial-kubeconfig/#how-it-works","title":"How It Works","text":""},{"location":"tutorial-kubeconfig/#authentication-flow","title":"Authentication Flow","text":"<ol> <li>Login: Browser opens to Keycloak login page</li> <li>OAuth2 PKCE: Secure token exchange without exposing credentials</li> <li>Token Storage: Encrypted tokens stored in <code>~/.kube-dc/credentials/</code></li> <li>kubectl Integration: Acts as credential plugin for kubectl</li> </ol>"},{"location":"tutorial-kubeconfig/#kubeconfig-integration","title":"Kubeconfig Integration","text":"<p>After login, your kubeconfig contains entries like:</p> <pre><code>contexts:\n- name: kube-dc/shalb/demo\n  context:\n    cluster: kube-dc-kube-dc.cloud-shalb\n    user: kube-dc@kube-dc.cloud/shalb\n    namespace: shalb-demo\n\nusers:\n- name: kube-dc@kube-dc.cloud/shalb\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1\n      command: kube-dc\n      args:\n        - credential\n        - --server\n        - https://kube-api.kube-dc.cloud:6443\n</code></pre>"},{"location":"tutorial-kubeconfig/#token-lifecycle","title":"Token Lifecycle","text":"<ul> <li>Access Token: Short-lived (5 minutes), automatically refreshed</li> <li>Refresh Token: 30-day validity with offline_access scope</li> <li>Auto-refresh: Tokens refresh transparently when kubectl runs</li> </ul>"},{"location":"tutorial-kubeconfig/#shell-completions","title":"Shell Completions","text":"<p>Enable tab completion for your shell:</p> <pre><code># Bash\nkube-dc completion bash &gt; /etc/bash_completion.d/kube-dc\n\n# Zsh\nkube-dc completion zsh &gt; \"${fpath[1]}/_kube-dc\"\n\n# Fish\nkube-dc completion fish &gt; ~/.config/fish/completions/kube-dc.fish\n</code></pre>"},{"location":"tutorial-kubeconfig/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-kubeconfig/#session-expired","title":"Session Expired","text":"<p>If you see \"session expired\", your refresh token has expired (after 30 days of inactivity):</p> <pre><code>kube-dc login --domain &lt;domain&gt; --org &lt;org&gt;\n</code></pre>"},{"location":"tutorial-kubeconfig/#context-not-found","title":"Context Not Found","text":"<p>If <code>kube-dc ns</code> shows \"not a kube-dc context\":</p> <pre><code># Check current context\nkubectl config current-context\n\n# Switch to a kube-dc context\nkube-dc use shalb/demo\n</code></pre>"},{"location":"tutorial-kubeconfig/#clear-all-credentials","title":"Clear All Credentials","text":"<p>To start fresh:</p> <pre><code>kube-dc logout --all\nrm -rf ~/.kube-dc/credentials/\n</code></pre>"},{"location":"tutorial-kubeconfig/#debug-mode","title":"Debug Mode","text":"<p>For troubleshooting connection issues:</p> <pre><code>kube-dc login --domain kube-dc.cloud --org shalb --debug\n</code></pre>"},{"location":"tutorial-kubeconfig/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Never share your <code>~/.kube-dc/credentials/</code> directory</li> <li>Use <code>--insecure</code> only for development/testing</li> <li>Logout when finished: <code>kube-dc logout</code></li> <li>Credentials are stored with 600 permissions</li> </ul>"},{"location":"tutorial-kubeconfig/#project-console-web-terminal","title":"Project Console (Web Terminal)","text":"<p>For quick access without CLI installation, use the Project Console from the UI:</p> <ol> <li>Click your username in the top-right</li> <li>Select \"Project console\"</li> <li>A web terminal opens with kubectl pre-configured</li> </ol> <p>The web console includes: - <code>kubectl</code>, <code>helm</code>, <code>k9s</code>, <code>stern</code>, <code>virtctl</code> - Shell completions for all tools - Common aliases: <code>k</code>, <code>kgp</code>, <code>kgs</code>, <code>kl</code>, etc.</p>"},{"location":"tutorial-kubeconfig/#next-steps","title":"Next Steps","text":"<ul> <li>User and Group Management: Learn about role-based access control</li> <li>Tutorial: Virtual Machines: Deploy your first VM</li> <li>Examples: Explore example manifests for various resources</li> </ul>"},{"location":"tutorial-networking-external/","title":"Additional External Network Configuration","text":"<p>This guide explains how to add additional external networks to Kube-DC alongside the default cloud network.</p>"},{"location":"tutorial-networking-external/#overview","title":"Overview","text":"<p>The configuration demonstrates how to add a second external network (public) to an existing Kube-DC setup that already has a cloud external network, using multiple VLANs on a single physical interface per node.</p>"},{"location":"tutorial-networking-external/#network-types-explained-by-example","title":"Network Types Explained by Example","text":""},{"location":"tutorial-networking-external/#cloud-network-egressnetworktype-cloud","title":"Cloud Network (<code>egressNetworkType: cloud</code>)","text":"<ul> <li>Purpose: Default external network for most workloads</li> <li>Subnet: <code>ext-cloud</code> (100.65.0.0/16) on VLAN 4013</li> <li>Use Cases: </li> <li>General internet access for applications</li> <li>Standard egress traffic from project workloads</li> <li>Cost-effective external connectivity</li> <li>IP Pool: Large address space (65,000+ IPs available)</li> </ul>"},{"location":"tutorial-networking-external/#public-network-egressnetworktype-public","title":"Public Network (<code>egressNetworkType: public</code>)","text":"<ul> <li>Purpose: Premium external network for specialized workloads</li> <li>Subnet: <code>ext-public</code> (168.119.17.48/28) on VLAN 4011</li> <li> <p>Use Cases:</p> </li> <li> <p>Production services requiring dedicated public IPs</p> </li> <li>Load balancers and ingress controllers</li> <li>Services needing specific public IP ranges or routing</li> <li>IP Pool: Limited address space with real ipv4 addresses (16 IPs total)</li> </ul>"},{"location":"tutorial-networking-external/#architecture","title":"Architecture","text":"<pre><code>Physical Interface (enp0s31f6)\n\u251c\u2500\u2500 VLAN 4013 (Cloud Network) - 100.65.0.0/16 (ext-cloud)\n\u2514\u2500\u2500 VLAN 4011 (Public Network) - 168.119.17.48/28 (ext-public)\n</code></pre>"},{"location":"tutorial-networking-external/#example-cluster-usage","title":"Example Cluster Usage","text":"<ul> <li>shalb-demo project: Uses <code>egressNetworkType: cloud</code> \u2192 EIP: 100.65.0.102 (development/testing)</li> <li>shalb-dev project: Uses <code>egressNetworkType: public</code> \u2192 EIP: 168.119.17.51 (development with public access)</li> <li>shalb-envoy project: Uses <code>egressNetworkType: public</code> \u2192 EIPs: 168.119.17.52, 168.119.17.54 (production load balancer)</li> </ul>"},{"location":"tutorial-networking-external/#choosing-the-right-network","title":"Choosing the Right Network","text":"<p>Use Cloud Network when: - Need basic internet connectivity - Don't require specific public IP ranges</p> <p>Use Public Network when: - Need dedicated public IP addresses - Have specific routing or compliance requirements - Running load balancers or ingress controllers</p>"},{"location":"tutorial-networking-external/#ovsovn-modifications-applied","title":"OVS/OVN Modifications Applied","text":""},{"location":"tutorial-networking-external/#1-ovs-bridge-configuration","title":"1. OVS Bridge Configuration","text":"<p>The system automatically creates the necessary OVS infrastructure:</p> <p>Bridge: <code>br-ext-cloud</code> - Physical interface <code>enp0s31f6</code> attached with VLAN trunking - Trunk VLANs: <code>[0, 4011, 4013]</code> - Patch ports for both external networks:   - <code>patch-localnet.ext-cloud-to-br-int</code> \u2194 <code>patch-br-int-to-localnet.ext-cloud</code>   - <code>patch-localnet.ext-public-to-br-int</code> \u2194 <code>patch-br-int-to-localnet.ext-public</code></p>"},{"location":"tutorial-networking-external/#2-ovn-logical-switches","title":"2. OVN Logical Switches","text":"<p>Two logical switches are created automatically: - <code>ext-cloud</code> (for VLAN 4013) - <code>ext-public</code> (for VLAN 4011)</p>"},{"location":"tutorial-networking-external/#3-providernetwork-status","title":"3. ProviderNetwork Status","text":"<p>The existing ProviderNetwork <code>ext-cloud</code> is updated to include both VLANs: <pre><code>status:\n  vlans: [\"vlan4013\", \"vlan4011\"]\n  ready: true\n  readyNodes:\n  - kube-dc-master-1\n  - kube-dc-worker-1\n</code></pre></p>"},{"location":"tutorial-networking-external/#configuration-steps","title":"Configuration Steps","text":""},{"location":"tutorial-networking-external/#1-apply-vlan-configuration","title":"1. Apply VLAN Configuration","text":"<pre><code>kubectl apply -f examples/networking/additional-external-network.yaml\n</code></pre>"},{"location":"tutorial-networking-external/#2-verify-configuration","title":"2. Verify Configuration","text":"<pre><code># Check ProviderNetwork VLANs\nkubectl get provider-network ext-cloud -o jsonpath='{.status.vlans}'\n# Expected output: [\"vlan4013\",\"vlan4011\"]\n\n# Check external subnets\nkubectl get subnets ext-cloud ext-public\n# Expected: ext-cloud (100.65.0.0/16) and ext-public (168.119.17.48/28)\n\n# Check EIP assignments\nkubectl get eips -A\n# Shows which projects are using which external networks\n\n# Check OVS bridge configuration\nkubectl exec -n kube-system [ovs-pod] -- ovs-vsctl show | grep -A 10 \"br-ext-cloud\"\n\n# Check OVN logical switches\nkubectl exec -n kube-system [ovn-central-pod] -- ovn-nbctl ls-list | grep ext\n</code></pre>"},{"location":"tutorial-networking-external/#3-test-with-project","title":"3. Test with Project","text":"<p>Create projects to test both network types:</p> <p>Project using Cloud Network: <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: test-project-cloud\n  namespace: test-org\nspec:\n  cidrBlock: 10.200.0.0/24\n  egressNetworkType: cloud  # Uses ext-cloud subnet (100.65.0.0/16)\n</code></pre></p> <p>Project using Public Network: <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: test-project-public\n  namespace: test-org\nspec:\n  cidrBlock: 10.201.0.0/24\n  egressNetworkType: public  # Uses ext-public subnet (168.119.17.48/28)\n</code></pre></p>"},{"location":"tutorial-networking-external/#key-points","title":"Key Points","text":"<ol> <li>Single ProviderNetwork: Use one ProviderNetwork per physical interface with multiple VLANs attached</li> <li>Automatic Configuration: OVS bridges, patch ports, and OVN logical switches are created automatically</li> <li>VLAN Trunking: The physical interface supports multiple VLANs simultaneously</li> <li>No Manual OVS Changes: All OVS/OVN modifications are handled by Kube-DC controllers</li> </ol>"},{"location":"tutorial-networking-external/#prerequisites","title":"Prerequisites","text":"<ul> <li>Physical network infrastructure supporting VLAN trunking</li> <li>vSwitch configured with appropriate VLAN IDs</li> </ul>"},{"location":"tutorial-networking-external/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-networking-external/#check-vlan-interface-on-nodes","title":"Check VLAN Interface on Nodes","text":"<pre><code># On cluster nodes\nip link show enp0s31f6.4011\nip addr show enp0s31f6.4011\n</code></pre>"},{"location":"tutorial-networking-external/#check-ovn-resources","title":"Check OVN Resources","text":"<pre><code># Check OVN-EIP resources\nkubectl get ovn-eip | grep ext-public\n\n# Check subnet status\nkubectl get subnet ext-public -o yaml\n</code></pre>"},{"location":"tutorial-networking-external/#test-connectivity","title":"Test Connectivity","text":"<pre><code># Test from pod\nkubectl exec -n [namespace] [pod] -- wget -qO- http://httpbin.org/ip\n</code></pre>"},{"location":"tutorial-service-exposure/","title":"Service Exposure Guide","text":"<p>This guide explains how to expose services in Kube-DC projects. The method you use depends on your project's network type and your requirements.</p>"},{"location":"tutorial-service-exposure/#quick-reference","title":"Quick Reference","text":"Network Type Default EIP Source Best For Recommended Method Cloud <code>ext-cloud</code> subnet Web apps, APIs Gateway Routes (<code>expose-route</code>) Public <code>ext-public</code> subnet VMs, custom protocols EIP + LoadBalancer <p>Note: Both network types support EIPs and LoadBalancers. The difference is where EIPs are allocated from.</p>"},{"location":"tutorial-service-exposure/#understanding-project-network-types","title":"Understanding Project Network Types","text":"<p>When creating a project, you choose an <code>egressNetworkType</code>:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: my-project\n  namespace: my-org\nspec:\n  egressNetworkType: cloud  # or \"public\"\n</code></pre>"},{"location":"tutorial-service-exposure/#cloud-network-egressnetworktype-cloud","title":"Cloud Network (<code>egressNetworkType: cloud</code>)","text":"<ul> <li>Default EIPs allocated from <code>ext-cloud</code> subnet (shared/NAT IPs)</li> <li>Outbound traffic goes through a shared NAT gateway</li> <li>Can create public EIPs by specifying <code>externalNetworkType: public</code></li> <li>Gateway Routes provide easy HTTPS exposure with auto-certificates</li> <li>Supports VMs, pods, and all workload types</li> <li>Best for: Web applications, APIs, microservices, cost-optimized workloads</li> <li>Cost: Lower (shared infrastructure, cloud IPs often included)</li> </ul>"},{"location":"tutorial-service-exposure/#public-network-egressnetworktype-public","title":"Public Network (<code>egressNetworkType: public</code>)","text":"<ul> <li>Default EIPs allocated from <code>ext-public</code> subnet (dedicated public IPs)</li> <li>Direct internet connectivity without NAT</li> <li>Each EIP is a dedicated public IP address</li> <li>Supports any TCP/UDP protocol</li> <li>Supports VMs, pods, and all workload types</li> <li>Best for: Game servers, custom protocols, direct IP requirements</li> <li>Cost: Higher (dedicated public IPs)</li> </ul>"},{"location":"tutorial-service-exposure/#feature-comparison","title":"Feature Comparison","text":"Feature Cloud Project Public Project Default EIP source <code>ext-cloud</code> <code>ext-public</code> Can get public EIPs \u2705 Yes (specify <code>externalNetworkType: public</code>) \u2705 Yes (default) Can use Gateway Routes \u2705 Yes \u2705 Yes Can use EIP + LB \u2705 Yes \u2705 Yes Can run VMs \u2705 Yes \u2705 Yes Can run Pods \u2705 Yes \u2705 Yes"},{"location":"tutorial-service-exposure/#part-1-cloud-network-projects","title":"Part 1: Cloud Network Projects","text":"<p>For projects with <code>egressNetworkType: cloud</code>, use Gateway Routes to expose services.</p>"},{"location":"tutorial-service-exposure/#all-service-annotations-reference","title":"All Service Annotations Reference","text":""},{"location":"tutorial-service-exposure/#gateway-route-annotations","title":"Gateway Route Annotations","text":"Annotation Description Example Values <code>expose-route</code> Enable Gateway route <code>http</code>, <code>https</code>, <code>tls-passthrough</code> <code>route-hostname</code> Custom hostname (optional) <code>api.example.com</code> <code>route-port</code> Target port (optional) <code>8080</code>, <code>50051</code> <code>tls-issuer</code> cert-manager Issuer name <code>letsencrypt</code> (default) <code>tls-secret</code> User-provided TLS secret <code>my-tls-secret</code>"},{"location":"tutorial-service-exposure/#eiploadbalancer-annotations","title":"EIP/LoadBalancer Annotations","text":"Annotation Description Example Values <code>bind-on-default-gw-eip</code> Use project's default EIP <code>\"true\"</code> <code>bind-on-eip</code> Use a specific EIP by name <code>my-eip</code> <code>autodelete</code> Auto-delete EIP when service deleted <code>\"true\"</code> <code>create-gateway-backend</code> Create Envoy Gateway backend <code>\"true\"</code> <p>Note: Prefix is <code>service.nlb.kube-dc.com/</code></p>"},{"location":"tutorial-service-exposure/#network-type-annotation","title":"Network Type Annotation","text":"Annotation Description Example Values <code>network.kube-dc.com/external-network-type</code> EIP type for auto-created EIP <code>cloud</code>, <code>public</code> <p>Tip: Use this on a LoadBalancer service to get a public EIP in a cloud project: <pre><code>annotations:\n  network.kube-dc.com/external-network-type: \"public\"\n</code></pre></p>"},{"location":"tutorial-service-exposure/#status-annotations-read-only","title":"Status Annotations (Read-Only)","text":"Annotation Description <code>route-hostname-status</code> Assigned hostname (set by controller) <p>Note: All annotations use prefix <code>service.nlb.kube-dc.com/</code></p>"},{"location":"tutorial-service-exposure/#gateway-route-annotations-details","title":"Gateway Route Annotations (Details)","text":"<p>Add these annotations to your <code>LoadBalancer</code> Service:</p>"},{"location":"tutorial-service-exposure/#route-type-comparison","title":"Route Type Comparison","text":"Route Type Port TLS App Serves Use Case <code>http</code> 80 None HTTP Plain HTTP traffic <code>https</code> 443 Gateway terminates HTTP \u2b50 Recommended - Auto TLS certs <code>tls-passthrough</code> 443 App terminates HTTPS End-to-end encryption"},{"location":"tutorial-service-exposure/#example-https-web-application-recommended","title":"Example: HTTPS Web Application (Recommended)","text":"<p>The simplest way to expose a web app with automatic TLS:</p>"},{"location":"tutorial-service-exposure/#step-1-create-the-issuer-once-per-namespace","title":"Step 1: Create the Issuer (once per namespace)","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt\n  namespace: my-project\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: your-email@example.com  # Replace with valid email\n    privateKeySecretRef:\n      name: letsencrypt-account-key\n    solvers:\n    - http01:\n        gatewayHTTPRoute:\n          parentRefs:\n          - group: gateway.networking.k8s.io\n            kind: Gateway\n            name: eg\n            namespace: envoy-gateway-system\n</code></pre>"},{"location":"tutorial-service-exposure/#step-2-deploy-your-application","title":"Step 2: Deploy your application","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\n  namespace: my-project\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: app\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"tutorial-service-exposure/#step-3-create-loadbalancer-service-with-https-route","title":"Step 3: Create LoadBalancer Service with HTTPS route","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  namespace: my-project\n  annotations:\n    # Expose via HTTPS with auto-provisioned certificate\n    service.nlb.kube-dc.com/expose-route: \"https\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-service-exposure/#step-4-verify-and-access","title":"Step 4: Verify and access","text":"<pre><code># Check assigned hostname\nkubectl get svc my-app -n my-project -o jsonpath='{.metadata.annotations.service\\.nlb\\.kube-dc\\.com/route-hostname-status}'\n# Output: my-app-my-project.stage.kube-dc.com\n\n# Check certificate status\nkubectl get certificate -n my-project\n\n# Test access\ncurl https://my-app-my-project.stage.kube-dc.com\n</code></pre>"},{"location":"tutorial-service-exposure/#example-plain-http","title":"Example: Plain HTTP","text":"<p>For non-TLS HTTP traffic:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/expose-route: \"http\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Access via: <code>http://my-app-my-project.stage.kube-dc.com</code></p>"},{"location":"tutorial-service-exposure/#example-tls-passthrough-kubernetes-api","title":"Example: TLS Passthrough (Kubernetes API)","text":"<p>For services that handle their own TLS (like Kubernetes control planes):</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: cluster-api\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/expose-route: \"tls-passthrough\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: kube-apiserver\n  ports:\n  - port: 6443\n    targetPort: 6443\n</code></pre> <p>Access via: <code>https://cluster-api-my-project.stage.kube-dc.com:6443</code></p>"},{"location":"tutorial-service-exposure/#example-custom-hostname","title":"Example: Custom Hostname","text":"<p>Override the auto-generated hostname:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/expose-route: \"https\"\n    service.nlb.kube-dc.com/route-hostname: \"api.mycompany.com\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Note: You must configure DNS to point <code>api.mycompany.com</code> to the Gateway IP.</p>"},{"location":"tutorial-service-exposure/#example-user-provided-certificate","title":"Example: User-Provided Certificate","text":"<p>Use your own TLS certificate instead of auto-provisioning:</p> <pre><code># First, create your TLS secret\nkubectl create secret tls my-tls-secret \\\n  --cert=path/to/tls.crt \\\n  --key=path/to/tls.key \\\n  -n my-project\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/expose-route: \"https\"\n    service.nlb.kube-dc.com/tls-secret: \"my-tls-secret\"\n    service.nlb.kube-dc.com/route-hostname: \"secure.mycompany.com\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-service-exposure/#example-grpc-service","title":"Example: gRPC Service","text":"<p>gRPC services work with HTTPS routes (HTTP/2):</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: grpc-api\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/expose-route: \"https\"\n    service.nlb.kube-dc.com/route-port: \"50051\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: grpc-server\n  ports:\n  - name: grpc\n    port: 50051\n    targetPort: 50051\n</code></pre>"},{"location":"tutorial-service-exposure/#part-2-eip-based-exposure-both-project-types","title":"Part 2: EIP-Based Exposure (Both Project Types)","text":"<p>Both cloud and public projects can use EIPs and LoadBalancer services.</p>"},{"location":"tutorial-service-exposure/#default-eip-allocation","title":"Default EIP Allocation","text":"Project Type Default EIP Source Can Request Cloud <code>ext-cloud</code> subnet Both <code>cloud</code> and <code>public</code> EIPs Public <code>ext-public</code> subnet Both <code>cloud</code> and <code>public</code> EIPs <p>When to use EIPs vs Gateway Routes: - Use Gateway Routes for HTTP/HTTPS/gRPC (simpler, auto-TLS) - Use EIPs for TCP/UDP protocols, VMs, or when you need a dedicated IP</p>"},{"location":"tutorial-service-exposure/#understanding-eips","title":"Understanding EIPs","text":"<p>External IPs (EIPs) provide IP addresses for your project from the configured external network.</p>"},{"location":"tutorial-service-exposure/#default-gateway-eip","title":"Default Gateway EIP","text":"<p>Every project automatically gets a default EIP (<code>default-gw</code>) that acts as: - NAT gateway for outbound traffic - Default endpoint for LoadBalancer services</p>"},{"location":"tutorial-service-exposure/#creating-additional-eips","title":"Creating Additional EIPs","text":"<p>For services that need dedicated IPs:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: web-server-eip\n  namespace: my-project\nspec:\n  externalNetworkType: public  # or \"cloud\"\n</code></pre> <p>EIP Types:</p> <code>externalNetworkType</code> Description Use Case <code>cloud</code> Shared/NAT pool IP Cost-effective, outbound NAT <code>public</code> Dedicated public IP Direct access, static IP, VMs <p>Tip: Cloud projects can request public EIPs for services that need dedicated IPs (e.g., game servers, VMs with direct access).</p>"},{"location":"tutorial-service-exposure/#loadbalancer-service-annotations","title":"LoadBalancer Service Annotations","text":"Annotation Description <code>service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"</code> Use project's default EIP <code>service.nlb.kube-dc.com/bind-on-eip: \"eip-name\"</code> Use a specific EIP"},{"location":"tutorial-service-exposure/#example-web-server-on-default-eip","title":"Example: Web Server on Default EIP","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-lb\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n  - name: https\n    port: 443\n    targetPort: 443\n</code></pre>"},{"location":"tutorial-service-exposure/#example-service-on-dedicated-eip","title":"Example: Service on Dedicated EIP","text":"<pre><code># Step 1: Create dedicated EIP\napiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: api-eip\n  namespace: my-project\nspec:\n  externalNetworkType: public\n---\n# Step 2: Bind service to the EIP\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-lb\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/bind-on-eip: \"api-eip\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: api-server\n  ports:\n  - port: 443\n    targetPort: 443\n</code></pre>"},{"location":"tutorial-service-exposure/#example-vm-ssh-access","title":"Example: VM SSH Access","text":"<p>Expose SSH access to a virtual machine:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: my-vm  # Target VM name\n  ports:\n  - name: ssh\n    port: 2222      # External port\n    targetPort: 22  # Internal SSH port\n</code></pre>"},{"location":"tutorial-service-exposure/#floating-ips-fips","title":"Floating IPs (FIPs)","text":"<p>Floating IPs map an internal IP directly to an EIP, providing 1:1 NAT.</p>"},{"location":"tutorial-service-exposure/#when-to-use-fips","title":"When to Use FIPs","text":"<ul> <li>Direct IP mapping for VMs</li> <li>Services that need to see their public IP</li> <li>Protocols that don't work behind NAT</li> </ul>"},{"location":"tutorial-service-exposure/#creating-a-fip","title":"Creating a FIP","text":"<pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: vm-fip\n  namespace: my-project\nspec:\n  ipAddress: 10.0.10.5    # Internal IP of VM or pod\n  eip: my-eip             # Name of existing EIP\n</code></pre>"},{"location":"tutorial-service-exposure/#important-limitation-fip-and-loadbalancer-conflicts","title":"\u26a0\ufe0f Important Limitation: FIP and LoadBalancer Conflicts","text":"<p>A pod/VM cannot simultaneously serve as: 1. A target for a public FIP 2. A backend for a cloud-network LoadBalancer service</p> <p>This is because public FIPs create source-based policy routes that redirect ALL outbound traffic from that IP to the public gateway, breaking cloud-network services.</p> <p>Example conflict: <pre><code>Pod IP: 10.0.0.30\n\u251c\u2500\u2500 Public FIP \u2192 Routes all traffic to public gateway (91.224.11.1)\n\u2514\u2500\u2500 Cloud LoadBalancer \u2192 Expects traffic via cloud gateway (100.65.0.1) \u274c BROKEN\n</code></pre></p> <p>Workarounds: - Use separate pods for FIP targets and cloud-service backends - Use the same network type for both (all public or all cloud) - Choose one exposure method per pod</p>"},{"location":"tutorial-service-exposure/#part-3-choosing-the-right-approach","title":"Part 3: Choosing the Right Approach","text":""},{"location":"tutorial-service-exposure/#decision-tree","title":"Decision Tree","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  What are you exposing?                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u25bc               \u25bc               \u25bc\n      Web App/API      VM Direct       Custom Protocol\n            \u2502           Access              \u2502\n            \u2502               \u2502               \u2502\n            \u25bc               \u25bc               \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502Cloud Project\u2502   \u2502  Public   \u2502   \u2502    Public     \u2502\n   \u2502expose-route \u2502   \u2502  Project  \u2502   \u2502    Project    \u2502\n   \u2502   : https   \u2502   \u2502  EIP+FIP  \u2502   \u2502   EIP + LB    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial-service-exposure/#comparison-table","title":"Comparison Table","text":"Feature Gateway Routes (Cloud) EIP + LoadBalancer (Public) IP Address Shared Gateway IP Dedicated per EIP Protocols HTTP, HTTPS, gRPC Any TCP/UDP TLS Termination Gateway (auto-cert) Application Cost Lower Higher Setup Simple annotation EIP + Service config DNS Auto hostname Manual Best For Web apps, APIs VMs, game servers"},{"location":"tutorial-service-exposure/#part-4-advanced-topics","title":"Part 4: Advanced Topics","text":""},{"location":"tutorial-service-exposure/#envoy-gateway-backend","title":"Envoy Gateway Backend","text":"<p>Use the <code>create-gateway-backend</code> annotation to register a service as an Envoy Gateway Backend for advanced routing scenarios:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-backend\n  namespace: my-project\n  annotations:\n    service.nlb.kube-dc.com/create-gateway-backend: \"true\"\nspec:\n  type: ClusterIP\n  selector:\n    app: my-app\n  ports:\n  - port: 8080\n    targetPort: 8080\n</code></pre> <p>This creates an Envoy Gateway <code>Backend</code> resource, enabling: - Cross-namespace routing from Gateway - Custom backend policies - Advanced load balancing configurations</p>"},{"location":"tutorial-service-exposure/#automatic-external-endpoints","title":"Automatic External Endpoints","text":"<p>For every LoadBalancer service, Kube-DC creates external endpoints for cross-VPC DNS access:</p> <ul> <li>External Service: <code>&lt;service-name&gt;-ext</code></li> <li>DNS: <code>&lt;service&gt;-ext.&lt;namespace&gt;.svc.cluster.local</code></li> </ul> <pre><code># Verify external endpoints\nkubectl get svc,endpoints -n my-project my-app-ext\n</code></pre>"},{"location":"tutorial-service-exposure/#namespace-scoped-ingress-controller","title":"Namespace-Scoped Ingress Controller","text":"<p>For advanced HTTP routing beyond Gateway capabilities, deploy a dedicated ingress-nginx:</p> <pre><code># ingress-values.yaml\ncontroller:\n  ingressClassResource:\n    enabled: false\n  scope:\n    enabled: true\n    namespace: my-project\n  admissionWebhooks:\n    enabled: false\n  service:\n    annotations:\n      service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nrbac:\n  create: true\n  scope: true\ndefaultBackend:\n  enabled: false\n</code></pre> <pre><code>helm install ingress ingress-nginx/ingress-nginx \\\n  --namespace my-project \\\n  --values ingress-values.yaml\n</code></pre>"},{"location":"tutorial-service-exposure/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-service-exposure/#gateway-routes-cloud-projects","title":"Gateway Routes (Cloud Projects)","text":"<pre><code># Check route hostname was assigned\nkubectl get svc my-app -o yaml | grep route-hostname-status\n\n# Check certificate status\nkubectl get certificate -n my-project\nkubectl describe certificate my-app-tls -n my-project\n\n# Check HTTPRoute created\nkubectl get httproute -n my-project\n\n# Check Gateway listener\nkubectl get gateway eg -n envoy-gateway-system -o yaml | grep -A5 \"https-my-app\"\n\n# Controller logs\nkubectl logs -n kube-dc deployment/kube-dc-manager | grep my-app\n</code></pre>"},{"location":"tutorial-service-exposure/#eiploadbalancer-public-projects","title":"EIP/LoadBalancer (Public Projects)","text":"<pre><code># Check EIP status\nkubectl get eip -n my-project\nkubectl describe eip my-eip -n my-project\n\n# Check LoadBalancer external IP\nkubectl get svc -n my-project\n\n# Check service events\nkubectl describe svc my-lb -n my-project\n</code></pre>"},{"location":"tutorial-service-exposure/#common-issues","title":"Common Issues","text":"Issue Cause Solution No hostname assigned Missing <code>expose-route</code> annotation Add annotation Certificate not ready Issuer not created Create Issuer first 503 error Backend not ready Check pod status EIP pending No available IPs Check subnet capacity Connection timeout DNS not configured Point DNS to Gateway/EIP Cloud LB stopped working after FIP created FIP policy route conflict Use separate pods or delete FIP (see limitation)"},{"location":"tutorial-service-exposure/#summary","title":"Summary","text":"Project Type Service Type Annotation Result Cloud LoadBalancer <code>expose-route: https</code> Auto HTTPS with cert Cloud LoadBalancer <code>expose-route: http</code> HTTP only Cloud LoadBalancer <code>expose-route: tls-passthrough</code> App handles TLS Public LoadBalancer <code>bind-on-default-gw-eip: \"true\"</code> Use default EIP Public LoadBalancer <code>bind-on-eip: \"name\"</code> Use specific EIP Public FIp N/A 1:1 NAT mapping"},{"location":"tutorial-sso-google-auth/","title":"Google SSO Authentication Setup","text":"<p>This guide explains how to enable Google OAuth authentication for Kube-DC using a central SSO Keycloak realm.</p>"},{"location":"tutorial-sso-google-auth/#overview","title":"Overview","text":"<p>Kube-DC supports Google OAuth authentication via a central <code>sso</code> Keycloak realm that brokers authentication to organization-specific realms. This allows:</p> <ul> <li>Single Google OAuth configuration - One Google client ID/secret for all organizations</li> <li>Per-organization isolation - Tokens issued by org realms with org-specific permissions</li> <li>Multi-org support - Users can belong to multiple organizations</li> <li>Self-service registration - Users can sign up and create organizations</li> <li>Feature flag - Enable/disable per deployment</li> </ul>"},{"location":"tutorial-sso-google-auth/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              Keycloak Server                                \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502                         Realm: sso                                 \u2502     \u2502\n\u2502  \u2502                                                                    \u2502     \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502     \u2502\n\u2502  \u2502  \u2502 Google IdP      \u2502  \u2502 Console Client  \u2502  \u2502 Broker Client    \u2502    \u2502     \u2502\n\u2502  \u2502  \u2502 (auto-link)     \u2502  \u2502 (kube-dc)       \u2502  \u2502 (sso-broker)     \u2502    \u2502     \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     \u2502\n\u2502  \u2502                                                                    \u2502     \u2502\n\u2502  \u2502  Registration: Passwordless (email verification required)          \u2502     \u2502\n\u2502  \u2502  Groups: /orgs/shalb, /orgs/acme, ...                              \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                              \u2502                                              \u2502\n\u2502                              \u2502 OIDC IdP Brokering                           \u2502\n\u2502                              \u25bc                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502  \u2502  Realm: shalb    \u2502  \u2502  Realm: acme     \u2502  \u2502  Realm: foo      \u2502           \u2502\n\u2502  \u2502  IdP: sso \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u253c\u2500\u2500\u25ba SSO Realm     \u2502           \u2502\n\u2502  \u2502  Users: admin    \u2502  \u2502  Users: admin    \u2502  \u2502  Users: admin    \u2502           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial-sso-google-auth/#user-journey","title":"User Journey","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         SELF-SERVICE REGISTRATION                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  1. SIGN UP                    2. VERIFY EMAIL                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502  \u2502 Enter:              \u2502       \u2502 Check inbox         \u2502                      \u2502\n\u2502  \u2502 \u2022 Email             \u2502 \u2500\u2500\u2500\u25ba  \u2502 Click verify link   \u2502                      \u2502\n\u2502  \u2502 \u2022 First/Last Name   \u2502       \u2502                     \u2502                      \u2502\n\u2502  \u2502 (No password yet!)  \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502                                  \u2502\n\u2502                                          \u25bc                                  \u2502\n\u2502  3. CREATE OR JOIN ORG         4. SET PASSWORD                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502  \u2502 Choose:             \u2502       \u2502 Set password        \u2502                      \u2502\n\u2502  \u2502 \u2022 Create new org    \u2502 \u2500\u2500\u2500\u25ba  \u2502 (only when creating \u2502                      \u2502\n\u2502  \u2502 \u2022 Join existing org \u2502       \u2502  organization)      \u2502                      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502                                          \u2502                                  \u2502\n\u2502                                          \u25bc                                  \u2502\n\u2502                                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502                                \u2502 \u2713 Organization      \u2502                      \u2502\n\u2502                                \u2502   created!          \u2502                      \u2502\n\u2502                                \u2502 \u2713 Auto-redirected   \u2502                      \u2502\n\u2502                                \u2502   to console        \u2502                      \u2502\n\u2502                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           GOOGLE SSO LOGIN                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  User clicks              Google OAuth              Auto-link by email      \u2502\n\u2502  \"Login with Google\"      authentication            (no extra prompts)      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502   Console   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502   Google    \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502  Keycloak   \u2502          \u2502\n\u2502  \u2502  (org page) \u2502         \u2502   Sign-in   \u2502           \u2502  SSO Realm  \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                                                           \u2502                 \u2502\n\u2502                                                           \u25bc                 \u2502\n\u2502                          Broker to org realm       Token issued             \u2502\n\u2502                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502                          \u2502  Org Realm  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502  Console    \u2502          \u2502\n\u2502                          \u2502  (via SSO)  \u2502           \u2502  (logged in)\u2502          \u2502\n\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial-sso-google-auth/#prerequisites","title":"Prerequisites","text":"<ol> <li>Google Cloud Console access to create OAuth credentials</li> <li>Keycloak admin access</li> <li>Kube-DC deployment with controller v0.1.34+</li> </ol>"},{"location":"tutorial-sso-google-auth/#setup-steps","title":"Setup Steps","text":""},{"location":"tutorial-sso-google-auth/#step-1-create-google-oauth-credentials","title":"Step 1: Create Google OAuth Credentials","text":""},{"location":"tutorial-sso-google-auth/#11-create-a-google-cloud-project","title":"1.1 Create a Google Cloud Project","text":"<ol> <li>Go to Google Cloud Console</li> <li>Click the project dropdown \u2192 New Project</li> <li>Enter a project name (e.g., <code>kube-dc-sso</code>)</li> <li>Click Create</li> </ol>"},{"location":"tutorial-sso-google-auth/#12-configure-oauth-consent-screen","title":"1.2 Configure OAuth Consent Screen","text":"<ol> <li>Navigate to APIs &amp; Services \u2192 OAuth consent screen</li> <li>Select External user type (or Internal for Google Workspace)</li> <li>Fill in required fields:</li> <li>App name: <code>Kube-DC</code></li> <li>User support email: Your email</li> <li>Developer contact: Your email</li> <li>Click Save and Continue</li> <li>Add scopes: <code>email</code>, <code>profile</code>, <code>openid</code></li> <li>Click Save and Continue through remaining steps</li> </ol>"},{"location":"tutorial-sso-google-auth/#13-create-oauth-20-client-id","title":"1.3 Create OAuth 2.0 Client ID","text":"<ol> <li>Navigate to APIs &amp; Services \u2192 Credentials</li> <li>Click Create Credentials \u2192 OAuth 2.0 Client ID</li> <li>Select Web application</li> <li>Configure:</li> <li>Name: <code>Kube-DC SSO</code></li> <li>Authorized JavaScript origins: <code>https://&lt;your-keycloak-url&gt;</code></li> <li>Authorized redirect URIs: <pre><code>https://&lt;your-keycloak-url&gt;/realms/sso/broker/google/endpoint\n</code></pre></li> <li>Click Create</li> <li>Copy and save the Client ID and Client Secret</li> </ol> <p>\u26a0\ufe0f Important: Keep the Client Secret secure. You'll need both values for the next step.</p>"},{"location":"tutorial-sso-google-auth/#step-2-bootstrap-sso-realm","title":"Step 2: Bootstrap SSO Realm","text":"<p>Run the bootstrap script to create the central SSO realm with Google IdP:</p> <pre><code># Required environment variables\nexport KEYCLOAK_URL=\"https://login.your-domain.com\"\nexport KEYCLOAK_ADMIN_USER=\"admin\"\nexport KEYCLOAK_ADMIN_PASSWORD=\"&lt;your-admin-password&gt;\"\nexport GOOGLE_CLIENT_ID=\"&lt;your-google-client-id&gt;\"\nexport GOOGLE_CLIENT_SECRET=\"&lt;your-google-client-secret&gt;\"\n\n# Optional\nexport CONSOLE_URL=\"https://console.your-domain.com\"  # Defaults to https://console.kube-dc.com\n\n# Run the bootstrap\n./hack/bootstrap-sso-realm.sh\n</code></pre> <p>Output: The script will generate and display <code>SSO_BROKER_SECRET</code>. Save this securely.</p>"},{"location":"tutorial-sso-google-auth/#what-the-bootstrap-script-configures","title":"What the Bootstrap Script Configures","text":"Component Description SSO Realm Central realm for authentication brokering Passwordless Registration Users sign up without password (set during org creation) Email Verification Required before organization setup Auto-link Flow Automatically links Google accounts by email Google IdP Configured with your OAuth credentials Console Client <code>kube-dc</code> client with PKCE for frontend Broker Client <code>sso-broker</code> for org realm federation Organization Groups <code>/orgs</code> group structure for membership"},{"location":"tutorial-sso-google-auth/#step-3-configure-kube-dc","title":"Step 3: Configure Kube-DC","text":""},{"location":"tutorial-sso-google-auth/#option-a-helm-values-recommended-for-new-deployments","title":"Option A: Helm Values (Recommended for new deployments)","text":"<p>Add SSO configuration to your Helm values:</p> <pre><code>manager:\n  keycloakSecret:\n    ssoEnabled: true\n    ssoBrokerSecret: \"&lt;from-bootstrap-output&gt;\"\n    googleClientId: \"&lt;your-google-client-id&gt;\"\n    googleClientSecret: \"&lt;your-google-client-secret&gt;\"\n</code></pre> <p>Then upgrade the Helm release:</p> <pre><code>helm upgrade kube-dc ./charts/kube-dc -n kube-dc -f values.yaml\n</code></pre>"},{"location":"tutorial-sso-google-auth/#option-b-kubectl-patch-existing-deployments","title":"Option B: kubectl patch (Existing deployments)","text":"<p>Add SSO configuration to the <code>master-config</code> secret:</p> <pre><code>export SSO_BROKER_SECRET=\"&lt;from-bootstrap-output&gt;\"\nexport GOOGLE_CLIENT_ID=\"&lt;your-google-client-id&gt;\"\nexport GOOGLE_CLIENT_SECRET=\"&lt;your-google-client-secret&gt;\"\n\nkubectl patch secret master-config -n kube-dc --type='json' -p=\"[\n  {\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/data/ssoEnabled\\\",\\\"value\\\":\\\"$(echo -n true | base64 -w0)\\\"},\n  {\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/data/ssoBrokerSecret\\\",\\\"value\\\":\\\"$(echo -n $SSO_BROKER_SECRET | base64 -w0)\\\"},\n  {\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/data/googleClientId\\\",\\\"value\\\":\\\"$(echo -n $GOOGLE_CLIENT_ID | base64 -w0)\\\"},\n  {\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/data/googleClientSecret\\\",\\\"value\\\":\\\"$(echo -n $GOOGLE_CLIENT_SECRET | base64 -w0)\\\"}\n]\"\n</code></pre>"},{"location":"tutorial-sso-google-auth/#step-4-restart-controller","title":"Step 4: Restart Controller","text":"<pre><code>kubectl rollout restart deployment kube-dc-manager -n kube-dc\n</code></pre> <p>The controller will now automatically configure SSO IdP for all new organizations.</p>"},{"location":"tutorial-sso-google-auth/#step-5-add-existing-organizations-to-sso-optional","title":"Step 5: Add Existing Organizations to SSO (Optional)","text":"<p>For organizations created before SSO was enabled, trigger a reconciliation:</p> <pre><code>kubectl annotate organization &lt;org-name&gt; -n &lt;org-name&gt; reconcile=$(date +%s) --overwrite\n</code></pre> <p>Or use the manual script:</p> <pre><code>export ORG_SLUG=\"&lt;organization-name&gt;\"\n./hack/add-org-to-sso.sh\n</code></pre>"},{"location":"tutorial-sso-google-auth/#configuration-reference","title":"Configuration Reference","text":""},{"location":"tutorial-sso-google-auth/#helm-values","title":"Helm Values","text":"<pre><code>manager:\n  keycloakSecret:\n    ssoEnabled: true                    # Enable Google SSO\n    ssoBrokerSecret: \"&lt;secret&gt;\"         # From bootstrap script output\n    googleClientId: \"&lt;client-id&gt;\"       # Google OAuth Client ID\n    googleClientSecret: \"&lt;secret&gt;\"      # Google OAuth Client Secret\n</code></pre> <p>The Helm chart automatically: - Stores SSO credentials in <code>master-config</code> secret - Configures frontend ConfigMap with <code>ssoEnabled</code> flag - Exposes \"Login with Google\" button when enabled</p>"},{"location":"tutorial-sso-google-auth/#master-config-secret-keys","title":"Master Config Secret Keys","text":"Key Type Description <code>ssoEnabled</code> string <code>\"true\"</code> to enable Google SSO <code>ssoBrokerSecret</code> string Secret for SSO broker client (from bootstrap) <code>googleClientId</code> string Google OAuth Client ID <code>googleClientSecret</code> string Google OAuth Client Secret"},{"location":"tutorial-sso-google-auth/#automatic-configuration-per-organization","title":"Automatic Configuration per Organization","text":"<p>When SSO is enabled, the controller automatically configures each organization realm with:</p> <ol> <li>SSO IdP - OIDC identity provider pointing to the <code>sso</code> realm</li> <li>Auto-link flow - Authentication flow that links existing users by email</li> <li>IdP mappers - Maps email, firstName, lastName from Google</li> <li>Org group - Creates <code>/orgs/&lt;org-slug&gt;</code> group in SSO realm</li> </ol>"},{"location":"tutorial-sso-google-auth/#user-experience","title":"User Experience","text":""},{"location":"tutorial-sso-google-auth/#self-service-registration","title":"Self-Service Registration","text":"<p>New users can sign up and create their own organization:</p> <ol> <li>User clicks \"Sign Up\" on the console login page</li> <li>Enters email, first name, and last name (no password required)</li> <li>Receives verification email and clicks the link</li> <li>After verification, chooses to:</li> <li>Create a new organization - Sets password and becomes org admin</li> <li>Join existing organization - Submits join request for admin approval</li> <li>Redirected to the console, fully authenticated</li> </ol> <p>\ud83d\udca1 Why passwordless registration? Users set their password only when creating an organization. This simplifies the signup flow and ensures passwords are only needed for org-level access.</p>"},{"location":"tutorial-sso-google-auth/#login-flow-existing-users","title":"Login Flow (Existing Users)","text":"<ol> <li>User navigates to the console</li> <li>Enters organization name</li> <li>Clicks \"Login with Google\" or uses username/password</li> <li>Authenticates with Google account (single click, no extra screens)</li> <li>Returns to console, authenticated to the organization</li> </ol>"},{"location":"tutorial-sso-google-auth/#organization-membership","title":"Organization Membership","text":"<p>For self-registered users, membership is automatic when they create an organization. For joining existing organizations:</p> <ol> <li>Log in to Keycloak admin console (<code>/admin/sso/console</code>)</li> <li>Navigate to Groups \u2192 orgs \u2192  <li>Add user to the group</li> <p>Or via API: <pre><code># Get user ID\nUSER_ID=$(curl -s -H \"Authorization: Bearer $TOKEN\" \\\n  \"$KEYCLOAK_URL/admin/realms/sso/users?email=user@example.com\" | jq -r '.[0].id')\n\n# Get group ID\nGROUP_ID=$(curl -s -H \"Authorization: Bearer $TOKEN\" \\\n  \"$KEYCLOAK_URL/admin/realms/sso/groups\" | jq -r '.[] | select(.name==\"orgs\") | .subGroups[] | select(.name==\"&lt;org-slug&gt;\") | .id')\n\n# Add user to group\ncurl -X PUT -H \"Authorization: Bearer $TOKEN\" \\\n  \"$KEYCLOAK_URL/admin/realms/sso/users/$USER_ID/groups/$GROUP_ID\"\n</code></pre></p>"},{"location":"tutorial-sso-google-auth/#verification","title":"Verification","text":"<p>After setup, verify the configuration is correct:</p> <pre><code># Get Keycloak credentials\nKC_URL=$(kubectl get secret -n kube-dc master-config -o jsonpath='{.data.url}' | base64 -d)\nKC_USER=$(kubectl get secret -n kube-dc master-config -o jsonpath='{.data.user}' | base64 -d)\nKC_PASS=$(kubectl get secret -n kube-dc master-config -o jsonpath='{.data.password}' | base64 -d)\n\n# Get admin token\nADMIN_TOKEN=$(curl -s -X POST \"$KC_URL/realms/master/protocol/openid-connect/token\" \\\n  -d \"username=$KC_USER\" -d \"password=$KC_PASS\" \\\n  -d \"grant_type=password\" -d \"client_id=admin-cli\" | jq -r '.access_token')\n\n# Check SSO realm configuration\necho \"Registration Flow:\"\ncurl -s -H \"Authorization: Bearer $ADMIN_TOKEN\" \"$KC_URL/admin/realms/sso\" | jq -r '.registrationFlow'\n# Expected: registration-no-password\n\necho \"Auto-link Flow:\"\ncurl -s -H \"Authorization: Bearer $ADMIN_TOKEN\" \"$KC_URL/admin/realms/sso/authentication/flows\" | \\\n  jq -r '.[] | select(.alias==\"auto-link-broker-login\") | .alias'\n# Expected: auto-link-broker-login\n\necho \"Google IdP Broker Flow:\"\ncurl -s -H \"Authorization: Bearer $ADMIN_TOKEN\" \"$KC_URL/admin/realms/sso/identity-provider/instances/google\" | \\\n  jq -r '.firstBrokerLoginFlowAlias'\n# Expected: auto-link-broker-login\n</code></pre>"},{"location":"tutorial-sso-google-auth/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-sso-google-auth/#sso-realm-not-found","title":"SSO realm not found","text":"<p>Error: <code>SSO realm 'sso' does not exist. Run bootstrap-sso-realm.sh first</code></p> <p>Solution: Run the bootstrap script to create the SSO realm.</p>"},{"location":"tutorial-sso-google-auth/#google-login-shows-account-already-exists-prompt","title":"Google login shows \"Account already exists\" prompt","text":"<p>Cause: Auto-link flow not configured on Google IdP.</p> <p>Solution: Verify the Google IdP uses <code>auto-link-broker-login</code> as its first broker login flow: <pre><code>curl -s -X PUT -H \"Authorization: Bearer $ADMIN_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"firstBrokerLoginFlowAlias\": \"auto-link-broker-login\"}' \\\n  \"$KC_URL/admin/realms/sso/identity-provider/instances/google\"\n</code></pre></p>"},{"location":"tutorial-sso-google-auth/#google-login-not-working","title":"Google login not working","text":"<ol> <li>Check Google OAuth redirect URI matches exactly:    <pre><code>https://&lt;your-keycloak-url&gt;/realms/sso/broker/google/endpoint\n</code></pre></li> <li>Verify <code>ssoEnabled</code> is <code>\"true\"</code> in master-config secret</li> <li>Check Google IdP has client secret configured</li> <li>Check controller logs: <code>kubectl logs -n kube-dc -l app.kubernetes.io/name=kube-dc-manager</code></li> </ol>"},{"location":"tutorial-sso-google-auth/#user-not-authorized","title":"User not authorized","text":"<p>Error: User can authenticate but cannot access organization</p> <p>Solution: Add user to <code>/orgs/&lt;org-slug&gt;</code> group in SSO realm.</p>"},{"location":"tutorial-sso-google-auth/#registration-email-not-received","title":"Registration email not received","text":"<ol> <li>Verify SMTP is configured in Keycloak SSO realm</li> <li>Check Keycloak logs for email sending errors</li> <li>Verify the email address is correct</li> </ol>"},{"location":"tutorial-sso-google-auth/#disabling-sso","title":"Disabling SSO","text":"<p>To disable Google SSO:</p> <pre><code>kubectl patch secret master-config -n kube-dc --type='json' -p='[\n  {\"op\":\"replace\",\"path\":\"/data/ssoEnabled\",\"value\":\"'$(echo -n false | base64 -w0)'\"}\n]'\n\nkubectl rollout restart deployment kube-dc-manager -n kube-dc\n</code></pre> <p>Users will fall back to direct organization login with username/password.</p>"},{"location":"tutorial-sso-google-auth/#security-considerations","title":"Security Considerations","text":"<ul> <li>Token isolation - SSO realm tokens are only used for authentication; final tokens come from org realms</li> <li>Org membership verification - Users cannot access organizations they're not members of</li> <li>Secrets management - All credentials stored in Kubernetes secrets, never in code</li> <li>TLS required - All Keycloak endpoints must use HTTPS</li> </ul> <p>See also: - Keycloak Identity Brokering Documentation - Google OAuth Setup Guide</p>"},{"location":"tutorial-user-groups/","title":"User and Group Management","text":"<p>This guide explains how to set up and manage users, groups, and roles in Kube-DC using Kubernetes RBAC and Keycloak integration.</p>"},{"location":"tutorial-user-groups/#overview","title":"Overview","text":"<p>Kube-DC implements a multi-tenant access control system that combines:</p> <ul> <li>Kubernetes RBAC: Handles resource-level permissions within namespaces</li> <li>Organization Groups: Manages project-level access across namespaces</li> <li>Keycloak Integration: Provides user authentication and group management</li> </ul>"},{"location":"tutorial-user-groups/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you're working from an Organization Admin perspective. You'll need:</p> <ul> <li>Access to the Kube-DC cluster with organization admin privileges</li> <li><code>kubectl</code> configured to access your cluster with organization admin privileges</li> <li>Access to the Keycloak organization admin console</li> </ul> <p>Before You Begin</p> <p>During organization and project creation you will get a namespace with organization name <code>&lt;orgname&gt;</code> created and project namespace with <code>&lt;orgname&gt;-&lt;projectname&gt;</code> pattern.</p>"},{"location":"tutorial-user-groups/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"tutorial-user-groups/#creating-project-roles","title":"Creating Project Roles","text":"<p>Create a Kubernetes Role to define permissions within a project namespace. These roles dictate what actions users can perform on specific resources.</p> <pre><code>apiVersion: rbac.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: shalb-demo  # Replace with your project namespace\n  name: resource-manager\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\", \"services\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"daemonsets\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n</code></pre> <p>Apply the role to your namespace using:</p> <pre><code>kubectl apply -f role.yaml\n</code></pre> <p>Role Scope</p> <p>Remember that Roles are namespace-scoped. If you need permissions across multiple namespaces, you need to create a separate Role in each Project namespace.</p>"},{"location":"tutorial-user-groups/#creating-organization-groups","title":"Creating Organization Groups","text":"<p>Create an OrganizationGroup Custom Resource (CR) to define group permissions across projects.</p> <p>Key Points</p> <ul> <li>The OrganizationGroup CR automatically creates a corresponding group in Keycloak</li> <li>This CR must be created in the organization namespace, not the project namespace</li> <li>Role bindings would be created by this CR</li> </ul> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb  # namespace of the organization (not the project)\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - resource-manager\n  # Additional projects and roles can be added:\n  # - project: \"prod\"\n  #   roles:\n  #   - resource-manager\n</code></pre> <p>Apply the group configuration:</p> <pre><code>kubectl apply -f organization-group.yaml\n</code></pre>"},{"location":"tutorial-user-groups/#managing-users-in-keycloak","title":"Managing Users in Keycloak","text":""},{"location":"tutorial-user-groups/#access-keycloak-admin-console","title":"Access Keycloak Admin Console","text":"<p>Retrieve Keycloak access credentials from your organization namespace:</p> <pre><code>kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.user}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\n</code></pre> <p>Remember</p> <p>Replace <code>shalb</code> with your own organization namespace in the commands above.</p>"},{"location":"tutorial-user-groups/#create-and-configure-users","title":"Create and Configure Users","text":"<ol> <li>Log in to the Keycloak admin console using the retrieved credentials</li> <li>Navigate to Users \u2192 Add User </li> <li>Fill in the required user information</li> <li>Set up initial password in the Credentials tab</li> <li>Add the user to the appropriate group (e.g., \"app-manager\") via the Groups tab </li> </ol> <p>User Group Mapping</p> <p>Any groups created via OrganizationGroup CRs will appear automatically in Keycloak. Changes to group membership in Keycloak are synchronized with Kubernetes RBAC.</p>"},{"location":"tutorial-user-groups/#accessing-kube-dc-ui","title":"Accessing Kube-DC UI","text":"<ol> <li>Navigate to the Kube-DC UI login page</li> <li>Log in using the credentials created in Keycloak</li> <li>Verify access to assigned project resources</li> </ol> <p>Permissions Troubleshooting</p> <p>If a user cannot access expected resources: - Verify they're assigned to the correct groups in Keycloak - Check that the OrganizationGroup CR includes the correct projects and roles - Ensure the underlying Kubernetes Roles have appropriate permissions - Examine the Keycloak logs for authentication issues</p> <p>Permission changes may take up to 5 minutes to propagate through the system.</p>"},{"location":"tutorial-virtual-machines/","title":"Deploying VMs &amp; Containers","text":"<p>This tutorial walks you through deploying virtual machines and containers in Kube-DC. You'll learn both the UI-based approach and how to use kubectl with YAML manifests.</p>"},{"location":"tutorial-virtual-machines/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Access to a Kube-DC cluster</li> <li>The <code>kubectl</code> command-line tool installed</li> <li>The <code>virtctl</code> plugin installed for KubeVirt (optional, but recommended)</li> <li>A project with the necessary permissions to create VMs and containers</li> </ul>"},{"location":"tutorial-virtual-machines/#understanding-vm-components-in-kube-dc","title":"Understanding VM Components in Kube-DC","text":"<p>Kube-DC's virtualization is powered by KubeVirt and consists of several components:</p> <ol> <li>VirtualMachine (VM): Defines the VM configuration and lifecycle</li> <li>DataVolume: Manages the VM's disk image(s)</li> <li>VirtualMachineInstance (VMI): Represents a running instance of a VM</li> </ol>"},{"location":"tutorial-virtual-machines/#creating-a-vm-using-the-kube-dc-ui","title":"Creating a VM Using the Kube-DC UI","text":""},{"location":"tutorial-virtual-machines/#step-1-navigate-to-vm-creation","title":"Step 1: Navigate to VM Creation","text":"<ol> <li>Log in to the Kube-DC dashboard</li> <li>Select your project from the dropdown menu (e.g., \"demo\")</li> <li>Navigate to \"Virtual Machines\" in the left sidebar</li> <li>Click the \"+\" button to create a new VM</li> </ol>"},{"location":"tutorial-virtual-machines/#step-2-configure-basic-vm-parameters","title":"Step 2: Configure Basic VM Parameters","text":"<p>In the VM creation wizard, specify the basic parameters:</p> <ol> <li>VM Name: Enter a name for your VM (e.g., \"new-vm-name\")</li> <li>Operation System: Select from the dropdown (e.g., \"Ubuntu 24.04\")</li> <li>Advanced Options: Expand this section if you want to customize the image source</li> </ol> <p></p>"},{"location":"tutorial-virtual-machines/#step-3-configure-vm-resources","title":"Step 3: Configure VM Resources","text":"<p>Continue configuring the VM:</p> <ol> <li>Number of vCPUs: Select the number of virtual CPUs</li> <li>RAM (GB): Specify the amount of memory</li> <li>Subnet: Choose the network for your VM</li> <li>Root Storage Size (GB): Set the disk size</li> <li>Root Storage Type: Select the storage class</li> </ol>"},{"location":"tutorial-virtual-machines/#step-4-review-and-create","title":"Step 4: Review and Create","text":"<ol> <li>Click \"Next\" to proceed to the review page</li> <li>Review the generated VM configuration</li> <li>The UI shows the actual YAML that will be applied</li> <li>Click \"Finish\" to create the VM</li> </ol>"},{"location":"tutorial-virtual-machines/#step-5-monitor-vm-creation","title":"Step 5: Monitor VM Creation","text":"<p>After creation:</p> <ol> <li>You'll be redirected to the VM list</li> <li>Wait for the VM to reach \"Running\" state</li> <li>Note the assigned IP address</li> </ol>"},{"location":"tutorial-virtual-machines/#managing-vms-via-the-ui","title":"Managing VMs via the UI","text":""},{"location":"tutorial-virtual-machines/#viewing-vm-details","title":"Viewing VM Details","text":"<p>Click on a VM name to view its details page, which includes:</p> <ol> <li>Guest OS: Information about the operating system</li> <li>VM Details: Status, VPC subnet, and node placement</li> <li>Performance Metrics: Real-time CPU, memory, and storage usage</li> <li>Conditions: Agent connection and other status indicators</li> </ol> <p></p>"},{"location":"tutorial-virtual-machines/#accessing-vm-console","title":"Accessing VM Console","text":"<p>From the VM details page, you have two options:</p> <ol> <li>Launch Remote Console: Opens a graphical console in your browser</li> <li>Launch SSH Terminal: Opens a web-based SSH terminal</li> </ol> <p>These options provide direct access to your VM without requiring SSH client configuration.</p>"},{"location":"tutorial-virtual-machines/#vm-actions","title":"VM Actions","text":"<p>The UI supports common VM management actions:</p> <ul> <li>Start/Stop: Control the VM power state</li> <li>Restart: Reboot the VM</li> <li>Delete: Remove the VM and its resources</li> <li>Configure: Modify VM settings</li> </ul>"},{"location":"tutorial-virtual-machines/#creating-a-vm-using-kubectl-manifests","title":"Creating a VM Using kubectl Manifests","text":"<p>For automation or GitOps workflows, you can create VMs using kubectl and YAML manifests.</p>"},{"location":"tutorial-virtual-machines/#step-1-create-datavolume","title":"Step 1: Create DataVolume","text":"<p>First, create a DataVolume to serve as the VM's disk:</p> <pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: ubuntu-vm-disk\n  namespace: shalb-demo\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f ubuntu-datavolume.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#step-2-create-the-vm-definition","title":"Step 2: Create the VM Definition","text":"<p>Create a VM manifest:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\n  namespace: shalb-demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: shalb-demo/default\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n        cpu:\n          cores: 2\n        memory:\n          guest: 4G\n      volumes:\n      - dataVolume:\n          name: ubuntu-vm-disk\n        name: root-volume\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |-\n            #cloud-config\n            chpasswd: { expire: False }\n            password: temppassword\n            ssh_pwauth: True\n            package_update: true\n            package_upgrade: true\n            packages:\n            - qemu-guest-agent\n            runcmd:\n            - [ systemctl, enable, qemu-guest-agent ]\n            - [ systemctl, start, qemu-guest-agent ]\n</code></pre> <p>Apply the VM manifest:</p> <pre><code>kubectl apply -f ubuntu-vm.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#step-3-monitor-vm-status","title":"Step 3: Monitor VM Status","text":"<p>Check the status of your VM:</p> <pre><code>kubectl get virtualmachines -n shalb-demo\nkubectl get virtualmachineinstances -n shalb-demo\n</code></pre>"},{"location":"tutorial-virtual-machines/#vm-examples-for-different-operating-systems","title":"VM Examples for Different Operating Systems","text":"<p>Kube-DC supports various operating systems. Here are examples for the most common ones:</p>"},{"location":"tutorial-virtual-machines/#debian","title":"Debian","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: debian-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 14G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-generic-amd64.qcow2\n---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: debian-vm\n  namespace: shalb-demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: shalb-demo/default\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n        cpu:\n          cores: 1\n        memory:\n          guest: 2G\n      volumes:\n      - dataVolume:\n          name: debian-base-img\n        name: root-volume\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |-\n            #cloud-config\n            chpasswd: { expire: False }\n            password: temppassword\n            ssh_pwauth: True\n            package_update: true\n            packages:\n            - qemu-guest-agent\n            runcmd:\n            - [ systemctl, start, qemu-guest-agent ]\n</code></pre>"},{"location":"tutorial-virtual-machines/#alpine-linux","title":"Alpine Linux","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: alpine-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 2G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://dl-cdn.alpinelinux.org/alpine/v3.19/releases/cloud/nocloud_alpine-3.19.1-x86_64-bios-cloudinit-r0.qcow2\n</code></pre>"},{"location":"tutorial-virtual-machines/#centos-stream-9","title":"CentOS Stream 9","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: centos-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud.centos.org/centos/9-stream/x86_64/images/CentOS-Stream-GenericCloud-9-latest.x86_64.qcow2\n</code></pre> <p>Note: CentOS Stream 9 requires additional SELinux configuration to enable guest agent SSH key injection. The OS configuration includes proper SELinux booleans and contexts to allow guest agent operations. See the CentOS example in <code>examples/virtual-machine/centos-8.yaml</code> for the complete cloud-init configuration with SELinux setup.</p>"},{"location":"tutorial-virtual-machines/#virtual-machine-health-checks","title":"Virtual Machine Health Checks","text":"<p>Kube-DC supports VM health checks to ensure your VMs are running properly:</p> <pre><code>spec:\n  template:\n    spec:\n      readinessProbe:\n        guestAgentPing: {}\n        failureThreshold: 10\n        initialDelaySeconds: 20\n        periodSeconds: 10\n        timeoutSeconds: 5\n      livenessProbe:\n        failureThreshold: 10\n        initialDelaySeconds: 120\n        periodSeconds: 20\n        timeoutSeconds: 5\n        httpGet:\n          port: 80\n</code></pre>"},{"location":"tutorial-virtual-machines/#exposing-vm-services","title":"Exposing VM Services","text":""},{"location":"tutorial-virtual-machines/#creating-a-service-for-vm","title":"Creating a Service for VM","text":"<p>To expose a service running on your VM:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh-service\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: ubuntu-vm\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222\n      targetPort: 22\n</code></pre> <p>Apply this service:</p> <pre><code>kubectl apply -f vm-service.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#using-floating-ips-for-vms","title":"Using Floating IPs for VMs","text":"<p>You can assign a floating IP to your VM for direct external access:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: ubuntu-vm-fip\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171\n  eip: vm-eip\n</code></pre> <p>First, ensure you have an EIP:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: vm-eip\n  namespace: shalb-demo\nspec: {}\n</code></pre>"},{"location":"tutorial-virtual-machines/#deploying-containers-alongside-vms","title":"Deploying Containers Alongside VMs","text":"<p>Kube-DC allows you to run containers alongside VMs. Here's how to deploy a simple Nginx container:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: shalb-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n</code></pre> <p>Create a service for the Nginx deployment:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-virtual-machines/#best-practices-for-vm-management","title":"Best Practices for VM Management","text":""},{"location":"tutorial-virtual-machines/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Allocate appropriate resources based on the OS and workload requirements</li> <li>Monitor VM performance to adjust resources as needed</li> <li>Use resource quotas to prevent resource exhaustion</li> </ul>"},{"location":"tutorial-virtual-machines/#security","title":"Security","text":"<ul> <li>Change default passwords immediately</li> <li>Use SSH keys instead of passwords when possible</li> <li>Keep guest OS updated with security patches</li> <li>Apply network policies to control VM traffic</li> </ul>"},{"location":"tutorial-virtual-machines/#efficiency","title":"Efficiency","text":"<ul> <li>Use cloud-init for automated VM configuration</li> <li>Create VM templates for standardized deployments</li> <li>Use the smallest OS image that meets your requirements</li> </ul>"},{"location":"tutorial-virtual-machines/#troubleshooting-vms","title":"Troubleshooting VMs","text":""},{"location":"tutorial-virtual-machines/#common-issues","title":"Common Issues","text":"<ol> <li> <p>VM stuck in provisioning: Check DataVolume status and events    <pre><code>kubectl get datavolume -n shalb-demo\nkubectl describe datavolume ubuntu-vm-disk -n shalb-demo\n</code></pre></p> </li> <li> <p>VM not accessible via network: Verify network configuration    <pre><code>kubectl get vmi -n shalb-demo -o jsonpath='{.items[*].status.interfaces[*].ipAddress}'\n</code></pre></p> </li> <li> <p>Cloud-init not running: Check cloud-init logs inside the VM    <pre><code># Inside the VM\nsudo cat /var/log/cloud-init.log\n</code></pre></p> </li> </ol>"},{"location":"tutorial-virtual-machines/#accessing-vm-logs","title":"Accessing VM Logs","text":"<pre><code>kubectl get events -n shalb-demo\nvirtctl console ubuntu-vm -n shalb-demo\nvirtctl logs ubuntu-vm -n shalb-demo\n</code></pre>"},{"location":"tutorial-virtual-machines/#advanced-kubevirt-features","title":"Advanced KubeVirt Features","text":"<p>For more advanced features, refer to the KubeVirt documentation:</p> <ul> <li>VM Snapshots</li> <li>Live Migration</li> <li>GPU Passthrough</li> <li>Storage Management</li> </ul>"},{"location":"tutorial-virtual-machines/#conclusion","title":"Conclusion","text":"<p>You've now learned how to deploy and manage VMs and containers in Kube-DC using both the intuitive UI and kubectl manifests. This hybrid approach allows you to choose the most appropriate method for your workflow, whether you prefer interactive management or automation through GitOps practices.</p>"},{"location":"tutorial-windows-vm/","title":"Windows 11 VM Tutorial - Complete Setup Guide","text":"<p>This comprehensive guide covers the complete process of setting up Windows 11 VMs in KubeVirt, from infrastructure setup to golden image creation and deployment.</p>"},{"location":"tutorial-windows-vm/#overview","title":"Overview","text":"<p>This tutorial provides two deployment methods:</p> <ol> <li>Golden Image Deployment (Recommended) - Deploy pre-configured VMs in 5-10 minutes</li> <li>Fresh Installation - Create custom Windows installations with full control</li> </ol>"},{"location":"tutorial-windows-vm/#prerequisites","title":"Prerequisites","text":"<ul> <li>KubeVirt and CDI installed and running</li> <li>Multus CNI with OVN network configured</li> <li>StorageClass <code>local-path</code> available</li> <li>Ingress controller for HTTP access to ISOs</li> </ul>"},{"location":"tutorial-windows-vm/#step-1-create-iso-hosting-environment","title":"Step 1: Create ISO Hosting Environment","text":""},{"location":"tutorial-windows-vm/#11-create-dedicated-namespace","title":"1.1 Create Dedicated Namespace","text":"<pre><code># hack/windows/iso-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: iso\n</code></pre>"},{"location":"tutorial-windows-vm/#12-create-storage-for-isos","title":"1.2 Create Storage for ISOs","text":"<pre><code># hack/windows/iso-storage-pvc-iso-ns.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: iso-storage\n  namespace: iso\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi  # Increased for Windows ISO + golden images\n  storageClassName: local-path\n</code></pre>"},{"location":"tutorial-windows-vm/#13-http-server-for-isos","title":"1.3 HTTP Server for ISOs","text":"<pre><code># hack/windows/nginx-iso-server-iso-ns.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-iso-server\n  namespace: iso\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-iso-server\n  template:\n    metadata:\n      labels:\n        app: nginx-iso-server\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: iso-storage\n          mountPath: /usr/share/nginx/html\n        - name: nginx-config\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: iso-storage\n        persistentVolumeClaim:\n          claimName: iso-storage\n      - name: nginx-config\n        configMap:\n          name: nginx-iso-config\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-iso-config\n  namespace: iso\ndata:\n  default.conf: |\n    server {\n        listen 80;\n        server_name _;\n        root /usr/share/nginx/html;\n\n        location / {\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n        }\n\n        location ~* \\.(iso|img|qcow2)$ {\n            add_header Content-Type application/octet-stream;\n            add_header Cache-Control \"public, max-age=3600\";\n        }\n    }\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-iso-server\n  namespace: iso\nspec:\n  selector:\n    app: nginx-iso-server\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-windows-vm/#14-ingress-configuration-with-tls","title":"1.4 Ingress Configuration with TLS","text":"<pre><code># hack/windows/iso-ingress-iso-ns.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: iso-server-ingress\n  namespace: iso\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod-http\n    nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"3600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"3600\"\n    nginx.ingress.kubernetes.io/proxy-buffering: \"off\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - iso.stage.kube-dc.com\n    secretName: iso-server-tls\n  rules:\n  - host: iso.stage.kube-dc.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: iso-server\n            port:\n              number: 80\n</code></pre>"},{"location":"tutorial-windows-vm/#15-deploy-infrastructure","title":"1.5 Deploy Infrastructure","text":"<pre><code># Deploy all infrastructure components\nkubectl apply -f hack/windows/iso-namespace.yaml\nkubectl apply -f hack/windows/iso-storage-pvc-iso-ns.yaml\nkubectl apply -f hack/windows/nginx-iso-server-iso-ns.yaml\nkubectl apply -f hack/windows/iso-ingress-iso-ns.yaml\n\n# Verify deployment\nkubectl get pods -n iso\nkubectl get ingress -n iso\n</code></pre>"},{"location":"tutorial-windows-vm/#step-2-download-and-upload-windows-iso","title":"Step 2: Download and Upload Windows ISO","text":""},{"location":"tutorial-windows-vm/#21-download-windows-11-enterprise-iso","title":"2.1 Download Windows 11 Enterprise ISO","text":"<ol> <li>Visit: https://www.microsoft.com/en-us/evalcenter/download-windows-11-enterprise</li> <li>Select: ISO \u2013 Enterprise download 64-bit edition (90-day evaluation)  </li> <li>Download the ISO file (approximately 5.4GB)</li> </ol>"},{"location":"tutorial-windows-vm/#22-download-virtio-drivers","title":"2.2 Download VirtIO Drivers","text":"<pre><code># Download latest VirtIO drivers\nwget https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso\n</code></pre>"},{"location":"tutorial-windows-vm/#23-upload-files-to-cluster","title":"2.3 Upload Files to Cluster","text":"<pre><code># Create temporary upload pod\nkubectl apply -f hack/windows/iso-upload-pod.yaml\n\n# Wait for pod to be ready\nkubectl wait --for=condition=Ready pod/iso-upload-pod -n iso --timeout=60s\n\n# Upload Windows 11 ISO (replace with your actual filename)\nkubectl cp ~/Win11_24H2_EnglishInternational_x64.iso iso/iso-upload-pod:/storage/win11-x64.iso\n\n# Upload VirtIO drivers\nkubectl cp ~/virtio-win.iso iso/iso-upload-pod:/storage/virtio-win.iso\n\n# Upload OpenSSH installation script\nkubectl cp hack/windows/install-openssh-windows.ps1 iso/iso-upload-pod:/storage/install-openssh-windows.ps1\n\n# Clean up upload pod\nkubectl delete pod iso-upload-pod -n iso --wait=true\n\n# Verify files are accessible\ncurl -I https://iso.stage.kube-dc.com/win11-x64.iso\ncurl -I https://iso.stage.kube-dc.com/virtio-win.iso\n</code></pre>"},{"location":"tutorial-windows-vm/#step-3-fresh-windows-installation","title":"Step 3: Fresh Windows Installation","text":""},{"location":"tutorial-windows-vm/#31-deploy-installation-vm","title":"3.1 Deploy Installation VM","text":"<p>Use the complete VM manifest that includes all required DataVolumes:</p> <pre><code># Deploy Windows VM with installation ISOs\nkubectl apply -f hack/windows/windows11-vm.yaml\n\n# Monitor DataVolume download progress\nkubectl get dv -n shalb-dev\n\n# Check VM status\nkubectl get vm,vmi -n shalb-dev | grep windows11\n</code></pre>"},{"location":"tutorial-windows-vm/#32-windows-installation-process","title":"3.2 Windows Installation Process","text":"<pre><code># Access VM console via VNC\nvirtctl vnc windows11-vm -n shalb-dev\n\n# Or use VNC proxy\nvirtctl vnc windows11-vm -n shalb-dev --proxy-only --port 5900\n# Then connect VNC client to localhost:5900\n</code></pre> <p>Installation Steps:</p> <ol> <li> <p>Boot from Windows ISO: VM boots from Windows installer (bootOrder: 1)</p> </li> <li> <p>Load VirtIO Drivers: </p> </li> <li>When prompted for disk drivers, click \"Load driver\"</li> <li>Browse to VirtIO drivers CDROM</li> <li>Navigate to <code>/amd64/w11/</code> folder</li> <li>Install VirtIO SCSI controller drivers (for disk access)</li> <li> <p>DO NOT install network drivers yet (to create local account)</p> </li> <li> <p>Install Windows: </p> </li> <li>Select the 60GB VirtIO disk for installation</li> <li> <p>Complete Windows 11 setup with local account</p> </li> <li> <p>Post-Installation:</p> </li> <li> <p>Install remaining VirtIO drivers from CDROM (network, balloon, RNG)</p> </li> <li>Install QEMU Guest Agent from VirtIO drivers CDROM</li> <li>Run Windows Updates</li> </ol>"},{"location":"tutorial-windows-vm/#33-configure-ssh-and-rdp","title":"3.3 Configure SSH and RDP","text":"<p>After Windows installation, configure SSH and RDP access:</p> <pre><code># Method 1: Download and run script\nInvoke-WebRequest -Uri \"https://iso.stage.kube-dc.com/install-openssh-windows.ps1\" -OutFile \"install-openssh-windows.ps1\"\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n.\\install-openssh-windows.ps1\n\n# Method 2: Direct execution (bypass execution policy)\nPowerShell -ExecutionPolicy Bypass -Command \"Invoke-Expression (Invoke-WebRequest -Uri 'https://iso.stage.kube-dc.com/install-openssh-windows.ps1').Content\"\n</code></pre> <p>Script Features:</p> <ul> <li>\u2705 Installs OpenSSH Server using Windows capabilities</li> <li>\u2705 Configures SSH service for automatic startup  </li> <li>\u2705 Opens SSH port 22 in Windows Firewall (all network profiles)</li> <li>\u2705 Enables Remote Desktop (port 3389)</li> <li>\u2705 Enables ICMP ping (IPv4 and IPv6)</li> <li>\u2705 Provides detailed verification and status reporting</li> </ul>"},{"location":"tutorial-windows-vm/#step-4-create-golden-image","title":"Step 4: Create Golden Image","text":""},{"location":"tutorial-windows-vm/#41-prepare-vm-for-golden-image","title":"4.1 Prepare VM for Golden Image","text":"<pre><code># 1. Inside Windows VM, run Sysprep (optional but recommended)\n# Navigate to: C:\\Windows\\System32\\Sysprep\\sysprep.exe\n# Options: Generalize, Enter System Out-of-Box Experience (OOBE), Shutdown\n\n# 2. Stop the source VM (CRITICAL for export)\nkubectl patch vm windows11-vm -n shalb-dev --type merge -p '{\"spec\":{\"runStrategy\":\"Halted\"}}'\nkubectl wait --for=delete vmi/windows11-vm -n shalb-dev --timeout=300s\n</code></pre>"},{"location":"tutorial-windows-vm/#42-export-to-qcow2-golden-image","title":"4.2 Export to QCOW2 Golden Image","text":"<pre><code># hack/windows/export-golden-image.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: export-golden-image\n  namespace: shalb-dev\nspec:\n  restartPolicy: Never\n  containers:\n    - name: exporter\n      image: ubuntu:22.04\n      command: [\"sh\", \"-c\"]\n      args:\n        - |\n          set -e\n          apt-get update &amp;&amp; apt-get install -y qemu-utils curl\n          cd /pvc\n\n          echo \"=== Disk Information ===\"\n          DISK_SIZE_BYTES=$(qemu-img info --output=json disk.img | grep '\"virtual-size\"' | cut -d: -f2 | tr -d ' ,')\n          DISK_SIZE_GB=$((DISK_SIZE_BYTES / 1024 / 1024 / 1024))\n          echo \"Source disk: ${DISK_SIZE_GB}GB (${DISK_SIZE_BYTES} bytes)\"\n\n          echo \"=== Converting to compressed QCOW2 ===\"\n          qemu-img convert -p -O qcow2 -c disk.img windows11-x64-golden.qcow2\n\n          echo \"=== Final Golden Image ===\"\n          ls -lh windows11-x64-golden.qcow2\n          qemu-img info windows11-x64-golden.qcow2\n\n          echo \"=== Upload to ISO server ===\"\n          # Upload to ISO server storage\n          curl -T windows11-x64-golden.qcow2 http://nginx-iso-server.iso.svc.cluster.local/windows11-x64-golden.qcow2 || echo \"Upload failed, manual copy required\"\n\n          echo \"=== Export completed, sleeping for inspection ===\"\n          sleep 3600\n      volumeMounts:\n        - name: source-disk\n          mountPath: /pvc\n  volumes:\n    - name: source-disk\n      persistentVolumeClaim:\n        claimName: windows11-disk\n</code></pre>"},{"location":"tutorial-windows-vm/#43-export-process","title":"4.3 Export Process","text":"<pre><code># Export golden image\nkubectl apply -f hack/windows/export-golden-image.yaml\nkubectl wait --for=condition=Ready pod/export-golden-image -n shalb-dev --timeout=120s\n\n# Monitor export progress\nkubectl logs -n shalb-dev export-golden-image -f\n\n# Manual copy to ISO server (if curl upload fails)\nkubectl cp shalb-dev/export-golden-image:/pvc/windows11-x64-golden.qcow2 /tmp/\nkubectl cp /tmp/windows11-x64-golden.qcow2 iso/iso-upload-pod:/storage/\n\n# Verify golden image is available\ncurl -I https://iso.stage.kube-dc.com/windows11-x64-golden.qcow2\n\n# Clean up export pod\nkubectl delete pod export-golden-image -n shalb-dev --wait=true\n</code></pre>"},{"location":"tutorial-windows-vm/#step-5-deploy-from-golden-image","title":"Step 5: Deploy from Golden Image","text":""},{"location":"tutorial-windows-vm/#51-golden-image-deployment-recommended","title":"5.1 Golden Image Deployment (Recommended)","text":"<pre><code># Deploy VM from golden image\nkubectl apply -f hack/windows/win11-x64.yaml\n\n# Create SSH key secret for key injection\nkubectl create secret generic authorized-keys-default \\\n  --from-file=key1=~/.ssh/id_rsa.pub \\\n  -n shalb-dev\n\n# Monitor deployment\nkubectl get vm,vmi,dv -n shalb-dev | grep win11-x64\n\n# Get VM IP when ready\nkubectl get vmi win11-x64 -n shalb-dev -o jsonpath='{.status.interfaces[0].ipAddress}'\n\n# SSH to VM (once guest agent is ready)\nssh kube-dc@&lt;vm-ip&gt;\n</code></pre>"},{"location":"tutorial-windows-vm/#52-golden-image-benefits","title":"5.2 Golden Image Benefits","text":"Aspect Golden Image Fresh Install Deployment Time 5-10 minutes 30+ minutes Download Size 21.3GB compressed 5.4GB + drivers Configuration Pre-configured Manual setup required SSH/RDP Ready immediately Requires script execution VirtIO Drivers Pre-installed Manual installation Use Case Production deployment Custom configurations"},{"location":"tutorial-windows-vm/#step-6-troubleshooting","title":"Step 6: Troubleshooting","text":""},{"location":"tutorial-windows-vm/#61-common-issues","title":"6.1 Common Issues","text":"<p>DataVolume stuck in ImportScheduled: <pre><code># Check CDI importer pods\nkubectl get pods -n cdi\nkubectl logs -n cdi &lt;importer-pod&gt;\n\n# Check storage provisioner\nkubectl get pods -n local-path-storage\n</code></pre></p> <p>VM won't start - CPU resources: <pre><code># Check node resources\nkubectl describe nodes | grep -A 10 \"Allocated resources\"\n\n# Reduce VM CPU if needed\nkubectl patch vm &lt;vm-name&gt; -n &lt;namespace&gt; --type merge -p '{\"spec\":{\"template\":{\"spec\":{\"domain\":{\"cpu\":{\"cores\":2}}}}}}'\n</code></pre></p> <p>SSH not working: <pre><code># Check guest agent connection\nkubectl describe vmi &lt;vm-name&gt; -n &lt;namespace&gt; | grep -i agent\n\n# Test network connectivity\nkubectl run test-pod --image=nicolaka/netshoot --rm -it -- ping &lt;vm-ip&gt;\nkubectl run test-pod --image=nicolaka/netshoot --rm -it -- nc -zv &lt;vm-ip&gt; 22\n</code></pre></p> <p>Storage issues with local-path: <pre><code># Check local-path provisioner\nkubectl get pods -n local-path-storage\nkubectl logs -n local-path-storage &lt;provisioner-pod&gt;\n\n# Note: local-path doesn't support Block mode, use Filesystem mode\n</code></pre></p>"},{"location":"tutorial-windows-vm/#62-verification-commands","title":"6.2 Verification Commands","text":"<pre><code># Check all Windows VMs\nkubectl get vm -A | grep -i win\n\n# Check DataVolume progress\nkubectl get dv -n &lt;namespace&gt;\n\n# Access VM console\nvirtctl vnc &lt;vm-name&gt; -n &lt;namespace&gt;\n\n# Check VM resource usage\nkubectl top pods -n &lt;namespace&gt; | grep virt-launcher\n</code></pre>"},{"location":"tutorial-windows-vm/#required-manifests-summary","title":"Required Manifests Summary","text":"<p>Infrastructure (Step 1): - <code>hack/windows/iso-namespace.yaml</code> - ISO namespace - <code>hack/windows/iso-storage-pvc-iso-ns.yaml</code> - Storage for ISOs - <code>hack/windows/nginx-iso-server-iso-ns.yaml</code> - HTTP server - <code>hack/windows/iso-ingress-iso-ns.yaml</code> - Ingress configuration</p> <p>Utilities: - <code>hack/windows/iso-upload-pod.yaml</code> - Upload files to cluster - <code>hack/windows/install-openssh-windows.ps1</code> - SSH/RDP configuration script</p> <p>VM Deployment: - <code>hack/windows/windows11-vm.yaml</code> - Fresh installation VM - <code>hack/windows/win11-x64.yaml</code> - Golden image deployment</p> <p>Golden Image Creation: - <code>hack/windows/export-golden-image.yaml</code> - Export VM to QCOW2 - <code>hack/windows/windows11-enterprise-golden-datasource.yaml</code> - DataSource for cloning</p> <p>Optional: - <code>hack/windows/windows11-from-datasource.yaml</code> - Deploy from DataSource (same namespace) - <code>hack/windows/pvc-init-windows11-disk.yaml</code> - Fix disk size issues if needed</p>"},{"location":"tutorial-windows-vm/#available-resources","title":"Available Resources","text":"<p>Once deployed, the following resources are available:</p> <ul> <li>Windows 11 ISO: <code>https://iso.stage.kube-dc.com/win11-x64.iso</code> (5.4GB)</li> <li>VirtIO Drivers: <code>https://iso.stage.kube-dc.com/virtio-win.iso</code> (700MB)</li> <li>SSH Script: <code>https://iso.stage.kube-dc.com/install-openssh-windows.ps1</code> (5KB)</li> <li>Golden Image: <code>https://iso.stage.kube-dc.com/windows11-x64-golden.qcow2</code> (21.3GB)</li> </ul>"},{"location":"tutorial-windows-vm/#security-considerations","title":"Security Considerations","text":"<ul> <li>SSH Keys: Use KubeVirt accessCredentials for secure key injection {{ ... }}</li> <li>Network Policies: Implement Kubernetes network policies for VM isolation</li> <li>Sysprep: Run before creating golden images to avoid SID conflicts</li> <li>Firewall: Script configures Windows Firewall appropriately for SSH, RDP, and ICMP</li> </ul> <p>\u2705 Complete Windows 11 VM infrastructure with golden image support ready for production use!</p>"},{"location":"user-groups/","title":"User and Group Management","text":"<p>This guide explains how to set up and manage users, groups, and roles in Kube-DC using Kubernetes RBAC and Keycloak integration.</p>"},{"location":"user-groups/#overview","title":"Overview","text":"<p>Kube-DC implements a multi-tenant access control system that combines:</p> <ul> <li>Kubernetes RBAC: Handles resource-level permissions within namespaces</li> <li>Organization Groups: Manages project-level access across namespaces</li> <li>Keycloak Integration: Provides user authentication and group management</li> </ul>"},{"location":"user-groups/#standard-roles","title":"Standard Roles","text":"<p>Kube-DC automatically creates standard roles when organizations and projects are provisioned. These provide a baseline permission structure that can be extended with custom roles.</p>"},{"location":"user-groups/#organization-level-roles","title":"Organization-Level Roles","text":"Role Group Permissions <code>{org}-admin</code> <code>org-admin</code> Full CRUD on organizations, projects, organizationgroups <code>{org}-user</code> <code>user</code> Read-only access to organization and projects list"},{"location":"user-groups/#project-level-roles","title":"Project-Level Roles","text":"Role Description Key Permissions <code>admin</code> Full project access All resources: <code>*</code> verbs, RBAC management <code>developer</code> VM/workload management VMs, pods, services: full CRUD; secrets: read-only; no RBAC <code>project-manager</code> View + console access All resources: get, list, watch; VM console/VNC access <code>user</code> Read-only access All resources: get, list; no console, no secrets"},{"location":"user-groups/#automatic-role-bindings","title":"Automatic Role Bindings","text":"<p>When a project is created, the following bindings are automatically configured:</p> RoleBinding Subject Role Purpose <code>org-admin</code> <code>{org}:org-admin</code> <code>admin</code> Org admins get full project access <code>user</code> <code>{org}:user</code> <code>user</code> All org users get read-only access"},{"location":"user-groups/#assigning-elevated-access","title":"Assigning Elevated Access","text":"<p>To grant users elevated access (developer or project-manager roles), create an OrganizationGroup:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: dev-team\n  namespace: shalb  # organization namespace\nspec:\n  permissions:\n  - project: demo\n    roles:\n    - developer  # grants developer role in shalb-demo project\n</code></pre> <p>This creates a Keycloak group <code>dev-team</code>. Add users to this group in Keycloak to grant them developer access to the specified project.</p>"},{"location":"user-groups/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you're working from an Organization Admin perspective. You'll need:</p> <ul> <li>Access to the Kube-DC cluster with organization admin privileges</li> <li><code>kubectl</code> configured to access your cluster with organization admin privileges</li> <li>Access to the Keycloak organization admin console</li> </ul> <p>Before You Begin</p> <p>During organization and project creation you will get a namespace with organization name <code>&lt;orgname&gt;</code> created and project namespace with <code>&lt;orgname&gt;-&lt;projectname&gt;</code> pattern.</p>"},{"location":"user-groups/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"user-groups/#creating-custom-project-roles","title":"Creating Custom Project Roles","text":"<p>While Kube-DC provides standard roles (<code>admin</code>, <code>developer</code>, <code>project-manager</code>, <code>user</code>) that cover most use cases, you can create custom Kubernetes Roles for specialized permission requirements.</p> <p>When to Use Custom Roles</p> <p>Custom roles are useful when:</p> <ul> <li>You need permissions not covered by standard roles</li> <li>You want to restrict access to specific resource types</li> <li>You need fine-grained control over particular workloads</li> </ul> <p>Example: Custom role for managing deployments and services</p> <pre><code>apiVersion: rbac.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: shalb-demo  # Replace with your project namespace\n  name: deployment-manager  # Custom role name\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\", \"services\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"daemonsets\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n</code></pre> <p>Apply the custom role:</p> <pre><code>kubectl apply -f custom-role.yaml\n</code></pre> <p>Role Scope</p> <p>Roles are namespace-scoped. If you need permissions across multiple namespaces, create the Role in each project namespace or reference it via OrganizationGroup.</p>"},{"location":"user-groups/#creating-organization-groups","title":"Creating Organization Groups","text":"<p>Create an OrganizationGroup Custom Resource (CR) to define group permissions across projects.</p> <p>Key Points</p> <ul> <li>The OrganizationGroup CR automatically creates a corresponding group in Keycloak</li> <li>This CR must be created in the organization namespace, not the project namespace</li> <li>Role bindings would be created by this CR</li> </ul> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb  # namespace of the organization (not the project)\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - developer  # standard role, or use custom role name like 'deployment-manager'\n  # Additional projects and roles can be added:\n  # - project: \"prod\"\n  #   roles:\n  #   - project-manager\n</code></pre> <p>Apply the group configuration:</p> <pre><code>kubectl apply -f organization-group.yaml\n</code></pre>"},{"location":"user-groups/#managing-users-in-keycloak","title":"Managing Users in Keycloak","text":""},{"location":"user-groups/#access-keycloak-admin-console","title":"Access Keycloak Admin Console","text":"<p>Retrieve Keycloak access credentials from your organization namespace:</p> <pre><code>kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.user}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\n</code></pre> <p>Remember</p> <p>Replace <code>shalb</code> with your own organization namespace in the commands above.</p>"},{"location":"user-groups/#create-and-configure-users","title":"Create and Configure Users","text":"<ol> <li>Log in to the Keycloak admin console using the retrieved credentials</li> <li>Navigate to Users \u2192 Add User </li> <li>Fill in the required user information</li> <li>Set up initial password in the Credentials tab</li> <li>Add the user to the appropriate group (e.g., \"app-manager\") via the Groups tab </li> </ol> <p>User Group Mapping</p> <p>Any groups created via OrganizationGroup CRs will appear automatically in Keycloak. Changes to group membership in Keycloak are synchronized with Kubernetes RBAC.</p>"},{"location":"user-groups/#accessing-kube-dc-ui","title":"Accessing Kube-DC UI","text":"<ol> <li>Navigate to the Kube-DC UI login page</li> <li>Log in using the credentials created in Keycloak</li> <li>Verify access to assigned project resources</li> </ol> <p>Permissions Troubleshooting</p> <p>If a user cannot access expected resources: - Verify they're assigned to the correct groups in Keycloak - Check that the OrganizationGroup CR includes the correct projects and roles - Ensure the underlying Kubernetes Roles have appropriate permissions - Examine the Keycloak logs for authentication issues</p> <p>Permission changes may take up to 5 minutes to propagate through the system.</p>"},{"location":"prd/acme_challenge_http_route/","title":"PRD: ACME HTTP-01 Challenge Integration with Envoy Gateway","text":""},{"location":"prd/acme_challenge_http_route/#status","title":"Status","text":"Component Status Gateway Backend auto-creation \u2705 Implemented (commit <code>dac0122</code>) Shared Envoy Gateway \u2705 Deployed (replaces nginx-ingress) HTTPRoute Controller (mutation) \ud83d\udd32 Pending ACME Solver Service detection \ud83d\udd32 Pending Manual test \u2705 Verified"},{"location":"prd/acme_challenge_http_route/#business-context","title":"Business Context","text":"<p>Goal: Minimize IPv4 usage per tenant by using shared gateways for traffic multiplexing.</p> <p>Architecture Vision: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Shared Envoy Gateway: eg (envoy-gateway-system)        \u2502\n\u2502                         88.99.29.250                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  HTTP :80 (ACME + redirect)  \u2502  TLS :6443 (passthrough)            \u2502\n\u2502           \u2193                  \u2502           \u2193                          \u2502\n\u2502  HTTPRoute \u2192 Backend         \u2502  TLSRoute \u2192 Backend                  \u2502\n\u2502  (solver ext-cloud IP)       \u2502  (tenant K8s API ext-cloud IP)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  HTTPS :443 (per-host TLS termination for platform services)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Customer A (ext-cloud)   \u2502  \u2502  Customer B (ext-cloud)   \u2502\n\u2502  100.65.0.10              \u2502  \u2502  100.65.0.20              \u2502\n\u2502  - Gets cert via ACME     \u2502  \u2502  - Gets cert via ACME     \u2502\n\u2502  - Terminates TLS itself  \u2502  \u2502  - Terminates TLS itself  \u2502\n\u2502  - No public IP needed    \u2502  \u2502  - No public IP needed    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Key Benefits: - 1000 customers, 1-3 public IPs (instead of 1000 public IPs) - No certificate sharing - gateway never sees customer certs (TLS passthrough) - Customer isolation - each customer terminates TLS in their own namespace - Standard Let's Encrypt - customers use HTTP-01 challenge, no DNS API needed</p>"},{"location":"prd/acme_challenge_http_route/#problem-statement","title":"Problem Statement","text":"<p>Customers want to use Let's Encrypt certificates with HTTP-01 challenges in their project namespaces. However, the current network isolation between Envoy Gateway (<code>envoy-gateway-system</code> / <code>ovn-default</code>) and project namespaces (isolated subnets) prevents cert-manager's ACME solver from working with the shared Gateway.</p>"},{"location":"prd/acme_challenge_http_route/#scope-cloud-networking-only","title":"Scope: Cloud Networking Only","text":"<p>This solution applies only to cloud networking (<code>ext-cloud</code> / <code>externalNetworkType: cloud</code>):</p> Network Type Gateway can reach Service? Solution Needed? Public (<code>ext-public</code>) \u2705 Yes - routable public IP \u274c No - standard routing works Cloud (<code>ext-cloud</code>) \u274c No - isolated VPC subnet \u2705 Yes - Backend required <p>For public network, the Gateway can directly route to services via their public IPs - no Backend workaround needed.</p> <p>For cloud network, services get private IPs (e.g., <code>100.65.x.x</code>) that are only reachable via static routes on nodes. The Gateway (in <code>ovn-default</code>) cannot reach these IPs directly, hence the Backend + ext-cloud IP pattern.</p>"},{"location":"prd/acme_challenge_http_route/#current-architecture","title":"Current Architecture","text":""},{"location":"prd/acme_challenge_http_route/#what-works-tls-passthrough-with-backend","title":"What Works: TLS Passthrough with Backend","text":"<pre><code>Gateway \u2192 TLSRoute \u2192 Backend (ext-cloud IP) \u2192 K8s API Service\n</code></pre> <p>We use a Backend resource pointing to an external IP (ext-cloud), which is reachable from the Gateway via static routes on nodes.</p> <pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: Backend\nmetadata:\n  name: demo-cluster-api\n  namespace: shalb-demo\nspec:\n  endpoints:\n  - ip:\n      address: 100.65.0.104  # ext-cloud IP - reachable from Gateway\n      port: 6443\n</code></pre>"},{"location":"prd/acme_challenge_http_route/#what-doesnt-work-acme-http-01-challenge","title":"What Doesn't Work: ACME HTTP-01 Challenge","text":"<p>cert-manager's <code>gatewayHTTPRoute</code> solver creates:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nspec:\n  backendRefs:\n  - kind: Service  # \u2190 Points to ClusterIP Service directly\n    name: cm-acme-http-solver-xyz\n    namespace: shalb-demo\n    port: 8089\n</code></pre> <p>Why it fails: 1. Gateway tries to reach ClusterIP (<code>10.x.x.x</code>) in isolated project subnet 2. Gateway (in <code>ovn-default</code>) can't reach project subnet due to network isolation 3. ACME challenge fails with connection timeout</p>"},{"location":"prd/acme_challenge_http_route/#root-cause","title":"Root Cause","text":"Resource Created By Points To Reachable? TLSRoute (K8s CP) User Backend with ext-cloud IP \u2713 Yes HTTPRoute (ACME) cert-manager Service (ClusterIP) \u2717 No <p>The difference: we control Backend creation for K8s CP, but cert-manager hardcodes Service references for ACME challenges.</p>"},{"location":"prd/acme_challenge_http_route/#proposed-solution","title":"Proposed Solution","text":""},{"location":"prd/acme_challenge_http_route/#controller-based-approach-recommended","title":"Controller-Based Approach (Recommended)","text":"<p>Instead of using a Mutating Admission Webhook, we use a controller-based reconciliation pattern. This approach:</p> <ol> <li>Follows Kubernetes best practices - Controllers are the standard way to manage resources</li> <li>No webhook latency - Webhooks add latency to API server requests</li> <li>Simpler deployment - No need for webhook certificates, TLS configuration</li> <li>Idempotent - Controllers naturally handle retries and eventual consistency</li> <li>Easier debugging - Controller logs are easier to trace than webhook calls</li> </ol> <p>Why not Mutating Webhook? - Webhooks intercept requests synchronously, adding latency - Webhook failures can block resource creation entirely - Requires additional TLS certificate management - More complex failure modes</p> <p>Controller Pattern: - Watch HTTPRoutes with ACME solver labels - Reconcile by modifying <code>backendRefs</code> from Service \u2192 Backend - Idempotent updates - if already correct, no change needed</p>"},{"location":"prd/acme_challenge_http_route/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  HTTPRoute Controller                                       \u2502\n\u2502  (existing kube-dc-manager)                                 \u2502\n\u2502                                                             \u2502\n\u2502  Watches:                                                   \u2502\n\u2502  - HTTPRoutes with label acme.cert-manager.io/http-domain   \u2502\n\u2502  - Services with label acme.cert-manager.io/http01-solver   \u2502\n\u2502                                                             \u2502\n\u2502  Actions:                                                   \u2502\n\u2502  1. When ACME solver Service created:                       \u2502\n\u2502     \u2192 Convert to LoadBalancer (bind to default-gw EIP)      \u2502\n\u2502     \u2192 Auto-creates Backend via existing annotation          \u2502\n\u2502                                                             \u2502\n\u2502  2. When ACME HTTPRoute created:                            \u2502\n\u2502     \u2192 Patch backendRef: Service \u2192 Backend                   \u2502\n\u2502     \u2192 Gateway can now route to ext-cloud IP                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/acme_challenge_http_route/#flow-diagram","title":"Flow Diagram","text":"<pre><code>1. Customer creates Certificate resource\n                \u2193\n2. cert-manager creates Challenge\n                \u2193\n3. cert-manager creates Solver Pod + Service (ClusterIP)\n   (label: acme.cert-manager.io/http01-solver=true)\n                \u2193\n4. [CONTROLLER] Detects ACME solver Service\n   \u2192 Patches Service: type=LoadBalancer, annotations for EIP + Backend\n   \u2192 Existing service controller creates:\n     - EIP (externalNetworkType: cloud)\n     - Backend pointing to EIP (via create-gateway-backend annotation)\n                \u2193\n5. cert-manager creates HTTPRoute (pointing to Service)\n   (label: acme.cert-manager.io/http-domain=&lt;domain&gt;)\n                \u2193\n6. [CONTROLLER] Detects ACME HTTPRoute\n   \u2192 Patches backendRef: Service \u2192 Backend\n   \u2192 Uses Backend name: &lt;service-name&gt;-backend\n                \u2193\n7. Gateway routes ACME challenge to Backend (ext-cloud IP)\n                \u2193\n8. ACME challenge succeeds\n                \u2193\n9. Certificate issued\n                \u2193\n10. cert-manager deletes solver resources\n    \u2192 Service deletion cascades to Backend (ownerReference)\n</code></pre>"},{"location":"prd/acme_challenge_http_route/#implementation-details","title":"Implementation Details","text":""},{"location":"prd/acme_challenge_http_route/#implemented-gateway-backend-auto-creation","title":"Implemented: Gateway Backend Auto-Creation","text":"<p>Already implemented in commit <code>dac0122</code>. When a LoadBalancer service has the annotation:</p> <pre><code>annotations:\n  service.nlb.kube-dc.com/create-gateway-backend: \"true\"\n</code></pre> <p>The service controller automatically creates a Backend resource pointing to the service's external IP.</p> <p>Usage: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  annotations:\n    service.nlb.kube-dc.com/bind-on-eip: default-gw\n    service.nlb.kube-dc.com/create-gateway-backend: \"true\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8089\n</code></pre></p> <p>This creates: - LoadBalancer with ext-cloud IP (e.g., <code>100.65.0.102</code>) - Backend <code>my-app-backend</code> pointing to <code>100.65.0.102:8089</code></p>"},{"location":"prd/acme_challenge_http_route/#part-1-acme-solver-service-controller","title":"Part 1: ACME Solver Service Controller","text":"<p>Extend the existing service controller to detect ACME solver services and automatically add the required annotations. Only applies to cloud networking.</p> <p>Watch Predicate: <pre><code>func isACMESolverService(obj client.Object) bool {\n    labels := obj.GetLabels()\n    return labels[\"acme.cert-manager.io/http01-solver\"] == \"true\"\n}\n</code></pre></p> <p>Reconcile Logic: <pre><code>func (r *ServiceReconciler) reconcileACMESolver(ctx context.Context, svc *corev1.Service, project *kubedcv1.Project) error {\n    // Check if this is an ACME solver service\n    if svc.Labels[\"acme.cert-manager.io/http01-solver\"] != \"true\" {\n        return nil\n    }\n\n    // Determine network type from:\n    // 1. Service annotation (explicit override)\n    // 2. Project default\n    netType := svc.GetAnnotations()[kubedccomv1.NetworkExternalTypeLabelKey]\n    if netType == \"\" {\n        netType = string(project.Spec.ExternalNetworkType)\n    }\n\n    // Only apply Backend workaround for cloud networking\n    // Public network can route directly - no Backend needed\n    if netType != \"cloud\" {\n        return nil\n    }\n\n    // Check if already patched\n    if svc.Spec.Type == corev1.ServiceTypeLoadBalancer {\n        return nil\n    }\n\n    // Patch service to LoadBalancer with Backend annotation\n    patch := client.MergeFrom(svc.DeepCopy())\n    svc.Spec.Type = corev1.ServiceTypeLoadBalancer\n    if svc.Annotations == nil {\n        svc.Annotations = make(map[string]string)\n    }\n    svc.Annotations[\"service.nlb.kube-dc.com/bind-on-eip\"] = \"default-gw\"\n    svc.Annotations[\"service.nlb.kube-dc.com/create-gateway-backend\"] = \"true\"\n\n    return r.Patch(ctx, svc, patch)\n}\n</code></pre></p> <p>Network Type Annotation: Services can override project's default network type using: <pre><code>annotations:\n  network.kube-dc.com/external-network-type: \"cloud\"  # or \"public\"\n</code></pre></p>"},{"location":"prd/acme_challenge_http_route/#part-2-httproute-controller-not-webhook","title":"Part 2: HTTPRoute Controller (Not Webhook)","text":"<p>Why Controller instead of Mutating Webhook:</p> Aspect Mutating Webhook Controller Timing Synchronous (blocks API) Asynchronous (eventual) Failure mode Blocks resource creation Retries automatically TLS certs Required for webhook Not needed Latency Adds to every request No API latency Debugging Harder to trace Controller logs Best practice For validation/defaults For resource management <p>Controller Implementation: <pre><code>// HTTPRouteReconciler watches HTTPRoutes and patches ACME solver routes\ntype HTTPRouteReconciler struct {\n    client.Client\n    Scheme *runtime.Scheme\n}\n\n// Watch predicate - only ACME HTTPRoutes\nfunc isACMEHTTPRoute(obj client.Object) bool {\n    labels := obj.GetLabels()\n    _, hasACMELabel := labels[\"acme.cert-manager.io/http-domain\"]\n    return hasACMELabel\n}\n\nfunc (r *HTTPRouteReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    route := &amp;gatewayv1.HTTPRoute{}\n    if err := r.Get(ctx, req.NamespacedName, route); err != nil {\n        return ctrl.Result{}, client.IgnoreNotFound(err)\n    }\n\n    // Get project to check network type\n    project := r.getProjectForNamespace(ctx, route.Namespace)\n    if project == nil {\n        return ctrl.Result{}, nil\n    }\n\n    // Only apply Backend workaround for cloud networking\n    // Public network routes directly - no patching needed\n    if project.Spec.ExternalNetworkType != \"cloud\" {\n        return ctrl.Result{}, nil\n    }\n\n    // Check if already patched (backendRef is Backend, not Service)\n    if r.isAlreadyPatched(route) {\n        return ctrl.Result{}, nil\n    }\n\n    // Get the solver service name from backendRef\n    serviceName := r.getServiceNameFromRoute(route)\n    if serviceName == \"\" {\n        return ctrl.Result{}, nil\n    }\n\n    // Check if Backend exists\n    backendName := serviceName + \"-backend\"\n    backend := &amp;egv1alpha1.Backend{}\n    if err := r.Get(ctx, types.NamespacedName{\n        Name:      backendName,\n        Namespace: route.Namespace,\n    }, backend); err != nil {\n        // Backend not ready yet, requeue\n        return ctrl.Result{RequeueAfter: 2 * time.Second}, nil\n    }\n\n    // Patch HTTPRoute to use Backend instead of Service\n    patch := client.MergeFrom(route.DeepCopy())\n    r.patchBackendRefs(route, backendName)\n\n    if err := r.Patch(ctx, route, patch); err != nil {\n        return ctrl.Result{}, err\n    }\n\n    return ctrl.Result{}, nil\n}\n\nfunc (r *HTTPRouteReconciler) patchBackendRefs(route *gatewayv1.HTTPRoute, backendName string) {\n    group := gatewayv1.Group(\"gateway.envoyproxy.io\")\n    kind := gatewayv1.Kind(\"Backend\")\n\n    for i, rule := range route.Spec.Rules {\n        for j, ref := range rule.BackendRefs {\n            if ref.Kind == nil || *ref.Kind == \"Service\" {\n                route.Spec.Rules[i].BackendRefs[j].Group = &amp;group\n                route.Spec.Rules[i].BackendRefs[j].Kind = &amp;kind\n                route.Spec.Rules[i].BackendRefs[j].Name = gatewayv1.ObjectName(backendName)\n            }\n        }\n    }\n}\n</code></pre></p> <p>Controller Setup: <pre><code>func (r *HTTPRouteReconciler) SetupWithManager(mgr ctrl.Manager) error {\n    return ctrl.NewControllerManagedBy(mgr).\n        For(&amp;gatewayv1.HTTPRoute{}).\n        WithEventFilter(predicate.Funcs{\n            CreateFunc: func(e event.CreateEvent) bool {\n                return isACMEHTTPRoute(e.Object)\n            },\n            UpdateFunc: func(e event.UpdateEvent) bool {\n                return isACMEHTTPRoute(e.ObjectNew)\n            },\n        }).\n        Complete(r)\n}\n</code></pre></p>"},{"location":"prd/acme_challenge_http_route/#timing-considerations","title":"Timing Considerations","text":"<p>Race Condition: HTTPRoute may be created before Backend exists.</p> <p>Controller Solution: The controller uses <code>RequeueAfter</code> to handle timing:</p> <ol> <li>HTTPRoute created by cert-manager</li> <li>Controller detects HTTPRoute, checks for Backend</li> <li>If Backend doesn't exist yet \u2192 <code>RequeueAfter: 2 * time.Second</code></li> <li>Controller retries until Backend is ready</li> <li>Once Backend exists, controller patches HTTPRoute</li> </ol> <p>This is more robust than webhook approach because: - No blocking of API requests - Automatic retries with exponential backoff - cert-manager doesn't see any errors (HTTPRoute created successfully) - Controller handles the eventual consistency</p>"},{"location":"prd/acme_challenge_http_route/#customer-experience","title":"Customer Experience","text":""},{"location":"prd/acme_challenge_http_route/#before-manual-process","title":"Before (Manual Process)","text":"<ol> <li>Create namespace-scoped Issuer with DNS-01 (requires DNS API access)</li> <li>OR use external certificate provider</li> <li>Manually manage certificate renewal</li> </ol>"},{"location":"prd/acme_challenge_http_route/#after-automated","title":"After (Automated)","text":"<ol> <li>Create namespace-scoped Issuer with HTTP-01 (standard Let's Encrypt)</li> <li>Create Certificate resource</li> <li>Automatic issuance and renewal via shared Gateway</li> </ol>"},{"location":"prd/acme_challenge_http_route/#end-to-end-customer-flow","title":"End-to-End Customer Flow","text":""},{"location":"prd/acme_challenge_http_route/#step-1-initial-setup-one-time","title":"Step 1: Initial Setup (One-time)","text":"<p>Customer creates their application with TLS termination:</p> <pre><code># Application with nginx for TLS termination\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\n  namespace: customer-project\nspec:\n  template:\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        volumeMounts:\n        - name: tls\n          mountPath: /etc/nginx/ssl\n      volumes:\n      - name: tls\n        secret:\n          secretName: my-app-tls  # Will be created by cert-manager\n---\n# Service with ext-cloud IP (no public IP)\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  namespace: customer-project\n  annotations:\n    ovn.kubernetes.io/service_external_ip_from_subnet: ext-cloud\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 443\n    targetPort: 443\n</code></pre>"},{"location":"prd/acme_challenge_http_route/#step-2-certificate-request","title":"Step 2: Certificate Request","text":"<pre><code># Issuer (points to shared gateway for HTTP-01)\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt\n  namespace: customer-project\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@customer.com\n    privateKeySecretRef:\n      name: letsencrypt-account-key\n    solvers:\n    - http01:\n        gatewayHTTPRoute:\n          parentRefs:\n          - name: shared-gateway\n            namespace: gateway-system\n---\n# Certificate\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: my-app-tls\n  namespace: customer-project\nspec:\n  secretName: my-app-tls\n  issuerRef:\n    name: letsencrypt\n  dnsNames:\n  - my-app.customer.example.com\n</code></pre>"},{"location":"prd/acme_challenge_http_route/#step-3-automatic-acme-challenge-our-controller","title":"Step 3: Automatic ACME Challenge (Our Controller)","text":"<pre><code>1. cert-manager creates solver pod + service\n2. [Controller] Creates EIP + Backend for solver\n3. [Webhook] Mutates HTTPRoute to use Backend\n4. ACME challenge succeeds\n5. Certificate issued to customer namespace\n6. [Controller] Cleans up solver EIP + Backend\n</code></pre>"},{"location":"prd/acme_challenge_http_route/#step-4-tlsroute-for-production-traffic","title":"Step 4: TLSRoute for Production Traffic","text":"<pre><code># Backend pointing to customer service ext-cloud IP\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: Backend\nmetadata:\n  name: my-app-backend\n  namespace: customer-project\nspec:\n  endpoints:\n  - ip:\n      address: 100.65.0.50  # Customer's ext-cloud IP\n      port: 443\n---\n# TLSRoute through shared gateway (passthrough)\napiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TLSRoute\nmetadata:\n  name: my-app\n  namespace: customer-project\nspec:\n  parentRefs:\n  - name: shared-gateway\n    namespace: gateway-system\n  hostnames:\n  - \"my-app.customer.example.com\"\n  rules:\n  - backendRefs:\n    - group: gateway.envoyproxy.io\n      kind: Backend\n      name: my-app-backend\n      port: 443\n</code></pre>"},{"location":"prd/acme_challenge_http_route/#result","title":"Result","text":"<pre><code>User browser\n    \u2193\nhttps://my-app.customer.example.com\n    \u2193\nShared Gateway (88.99.29.250:443)\n    \u2193 TLS passthrough (SNI routing)\nCustomer Service (100.65.0.50:443)\n    \u2193 TLS termination with Let's Encrypt cert\nApplication\n</code></pre> <p>Customer uses 0 public IPs, gets valid Let's Encrypt certificate.</p>"},{"location":"prd/acme_challenge_http_route/#example-customer-resources","title":"Example Customer Resources","text":"<pre><code># 1. Issuer (customer creates)\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt\n  namespace: my-project\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@example.com\n    privateKeySecretRef:\n      name: letsencrypt-account-key\n    solvers:\n    - http01:\n        gatewayHTTPRoute:\n          parentRefs:\n          - name: shared-gateway\n            namespace: gateway-system\n---\n# 2. Certificate (customer creates)\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: my-app-tls\n  namespace: my-project\nspec:\n  secretName: my-app-tls-secret\n  issuerRef:\n    name: letsencrypt\n  dnsNames:\n  - my-app.example.com\n</code></pre> <p>Everything else (EIP, Backend, HTTPRoute mutation) happens automatically.</p>"},{"location":"prd/acme_challenge_http_route/#complexity-assessment","title":"Complexity Assessment","text":"Component Complexity Effort Status Gateway Backend auto-creation Low 1 day \u2705 Done (commit <code>dac0122</code>) ACME Solver Service detection Low 1 day \ud83d\udd32 Pending HTTPRoute Controller Low 1-2 days \ud83d\udd32 Pending Testing Medium 2-3 days E2E with Let's Encrypt staging Total Remaining 4-6 days"},{"location":"prd/acme_challenge_http_route/#current-gateway-setup","title":"Current Gateway Setup","text":"<p>Shared Envoy Gateway: Single gateway shared across all projects.</p> <pre><code>Gateway: eg (envoy-gateway-system)\nAddress: 88.99.29.250\n</code></pre> <p>Listeners: | Port | Protocol | Purpose | Allowed Routes | |------|----------|---------|----------------| | 80 | HTTP | ACME challenges, HTTP\u2192HTTPS redirect | All namespaces | | 443 | HTTPS | Per-host TLS termination (platform services) | All namespaces | | 6443 | TLS | Passthrough for tenant K8s APIs | All namespaces |</p> <p>Key Design: - Single Gateway - All projects share one gateway IP - All namespaces allowed - Projects can create HTTPRoutes/TLSRoutes in their namespace - HTTP :80 available - ACME HTTP-01 challenges can use this listener - TLS Passthrough :6443 - Tenant clusters use Backend \u2192 ext-cloud IP pattern</p>"},{"location":"prd/acme_challenge_http_route/#prerequisites","title":"Prerequisites","text":"<ol> <li>\u2705 cert-manager installed with Gateway API support</li> <li>\u2705 Shared Gateway with HTTP listener (port 80)</li> <li>\u2705 Gateway allows routes from all namespaces</li> <li>\u2705 Envoy Gateway deployed (replaces nginx-ingress)</li> <li>\u2705 DNS pointing to Gateway IP (88.99.29.250)</li> </ol>"},{"location":"prd/acme_challenge_http_route/#risks-and-mitigations","title":"Risks and Mitigations","text":"Risk Mitigation EIP exhaustion Solver uses project's default-gw EIP (shared, no new EIP) Controller timing RequeueAfter handles eventual consistency cert-manager version changes Watch cert-manager release notes for HTTPRoute changes Stale Backends OwnerReferences ensure cleanup when solver Service deleted"},{"location":"prd/acme_challenge_http_route/#alternatives-considered","title":"Alternatives Considered","text":"Alternative Pros Cons Decision Mutating Webhook Immediate mutation Latency, TLS certs, complex failure modes \u274c Rejected Controller-based Eventual consistency, simpler Slight delay \u2705 Chosen DNS-01 challenge No HTTP routing Requires customer DNS API access For advanced users Per-project Gateway Full isolation More resources, more EIPs Not scalable"},{"location":"prd/acme_challenge_http_route/#success-criteria","title":"Success Criteria","text":"<ol> <li>Customer can issue Let's Encrypt certificates using HTTP-01 challenge</li> <li>Certificates automatically renew without customer intervention</li> <li>No manual EIP/Backend management required</li> <li>Works with shared Gateway across multiple projects</li> <li>Proper cleanup when challenges complete</li> </ol>"},{"location":"prd/acme_challenge_http_route/#implementation-order","title":"Implementation Order","text":"<ol> <li>Replace nginx-ingress with Envoy Gateway (external task)</li> <li>Move HTTP :80 and HTTPS :443 to Envoy Gateway</li> <li>Update DNS records</li> <li> <p>Verify existing routes work</p> </li> <li> <p>Implement ACME Solver Service detection</p> </li> <li>Add predicate for <code>acme.cert-manager.io/http01-solver</code> label</li> <li>Auto-patch ClusterIP \u2192 LoadBalancer with annotations</li> <li> <p>Test with manual Certificate creation</p> </li> <li> <p>Implement HTTPRoute Controller</p> </li> <li>Add HTTPRoute reconciler</li> <li>Watch for ACME HTTPRoutes</li> <li>Patch backendRef Service \u2192 Backend</li> <li> <p>Test end-to-end with Let's Encrypt staging</p> </li> <li> <p>E2E Testing</p> </li> <li>Test with Let's Encrypt staging environment</li> <li>Verify certificate issuance and renewal</li> <li>Test cleanup after challenge completion</li> </ol>"},{"location":"prd/acme_challenge_http_route/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Rate limiting: Limit concurrent ACME challenges per project</li> <li>Metrics: Track challenge success/failure rates</li> <li>Alerts: Notify on certificate expiration</li> <li>ClusterIssuer support: Platform-wide issuer for all projects</li> </ol>"},{"location":"prd/auth-config-sync/","title":"Auth Config Sync to Control Plane Nodes","text":""},{"location":"prd/auth-config-sync/#problem-statement","title":"Problem Statement","text":"<p>The kube-dc manager writes OIDC authentication configuration (<code>auth-conf.yaml</code>) to the host filesystem. This configuration is read by the Kubernetes API server for OIDC authentication. However, in a multi-master setup, the manager runs on only one node (via leader election), leaving other control plane nodes without the updated auth configuration.</p>"},{"location":"prd/auth-config-sync/#current-behavior","title":"Current Behavior","text":"<ul> <li>Manager writes <code>auth-conf.yaml</code> to <code>/etc/rancher/auth-conf.yaml</code> on its local node</li> <li>Other control plane nodes have stale or missing auth configuration</li> <li>Users authenticating against API servers on other nodes may fail or see outdated realm configurations</li> </ul>"},{"location":"prd/auth-config-sync/#impact","title":"Impact","text":"<ul> <li>Authentication failures after organization creation/deletion</li> <li>Inconsistent login behavior depending on which API server handles the request</li> <li>Delayed propagation of OIDC configuration changes</li> </ul>"},{"location":"prd/auth-config-sync/#requirements","title":"Requirements","text":""},{"location":"prd/auth-config-sync/#functional-requirements","title":"Functional Requirements","text":"ID Requirement Priority FR-1 Auth config must be synchronized to ALL control plane nodes High FR-2 Sync must occur within 2 seconds of configuration change High FR-3 Solution must work with existing leader election model High FR-4 Solution must handle node additions/removals gracefully Medium FR-5 Solution must be deployed via existing Helm chart Medium"},{"location":"prd/auth-config-sync/#non-functional-requirements","title":"Non-Functional Requirements","text":"ID Requirement Priority NFR-1 Minimal resource overhead (&lt; 50m CPU, &lt; 32Mi memory per node) Medium NFR-2 No external dependencies (SSH, shared storage) High NFR-3 Solution must be idempotent and crash-safe High"},{"location":"prd/auth-config-sync/#solution-design","title":"Solution Design","text":""},{"location":"prd/auth-config-sync/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Control Plane Node 1                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    writes    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502  \u2502   Manager    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   ConfigMap      \u2502                     \u2502\n\u2502  \u2502   (Leader)   \u2502              \u2502 kube-dc-auth-cfg \u2502                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502                                         \u2502                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    mounts    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    writes           \u2502\n\u2502  \u2502  auth-sync   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  ConfigMap Vol   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba/etc/... \u2502\n\u2502  \u2502  DaemonSet   \u2502   inotify    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Control Plane Node 2                              \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    mounts    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    writes           \u2502\n\u2502  \u2502  auth-sync   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  ConfigMap Vol   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba/etc/... \u2502\n\u2502  \u2502  DaemonSet   \u2502   inotify    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Control Plane Node 3                              \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    mounts    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    writes           \u2502\n\u2502  \u2502  auth-sync   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  ConfigMap Vol   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba/etc/... \u2502\n\u2502  \u2502  DaemonSet   \u2502   inotify    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/auth-config-sync/#components","title":"Components","text":""},{"location":"prd/auth-config-sync/#1-manager-modified","title":"1. Manager (Modified)","text":"<ul> <li>Change: After writing <code>auth-conf.yaml</code> to local file, also write to ConfigMap</li> <li>ConfigMap Name: <code>kube-dc-auth-config</code></li> <li>ConfigMap Key: <code>auth-conf.yaml</code></li> <li>Namespace: <code>kube-dc</code> (same as manager)</li> </ul>"},{"location":"prd/auth-config-sync/#2-auth-config-sync-daemonset-new","title":"2. Auth Config Sync DaemonSet (New)","text":"<ul> <li>Name: <code>kube-dc-auth-config-sync</code></li> <li>Runs on: Control plane nodes only (<code>node-role.kubernetes.io/control-plane</code>)</li> <li>Image: <code>busybox:1.36</code> (lightweight, has inotifyd)</li> <li>Function: Watch ConfigMap volume mount, copy to host path on change</li> </ul>"},{"location":"prd/auth-config-sync/#data-flow","title":"Data Flow","text":"<ol> <li>Organization created/updated/deleted</li> <li>Manager reconciles and updates <code>auth-conf.yaml</code></li> <li>Manager writes content to local file (existing behavior)</li> <li>Manager writes content to ConfigMap <code>kube-dc-auth-config</code></li> <li>Kubernetes propagates ConfigMap to all volume mounts (~1-2 seconds)</li> <li>DaemonSet pod on each node detects hash change (polling every 2s)</li> <li>DaemonSet copies file to host path <code>/etc/rancher/auth-conf.yaml</code> with 0666 permissions</li> <li>API server reloads OIDC config (Kubernetes 1.30+ dynamic reload)</li> </ol>"},{"location":"prd/auth-config-sync/#timing-analysis","title":"Timing Analysis","text":"Step Duration Manager writes ConfigMap &lt; 100ms Kubelet syncs ConfigMap to volume ~1-2 seconds Polling detects change &lt; 2 seconds Total end-to-end ~1-2 seconds"},{"location":"prd/auth-config-sync/#implementation-plan","title":"Implementation Plan","text":""},{"location":"prd/auth-config-sync/#phase-1-modify-manager","title":"Phase 1: Modify Manager","text":"<ol> <li>Add Kubernetes client to <code>KubeAuthClient</code> struct</li> <li>Create <code>saveToConfigMap()</code> method</li> <li>Update <code>SaveAuthFile()</code> to also call <code>saveToConfigMap()</code></li> <li>Handle ConfigMap create/update logic</li> </ol> <p>Files to modify: - <code>internal/organization/client_kube_auth.go</code> - <code>internal/organization/res_kube_auth.go</code> - <code>internal/organization/organization.go</code></p>"},{"location":"prd/auth-config-sync/#phase-2-create-daemonset","title":"Phase 2: Create DaemonSet","text":"<ol> <li>Create Helm template for DaemonSet</li> <li>Add values.yaml configuration</li> <li>Configure:</li> <li>nodeSelector for control-plane nodes</li> <li>tolerations for control-plane taints</li> <li>hostPath volume mount</li> <li>ConfigMap volume mount</li> <li>inotifyd watch script</li> </ol> <p>Files to create/modify: - <code>charts/kube-dc/templates/auth-config-sync-daemonset.yaml</code> (new) - <code>charts/kube-dc/values.yaml</code></p>"},{"location":"prd/auth-config-sync/#phase-3-testing","title":"Phase 3: Testing","text":"<ol> <li>Deploy to test cluster with 3 control plane nodes</li> <li>Create new organization</li> <li>Verify auth config appears on all nodes within 2 seconds</li> <li>Delete organization</li> <li>Verify auth config updated on all nodes</li> <li>Test node failure/recovery scenarios</li> </ol>"},{"location":"prd/auth-config-sync/#daemonset-specification","title":"DaemonSet Specification","text":"<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-dc-auth-config-sync\nspec:\n  selector:\n    matchLabels:\n      app: kube-dc-auth-config-sync\n  template:\n    spec:\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      containers:\n      - name: sync\n        image: busybox:1.36\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n        - |\n          echo \"Starting auth config sync daemon...\"\n          TARGET=\"/etc/rancher/auth-conf.yaml\"\n          SOURCE=\"/config/auth-conf.yaml\"\n          LAST_HASH=\"\"\n\n          sync_file() {\n            if [ -f \"$SOURCE\" ]; then\n              CURRENT_HASH=$(md5sum \"$SOURCE\" 2&gt;/dev/null | cut -d' ' -f1)\n              if [ \"$CURRENT_HASH\" != \"$LAST_HASH\" ]; then\n                cp \"$SOURCE\" \"$TARGET\"\n                chmod 0666 \"$TARGET\"  # Allow non-root manager to write\n                LAST_HASH=\"$CURRENT_HASH\"\n                echo \"$(date): Synced $SOURCE -&gt; $TARGET (hash: $CURRENT_HASH)\"\n              fi\n            fi\n          }\n\n          # Initial sync\n          sync_file\n\n          # Poll for changes every 2 seconds\n          # (ConfigMap symlink updates don't trigger inotify reliably)\n          echo \"Watching for changes...\"\n          while true; do\n            sync_file\n            sleep 2\n          done\n        securityContext:\n          privileged: true\n          runAsUser: 0\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: host-path\n          mountPath: /etc/rancher\n        resources:\n          limits:\n            cpu: 50m\n            memory: 32Mi\n          requests:\n            cpu: 10m\n            memory: 16Mi\n      volumes:\n      - name: config\n        configMap:\n          name: kube-dc-auth-config\n          optional: true\n      - name: host-path\n        hostPath:\n          path: /etc/rancher\n          type: DirectoryOrCreate\n</code></pre>"},{"location":"prd/auth-config-sync/#rollback-plan","title":"Rollback Plan","text":"<p>If issues occur: 1. Delete the DaemonSet: <code>kubectl delete daemonset kube-dc-auth-config-sync -n kube-dc</code> 2. Manager continues writing to local file (existing behavior) 3. Manually copy auth config to other nodes if needed</p>"},{"location":"prd/auth-config-sync/#success-criteria","title":"Success Criteria","text":"<ul> <li> Auth config synced to all control plane nodes within 2-4 seconds</li> <li> No authentication failures after organization changes</li> <li> DaemonSet uses &lt; 50m CPU, &lt; 32Mi memory per pod</li> <li> Solution works with 1, 3, or more control plane nodes</li> <li> Helm chart upgrade is seamless</li> </ul>"},{"location":"prd/auth-config-sync/#single-master-compatibility","title":"Single Master Compatibility","text":"<p>For single master installations, the solution works identically: - Manager writes to local file (existing behavior) - Manager writes to ConfigMap (new) - DaemonSet copies ConfigMap to host path (overwrites same file)</p> <p>This is intentionally redundant but safe - the same deployment works for any number of masters without conditional logic. When additional masters are added, sync works automatically.</p>"},{"location":"prd/auth-config-sync/#open-questions","title":"Open Questions","text":"<ol> <li>ConfigMap size limit: Auth config with 100+ organizations - will it exceed 1MB limit?</li> <li> <p>Estimate: ~500 bytes per organization = 50KB for 100 orgs. Well within limit.</p> </li> <li> <p>Initial bootstrap: How to handle first deployment when ConfigMap doesn't exist?</p> </li> <li>Solution: DaemonSet uses <code>optional: true</code> for ConfigMap mount, waits for it to appear.</li> </ol>"},{"location":"prd/auth-config-sync/#references","title":"References","text":"<ul> <li>Kubernetes 1.30 Structured Authentication</li> <li>Kubernetes ConfigMap Updates</li> <li>DaemonSet Documentation</li> </ul>"},{"location":"prd/certificate-management-monitoring/","title":"Certificate Management and Monitoring Requirements","text":"<p>Document Version: 1.0 Last Updated: January 12, 2026 Status: Production Critical</p>"},{"location":"prd/certificate-management-monitoring/#executive-summary","title":"Executive Summary","text":"<p>Kube-DC relies on numerous webhook-based components that require valid TLS certificates. A certificate audit revealed that while most components use cert-manager for automatic renewal, several critical components use self-managed certificates with varying expiration policies. Failure to monitor and maintain these certificates can result in complete platform outages.</p> <p>Critical Issue Resolved: CAPK webhook certificate expired on January 3, 2026, preventing all KdcCluster worker pool creation. This has been fixed and integrated into the installer for future deployments.</p>"},{"location":"prd/certificate-management-monitoring/#certificate-management-architecture","title":"Certificate Management Architecture","text":""},{"location":"prd/certificate-management-monitoring/#components-with-cert-manager-auto-renewal-safe","title":"Components with cert-manager Auto-Renewal (\u2705 Safe)","text":"<p>These components are properly managed and will automatically renew:</p>"},{"location":"prd/certificate-management-monitoring/#cluster-api-webhooks","title":"Cluster API Webhooks","text":"<ul> <li>capi-system/capi-webhook-service-cert</li> <li>Validity: 1 year</li> <li>Renewal: 30 days before expiry</li> <li> <p>Management: cert-manager + self-signed issuer</p> </li> <li> <p>capi-k3s-bootstrap-system/capi-k3s-bootstrap-webhook-service-cert</p> </li> <li>capi-k3s-control-plane-system/capi-k3s-control-plane-webhook-service-cert</li> <li>capi-kubeadm-bootstrap-system/capi-kubeadm-bootstrap-webhook-service-cert</li> <li>capk-system/capk-webhook-service-cert \u26a1 Fixed January 2026</li> <li>Validity: 1 year</li> <li>Renewal: 30 days before expiry</li> <li>Management: cert-manager + self-signed issuer</li> <li>Now integrated into installer (see Installer Integration section)</li> </ul>"},{"location":"prd/certificate-management-monitoring/#kamaji-certificates","title":"Kamaji Certificates","text":"<ul> <li>kamaji-system/kamaji-webhook-server-cert</li> <li>Validity: 1 year</li> <li> <p>Management: cert-manager</p> </li> <li> <p>44+ etcd certificate resources (tenant cluster datastores)</p> </li> <li>Issuer: <code>etcd-ca-issuer</code> (ClusterIssuer)</li> <li>Automatically created per KdcClusterDatastore</li> <li>Management: cert-manager</li> </ul>"},{"location":"prd/certificate-management-monitoring/#components-with-self-managed-certificates-requires-monitoring","title":"Components with Self-Managed Certificates (\u26a0\ufe0f Requires Monitoring)","text":""},{"location":"prd/certificate-management-monitoring/#kubevirt-certificates-high-risk-24h-validity","title":"KubeVirt Certificates (\u26a0\ufe0f High Risk - 24h Validity)","text":"<p>Certificate Details: <pre><code>kubevirt-operator-certs:       24 hours validity\nkubevirt-virt-api-certs:       24 hours validity\nkubevirt-controller-certs:     24 hours validity\nkubevirt-virt-handler-certs:   24 hours validity\n</code></pre></p> <p>Affected Webhooks: - <code>virt-api-validator</code> (ValidatingWebhookConfiguration) - <code>virt-api-mutator</code> (MutatingWebhookConfiguration) - <code>virt-operator-validator</code> (ValidatingWebhookConfiguration)</p> <p>Risk Assessment: - \u26a0\ufe0f Very short validity period (24 hours) - \u2705 Built-in automatic rotation via <code>virt-operator</code> - \u26a0\ufe0f Single point of failure: If virt-operator pod fails/crashes, certificates won't rotate - \u26a0\ufe0f Impact if expired: All VM operations fail, including creation, deletion, and live migration</p> <p>Monitoring Requirements: <pre><code># Check virt-operator health\nkubectl get pods -n kubevirt -l kubevirt.io=virt-operator\n\n# Check certificate rotation logs\nkubectl logs -n kubevirt deployment/virt-operator --tail=100 | grep -i cert\n\n# Verify certificate validity\nkubectl get secret -n kubevirt kubevirt-virt-api-certs -o jsonpath='{.data.tls\\.crt}' | \\\n  base64 -d | openssl x509 -noout -dates\n</code></pre></p>"},{"location":"prd/certificate-management-monitoring/#cdi-certificates-high-risk-24h-validity","title":"CDI Certificates (\u26a0\ufe0f High Risk - 24h Validity)","text":"<p>Certificate Details: <pre><code>cdi-apiserver-server-cert:     24 hours validity\ncdi-uploadproxy-server-cert:   24 hours validity\ncdi-uploadserver-client-cert:  24 hours validity\n</code></pre></p> <p>Affected Webhooks: - <code>cdi-api-dataimportcron-validate</code> (ValidatingWebhookConfiguration) - <code>cdi-api-datavolume-validate</code> (ValidatingWebhookConfiguration) - <code>cdi-api-datavolume-mutate</code> (MutatingWebhookConfiguration) - <code>objecttransfer-api-validate</code> (ValidatingWebhookConfiguration)</p> <p>Risk Assessment: - \u26a0\ufe0f Very short validity period (24 hours) - \u2705 Built-in automatic rotation via <code>cdi-operator</code> - \u26a0\ufe0f Single point of failure: If cdi-operator pod fails/crashes, certificates won't rotate - \u26a0\ufe0f Impact if expired: DataVolume operations fail, preventing disk provisioning and VM creation</p> <p>Monitoring Requirements: <pre><code># Check cdi-operator health\nkubectl get pods -n cdi -l cdi.kubevirt.io=cdi-operator\n\n# Check certificate rotation logs\nkubectl logs -n cdi deployment/cdi-operator --tail=100 | grep -i cert\n\n# Verify certificate validity\nkubectl get secret -n cdi cdi-apiserver-server-cert -o jsonpath='{.data.tls\\.crt}' | \\\n  base64 -d | openssl x509 -noout -dates\n</code></pre></p>"},{"location":"prd/certificate-management-monitoring/#kyverno-certificates-medium-risk-5-months","title":"Kyverno Certificates (\u26a0\ufe0f Medium Risk - ~5 Months)","text":"<p>Certificate Details (as of Jan 12, 2026): <pre><code>kyverno-svc CA:                     Valid until Aug 6, 2026 (1 year)\nkyverno-svc certificate:            Valid until May 31, 2026 (~5 months)\nkyverno-cleanup-controller CA:      Valid until Aug 6, 2026 (1 year)\nkyverno-cleanup-controller cert:    Valid until May 31, 2026 (~5 months)\n</code></pre></p> <p>Affected Webhooks: - <code>kyverno-policy-validating-webhook-cfg</code> - <code>kyverno-resource-validating-webhook-cfg</code> - <code>kyverno-exception-validating-webhook-cfg</code> - <code>kyverno-cleanup-validating-webhook-cfg</code> - <code>kyverno-policy-mutating-webhook-cfg</code> - <code>kyverno-resource-mutating-webhook-cfg</code> - <code>kyverno-verify-mutating-webhook-cfg</code></p> <p>Risk Assessment: - \u26a0\ufe0f Manual renewal required - No automatic rotation - \u26a0\ufe0f Next renewal deadline: May 31, 2026 - \u26a0\ufe0f Impact if expired: Policy enforcement fails, security policies not applied</p> <p>Action Required: - Before May 2026: Migrate Kyverno certificates to cert-manager management - Alternative: Manual certificate renewal procedure</p>"},{"location":"prd/certificate-management-monitoring/#monitoring-and-alerting-requirements","title":"Monitoring and Alerting Requirements","text":""},{"location":"prd/certificate-management-monitoring/#prometheus-alerts","title":"Prometheus Alerts","text":"<p>Certificate Expiration Alert: <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: certificate-expiration-alerts\n  namespace: monitoring\nspec:\n  groups:\n  - name: certificates\n    interval: 1h\n    rules:\n    - alert: WebhookCertificateExpiringSoon\n      expr: |\n        (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 &lt; 30\n      for: 1h\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Certificate {{ $labels.namespace }}/{{ $labels.name }} expires in &lt; 30 days\"\n        description: \"Certificate will expire in {{ $value | humanizeDuration }}\"\n\n    - alert: WebhookCertificateExpired\n      expr: |\n        cert_manager_certificate_expiration_timestamp_seconds - time() &lt; 0\n      for: 1m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Certificate {{ $labels.namespace }}/{{ $labels.name }} has EXPIRED\"\n        description: \"Immediate action required to restore service\"\n</code></pre></p>"},{"location":"prd/certificate-management-monitoring/#manual-verification-commands","title":"Manual Verification Commands","text":"<p>Check all webhook certificates: <pre><code>#!/bin/bash\n# certificate-check.sh\n\necho \"=== CAPK Webhook Certificate ===\"\nkubectl get certificate capk-serving-cert -n capk-system -o wide\n\necho -e \"\\n=== KubeVirt Operator Health ===\"\nkubectl get pods -n kubevirt -l kubevirt.io=virt-operator\n\necho -e \"\\n=== KubeVirt Certificate Expiry ===\"\nkubectl get secret -n kubevirt kubevirt-virt-api-certs -o jsonpath='{.data.tls\\.crt}' | \\\n  base64 -d | openssl x509 -noout -dates\n\necho -e \"\\n=== CDI Operator Health ===\"\nkubectl get pods -n cdi -l cdi.kubevirt.io=cdi-operator\n\necho -e \"\\n=== CDI Certificate Expiry ===\"\nkubectl get secret -n cdi cdi-apiserver-server-cert -o jsonpath='{.data.tls\\.crt}' | \\\n  base64 -d | openssl x509 -noout -dates\n\necho -e \"\\n=== Kyverno Certificate Expiry ===\"\nkubectl get secret -n kyverno kyverno-svc.kyverno.svc.kyverno-tls-pair -o jsonpath='{.data.tls\\.crt}' | \\\n  base64 -d | openssl x509 -noout -dates\n\necho -e \"\\n=== All cert-manager Certificates ===\"\nkubectl get certificate -A -o wide | grep -v \"True.*up to date\"\n</code></pre></p> <p>Add to cron for daily checks: <pre><code># Run daily at 8 AM\n0 8 * * * /path/to/certificate-check.sh | mail -s \"Kube-DC Certificate Status\" ops@example.com\n</code></pre></p>"},{"location":"prd/certificate-management-monitoring/#incident-response-procedures","title":"Incident Response Procedures","text":""},{"location":"prd/certificate-management-monitoring/#scenario-1-kubevirtcdi-certificate-expired-24h","title":"Scenario 1: KubeVirt/CDI Certificate Expired (24h)","text":"<p>Symptoms: - VM creation fails with webhook errors - DataVolume operations rejected - Webhook timeout errors in logs</p> <p>Emergency Recovery: <pre><code># 1. Restart the operator to trigger certificate rotation\nkubectl rollout restart deployment/virt-operator -n kubevirt\n# or\nkubectl rollout restart deployment/cdi-operator -n cdi\n\n# 2. Wait for operator to issue new certificates\nkubectl rollout status deployment/virt-operator -n kubevirt --timeout=5m\n\n# 3. Verify webhook pods reload certificates\nkubectl rollout restart deployment/virt-api -n kubevirt\n\n# 4. Verify certificate validity\nkubectl get secret -n kubevirt kubevirt-virt-api-certs -o jsonpath='{.data.tls\\.crt}' | \\\n  base64 -d | openssl x509 -noout -dates\n</code></pre></p>"},{"location":"prd/certificate-management-monitoring/#scenario-2-capk-webhook-certificate-expired","title":"Scenario 2: CAPK Webhook Certificate Expired","text":"<p>Symptoms: - KdcCluster worker pool creation fails - Error: \"x509: certificate has expired\" - Cannot create KubevirtMachineTemplate resources</p> <p>Emergency Recovery: <pre><code># 1. Manually apply certificate resources\nkubectl apply -f /path/to/kube-dc/installer/kube-dc/templates/kube-dc/cluster-api/capk-webhook-cert.yaml\n\n# 2. Wait for cert-manager to issue certificate\nkubectl wait --for=condition=Ready certificate/capk-serving-cert -n capk-system --timeout=60s\n\n# 3. Restart CAPK controller to reload certificate\nkubectl rollout restart deployment/capk-controller-manager -n capk-system\n\n# 4. Verify webhook is operational\nkubectl rollout status deployment/capk-controller-manager -n capk-system\n</code></pre></p> <p>Prevention: - \u2705 Fixed: Certificate now managed by cert-manager in installer - \u2705 Automatic renewal: 30 days before expiry</p>"},{"location":"prd/certificate-management-monitoring/#scenario-3-kyverno-certificate-expired-may-2026","title":"Scenario 3: Kyverno Certificate Expired (May 2026)","text":"<p>Symptoms: - Policy validation fails - Resources created without policy enforcement - Webhook errors in admission controller</p> <p>Manual Renewal (Temporary): <pre><code># Force Kyverno to regenerate certificates\nkubectl delete secret -n kyverno kyverno-svc.kyverno.svc.kyverno-tls-pair\nkubectl delete secret -n kyverno kyverno-cleanup-controller.kyverno.svc.kyverno-tls-pair\n\n# Restart Kyverno controllers\nkubectl rollout restart deployment -n kyverno\n</code></pre></p> <p>Permanent Solution (Recommended): Migrate Kyverno to cert-manager management (see Future Improvements section).</p>"},{"location":"prd/certificate-management-monitoring/#installer-integration","title":"Installer Integration","text":""},{"location":"prd/certificate-management-monitoring/#capk-webhook-certificate-auto-configuration","title":"CAPK Webhook Certificate Auto-Configuration","text":"<p>Location: <code>/home/voa/projects/kube-dc/installer/kube-dc/templates/kube-dc/cluster-api/</code></p> <p>Files Added: 1. <code>capk-webhook-cert.yaml</code> - Certificate and Issuer manifests 2. <code>install.sh</code> - Updated to apply certificates after clusterctl init</p> <p>Installation Flow: <pre><code># 1. clusterctl init creates capk-system namespace and webhook\n# 2. Script waits for namespace to be ready\n# 3. Applies Issuer + Certificate resources\n# 4. cert-manager issues certificate immediately\n# 5. CAPK webhook loads new certificate\n</code></pre></p> <p>Certificate Specification: <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: capk-serving-cert\n  namespace: capk-system\nspec:\n  dnsNames:\n  - capk-webhook-service.capk-system.svc\n  - capk-webhook-service.capk-system.svc.cluster.local\n  issuerRef:\n    kind: Issuer\n    name: capk-selfsigned-issuer\n  secretName: capk-webhook-service-cert\n  duration: 8760h  # 1 year\n  renewBefore: 720h  # 30 days before expiry\n</code></pre></p> <p>Template Integration: <pre><code># In template.yaml\ncreate_files:\n  - file: capk-webhook-cert.yaml\n    content: {{ insertYAML (readFile \"./cluster-api/capk-webhook-cert.yaml\") }}\n</code></pre></p> <p>Benefits: - \u2705 Automatic deployment on new installations - \u2705 cert-manager handles renewal - \u2705 No manual intervention required - \u2705 30-day renewal window ensures safety margin</p>"},{"location":"prd/certificate-management-monitoring/#future-improvements","title":"Future Improvements","text":""},{"location":"prd/certificate-management-monitoring/#priority-1-migrate-kyverno-to-cert-manager","title":"Priority 1: Migrate Kyverno to cert-manager","text":"<p>Deadline: Before May 2026</p> <p>Implementation: 1. Create Certificate resources for Kyverno webhooks 2. Configure cert-manager Issuer 3. Update Kyverno webhook configurations to use cert-manager certs 4. Test certificate renewal process 5. Document rollback procedure</p>"},{"location":"prd/certificate-management-monitoring/#priority-2-enhanced-monitoring-dashboard","title":"Priority 2: Enhanced Monitoring Dashboard","text":"<p>Grafana Dashboard: - Certificate expiration timeline (all components) - Webhook health status - Certificate rotation events - Alert history</p>"},{"location":"prd/certificate-management-monitoring/#priority-3-automated-certificate-health-checks","title":"Priority 3: Automated Certificate Health Checks","text":"<p>Kubernetes CronJob: <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: certificate-health-check\n  namespace: monitoring\nspec:\n  schedule: \"0 */6 * * *\"  # Every 6 hours\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cert-check\n            image: alpine/openssl\n            command:\n            - /bin/sh\n            - -c\n            - |\n              # Check all webhook certificates\n              # Send alerts if &lt; 7 days remaining\n              # Report to monitoring system\n</code></pre></p>"},{"location":"prd/certificate-management-monitoring/#maintenance-calendar","title":"Maintenance Calendar","text":""},{"location":"prd/certificate-management-monitoring/#monthly-tasks","title":"Monthly Tasks","text":"<ul> <li> Review certificate expiration status</li> <li> Verify cert-manager health</li> <li> Check operator pod logs for cert rotation</li> <li> Review certificate monitoring alerts</li> </ul>"},{"location":"prd/certificate-management-monitoring/#quarterly-tasks","title":"Quarterly Tasks","text":"<ul> <li> Audit all webhook configurations</li> <li> Verify certificate backup procedures</li> <li> Test certificate renewal process</li> <li> Update documentation</li> </ul>"},{"location":"prd/certificate-management-monitoring/#critical-dates-2026","title":"Critical Dates (2026)","text":"<ul> <li>May 31, 2026: Kyverno certificates expire \u26a0\ufe0f ACTION REQUIRED</li> <li>August 6, 2026: Kyverno CA certificates expire \u26a0\ufe0f ACTION REQUIRED</li> <li>December 2026: Review all cert-manager certificates for 2027 renewal</li> </ul>"},{"location":"prd/certificate-management-monitoring/#references","title":"References","text":""},{"location":"prd/certificate-management-monitoring/#related-documentation","title":"Related Documentation","text":"<ul> <li>CAPK Webhook Certificate Fix (if created)</li> <li>cert-manager Documentation</li> <li>KubeVirt Certificate Management</li> <li>Kyverno Certificate Management</li> </ul>"},{"location":"prd/certificate-management-monitoring/#support-contacts","title":"Support Contacts","text":"<ul> <li>Platform Team: Contact for certificate issues</li> <li>On-Call Rotation: Emergency certificate expiration response</li> <li>Vendor Support: KubeVirt, Kyverno for upstream certificate issues</li> </ul>"},{"location":"prd/certificate-management-monitoring/#revision-history","title":"Revision History","text":"Version Date Author Changes 1.0 Jan 12, 2026 System Initial documentation after CAPK certificate incident <p>\u26a0\ufe0f CRITICAL REMINDER:</p> <p>Certificates are security-critical infrastructure components. Expired certificates can cause complete platform outages affecting all tenant clusters. This documentation must be reviewed quarterly and updated whenever new components are added to the platform.</p>"},{"location":"prd/cloud_current_network/","title":"Current OVN Network Architecture","text":""},{"location":"prd/cloud_current_network/#overview","title":"Overview","text":"<p>This document provides a complete view of the current OVN-based network architecture in the kube-dc management cluster, including VPCs, subnets, EIPs, service exposure, and Envoy Gateway integration.</p>"},{"location":"prd/cloud_current_network/#physical-network-layer","title":"Physical Network Layer","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         PHYSICAL NETWORK                                     \u2502\n\u2502                                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502      VLAN 4011          \u2502              \u2502      VLAN 4013          \u2502       \u2502\n\u2502  \u2502      ext-public         \u2502              \u2502      ext-cloud          \u2502       \u2502\n\u2502  \u2502   168.119.17.48/28      \u2502              \u2502   100.65.0.0/16         \u2502       \u2502\n\u2502  \u2502                         \u2502              \u2502                         \u2502       \u2502\n\u2502  \u2502   Gateway: 168.119.17.49\u2502              \u2502   Gateway: 100.65.0.1   \u2502       \u2502\n\u2502  \u2502   Internet-routable \u2705  \u2502              \u2502   Internal-only \u274c      \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502              \u2502                                        \u2502                      \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502                               \u2502                                              \u2502\n\u2502                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u2502\n\u2502                     \u2502   Provider Bridge  \u2502                                    \u2502\n\u2502                     \u2502   br-ext-cloud     \u2502                                    \u2502\n\u2502                     \u2502   (on each node)   \u2502                                    \u2502\n\u2502                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              OVN NETWORK                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#ovn-logical-network-architecture","title":"OVN Logical Network Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                    OVN LOGICAL NETWORK                                           \u2502\n\u2502                                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                              ovn-cluster VPC (Management)                                   \u2502 \u2502\n\u2502  \u2502                                                                                             \u2502 \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502   \u2502   ovn-default   \u2502    \u2502    ext-cloud    \u2502    \u2502   ext-public    \u2502    \u2502     join      \u2502  \u2502 \u2502\n\u2502  \u2502   \u2502  10.100.0.0/16  \u2502    \u2502  100.65.0.0/16  \u2502    \u2502168.119.17.48/28 \u2502    \u2502 172.30.0.0/22 \u2502  \u2502 \u2502\n\u2502  \u2502   \u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502    \u2502               \u2502  \u2502 \u2502\n\u2502  \u2502   \u2502 \u2022 kube-system   \u2502    \u2502 \u2022 LB VIPs       \u2502    \u2502 \u2022 Public LB VIPs\u2502    \u2502 \u2022 Node IPs    \u2502  \u2502 \u2502\n\u2502  \u2502   \u2502 \u2022 kamaji-system \u2502    \u2502 \u2022 Cloud EIPs    \u2502    \u2502 \u2022 Public EIPs   \u2502    \u2502 \u2022 kube-proxy  \u2502  \u2502 \u2502\n\u2502  \u2502   \u2502 \u2022 envoy-gateway \u2502    \u2502                 \u2502    \u2502                 \u2502    \u2502   SNAT        \u2502  \u2502 \u2502\n\u2502  \u2502   \u2502 \u2022 ingress-nginx \u2502    \u2502                 \u2502    \u2502                 \u2502    \u2502               \u2502  \u2502 \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2502            \u2502                      \u2502                      \u2502                     \u2502          \u2502 \u2502\n\u2502  \u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502 \u2502\n\u2502  \u2502                                   \u2502                      \u2502                                \u2502 \u2502\n\u2502  \u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502 \u2502\n\u2502  \u2502                         \u2502         ovn-cluster Router               \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2502                                          \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2502  Ports:                                  \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2502  \u2022 ovn-cluster-ovn-default: 10.100.0.1   \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2502  \u2022 ovn-cluster-ext-cloud: 100.65.0.101   \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2502  \u2022 ovn-cluster-join: 172.30.0.1          \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2502                                          \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2502  SNAT: 10.100.0.0/16 \u2192 100.65.0.101      \u2502                      \u2502 \u2502\n\u2502  \u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n\u2502  \u2502     shalb-demo VPC (Project)    \u2502  \u2502     shalb-dev VPC (Project)     \u2502                       \u2502\n\u2502  \u2502                                 \u2502  \u2502                                 \u2502                       \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                       \u2502\n\u2502  \u2502   \u2502  shalb-demo-default     \u2502   \u2502  \u2502   \u2502  shalb-dev-default      \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2502     10.0.10.0/24        \u2502   \u2502  \u2502   \u2502     10.1.0.0/16         \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2502                         \u2502   \u2502  \u2502   \u2502                         \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2502  \u2022 Customer pods        \u2502   \u2502  \u2502   \u2502  \u2022 Customer pods        \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2502  \u2022 test-http-app        \u2502   \u2502  \u2502   \u2502  \u2022 Development workloads\u2502   \u2502                       \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                       \u2502\n\u2502  \u2502               \u2502                 \u2502  \u2502               \u2502                 \u2502                       \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                       \u2502\n\u2502  \u2502   \u2502  shalb-demo Router      \u2502   \u2502  \u2502   \u2502  shalb-dev Router       \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2502                         \u2502   \u2502  \u2502   \u2502                         \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2502  ext-cloud: 100.65.0.102\u2502   \u2502  \u2502   \u2502  ext-public:168.119.17.51\u2502  \u2502                       \u2502\n\u2502  \u2502   \u2502  SNAT: 10.0.10.0/24     \u2502   \u2502  \u2502   \u2502  ext-cloud: (via extra) \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2502       \u2192 100.65.0.102    \u2502   \u2502  \u2502   \u2502  SNAT: 10.1.0.0/16      \u2502   \u2502                       \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502   \u2502       \u2192 168.119.17.51   \u2502   \u2502                       \u2502\n\u2502  \u2502                                 \u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                       \u2502\n\u2502  \u2502   extraExternalSubnets: []      \u2502  \u2502   extraExternalSubnets:         \u2502                       \u2502\n\u2502  \u2502   Default GW: ext-cloud         \u2502  \u2502   [ext-cloud, ext-public]       \u2502                       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n\u2502                                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                            \u2502\n\u2502  \u2502    shalb-envoy VPC (Project)    \u2502                                                            \u2502\n\u2502  \u2502                                 \u2502                                                            \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                                                            \u2502\n\u2502  \u2502   \u2502  shalb-envoy-default    \u2502   \u2502                                                            \u2502\n\u2502  \u2502   \u2502     10.0.40.0/24        \u2502   \u2502                                                            \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                                                            \u2502\n\u2502  \u2502               \u2502                 \u2502                                                            \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                                                            \u2502\n\u2502  \u2502   \u2502  shalb-envoy Router     \u2502   \u2502                                                            \u2502\n\u2502  \u2502   \u2502                         \u2502   \u2502                                                            \u2502\n\u2502  \u2502   \u2502  ext-public:168.119.17.52\u2502  \u2502                                                            \u2502\n\u2502  \u2502   \u2502  SNAT: 10.0.40.0/24     \u2502   \u2502                                                            \u2502\n\u2502  \u2502   \u2502       \u2192 168.119.17.52   \u2502   \u2502                                                            \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                                                            \u2502\n\u2502  \u2502                                 \u2502                                                            \u2502\n\u2502  \u2502   extraExternalSubnets:         \u2502                                                            \u2502\n\u2502  \u2502   [ext-public]                  \u2502                                                            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#vpc-and-subnet-summary","title":"VPC and Subnet Summary","text":""},{"location":"prd/cloud_current_network/#vpcs","title":"VPCs","text":"VPC Purpose Namespaces External Subnets Default GW <code>ovn-cluster</code> Management cluster kube-system, kamaji-system, envoy-gateway-system ext-cloud, ext-public N/A <code>shalb-demo</code> Customer project shalb-demo ext-cloud (default) 100.65.0.1 <code>shalb-dev</code> Customer project shalb-dev ext-cloud, ext-public 168.119.17.49 <code>shalb-envoy</code> Customer project shalb-envoy ext-public 168.119.17.49"},{"location":"prd/cloud_current_network/#subnets","title":"Subnets","text":"Subnet VPC CIDR VLAN Purpose <code>ovn-default</code> ovn-cluster 10.100.0.0/16 - Management pods <code>ext-cloud</code> ovn-cluster 100.65.0.0/16 4013 Cloud LB VIPs <code>ext-public</code> ovn-cluster 168.119.17.48/28 4011 Public LB VIPs <code>join</code> ovn-cluster 172.30.0.0/22 - Node-to-OVN connectivity <code>shalb-demo-default</code> shalb-demo 10.0.10.0/24 - Customer pods <code>shalb-dev-default</code> shalb-dev 10.1.0.0/16 - Customer pods <code>shalb-envoy-default</code> shalb-envoy 10.0.40.0/24 - Customer pods"},{"location":"prd/cloud_current_network/#join-subnet-and-node-connectivity","title":"Join Subnet and Node Connectivity","text":"<p>The <code>join</code> subnet (172.30.0.0/22) connects Kubernetes nodes to the OVN network:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           NODE NETWORKING                                 \u2502\n\u2502                                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                    kube-dc-worker-1                                  \u2502 \u2502\n\u2502  \u2502                                                                      \u2502 \u2502\n\u2502  \u2502   Physical Interfaces:                                               \u2502 \u2502\n\u2502  \u2502   \u2022 br-ext-cloud: 138.201.132.165/26 (provider bridge)              \u2502 \u2502\n\u2502  \u2502   \u2022 enp0s31f6.4012: 192.168.1.4 (internal VLAN)                     \u2502 \u2502\n\u2502  \u2502                                                                      \u2502 \u2502\n\u2502  \u2502   OVN Interfaces:                                                    \u2502 \u2502\n\u2502  \u2502   \u2022 ovn0: 172.30.0.2/22 (join subnet - node IP)                     \u2502 \u2502\n\u2502  \u2502                                                                      \u2502 \u2502\n\u2502  \u2502   Routes:                                                            \u2502 \u2502\n\u2502  \u2502   \u2022 10.100.0.0/16 via 172.30.0.1 dev ovn0 (pod network)             \u2502 \u2502\n\u2502  \u2502   \u2022 default via 138.201.132.129 dev br-ext-cloud (internet)         \u2502 \u2502\n\u2502  \u2502                                                                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                           \u2502\n\u2502              \u2502                                                            \u2502\n\u2502              \u2502 ovn0 (172.30.0.2)                                          \u2502\n\u2502              \u2502                                                            \u2502\n\u2502              \u25bc                                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                      OVN Join Switch                                 \u2502 \u2502\n\u2502  \u2502                      172.30.0.0/22                                   \u2502 \u2502\n\u2502  \u2502                                                                      \u2502 \u2502\n\u2502  \u2502   Connected to:                                                      \u2502 \u2502\n\u2502  \u2502   \u2022 ovn-cluster router (172.30.0.1) - gateway to OVN network        \u2502 \u2502\n\u2502  \u2502   \u2022 All nodes (172.30.0.x)                                          \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#join-subnet-role","title":"Join Subnet Role","text":"<ol> <li>Node Registration: Each node gets an IP on the join subnet (172.30.0.x)</li> <li>Pod Network Access: Nodes route pod traffic (10.100.x.x) via join subnet gateway (172.30.0.1)</li> <li>kube-proxy SNAT: When external traffic arrives via externalIPs, kube-proxy SNATs to 172.30.0.x</li> </ol>"},{"location":"prd/cloud_current_network/#eip-external-ip-resources","title":"EIP (External IP) Resources","text":""},{"location":"prd/cloud_current_network/#eip-types","title":"EIP Types","text":"Type Network Purpose Internet Routable <code>cloud</code> ext-cloud (100.65.x.x) Internal services, Kamaji, etcd \u274c No <code>public</code> ext-public (168.119.x.x) Public-facing services \u2705 Yes"},{"location":"prd/cloud_current_network/#current-eips","title":"Current EIPs","text":"Namespace EIP Name IP Type Usage shalb-demo default-gw 100.65.0.102 cloud SNAT for project shalb-demo slb-test-http-app-bc6y7 100.65.0.112 cloud LoadBalancer service shalb-demo demo-cluster-api-eip 100.65.0.105 cloud Kamaji control plane shalb-demo mt-api-eip 100.65.0.108 cloud MT control plane shalb-dev default-gw 168.119.17.51 public SNAT for project shalb-dev bohdan-prod-eip 168.119.17.57 public FIP shalb-envoy default-gw 168.119.17.52 public SNAT for project"},{"location":"prd/cloud_current_network/#service-loadbalancer-architecture","title":"Service LoadBalancer Architecture","text":""},{"location":"prd/cloud_current_network/#loadbalancer-service-flow","title":"LoadBalancer Service Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     SERVICE LOADBALANCER ARCHITECTURE                                 \u2502\n\u2502                                                                                       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                    LoadBalancer Service Creation                               \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   apiVersion: v1                                                               \u2502  \u2502\n\u2502   \u2502   kind: Service                                                                \u2502  \u2502\n\u2502   \u2502   metadata:                                                                    \u2502  \u2502\n\u2502   \u2502     name: test-http-app                                                        \u2502  \u2502\n\u2502   \u2502     namespace: shalb-demo                                                      \u2502  \u2502\n\u2502   \u2502     annotations:                                                               \u2502  \u2502\n\u2502   \u2502       service.nlb.kube-dc.com/bind-on-eip: default-gw                         \u2502  \u2502\n\u2502   \u2502       gateway.kube-dc.com/create-backend: \"true\"  # Optional: creates Backend \u2502  \u2502\n\u2502   \u2502   spec:                                                                        \u2502  \u2502\n\u2502   \u2502     type: LoadBalancer                                                         \u2502  \u2502\n\u2502   \u2502     ports:                                                                     \u2502  \u2502\n\u2502   \u2502       - port: 80                                                               \u2502  \u2502\n\u2502   \u2502     selector:                                                                  \u2502  \u2502\n\u2502   \u2502       app: test-http-app                                                       \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                        \u2502                                              \u2502\n\u2502                                        \u25bc                                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                    Service Controller (kube-dc)                                \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   1. Allocate EIP (if not exists)                                             \u2502  \u2502\n\u2502   \u2502      \u2192 Creates EIp resource                                                    \u2502  \u2502\n\u2502   \u2502      \u2192 Creates OvnEip resource                                                 \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   2. Create OVN LoadBalancer                                                   \u2502  \u2502\n\u2502   \u2502      \u2192 VIP: 100.65.0.112:80                                                    \u2502  \u2502\n\u2502   \u2502      \u2192 Backends: 10.0.10.28:80 (pod IPs)                                       \u2502  \u2502\n\u2502   \u2502      \u2192 Attach to VPC router                                                    \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   3. Create External Service + Endpoints                                       \u2502  \u2502\n\u2502   \u2502      \u2192 test-http-app-ext (headless)                                            \u2502  \u2502\n\u2502   \u2502      \u2192 Endpoints: 100.65.0.112:80                                              \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   4. Create Gateway Backend (if annotated)                                     \u2502  \u2502\n\u2502   \u2502      \u2192 test-http-app-backend                                                   \u2502  \u2502\n\u2502   \u2502      \u2192 Endpoint: 100.65.0.112:80                                               \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                        \u2502                                              \u2502\n\u2502                                        \u25bc                                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                        Created Resources                                       \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502  \u2502\n\u2502   \u2502   \u2502      EIp        \u2502  \u2502    OvnEip       \u2502  \u2502  OVN LB         \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 slb-test-http-  \u2502  \u2502 eip-slb-test-   \u2502  \u2502 VIP:            \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 app-bc6y7       \u2502  \u2502 http-app-bc6y7  \u2502  \u2502 100.65.0.112:80 \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 IP: 100.65.0.112\u2502  \u2502 V4IP:           \u2502  \u2502 Backends:       \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 Type: cloud     \u2502  \u2502 100.65.0.112    \u2502  \u2502 10.0.10.28:80   \u2502               \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502  \u2502\n\u2502   \u2502   \u2502 Service -ext    \u2502  \u2502  Endpoints -ext \u2502  \u2502 Backend (GW API)\u2502               \u2502  \u2502\n\u2502   \u2502   \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 test-http-app-  \u2502  \u2502 test-http-app-  \u2502  \u2502 test-http-app-  \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 ext             \u2502  \u2502 ext             \u2502  \u2502 backend         \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 ClusterIP: None \u2502  \u2502 IP:             \u2502  \u2502 Endpoint:       \u2502               \u2502  \u2502\n\u2502   \u2502   \u2502 (headless)      \u2502  \u2502 100.65.0.112    \u2502  \u2502 100.65.0.112:80 \u2502               \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#-ext-service-purpose","title":"-ext Service Purpose","text":"<p>The <code>-ext</code> suffix services provide a way for other components to discover the external LoadBalancer IP:</p> <pre><code># Original Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-http-app\n  namespace: shalb-demo\nspec:\n  type: LoadBalancer\n  clusterIP: 10.101.61.188\n  ports:\n    - port: 80\n  selector:\n    app: test-http-app\nstatus:\n  loadBalancer:\n    ingress:\n      - ip: 100.65.0.112\n\n---\n# Auto-created -ext Service (headless)\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-http-app-ext\n  namespace: shalb-demo\n  labels:\n    kube-dc.com/endpoint-type: external\n    kube-dc.com/managed-by: service-lb-controller\n    kube-dc.com/source-service: test-http-app\nspec:\n  clusterIP: None  # Headless\n  ports:\n    - name: http\n      port: 80\n\n---\n# Auto-created Endpoints\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: test-http-app-ext\n  namespace: shalb-demo\nsubsets:\n  - addresses:\n      - ip: 100.65.0.112  # External LB IP\n    ports:\n      - name: http\n        port: 80\n</code></pre>"},{"location":"prd/cloud_current_network/#envoy-gateway-architecture","title":"Envoy Gateway Architecture","text":""},{"location":"prd/cloud_current_network/#current-deployment","title":"Current Deployment","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         ENVOY GATEWAY ARCHITECTURE                                    \u2502\n\u2502                                                                                       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                    envoy-gateway-system namespace                              \u2502  \u2502\n\u2502   \u2502                    (in ovn-default subnet: 10.100.0.0/16)                      \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502\n\u2502   \u2502   \u2502  Envoy Gateway         \u2502     \u2502  Envoy Proxy Pod                       \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  Controller            \u2502     \u2502                                        \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502                        \u2502     \u2502  Pod IP: 10.100.0.250                  \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 Watches Gateway,    \u2502     \u2502                                        \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502    HTTPRoute, TLSRoute \u2502     \u2502  Listens on:                           \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 Configures Envoy    \u2502     \u2502  \u2022 7443 (TLS passthrough)              \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502    proxy               \u2502     \u2502  \u2022 8080 (HTTP)                         \u2502   \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502                                        \u2502   \u2502  \u2502\n\u2502   \u2502                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502\n\u2502   \u2502                                                   \u2502                           \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502\n\u2502   \u2502   \u2502  Service: envoy-envoy-gateway-system-eg-tls-passthrough-24835ac8      \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502                                                                        \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  Type: ClusterIP                                                       \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  ClusterIP: 10.101.148.184                                             \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  externalIPs: [88.99.29.250]  \u25c4\u2500\u2500 Shared with nginx-ingress            \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502                                                                        \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  Ports:                                                                \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 7443 \u2192 7443 (TLS passthrough)                                       \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  \u2022 8080 \u2192 8080 (HTTP)                                                  \u2502   \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                                       \u2502\n\u2502                                        \u2502                                              \u2502\n\u2502                                        \u2502 externalIPs: 88.99.29.250                    \u2502\n\u2502                                        \u2502                                              \u2502\n\u2502                                        \u25bc                                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                         EXTERNAL TRAFFIC FLOW                                  \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   Internet Client (88.99.218.47)                                               \u2502  \u2502\n\u2502   \u2502         \u2502                                                                      \u2502  \u2502\n\u2502   \u2502         \u25bc                                                                      \u2502  \u2502\n\u2502   \u2502   Node External IP (88.99.29.250:7443)                                         \u2502  \u2502\n\u2502   \u2502         \u2502                                                                      \u2502  \u2502\n\u2502   \u2502         \u25bc kube-proxy intercepts                                                \u2502  \u2502\n\u2502   \u2502         \u2502 SNAT: src 88.99.218.47 \u2192 172.30.0.2 (join IP) \u25c4\u2500\u2500 CLIENT IP LOST!   \u2502  \u2502\n\u2502   \u2502         \u2502                                                                      \u2502  \u2502\n\u2502   \u2502         \u25bc                                                                      \u2502  \u2502\n\u2502   \u2502   Envoy Pod (10.100.0.250:7443)                                                \u2502  \u2502\n\u2502   \u2502         \u2502 Sees client IP: 172.30.0.2 \u274c                                        \u2502  \u2502\n\u2502   \u2502         \u2502                                                                      \u2502  \u2502\n\u2502   \u2502         \u25bc Routes via HTTPRoute/TLSRoute                                        \u2502  \u2502\n\u2502   \u2502   Backend (via Backend resource or Service)                                    \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#gateway-api-resources","title":"Gateway API Resources","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         GATEWAY API RESOURCE HIERARCHY                                \u2502\n\u2502                                                                                       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                    GatewayClass (cluster-scoped)                               \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   name: eg                                                                     \u2502  \u2502\n\u2502   \u2502   controller: gateway.envoyproxy.io/gatewayclass-controller                   \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                        \u2502                                              \u2502\n\u2502                                        \u25bc                                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                    Gateway (envoy-gateway-system/eg-tls-passthrough)           \u2502  \u2502\n\u2502   \u2502                                                                                \u2502  \u2502\n\u2502   \u2502   spec:                                                                        \u2502  \u2502\n\u2502   \u2502     gatewayClassName: eg                                                       \u2502  \u2502\n\u2502   \u2502     listeners:                                                                 \u2502  \u2502\n\u2502   \u2502       - name: tls                                                              \u2502  \u2502\n\u2502   \u2502         port: 7443                                                             \u2502  \u2502\n\u2502   \u2502         protocol: TLS                                                          \u2502  \u2502\n\u2502   \u2502         tls:                                                                   \u2502  \u2502\n\u2502   \u2502           mode: Passthrough                                                    \u2502  \u2502\n\u2502   \u2502       - name: http                                                             \u2502  \u2502\n\u2502   \u2502         port: 8080                                                             \u2502  \u2502\n\u2502   \u2502         protocol: HTTP                                                         \u2502  \u2502\n\u2502   \u2502   status:                                                                      \u2502  \u2502\n\u2502   \u2502     addresses:                                                                 \u2502  \u2502\n\u2502   \u2502       - value: 10.101.148.184                                                  \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                        \u2502                                              \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502                    \u2502                                       \u2502                         \u2502\n\u2502                    \u25bc                                       \u25bc                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502   \u2502  TLSRoute (shalb-demo/demo-    \u2502   \u2502  HTTPRoute (shalb-demo/test-   \u2502            \u2502\n\u2502   \u2502  cluster)                      \u2502   \u2502  http-app)                     \u2502            \u2502\n\u2502   \u2502                                \u2502   \u2502                                \u2502            \u2502\n\u2502   \u2502  hostnames:                    \u2502   \u2502  hostnames:                    \u2502            \u2502\n\u2502   \u2502  - demo-cluster.stage.kube-   \u2502   \u2502  - test-http-app.stage.kube-  \u2502            \u2502\n\u2502   \u2502    dc.com                      \u2502   \u2502    dc.com                      \u2502            \u2502\n\u2502   \u2502                                \u2502   \u2502                                \u2502            \u2502\n\u2502   \u2502  backendRefs:                  \u2502   \u2502  backendRefs:                  \u2502            \u2502\n\u2502   \u2502  - name: demo-cluster-backend  \u2502   \u2502  - name: test-http-app-backend \u2502            \u2502\n\u2502   \u2502    kind: Backend               \u2502   \u2502    kind: Backend               \u2502            \u2502\n\u2502   \u2502    group: gateway.envoyproxy.io\u2502   \u2502    group: gateway.envoyproxy.io\u2502            \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502                    \u2502                                       \u2502                         \u2502\n\u2502                    \u25bc                                       \u25bc                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502   \u2502  Backend (shalb-demo/demo-     \u2502   \u2502  Backend (shalb-demo/test-     \u2502            \u2502\n\u2502   \u2502  cluster-backend)              \u2502   \u2502  http-app-backend)             \u2502            \u2502\n\u2502   \u2502                                \u2502   \u2502                                \u2502            \u2502\n\u2502   \u2502  spec:                         \u2502   \u2502  spec:                         \u2502            \u2502\n\u2502   \u2502    endpoints:                  \u2502   \u2502    endpoints:                  \u2502            \u2502\n\u2502   \u2502    - ip:                       \u2502   \u2502    - ip:                       \u2502            \u2502\n\u2502   \u2502        address: 100.65.0.105   \u2502   \u2502        address: 100.65.0.112   \u2502            \u2502\n\u2502   \u2502        port: 6443              \u2502   \u2502        port: 80                \u2502            \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502                    \u2502                                       \u2502                         \u2502\n\u2502                    \u25bc                                       \u25bc                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502   \u2502  OVN LoadBalancer (cloud)      \u2502   \u2502  OVN LoadBalancer (cloud)      \u2502            \u2502\n\u2502   \u2502                                \u2502   \u2502                                \u2502            \u2502\n\u2502   \u2502  VIP: 100.65.0.105:6443        \u2502   \u2502  VIP: 100.65.0.112:80          \u2502            \u2502\n\u2502   \u2502  Backends: 10.0.10.x:6443      \u2502   \u2502  Backends: 10.0.10.28:80       \u2502            \u2502\n\u2502   \u2502  (Kamaji control plane pods)   \u2502   \u2502  (test-http-app pod)           \u2502            \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#complete-traffic-flow-examples","title":"Complete Traffic Flow Examples","text":""},{"location":"prd/cloud_current_network/#example-1-tls-passthrough-to-kamaji-control-plane","title":"Example 1: TLS Passthrough to Kamaji Control Plane","text":"<pre><code>Internet Client\n      \u2502\n      \u2502 https://demo-cluster.stage.kube-dc.com:7443\n      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Node (88.99.29.250:7443)                                                \u2502\n\u2502      \u2502                                                                   \u2502\n\u2502      \u25bc kube-proxy                                                        \u2502\n\u2502      \u2502 SNAT: client IP \u2192 172.30.0.x                                      \u2502\n\u2502      \u25bc                                                                   \u2502\n\u2502  Envoy Pod (10.100.0.250:7443)                                           \u2502\n\u2502      \u2502                                                                   \u2502\n\u2502      \u2502 TLSRoute matches SNI: demo-cluster.stage.kube-dc.com              \u2502\n\u2502      \u2502 Backend: demo-cluster-backend                                     \u2502\n\u2502      \u25bc                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 OVN Network (ovn-default \u2192 ext-cloud)                                \u2502\u2502\n\u2502  \u2502                                                                      \u2502\u2502\n\u2502  \u2502 Envoy \u2192 100.65.0.105:6443 (Backend IP endpoint)                     \u2502\u2502\n\u2502  \u2502           \u2502                                                          \u2502\u2502\n\u2502  \u2502           \u25bc OVN LoadBalancer DNAT                                    \u2502\u2502\n\u2502  \u2502       10.0.10.x:6443 (Kamaji pod in shalb-demo VPC)                  \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#example-2-http-route-to-test-app","title":"Example 2: HTTP Route to Test App","text":"<pre><code>Internet Client\n      \u2502\n      \u2502 http://test-http-app.stage.kube-dc.com:8080\n      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Node (88.99.29.250:8080)                                                \u2502\n\u2502      \u2502                                                                   \u2502\n\u2502      \u25bc kube-proxy                                                        \u2502\n\u2502      \u2502 SNAT: client IP \u2192 172.30.0.x                                      \u2502\n\u2502      \u25bc                                                                   \u2502\n\u2502  Envoy Pod (10.100.0.250:8080)                                           \u2502\n\u2502      \u2502                                                                   \u2502\n\u2502      \u2502 HTTPRoute matches Host: test-http-app.stage.kube-dc.com           \u2502\n\u2502      \u2502 Backend: test-http-app-backend                                    \u2502\n\u2502      \u25bc                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 OVN Network (ovn-default \u2192 ext-cloud)                                \u2502\u2502\n\u2502  \u2502                                                                      \u2502\u2502\n\u2502  \u2502 Envoy \u2192 100.65.0.112:80 (Backend IP endpoint)                       \u2502\u2502\n\u2502  \u2502           \u2502                                                          \u2502\u2502\n\u2502  \u2502           \u25bc OVN LoadBalancer DNAT                                    \u2502\u2502\n\u2502  \u2502       10.0.10.28:80 (test-http-app pod in shalb-demo VPC)            \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#nginx-ingress-comparison","title":"Nginx Ingress (Comparison)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     NGINX INGRESS CONFIGURATION                           \u2502\n\u2502                                                                           \u2502\n\u2502   Namespace: ingress-nginx                                                \u2502\n\u2502                                                                           \u2502\n\u2502   Service: ingress-nginx-controller                                       \u2502\n\u2502   Type: LoadBalancer                                                      \u2502\n\u2502   ClusterIP: 10.101.151.121                                               \u2502\n\u2502   externalIPs: [88.99.29.250]  \u25c4\u2500\u2500 Same IP as Envoy!                     \u2502\n\u2502                                                                           \u2502\n\u2502   Ports:                                                                  \u2502\n\u2502   \u2022 80:31928 (HTTP)                                                       \u2502\n\u2502   \u2022 443:31891 (HTTPS)                                                     \u2502\n\u2502   \u2022 6443:30504 (TCP - for Kamaji?)                                        \u2502\n\u2502                                                                           \u2502\n\u2502   Note: nginx-ingress uses pod networking (10.100.0.153)                  \u2502\n\u2502         NOT hostNetwork                                                   \u2502\n\u2502         Same SNAT issue as Envoy - client IP lost                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_current_network/#known-issues","title":"Known Issues","text":""},{"location":"prd/cloud_current_network/#1-client-ip-preservation","title":"1. Client IP Preservation","text":"<p>Problem: kube-proxy SNATs incoming traffic through the join subnet (172.30.0.x), losing the real client IP.</p> <p>Impact: SecurityPolicy IP-based filtering doesn't work correctly.</p> <p>Current Workaround: None implemented.</p> <p>Proposed Solutions: - Cloud Bridge Router (see <code>cloud_bridge_network_routing_ingress.md</code>) - <code>externalTrafficPolicy: Local</code> - Proxy Protocol</p>"},{"location":"prd/cloud_current_network/#2-cross-vpc-communication","title":"2. Cross-VPC Communication","text":"<p>Problem: Pods in different VPCs (e.g., ovn-default vs shalb-demo) cannot communicate directly.</p> <p>Solution: Use OVN LoadBalancer VIPs on ext-cloud network as intermediary. The Backend resource points to the LB VIP (100.65.x.x), which DNATs to the actual pod IPs.</p>"},{"location":"prd/cloud_current_network/#3-shared-externalip","title":"3. Shared externalIP","text":"<p>Problem: Both nginx-ingress and Envoy Gateway share 88.99.29.250. Port conflicts are managed by using different ports: - nginx: 80, 443, 6443 - Envoy: 7443, 8080</p>"},{"location":"prd/cloud_current_network/#resource-summary","title":"Resource Summary","text":"Resource Type Count Purpose VPCs 5 Network isolation per project Subnets 8 IP allocation for pods and LBs EIps 14 External IP management OvnEips 14 OVN external IP binding OvnSnatRules 5 Outbound NAT for VPCs Backends 3 Gateway API backend endpoints HTTPRoutes 1 HTTP routing rules TLSRoutes 2 TLS passthrough routing Gateways 1 Envoy Gateway listener"},{"location":"prd/cloud_current_network/#references","title":"References","text":"<ul> <li>Kube-OVN VPC Documentation</li> <li>Envoy Gateway Documentation</li> <li>Gateway API Specification</li> <li>Cloud Bridge PRD: <code>cloud_bridge_network_routing_ingress.md</code></li> <li>Cloud Network Enable: <code>cloud_network_enable_cluster.md</code></li> <li>Gateway Primitives: <code>gateway_primitives.md</code></li> </ul>"},{"location":"prd/cloud_network_enable_cluster/","title":"Enabling Cloud Network Access for Management Cluster Components","text":""},{"location":"prd/cloud_network_enable_cluster/#problem-statement","title":"Problem Statement","text":"<p>The management cluster has two external networks available: - ext-public (<code>168.119.17.48/28</code>) - Public internet-facing IPs on VLAN 4011 - ext-cloud (<code>100.65.0.0/16</code>) - Private cloud network on VLAN 4013</p> <p>By default, pods running in the <code>ovn-default</code> subnet (e.g., <code>kamaji-system</code>, <code>kube-system</code>) cannot reach services exposed on the <code>ext-cloud</code> network, even though both subnets are within the same <code>ovn-cluster</code> VPC.</p> <p>This causes issues when: - Kamaji controller needs to connect to dedicated etcd datastores exposed via LoadBalancer on ext-cloud - Any management component needs to access services on the cloud network</p>"},{"location":"prd/cloud_network_enable_cluster/#root-cause","title":"Root Cause","text":"<p>The <code>ovn-default</code> subnet has <code>natOutgoing: true</code>, which means outbound traffic is NAT'd through the node's external interface (join network). However:</p> <ol> <li>Traffic to <code>ext-cloud</code> (100.65.0.x) is routed via the physical VLAN 4013</li> <li>Return traffic from ext-cloud doesn't know how to reach the OVN internal network (10.100.0.0/16)</li> <li>No SNAT is configured for traffic from <code>ovn-default</code> to <code>ext-cloud</code></li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#solution","title":"Solution","text":"<p>Three configurations were required on the <code>ovn-cluster</code> VPC:</p>"},{"location":"prd/cloud_network_enable_cluster/#1-static-route-to-ext-cloud-gateway","title":"1. Static Route to ext-cloud Gateway","text":"<pre><code># Added to vpc/ovn-cluster spec.staticRoutes\n- cidr: 100.65.0.0/16\n  nextHopIP: 100.65.0.1  # ext-cloud gateway\n  policy: policyDst\n</code></pre> <p>This tells the OVN router where to send traffic destined for ext-cloud.</p>"},{"location":"prd/cloud_network_enable_cluster/#2-policy-route-to-allow-traffic","title":"2. Policy Route to Allow Traffic","text":"<pre><code># Added to vpc/ovn-cluster spec.policyRoutes  \n- action: allow\n  match: ip4.dst == 100.65.0.0/16\n  priority: 31000\n</code></pre> <p>Without this, traffic to ext-cloud would be dropped by default policies that only allow traffic to known internal subnets.</p>"},{"location":"prd/cloud_network_enable_cluster/#3-snat-rule-for-return-traffic","title":"3. SNAT Rule for Return Traffic","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: OvnSnatRule\nmetadata:\n  name: ovn-cluster-to-ext-cloud\nspec:\n  ovnEip: ovn-cluster-ext-cloud  # Uses 100.65.0.101\n  vpcSubnet: ovn-default\n</code></pre> <p>This NATs traffic from <code>10.100.0.0/16</code> to <code>100.65.0.101</code>, allowing return traffic to find its way back.</p>"},{"location":"prd/cloud_network_enable_cluster/#current-configuration","title":"Current Configuration","text":""},{"location":"prd/cloud_network_enable_cluster/#vpc-ovn-cluster","title":"VPC ovn-cluster","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Vpc\nmetadata:\n  name: ovn-cluster\nspec:\n  enableExternal: true\n  staticRoutes:\n  - cidr: 100.65.0.0/16\n    nextHopIP: 100.65.0.1\n    policy: policyDst\n  policyRoutes:\n  - action: allow\n    match: ip4.dst == 100.65.0.0/16\n    priority: 31000\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#ovnsnatrule","title":"OvnSnatRule","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: OvnSnatRule\nmetadata:\n  name: ovn-cluster-to-ext-cloud\nspec:\n  ovnEip: ovn-cluster-ext-cloud\n  vpcSubnet: ovn-default\nstatus:\n  ready: true\n  v4Eip: 100.65.0.101\n  v4IpCidr: 10.100.0.0/16\n  vpc: ovn-cluster\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#verification","title":"Verification","text":"<p>Test connectivity from kamaji-system to ext-cloud:</p> <pre><code># Create test service on ext-cloud\nkubectl -n shalb-demo run nginx-test --image=nginx:alpine\nkubectl -n shalb-demo expose pod nginx-test --port=8080 --target-port=80 --type=LoadBalancer \\\n  --dry-run=client -o yaml | \\\n  kubectl annotate -f - service.nlb.kube-dc.com/bind-on-eip=default-gw --local -o yaml | \\\n  kubectl apply -f -\n\n# Test from kamaji-system\nkubectl -n kamaji-system run test --image=busybox --rm -it --restart=Never -- \\\n  wget -qO- --timeout=5 http://100.65.0.102:8080\n\n# Cleanup\nkubectl -n shalb-demo delete pod nginx-test svc nginx-test\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#ovn-commands-for-debugging","title":"OVN Commands for Debugging","text":"<pre><code># View routes on ovn-cluster router\nkubectl ko nbctl lr-route-list ovn-cluster\n\n# View policy routes\nkubectl ko nbctl lr-policy-list ovn-cluster\n\n# View NAT rules\nkubectl ko nbctl lr-nat-list ovn-cluster\n\n# Trace packet path\nkubectl ko trace &lt;namespace&gt;/&lt;pod&gt; &lt;dest-ip&gt; tcp &lt;port&gt;\n\n# View router ports\nkubectl ko nbctl show ovn-cluster\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#project-vpcs-and-ext-cloud-access","title":"Project VPCs and ext-cloud Access","text":""},{"location":"prd/cloud_network_enable_cluster/#existing-project-vpcs","title":"Existing Project VPCs","text":"<p>Project VPCs (like <code>shalb-demo</code>, <code>shalb-dev</code>) are isolated from <code>ovn-cluster</code> VPC. They have their own configuration:</p> VPC ext-cloud Access Configuration <code>shalb-dev</code> \u2705 Yes <code>extraExternalSubnets: [\"ext-cloud\", \"ext-public\"]</code> <code>shalb-demo</code> \u2705 Yes Default gateway on ext-cloud (<code>100.65.0.1</code>) <code>shalb-envoy</code> \u274c No (public only) <code>extraExternalSubnets: [\"ext-public\"]</code>"},{"location":"prd/cloud_network_enable_cluster/#creating-new-vpcs-with-ext-cloud-access","title":"Creating New VPCs with ext-cloud Access","text":"<p>When creating a new project VPC that needs ext-cloud access:</p>"},{"location":"prd/cloud_network_enable_cluster/#option-1-set-ext-cloud-as-default-gateway-recommended","title":"Option 1: Set ext-cloud as Default Gateway (Recommended)","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Vpc\nmetadata:\n  name: my-project\n  annotations:\n    network.kube-dc.com/default-gw-subnet-name: ext-cloud\nspec:\n  enableExternal: true\n  namespaces:\n  - my-project\n  staticRoutes:\n  - cidr: 0.0.0.0/0\n    nextHopIP: 100.65.0.1  # ext-cloud gateway\n    policy: policyDst\n</code></pre> <p>This routes all external traffic through ext-cloud.</p>"},{"location":"prd/cloud_network_enable_cluster/#option-2-add-ext-cloud-as-extra-external-subnet","title":"Option 2: Add ext-cloud as Extra External Subnet","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Vpc\nmetadata:\n  name: my-project\nspec:\n  enableExternal: true\n  extraExternalSubnets:\n  - ext-cloud\n  - ext-public  # Optional: include both\n  namespaces:\n  - my-project\n</code></pre> <p>This allows the VPC to use both networks.</p>"},{"location":"prd/cloud_network_enable_cluster/#key-points-for-new-vpcs","title":"Key Points for New VPCs","text":"<ol> <li>No changes needed to ovn-cluster VPC - The configuration above is global and allows <code>ovn-default</code> pods to reach ext-cloud</li> <li>Project VPCs are independent - Each project VPC needs its own routing configuration</li> <li>SNAT is automatic - When using EIP resources, SNAT is configured automatically</li> <li>extraExternalSubnets - This setting allows the VPC to connect to external subnets on physical VLANs</li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#network-architecture-diagram","title":"Network Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Physical Network                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502   VLAN 4011      \u2502                 \u2502   VLAN 4013      \u2502      \u2502\n\u2502  \u2502   ext-public     \u2502                 \u2502   ext-cloud      \u2502      \u2502\n\u2502  \u2502 168.119.17.48/28 \u2502                 \u2502 100.65.0.0/16    \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502                                     \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       OVN Logical Network                        \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                    ovn-cluster VPC                          \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  ovn-default \u2502    \u2502   ext-cloud  \u2502    \u2502  ext-public  \u2502  \u2502 \u2502\n\u2502  \u2502  \u250210.100.0.0/16 \u2502\u2500\u2500\u2500\u25b6\u2502100.65.0.0/16 \u2502    \u2502168.119.17/28 \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502              \u2502    \u2502              \u2502    \u2502              \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502 kamaji-sys   \u2502    \u2502  SNAT via    \u2502    \u2502              \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502 kube-system  \u2502    \u2502 100.65.0.101 \u2502    \u2502              \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  shalb-dev VPC  \u2502  \u2502 shalb-demo VPC  \u2502  \u2502 shalb-envoy VPC \u2502  \u2502\n\u2502  \u2502  10.1.0.0/16    \u2502  \u2502  10.0.10.0/24   \u2502  \u2502  10.0.40.0/24   \u2502  \u2502\n\u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502\n\u2502  \u2502 ext-cloud: \u2705   \u2502  \u2502 ext-cloud: \u2705   \u2502  \u2502 ext-cloud: \u274c   \u2502  \u2502\n\u2502  \u2502 ext-public: \u2705  \u2502  \u2502 ext-public: \u274c  \u2502  \u2502 ext-public: \u2705  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/cloud_network_enable_cluster/#troubleshooting","title":"Troubleshooting","text":""},{"location":"prd/cloud_network_enable_cluster/#symptom-timeout-when-accessing-ext-cloud-from-kamaji-system","title":"Symptom: Timeout when accessing ext-cloud from kamaji-system","text":"<ol> <li> <p>Check static route exists: <pre><code>kubectl ko nbctl lr-route-list ovn-cluster | grep 100.65\n</code></pre></p> </li> <li> <p>Check policy route allows traffic: <pre><code>kubectl ko nbctl lr-policy-list ovn-cluster | grep 100.65\n</code></pre></p> </li> <li> <p>Check SNAT rule exists: <pre><code>kubectl ko nbctl lr-nat-list ovn-cluster\nkubectl get ovn-snat-rules ovn-cluster-to-ext-cloud\n</code></pre></p> </li> <li> <p>Trace the packet: <pre><code>kubectl ko trace kamaji-system/&lt;pod-name&gt; 100.65.0.102 tcp 8080\n</code></pre></p> </li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#symptom-project-vpc-cannot-reach-ext-cloud","title":"Symptom: Project VPC cannot reach ext-cloud","text":"<ol> <li> <p>Check VPC has extraExternalSubnets or staticRoute: <pre><code>kubectl get vpc &lt;vpc-name&gt; -o yaml\n</code></pre></p> </li> <li> <p>Verify EIP and SNAT are configured: <pre><code>kubectl get ovn-eip,ovn-snat-rules | grep &lt;vpc-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"prd/cloud_network_enable_cluster/#references","title":"References","text":"<ul> <li>Kube-OVN VPC Documentation</li> <li>Kube-OVN External Gateway</li> </ul>"},{"location":"prd/components-inventory/","title":"Kube-DC Components Inventory","text":""},{"location":"prd/components-inventory/#kube-dc-product-components","title":"Kube-DC Product Components","text":"Component Description Kube-DC Manager (Controller) Core Kubernetes operator managing custom resources (Organizations, Projects, EIp, FIp, etc.), implementing multi-tenancy, networking policies, and resource lifecycle management UI Frontend React-based web console built with PatternFly UI framework, providing dashboard for managing organizations, projects, virtual machines, and monitoring UI Backend (API Server) Node.js/Express API server handling authentication, Kubernetes API proxying, metrics collection (Prometheus), VM management, and cloud shell access Authentication Service JWT-based authentication integration with Keycloak, managing user sessions, organization/project access tokens, and RBAC mapping Virtual Machine Management KubeVirt integration providing VM lifecycle management, live migration, snapshots, cloud-init configuration, and performance monitoring Networking Service Kube-OVN integration managing VPCs, subnets, external IPs (EIp/FIp), SNAT rules, load balancers, and multi-network connectivity Organization &amp; Project Management Multi-tenant resource isolation, namespace provisioning, Keycloak realm/group synchronization, and hierarchical RBAC Monitoring &amp; Metrics Service Prometheus integration for real-time and historical metrics collection, VM performance tracking, cluster observability Billing Integration Resource usage tracking, cost allocation per organization/project, usage reporting and quota management Kube-DC K8s Manager Kamaji-based multi-tenant Kubernetes control plane manager, enabling creation of isolated tenant Kubernetes clusters with dedicated/shared etcd Cluster API Provider CloudSigma Infrastructure provider for Cluster API enabling declarative Kubernetes cluster management on CloudSigma cloud platform CloudSigma Cloud Controller Manager (CCM) Kubernetes cloud-provider implementation for CloudSigma, providing node registration with metadata, LoadBalancer service support, and node lifecycle management CloudSigma CSI Driver Container Storage Interface driver enabling dynamic volume provisioning, attachment, expansion (offline), snapshots, and persistent storage management on CloudSigma infrastructure Kubernetes Image Builder Packer-based automation for building Ubuntu 24.04 images with pre-installed Kubernetes components (kubelet, kubeadm, kubectl, containerd) for CloudSigma worker nodes"},{"location":"prd/components-inventory/#open-source-components","title":"Open Source Components","text":"Component Open Source License Open Source Link Features/Purpose Kubernetes Apache 2.0 https://github.com/kubernetes/kubernetes Container orchestration platform, base for all Kube-DC operations Kube-OVN Apache 2.0 https://github.com/kubeovn/kube-ovn Software-defined networking with VPC, subnet, VLAN support, SNAT/DNAT, load balancing Multus CNI Apache 2.0 https://github.com/k8snetworkplumbingwg/multus-cni Multi-network plugin enabling multiple network interfaces for pods KubeVirt Apache 2.0 https://github.com/kubevirt/kubevirt Virtual machine management on Kubernetes, enabling VM workloads alongside containers CDI (Containerized Data Importer) Apache 2.0 https://github.com/kubevirt/containerized-data-importer Persistent storage management for KubeVirt VMs, image import/upload, DataVolumes Cert-Manager Apache 2.0 https://github.com/cert-manager/cert-manager Automatic TLS certificate provisioning and management, Let's Encrypt integration Envoy Gateway Apache 2.0 https://github.com/envoyproxy/gateway Kubernetes Gateway API implementation for advanced ingress, routing, and TLS passthrough Keycloak Apache 2.0 https://github.com/keycloak/keycloak Identity and access management, SSO, OIDC/SAML provider for multi-tenant authentication Prometheus Operator Apache 2.0 https://github.com/prometheus-operator/prometheus-operator Kubernetes-native Prometheus deployment, ServiceMonitors, alerting rules management Grafana AGPL-3.0 https://github.com/grafana/grafana Observability dashboards, metrics visualization, alerting UI Grafana Loki AGPL-3.0 https://github.com/grafana/loki Log aggregation system optimized for Kubernetes, stores and queries logs Grafana Alloy Apache 2.0 https://github.com/grafana/alloy OpenTelemetry collector for metrics, logs, and traces, Kubernetes events collection Kamaji Apache 2.0 https://github.com/clastix/kamaji Multi-tenant Kubernetes control plane manager, runs tenant API servers as pods Kamaji-etcd Apache 2.0 https://github.com/clastix/kamaji-etcd Shared or dedicated etcd datastore management for Kamaji tenant clusters Cluster API Apache 2.0 https://github.com/kubernetes-sigs/cluster-api Declarative Kubernetes cluster lifecycle management, infrastructure abstraction CAPI Provider KubeVirt Apache 2.0 https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt KubeVirt infrastructure provider for Cluster API, enables VM-based worker nodes CAPI Provider K3s Apache 2.0 https://github.com/cluster-api-provider-k3s K3s bootstrap and control plane provider for lightweight Kubernetes clusters Sveltos Apache 2.0 https://github.com/projectsveltos/sveltos Kubernetes addon controller for automated application deployment across clusters Kyverno Apache 2.0 https://github.com/kyverno/kyverno Kubernetes policy engine for security, compliance, and resource management policies Local Path Provisioner Apache 2.0 https://github.com/rancher/local-path-provisioner Dynamic local storage provisioner, default StorageClass for persistent volumes noVNC MPL 2.0 https://github.com/novnc/noVNC Browser-based VNC client for VM console access via WebSocket CloudSigma Cloud Controller Manager Apache 2.0 https://github.com/kube-dc/cluster-api-provider-cloudsigma/tree/main/ccm Cloud provider implementation for CloudSigma infrastructure, node initialization, LoadBalancer services CloudSigma CSI Driver Apache 2.0 https://github.com/kube-dc/cluster-api-provider-cloudsigma/tree/main/csi Container Storage Interface driver for CloudSigma persistent storage, dynamic volume provisioning, snapshots CloudSigma Go SDK Apache 2.0 https://github.com/cloudsigma/cloudsigma-sdk-go Official CloudSigma API client library for Go, used by CAPCS, CCM, and CSI Packer MPL 2.0 https://github.com/hashicorp/packer HashiCorp image builder tool for creating Kubernetes node images"},{"location":"prd/components-inventory/#frontend-dependencies-ui-stack","title":"Frontend Dependencies (UI Stack)","text":"Component License Purpose React MIT UI framework for building interactive user interfaces PatternFly React MIT Enterprise UI component library with accessible, responsive design React Router MIT Client-side routing for single-page application navigation Victory Charts MIT Data visualization library for performance and metrics charts Keycloak-js Apache 2.0 JavaScript adapter for Keycloak authentication Webpack MIT Module bundler for frontend assets and code optimization TypeScript Apache 2.0 Type-safe JavaScript for improved developer experience"},{"location":"prd/components-inventory/#backend-dependencies-api-stack","title":"Backend Dependencies (API Stack)","text":"Component License Purpose Node.js + Express MIT JavaScript runtime and web framework for REST API server @kubernetes/client-node Apache 2.0 Official Kubernetes JavaScript client library @kubevirt-ui/kubevirt-api Apache 2.0 KubeVirt API TypeScript definitions and utilities openid-client MIT OpenID Connect relying party implementation for OIDC authentication jose MIT JavaScript Object Signing and Encryption, JWT handling axios MIT HTTP client for REST API requests to Kubernetes and external services express-ws BSD-2-Clause WebSocket support for Express, enables VM console streaming http-proxy-middleware MIT Proxy middleware for routing requests to Kubernetes API js-yaml MIT YAML parser for Kubernetes manifests and configuration mustache MIT Template engine for dynamic resource generation moment MIT Date/time manipulation for metrics and timestamps swagger-jsdoc MIT OpenAPI documentation generation from JSDoc comments swagger-ui-express MIT Interactive API documentation UI"},{"location":"prd/components-inventory/#version-information-as-of-latest-installer","title":"Version Information (as of latest installer)","text":"Component Version Kube-OVN v1.14.10 Multus CNI v4.1.0 KubeVirt v1.6.0 CDI v1.59.0 Cert-Manager v1.14.4 Envoy Gateway v1.2.6 Keycloak 24.3.0 Prometheus Operator (kube-prometheus-stack) 67.4.0 Grafana Loki 6.11.0 Grafana Alloy 0.10.1 Kamaji 1.0.0 Kamaji-etcd 0.14.0 Cluster API v1.8.1 CAPI K3s Provider v1.2.2 Kyverno v1.15.2 Sveltos v0.57.3 Local Path Provisioner v0.0.31"},{"location":"prd/components-inventory/#architecture-overview","title":"Architecture Overview","text":""},{"location":"prd/components-inventory/#component-interaction-flow","title":"Component Interaction Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         User Interface Layer                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502 Web Browser  \u2502\u2500\u2500\u2500\u25b6\u2502 UI Frontend  \u2502\u2500\u2500\u2500\u25b6\u2502  UI Backend  \u2502          \u2502\n\u2502  \u2502  (React)     \u2502    \u2502 (PatternFly) \u2502    \u2502  (Node.js)   \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                     \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Authentication Layer            \u2502                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502                 \u2502\n\u2502  \u2502  Keycloak    \u2502\u25c0\u2500\u2500\u2500\u2502 Auth Service \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502  \u2502   (OIDC)     \u2502    \u2502   (JWT)      \u2502                               \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Kubernetes API Layer (Management Cluster)               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502   Kube-DC    \u2502    \u2502    Kamaji    \u2502    \u2502 Cluster API  \u2502          \u2502\n\u2502  \u2502  Controller  \u2502    \u2502  Controller  \u2502    \u2502 Controllers  \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          \u2502   Infrastructure &amp; Platform Layer     \u2502                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502  Kube-OVN    \u2502    \u2502  KubeVirt  \u2502    \u2502   Prometheus   \u2502          \u2502\n\u2502  \u2502 (Networking) \u2502    \u2502   (VMs)    \u2502    \u2502  (Monitoring)  \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/components-inventory/#cloudsigma-integration-components","title":"CloudSigma Integration Components","text":"<p>The cluster-api-provider-cloudsigma repository includes three key components for CloudSigma cloud integration:</p>"},{"location":"prd/components-inventory/#1-cloudsigma-cloud-controller-manager-ccm","title":"1. CloudSigma Cloud Controller Manager (CCM)","text":"<p>Location: <code>/ccm</code></p> <p>Purpose: Implements the Kubernetes cloud-provider interface for CloudSigma infrastructure</p> <p>Features: - Node Controller: Automatic node registration with CloudSigma metadata, providerID assignment, topology labels - Service Controller: LoadBalancer service type support with CloudSigma load balancer API integration - Route Controller: Optional pod network route management (typically disabled with CNI)</p> <p>Deployment: DaemonSet on control plane nodes with external cloud provider configuration</p> <p>Key Capabilities: - Initializes nodes with <code>providerID: cloudsigma://server-uuid</code> - Adds topology labels: <code>topology.kubernetes.io/region</code>, <code>node.kubernetes.io/instance-type</code> - Creates CloudSigma load balancers for <code>type: LoadBalancer</code> services - Updates service status with external IPs</p>"},{"location":"prd/components-inventory/#2-cloudsigma-csi-driver","title":"2. CloudSigma CSI Driver","text":"<p>Location: <code>/csi</code></p> <p>Purpose: Container Storage Interface driver for persistent volume management on CloudSigma</p> <p>Features: - Dynamic Volume Provisioning: Automatically create CloudSigma drives for PersistentVolumeClaims - Volume Attachment: Hot-plug/unplug volumes to running nodes with detachment verification - Volume Expansion: Offline resize support (CloudSigma platform limitation) - Volume Snapshots: Create and restore volume snapshots via CloudSigma API - Battle-proof Device Discovery: Stable <code>/dev/disk/by-path/</code> detection with mutex serialization - Storage Classes: Support for DSSD (Distributed SSD) storage type</p> <p>Architecture: - Controller Plugin: Deployment handling volume lifecycle operations (create, delete, attach, detach, expand, snapshot) - Node Plugin: DaemonSet on each node for volume staging, publishing, and filesystem operations</p> <p>Performance: Sequential R/W 200-500 MB/s, Random IOPS 5k-15k (varies by VM size)</p> <p>Current Version: v1.2.7 with offline expansion and detachment verification</p>"},{"location":"prd/components-inventory/#3-kubernetes-image-builder","title":"3. Kubernetes Image Builder","text":"<p>Location: <code>/images/ubuntu-k8s</code></p> <p>Purpose: Packer-based automation for building Kubernetes-ready Ubuntu images for CloudSigma</p> <p>Build Methods: - CloudSigma Build (Recommended): Builds directly on CloudSigma infrastructure, no upload required - Local QEMU Build: Builds locally, requires manual upload via CloudSigma Web UI</p> <p>What's Included: - Ubuntu 24.04 LTS base image - Containerd container runtime - Kubernetes components: kubelet, kubeadm, kubectl (version configurable) - CNI plugins pre-installed - Kernel modules and sysctl configuration - Cloud-init configured for CAPI bootstrap - Common Kubernetes images pre-pulled for faster node startup</p> <p>Build Configuration: <pre><code># Build on CloudSigma (recommended)\nmake build-on-cloudsigma K8S_VERSION=1.34.1\n\n# Build locally with QEMU\nmake build K8S_VERSION=1.34.1 UBUNTU_VERSION=24.04\n</code></pre></p> <p>Provisioning Scripts: 1. Base packages installation 2. Containerd installation and configuration 3. Kubernetes component installation 4. System configuration (kernel modules, sysctl) 5. Cleanup and image preparation</p>"},{"location":"prd/components-inventory/#multi-repository-structure","title":"Multi-Repository Structure","text":"<ol> <li>kube-dc (Main Repository)</li> <li>Core controller managing Organizations, Projects, EIp, FIp</li> <li>UI Frontend and Backend</li> <li>Multi-tenancy and networking logic</li> <li> <p>Installation templates (cdev/helm)</p> </li> <li> <p>kube-dc-k8-manager (Tenant Cluster Manager)</p> </li> <li>Manages tenant Kubernetes control planes via Kamaji</li> <li>Cluster API integration for worker node provisioning</li> <li>Datastore management (shared/dedicated etcd)</li> <li> <p>CloudSigma and KubeVirt infrastructure support</p> </li> <li> <p>cluster-api-provider-cloudsigma (Cloud Provider)</p> </li> <li>CAPI infrastructure provider for CloudSigma</li> <li>CloudSigma SDK integration</li> <li>Worker node provisioning on CloudSigma platform</li> <li>CCM (<code>/ccm</code>): Cloud Controller Manager for node initialization and LoadBalancer services</li> <li>CSI (<code>/csi</code>): Container Storage Interface driver for persistent volume management</li> <li>Image Builder (<code>/images/ubuntu-k8s</code>): Packer-based Kubernetes node image automation</li> </ol>"},{"location":"prd/components-inventory/#integration-points","title":"Integration Points","text":""},{"location":"prd/components-inventory/#external-systems-integration","title":"External Systems Integration","text":"System Integration Method Purpose Keycloak OIDC/OAuth2 User authentication, organization realms, group management Kubernetes API client-go, @kubernetes/client-node Resource management, RBAC, CRD operations Prometheus HTTP API Metrics collection, VM performance data, alerting CloudSigma API Go SDK Infrastructure provisioning for CAPI clusters OVN Database ovn-nbctl/ovs-vsctl Network configuration, VPC management, routing"},{"location":"prd/components-inventory/#component-communication","title":"Component Communication","text":"<ul> <li>UI Frontend \u2194 UI Backend: REST API (HTTP/HTTPS), WebSocket (VM console)</li> <li>UI Backend \u2194 Kubernetes API: Kubernetes API client, token-based auth</li> <li>Kube-DC Controller \u2194 Kube-OVN: Custom Resource updates, OVN database queries</li> <li>Kamaji \u2194 Tenant Clusters: TCP/6443 (Kubernetes API), etcd connection</li> <li>CAPI \u2194 Infrastructure Providers: Infrastructure machine creation, health checks</li> </ul>"},{"location":"prd/components-inventory/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"prd/components-inventory/#installation-method","title":"Installation Method","text":"<ul> <li>Primary: cdev (Cloud Development Environment) with Helm charts</li> <li>Templates: YAML-based templating with variable substitution</li> <li>Namespace Organization: </li> <li><code>kube-system</code>: Kube-OVN, Multus</li> <li><code>kube-dc</code>: Main controller, UI</li> <li><code>kubevirt</code>: VM management</li> <li><code>monitoring</code>: Prometheus, Grafana, Loki</li> <li><code>keycloak</code>: Identity management</li> <li><code>cert-manager</code>: Certificate automation</li> <li><code>kamaji-system</code>: Tenant control plane manager</li> <li><code>projectsveltos</code>: Addon controller</li> <li>Organization namespaces: <code>&lt;org-name&gt;</code></li> <li>Project namespaces: <code>&lt;org-name&gt;-&lt;project-name&gt;</code></li> </ul>"},{"location":"prd/components-inventory/#high-availability-considerations","title":"High Availability Considerations","text":"<ul> <li>Multi-replica controller deployments</li> <li>Kamaji etcd cluster (3+ replicas)</li> <li>Kube-OVN OVN database HA</li> <li>LoadBalancer services for critical components</li> <li>StatefulSet for stateful components (etcd, Loki)</li> </ul> <p>Last Updated: January 2025 Document Version: 1.0</p>"},{"location":"prd/dynamic_https_listeners/","title":"PRD: Dynamic HTTPS Listeners - Gateway-Terminated TLS","text":""},{"location":"prd/dynamic_https_listeners/#executive-summary","title":"Executive Summary","text":"<p>This PRD defines the automatic HTTPS listener creation mechanism for services that want the Gateway to terminate TLS. When a Service is annotated with <code>expose-route: https</code>, the controller automatically creates: 1. A TLS Certificate (via cert-manager) 2. A dedicated Gateway listener for the hostname 3. An HTTPRoute pointing to the service's Backend</p> <p>This enables users to expose services over HTTPS without handling TLS in their application.</p>"},{"location":"prd/dynamic_https_listeners/#problem-statement","title":"Problem Statement","text":"<p>Currently, there are two options for HTTPS exposure:</p>"},{"location":"prd/dynamic_https_listeners/#option-1-tls-passthrough-current","title":"Option 1: TLS Passthrough (Current)","text":"<p><pre><code>service.nlb.kube-dc.com/expose-route: \"tls-passthrough\"\n</code></pre> - \u2705 Works today - \u274c App must terminate TLS (configure nginx/app with certificates) - \u274c More complex application setup</p>"},{"location":"prd/dynamic_https_listeners/#option-2-platform-style-https-manual","title":"Option 2: Platform-Style HTTPS (Manual)","text":"<p><pre><code># Requires manual Gateway listener configuration by admin\n# Only available for platform services (console, keycloak, etc.)\n</code></pre> - \u2705 App serves plain HTTP - \u274c Requires admin intervention - \u274c Not self-service for users</p>"},{"location":"prd/dynamic_https_listeners/#desired-state","title":"Desired State","text":"<p><pre><code>service.nlb.kube-dc.com/expose-route: \"https\"\n</code></pre> - \u2705 App serves plain HTTP (no TLS handling needed) - \u2705 Gateway terminates TLS automatically - \u2705 Certificate auto-provisioned via cert-manager - \u2705 Self-service for project users</p>"},{"location":"prd/dynamic_https_listeners/#solution-dynamic-listener-creation","title":"Solution: Dynamic Listener Creation","text":""},{"location":"prd/dynamic_https_listeners/#user-experience","title":"User Experience","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app\n  annotations:\n    # Single annotation for full HTTPS automation\n    service.nlb.kube-dc.com/expose-route: \"https\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-web-app\n  ports:\n  - port: 80        # App serves plain HTTP\n    targetPort: 8080\n</code></pre> <p>Result: - \u2705 Certificate requested via cert-manager (ACME HTTP-01) - \u2705 Gateway listener created for <code>my-web-app-{namespace}.{base_domain}</code> - \u2705 HTTPRoute created pointing to Backend - \u2705 User accesses <code>https://my-web-app-demo.stage.kube-dc.com</code></p>"},{"location":"prd/dynamic_https_listeners/#traffic-flow","title":"Traffic Flow","text":"<pre><code>User Browser\n    \u2502\n    \u2502 HTTPS (encrypted)\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Envoy Gateway               \u2502\n\u2502  Listener: https-my-web-app-demo    \u2502\n\u2502  Port: 443                          \u2502\n\u2502  TLS: Terminate (uses certificate)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u2502 HTTP (plain, internal)\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Backend                     \u2502\n\u2502  Target: 100.65.x.x:80              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u2502 HTTP\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         User Application            \u2502\n\u2502  Listens on: 0.0.0.0:8080           \u2502\n\u2502  No TLS configuration needed        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#annotation-schema","title":"Annotation Schema","text":""},{"location":"prd/dynamic_https_listeners/#route-types-extended","title":"Route Types (Extended)","text":"Annotation Value Description App Requirements <code>http</code> Plain HTTP on port 80 Serves HTTP <code>tls-passthrough</code> TLS passthrough on port 443 Serves HTTPS (app terminates TLS) <code>https</code> NEW Gateway-terminated TLS on port 443 Serves HTTP (gateway terminates TLS)"},{"location":"prd/dynamic_https_listeners/#additional-annotations","title":"Additional Annotations","text":"Annotation Values Default Description <code>service.nlb.kube-dc.com/expose-route</code> <code>http</code>, <code>tls-passthrough</code>, <code>https</code> - Route type <code>service.nlb.kube-dc.com/route-hostname</code> string auto-generated Custom hostname <code>service.nlb.kube-dc.com/route-port</code> number first port Backend port <code>service.nlb.kube-dc.com/tls-issuer</code> string <code>letsencrypt</code> cert-manager Issuer name"},{"location":"prd/dynamic_https_listeners/#architecture","title":"Architecture","text":""},{"location":"prd/dynamic_https_listeners/#resources-created-for-expose-route-https","title":"Resources Created for <code>expose-route: https</code>","text":"<pre><code>Service (user creates)\n    \u2502\n    \u251c\u2500\u2500 EIP (auto-created)\n    \u2502\n    \u251c\u2500\u2500 Backend (auto-created)\n    \u2502\n    \u251c\u2500\u2500 Certificate (auto-created)\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500 Secret (cert-manager creates)\n    \u2502\n    \u251c\u2500\u2500 Gateway Listener (auto-created via patch)\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500 References Certificate Secret\n    \u2502\n    \u251c\u2500\u2500 ReferenceGrant (auto-created)\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500 Allows Gateway to access Certificate Secret\n    \u2502\n    \u2514\u2500\u2500 HTTPRoute (auto-created)\n            \u2502\n            \u2514\u2500\u2500 References Gateway Listener and Backend\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#controller-flow","title":"Controller Flow","text":"<pre><code>func (m *GatewayRouteManager) syncHTTPSRoute(ctx context.Context, hostname string, port int32) error {\n    // 1. Ensure Issuer exists (or use default)\n    issuer := m.getIssuer()\n\n    // 2. Create Certificate\n    cert := m.createCertificate(hostname, issuer)\n\n    // 3. Wait for Certificate to be ready\n    if !m.isCertificateReady(cert) {\n        return requeueAfter(30 * time.Second)\n    }\n\n    // 4. Create ReferenceGrant (allows Gateway to access cert secret)\n    m.createReferenceGrant(hostname)\n\n    // 5. Patch Gateway to add listener\n    m.patchGatewayAddListener(hostname, cert.Spec.SecretName)\n\n    // 6. Create HTTPRoute\n    m.createHTTPRoute(hostname, port)\n\n    return nil\n}\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#implementation-details","title":"Implementation Details","text":""},{"location":"prd/dynamic_https_listeners/#1-certificate-creation","title":"1. Certificate Creation","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: my-web-app-tls\n  namespace: shalb-demo\n  ownerReferences:\n  - apiVersion: v1\n    kind: Service\n    name: my-web-app\n    uid: &lt;service-uid&gt;\nspec:\n  secretName: my-web-app-tls-secret\n  issuerRef:\n    name: letsencrypt\n    kind: Issuer\n  dnsNames:\n  - my-web-app-demo.stage.kube-dc.com\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#2-referencegrant","title":"2. ReferenceGrant","text":"<p>Allows the Gateway in <code>envoy-gateway-system</code> to reference the certificate Secret in the user's namespace:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: allow-gateway-cert-my-web-app\n  namespace: shalb-demo\n  ownerReferences:\n  - apiVersion: v1\n    kind: Service\n    name: my-web-app\nspec:\n  from:\n  - group: gateway.networking.k8s.io\n    kind: Gateway\n    namespace: envoy-gateway-system\n  to:\n  - group: \"\"\n    kind: Secret\n    name: my-web-app-tls-secret\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#3-gateway-listener-patch","title":"3. Gateway Listener Patch","text":"<pre><code># Controller patches Gateway to add:\n- name: https-my-web-app-shalb-demo\n  hostname: my-web-app-demo.stage.kube-dc.com\n  port: 443\n  protocol: HTTPS\n  tls:\n    mode: Terminate\n    certificateRefs:\n    - group: \"\"\n      kind: Secret\n      name: my-web-app-tls-secret\n      namespace: shalb-demo\n  allowedRoutes:\n    namespaces:\n      from: Same\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#4-httproute","title":"4. HTTPRoute","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: my-web-app-route\n  namespace: shalb-demo\n  ownerReferences:\n  - apiVersion: v1\n    kind: Service\n    name: my-web-app\nspec:\n  parentRefs:\n  - group: gateway.networking.k8s.io\n    kind: Gateway\n    name: eg\n    namespace: envoy-gateway-system\n    sectionName: https-my-web-app-shalb-demo\n  hostnames:\n  - my-web-app-demo.stage.kube-dc.com\n  rules:\n  - backendRefs:\n    - group: gateway.envoyproxy.io\n      kind: Backend\n      name: my-web-app-backend\n      port: 80\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#rbac-requirements","title":"RBAC Requirements","text":""},{"location":"prd/dynamic_https_listeners/#controller-permissions-new","title":"Controller Permissions (New)","text":"<pre><code># +kubebuilder:rbac:groups=gateway.networking.k8s.io,resources=gateways,verbs=get;list;watch;patch\n# +kubebuilder:rbac:groups=gateway.networking.k8s.io,resources=referencegrants,verbs=get;list;watch;create;update;patch;delete\n# +kubebuilder:rbac:groups=cert-manager.io,resources=certificates,verbs=get;list;watch;create;update;patch;delete\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#security-considerations","title":"Security Considerations","text":"Concern Mitigation Controller can modify Gateway Limited to adding/removing listeners, not core config Certificate in user namespace ReferenceGrant scoped to specific secret Listener naming conflicts Use <code>https-{service}-{namespace}</code> pattern Orphaned listeners OwnerReference on Service triggers cleanup"},{"location":"prd/dynamic_https_listeners/#cleanup","title":"Cleanup","text":"<p>When Service is deleted:</p> <ol> <li>OwnerReferences automatically delete:</li> <li>Certificate</li> <li>ReferenceGrant</li> <li>HTTPRoute</li> <li>Backend</li> <li> <p>EIP</p> </li> <li> <p>Controller explicitly removes:</p> </li> <li>Gateway listener (via patch)</li> </ol> <pre><code>func (m *GatewayRouteManager) deleteHTTPSRoute(ctx context.Context) error {\n    hostname := m.getHostname()\n    listenerName := m.getListenerName(hostname)\n\n    // Remove listener from Gateway\n    m.patchGatewayRemoveListener(listenerName)\n\n    // Other resources cleaned up by OwnerReferences\n    return nil\n}\n</code></pre>"},{"location":"prd/dynamic_https_listeners/#comparison-route-types","title":"Comparison: Route Types","text":"Feature <code>http</code> <code>tls-passthrough</code> <code>https</code> Gateway Port 80 443 443 TLS Termination None App Gateway App Complexity Low High Low Certificate Not needed App manages Auto-provisioned Gateway Listener Shared Shared wildcard Per-service Gateway Modification No No Yes"},{"location":"prd/dynamic_https_listeners/#example-usage","title":"Example Usage","text":""},{"location":"prd/dynamic_https_listeners/#simple-web-application","title":"Simple Web Application","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-web-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: my-web-app\n  template:\n    metadata:\n      labels:\n        app: my-web-app\n    spec:\n      containers:\n      - name: app\n        image: nginx:alpine\n        ports:\n        - containerPort: 80  # Plain HTTP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app\n  annotations:\n    service.nlb.kube-dc.com/expose-route: \"https\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-web-app\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Access: <code>https://my-web-app-{namespace}.{base_domain}</code></p>"},{"location":"prd/dynamic_https_listeners/#custom-hostname","title":"Custom Hostname","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app\n  annotations:\n    service.nlb.kube-dc.com/expose-route: \"https\"\n    service.nlb.kube-dc.com/route-hostname: \"app.example.com\"\nspec:\n  type: LoadBalancer\n  ...\n</code></pre> <p>Note: Custom hostname requires: 1. DNS pointing to Gateway IP 2. Issuer that can validate that domain (may need DNS-01)</p>"},{"location":"prd/dynamic_https_listeners/#prerequisites","title":"Prerequisites","text":""},{"location":"prd/dynamic_https_listeners/#existing-infrastructure","title":"Existing Infrastructure","text":"Component Status Notes cert-manager \u2705 Installed Handles certificate lifecycle Issuer (letsencrypt) \u2705 Available HTTP-01 challenge via Gateway Gateway API CRDs \u2705 Installed ReferenceGrant, HTTPRoute Envoy Gateway \u2705 Running Supports dynamic listeners"},{"location":"prd/dynamic_https_listeners/#new-requirements","title":"New Requirements","text":"Component Action Priority Controller RBAC Add Gateway patch permission High Controller Logic Implement HTTPS route sync High Default Issuer Configure in master-config Medium Listener cleanup Implement deletion logic High"},{"location":"prd/dynamic_https_listeners/#testing-plan","title":"Testing Plan","text":""},{"location":"prd/dynamic_https_listeners/#unit-tests","title":"Unit Tests","text":"<ul> <li> Certificate name generation</li> <li> Listener name generation (avoid conflicts)</li> <li> ReferenceGrant creation</li> <li> Gateway patch logic</li> </ul>"},{"location":"prd/dynamic_https_listeners/#integration-tests","title":"Integration Tests","text":"<ul> <li> Certificate issuance flow</li> <li> Gateway listener addition/removal</li> <li> HTTPRoute attachment to listener</li> <li> End-to-end HTTPS access</li> </ul>"},{"location":"prd/dynamic_https_listeners/#e2e-tests","title":"E2E Tests","text":"<ul> <li> Create service with <code>expose-route: https</code></li> <li> Verify certificate issued</li> <li> Verify HTTPS access works</li> <li> Verify cleanup on service deletion</li> </ul>"},{"location":"prd/dynamic_https_listeners/#rollout-plan","title":"Rollout Plan","text":""},{"location":"prd/dynamic_https_listeners/#phase-1-implementation","title":"Phase 1: Implementation","text":"<ul> <li>Implement controller logic</li> <li>Add RBAC permissions</li> <li>Unit tests</li> </ul>"},{"location":"prd/dynamic_https_listeners/#phase-2-testing","title":"Phase 2: Testing","text":"<ul> <li>Integration tests in dev cluster</li> <li>E2E tests</li> </ul>"},{"location":"prd/dynamic_https_listeners/#phase-3-documentation","title":"Phase 3: Documentation","text":"<ul> <li>Update examples</li> <li>Update README</li> </ul>"},{"location":"prd/dynamic_https_listeners/#phase-4-release","title":"Phase 4: Release","text":"<ul> <li>Deploy to staging</li> <li>User acceptance testing</li> <li>Production rollout</li> </ul>"},{"location":"prd/dynamic_https_listeners/#open-questions","title":"Open Questions","text":"<ol> <li>Issuer Configuration: Should we require an Issuer per namespace or use a ClusterIssuer?</li> <li> <p>Recommendation: Support both, default to namespace Issuer named <code>letsencrypt</code></p> </li> <li> <p>Certificate Renewal: How to handle certificate renewal with listener?</p> </li> <li> <p>cert-manager handles renewal automatically, secret is updated in place</p> </li> <li> <p>Listener Limits: Is there a max number of listeners on Gateway?</p> </li> <li> <p>Envoy Gateway: No hard limit, but monitor performance</p> </li> <li> <p>Custom Domains: How to handle DNS validation for non-base-domain hostnames?</p> </li> <li>Recommendation: Require DNS-01 capable Issuer for custom domains</li> </ol>"},{"location":"prd/dynamic_https_listeners/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Wildcard Certificate Option: Single <code>*.{base_domain}</code> cert for simpler setup</li> <li>mTLS Support: Client certificate verification</li> <li>HTTP\u2192HTTPS Redirect: Automatic redirect from HTTP to HTTPS</li> <li>HSTS Headers: Automatic HSTS header injection</li> </ol>"},{"location":"prd/endpoint_for_lb/","title":"PRD: Automatic External Endpoints for LoadBalancer Services","text":"<p>Status: \u2705 Implemented Version: v0.1.34-dev1 Date: 2025-11-19  </p>"},{"location":"prd/endpoint_for_lb/#problem-statement","title":"Problem Statement","text":"<p>When using LoadBalancer services in kube-dc multi-tenant VPC environments, external clients (such as Kamaji controllers, CI/CD systems, or other tenants) need stable DNS-based access to these services. Currently, users must:</p> <ol> <li>Manually discover the LoadBalancer's external IP address</li> <li>Hardcode IPs in configurations, certificates, and kubeconfigs</li> <li>Manually create and maintain Service/Endpoints pairs for external access</li> <li>Track and update these endpoints whenever LoadBalancer IPs change</li> </ol> <p>This creates operational burden and increases the risk of configuration drift, especially in multi-tenant scenarios where LoadBalancers are frequently created, updated, or recreated.</p>"},{"location":"prd/endpoint_for_lb/#real-world-impact","title":"Real-World Impact","text":"<p>Scenario: Kamaji Multi-Tenant Setup - Kamaji controller runs in <code>kamaji-system</code> namespace - Tenant control planes run in tenant VPC namespaces (e.g., <code>shalb-envoy</code>) - etcd cluster exposed via LoadBalancer with external IP <code>168.119.17.55</code> - Kamaji cannot reach <code>etcd.shalb-envoy.svc.cluster.local</code> (internal ClusterIP) due to network isolation - Must use external IP <code>168.119.17.55:2379</code> in DataStore configuration - When LoadBalancer is recreated, IP changes, breaking all references</p> <p>Current Workaround: <pre><code># Manual Service + Endpoints creation\napiVersion: v1\nkind: Service\nmetadata:\n  name: etcd-lb-ext\n  namespace: shalb-envoy\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - port: 2379\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: etcd-lb-ext\n  namespace: shalb-envoy\nsubsets:\n  - addresses:\n      - ip: 168.119.17.55  # Must be manually updated!\n    ports:\n      - port: 2379\n</code></pre></p> <p>Then Kamaji can use: <code>etcd-lb-ext.shalb-envoy.svc.cluster.local:2379</code></p>"},{"location":"prd/endpoint_for_lb/#implemented-solution","title":"Implemented Solution","text":"<p>Automatically create and manage external endpoints for every LoadBalancer service managed by kube-dc. The service controller:</p> <ol> <li>Creates a headless Service + Endpoints pair when a LoadBalancer is created</li> <li>Updates the Endpoints IP when the LoadBalancer's external IP changes</li> <li>Deletes the external endpoint pair when the LoadBalancer is deleted</li> <li>Supports multiple endpoints if a LoadBalancer has multiple external IPs</li> </ol>"},{"location":"prd/endpoint_for_lb/#naming-convention","title":"Naming Convention","text":"<p>For a LoadBalancer service named <code>&lt;service-name&gt;</code>, create: - Service: <code>&lt;service-name&gt;-ext</code> (headless ClusterIP: None) - Endpoints: <code>&lt;service-name&gt;-ext</code> (pointing to LoadBalancer external IP)</p> <p>Examples: - <code>etcd-lb</code> \u2192 <code>etcd-lb-ext.shalb-envoy.svc.cluster.local</code> - <code>cluster-a-cp</code> \u2192 <code>cluster-a-cp-ext.shalb-envoy.svc.cluster.local</code> - <code>api-gateway</code> \u2192 <code>api-gateway-ext.tenant-ns.svc.cluster.local</code></p>"},{"location":"prd/endpoint_for_lb/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 kube-dc Service Controller (internal/controller/core/service_controller.go) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u251c\u2500 Reconcile LoadBalancer Service\n                              \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                              \u2502\n                \u25bc                              \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 EIP Management     \u2502        \u2502 External Endpoint Mgmt \u2502\n    \u2502 (existing)         \u2502        \u2502 (NEW)                  \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502 - Create EIP       \u2502        \u2502 - Create Service-ext   \u2502\n    \u2502 - Bind to LB       \u2502        \u2502 - Create Endpoints     \u2502\n    \u2502 - Update status    \u2502        \u2502 - Update on IP change  \u2502\n    \u2502 - Delete on remove \u2502        \u2502 - Delete on LB delete  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                              \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u25bc\n                    LoadBalancer gets external IP\n                               \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u25bc                             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Service: etcd-lb    \u2502      \u2502 Service: etcd-lb-ext\u2502\n    \u2502 Type: LoadBalancer  \u2502      \u2502 Type: ClusterIP     \u2502\n    \u2502 ExternalIP:         \u2502      \u2502 ClusterIP: None     \u2502\n    \u2502   168.119.17.55     \u2502      \u2502                     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                           \u2502\n                                           \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Endpoints: etcd-lb-ext   \u2502\n                              \u2502 IP: 168.119.17.55        \u2502\n                              \u2502 Port: 2379               \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/endpoint_for_lb/#technical-specification","title":"Technical Specification","text":""},{"location":"prd/endpoint_for_lb/#1-service-resource-structure","title":"1. Service Resource Structure","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: &lt;service-name&gt;-ext\n  namespace: &lt;service-namespace&gt;\n  labels:\n    kube-dc.com/managed-by: service-lb-controller\n    kube-dc.com/source-service: &lt;service-name&gt;\n    kube-dc.com/endpoint-type: external\n  ownerReferences:\n  - apiVersion: v1\n    kind: Service\n    name: &lt;service-name&gt;\n    uid: &lt;service-uid&gt;\n    controller: true\n    blockOwnerDeletion: true\nspec:\n  type: ClusterIP\n  clusterIP: None  # Headless service\n  ports:\n  - name: &lt;port-name&gt;\n    port: &lt;port&gt;\n    protocol: &lt;protocol&gt;\n  # Copy all ports from source LoadBalancer service\n</code></pre>"},{"location":"prd/endpoint_for_lb/#2-endpoints-resource-structure","title":"2. Endpoints Resource Structure","text":"<pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  name: &lt;service-name&gt;-ext\n  namespace: &lt;service-namespace&gt;\n  labels:\n    kube-dc.com/managed-by: service-lb-controller\n    kube-dc.com/source-service: &lt;service-name&gt;\n    kube-dc.com/endpoint-type: external\n  ownerReferences:\n  - apiVersion: v1\n    kind: Service\n    name: &lt;service-name&gt;-ext\n    uid: &lt;service-ext-uid&gt;\n    controller: true\n    blockOwnerDeletion: true\nsubsets:\n  - addresses:\n      - ip: &lt;loadbalancer-external-ip-1&gt;\n      - ip: &lt;loadbalancer-external-ip-2&gt;  # If multiple IPs\n    ports:\n      - name: &lt;port-name&gt;\n        port: &lt;port&gt;\n        protocol: &lt;protocol&gt;\n</code></pre>"},{"location":"prd/endpoint_for_lb/#3-controller-logic-implemented","title":"3. Controller Logic (Implemented)","text":""},{"location":"prd/endpoint_for_lb/#file-internalservice_lbexternal_endpointgo","title":"File: <code>internal/service_lb/external_endpoint.go</code>","text":"<pre><code>package servicelb\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    corev1 \"k8s.io/api/core/v1\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n    \"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\"\n)\n\nconst (\n    ExternalEndpointSuffix = \"-ext\"\n    ManagedByLabel         = \"kube-dc.com/managed-by\"\n    SourceServiceLabel     = \"kube-dc.com/source-service\"\n    EndpointTypeLabel      = \"kube-dc.com/endpoint-type\"\n    ControllerName         = \"service-lb-controller\"\n)\n\n// ExternalEndpointManager manages external endpoint resources for LoadBalancer services\ntype ExternalEndpointManager struct {\n    client.Client\n    Service *corev1.Service\n}\n\n// Sync creates or updates external Service and Endpoints\nfunc (m *ExternalEndpointManager) Sync(ctx context.Context) error {\n    if m.Service.Spec.Type != corev1.ServiceTypeLoadBalancer {\n        return nil // Only for LoadBalancer services\n    }\n\n    externalIPs := m.getExternalIPs()\n    if len(externalIPs) == 0 {\n        // LoadBalancer not ready yet, skip\n        return nil\n    }\n\n    // Create or update external Service\n    if err := m.syncExternalService(ctx); err != nil {\n        return fmt.Errorf(\"failed to sync external service: %w\", err)\n    }\n\n    // Create or update Endpoints\n    if err := m.syncEndpoints(ctx, externalIPs); err != nil {\n        return fmt.Errorf(\"failed to sync endpoints: %w\", err)\n    }\n\n    return nil\n}\n\n// Delete removes external Service and Endpoints\nfunc (m *ExternalEndpointManager) Delete(ctx context.Context) error {\n    extSvcName := m.getExternalServiceName()\n\n    // Delete external Service (Endpoints will be cascaded via ownerReference)\n    extSvc := &amp;corev1.Service{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        },\n    }\n\n    if err := m.Client.Delete(ctx, extSvc); client.IgnoreNotFound(err) != nil {\n        return fmt.Errorf(\"failed to delete external service: %w\", err)\n    }\n\n    return nil\n}\n\nfunc (m *ExternalEndpointManager) getExternalServiceName() string {\n    return m.Service.Name + ExternalEndpointSuffix\n}\n\nfunc (m *ExternalEndpointManager) getExternalIPs() []string {\n    ips := []string{}\n    for _, ingress := range m.Service.Status.LoadBalancer.Ingress {\n        if ingress.IP != \"\" {\n            ips = append(ips, ingress.IP)\n        }\n    }\n    return ips\n}\n\nfunc (m *ExternalEndpointManager) syncExternalService(ctx context.Context) error {\n    extSvcName := m.getExternalServiceName()\n    extSvc := &amp;corev1.Service{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        },\n    }\n\n    _, err := controllerutil.CreateOrUpdate(ctx, m.Client, extSvc, func() error {\n        // Set labels\n        if extSvc.Labels == nil {\n            extSvc.Labels = make(map[string]string)\n        }\n        extSvc.Labels[ManagedByLabel] = ControllerName\n        extSvc.Labels[SourceServiceLabel] = m.Service.Name\n        extSvc.Labels[EndpointTypeLabel] = \"external\"\n\n        // Set controller reference (ensures garbage collection)\n        if err := controllerutil.SetControllerReference(m.Service, extSvc, m.Scheme()); err != nil {\n            return err\n        }\n\n        // Configure as headless service\n        extSvc.Spec.Type = corev1.ServiceTypeClusterIP\n        extSvc.Spec.ClusterIP = corev1.ClusterIPNone\n\n        // Copy ports from source service\n        extSvc.Spec.Ports = []corev1.ServicePort{}\n        for _, port := range m.Service.Spec.Ports {\n            extSvc.Spec.Ports = append(extSvc.Spec.Ports, corev1.ServicePort{\n                Name:     port.Name,\n                Port:     port.Port,\n                Protocol: port.Protocol,\n            })\n        }\n\n        return nil\n    })\n\n    return err\n}\n\nfunc (m *ExternalEndpointManager) syncEndpoints(ctx context.Context, externalIPs []string) error {\n    extSvcName := m.getExternalServiceName()\n    endpoints := &amp;corev1.Endpoints{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        },\n    }\n\n    _, err := controllerutil.CreateOrUpdate(ctx, m.Client, endpoints, func() error {\n        // Set labels\n        if endpoints.Labels == nil {\n            endpoints.Labels = make(map[string]string)\n        }\n        endpoints.Labels[ManagedByLabel] = ControllerName\n        endpoints.Labels[SourceServiceLabel] = m.Service.Name\n        endpoints.Labels[EndpointTypeLabel] = \"external\"\n\n        // Get external service for owner reference\n        extSvc := &amp;corev1.Service{}\n        if err := m.Client.Get(ctx, client.ObjectKey{\n            Name:      extSvcName,\n            Namespace: m.Service.Namespace,\n        }, extSvc); err != nil {\n            return fmt.Errorf(\"failed to get external service: %w\", err)\n        }\n\n        // Set controller reference to external service\n        if err := controllerutil.SetControllerReference(extSvc, endpoints, m.Scheme()); err != nil {\n            return err\n        }\n\n        // Build addresses\n        addresses := []corev1.EndpointAddress{}\n        for _, ip := range externalIPs {\n            addresses = append(addresses, corev1.EndpointAddress{\n                IP: ip,\n            })\n        }\n\n        // Build ports\n        ports := []corev1.EndpointPort{}\n        for _, port := range m.Service.Spec.Ports {\n            ports = append(ports, corev1.EndpointPort{\n                Name:     port.Name,\n                Port:     port.Port,\n                Protocol: port.Protocol,\n            })\n        }\n\n        // Set subsets\n        endpoints.Subsets = []corev1.EndpointSubset{\n            {\n                Addresses: addresses,\n                Ports:     ports,\n            },\n        }\n\n        return nil\n    })\n\n    return err\n}\n</code></pre>"},{"location":"prd/endpoint_for_lb/#integration-in-service_controllergo","title":"Integration in <code>service_controller.go</code>","text":"<pre><code>// In reconcileSync function, after EIP and LoadBalancer sync:\n\nfunc (r *ServiceReconciler) reconcileSync(ctx context.Context, req ctrl.Request, svc *corev1.Service, endpoints *corev1.Endpoints, project *kubedccomv1.Project) (ctrl.Result, error) {\n    log := log.FromContext(ctx).WithName(\"Sync:\").WithValues(\"ServiceLoadBalancer\", req.Name)\n\n    // ... existing EIP sync ...\n\n    // ... existing LoadBalancer sync ...\n\n    // ... existing external IP status update ...\n\n    // NEW: Sync external endpoints for cross-VPC access\n    extEndpointMgr := &amp;serviceLb.ExternalEndpointManager{\n        Client:  r.Client,\n        Service: svc,\n    }\n    if err := extEndpointMgr.Sync(ctx); err != nil {\n        log.Error(err, \"Failed to sync external endpoints\")\n        // Don't fail the reconciliation, just log the error\n    }\n\n    return ctrl.Result{}, nil\n}\n\n// In reconcileDelete function:\n\nfunc (r *ServiceReconciler) reconcileDelete(ctx context.Context, req ctrl.Request, svc *corev1.Service, endpoints *corev1.Endpoints, project *kubedccomv1.Project) (ctrl.Result, error) {\n    log := log.FromContext(ctx).WithName(\"Delete:\").WithValues(\"ServiceLoadBalancer\", req.Name)\n\n    // NEW: Delete external endpoints first\n    extEndpointMgr := &amp;serviceLb.ExternalEndpointManager{\n        Client:  r.Client,\n        Service: svc,\n    }\n    if err := extEndpointMgr.Delete(ctx); err != nil {\n        log.Error(err, \"Failed to delete external endpoints\")\n    }\n\n    // ... existing EIP and LoadBalancer delete logic ...\n\n    return ctrl.Result{}, nil\n}\n</code></pre>"},{"location":"prd/endpoint_for_lb/#4-rbac-permissions","title":"4. RBAC Permissions","text":"<p>Add to <code>config/rbac/role.yaml</code>:</p> <pre><code>- apiGroups: [\"\"]\n  resources: [\"services\", \"endpoints\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"prd/endpoint_for_lb/#benefits","title":"Benefits","text":""},{"location":"prd/endpoint_for_lb/#1-operational-simplicity","title":"1. Operational Simplicity","text":"<ul> <li>Zero manual intervention: Endpoints created/updated automatically</li> <li>Self-healing: Endpoints always reflect current LoadBalancer IPs</li> <li>Consistent naming: Predictable <code>-ext</code> suffix convention</li> </ul>"},{"location":"prd/endpoint_for_lb/#2-multi-tenant-support","title":"2. Multi-Tenant Support","text":"<ul> <li>Cross-VPC access: External endpoints work across network boundaries</li> <li>Stable DNS: Use <code>&lt;service&gt;-ext.namespace.svc.cluster.local</code> in all configs</li> <li>No IP hardcoding: Certificates, kubeconfigs, and DataStores use DNS names</li> </ul>"},{"location":"prd/endpoint_for_lb/#3-resilience","title":"3. Resilience","text":"<ul> <li>Owner references: Automatic cleanup when services are deleted</li> <li>Reconciliation: Controller ensures consistency even after disruptions</li> <li>Multiple IPs: Supports LoadBalancers with multiple external IPs</li> </ul>"},{"location":"prd/endpoint_for_lb/#use-cases","title":"Use Cases","text":""},{"location":"prd/endpoint_for_lb/#1-kamaji-multi-tenant-control-planes-primary-use-case","title":"1. Kamaji Multi-Tenant Control Planes \u2b50 PRIMARY USE CASE","text":"<p>Problem: Kamaji controller in <code>kamaji-system</code> cannot reach etcd in tenant VPC via ClusterIP.</p> <p>Current (manual): <pre><code>apiVersion: kamaji.clastix.io/v1alpha1\nkind: DataStore\nmetadata:\n  name: shalb-envoy-etcd\nspec:\n  driver: etcd\n  endpoints:\n  - 168.119.17.55:2379  # \u274c Hardcoded IP - breaks when service recreated\n</code></pre></p> <p>With auto-managed endpoints: <pre><code>apiVersion: kamaji.clastix.io/v1alpha1\nkind: DataStore\nmetadata:\n  name: shalb-envoy-etcd\nspec:\n  driver: etcd\n  endpoints:\n  - etcd-lb-ext.shalb-envoy.svc.cluster.local:2379  # \u2705 Stable DNS name\n  tlsConfig:\n    # ... certificates with DNS SANs (not IP SANs)\n</code></pre></p> <p>How it works: 1. LoadBalancer <code>etcd-lb</code> gets external IP <code>168.119.17.55</code> 2. Controller auto-creates Service <code>etcd-lb-ext</code> (headless) 3. Controller auto-creates Endpoints <code>etcd-lb-ext</code> pointing to <code>168.119.17.55</code> 4. Kamaji resolves <code>etcd-lb-ext.shalb-envoy.svc.cluster.local</code> \u2192 <code>168.119.17.55</code> 5. When IP changes, controller updates Endpoints automatically 6. No DataStore configuration change needed!</p>"},{"location":"prd/endpoint_for_lb/#2-cross-tenant-api-access","title":"2. Cross-Tenant API Access","text":"<pre><code># Tenant A accessing Tenant B's API\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tenant-b-access\n  namespace: tenant-a\ndata:\n  api-endpoint: https://api-gateway-ext.tenant-b.svc.cluster.local:8443\n</code></pre>"},{"location":"prd/endpoint_for_lb/#3-cicd-integration","title":"3. CI/CD Integration","text":"<pre><code># CI pipeline can use stable DNS names\nkubectl --kubeconfig=/tmp/kubeconfig \\\n  --server=https://cluster-ext.tenant-prod.svc.cluster.local:6443 \\\n  get nodes\n</code></pre>"},{"location":"prd/endpoint_for_lb/#testing-strategy","title":"Testing Strategy","text":""},{"location":"prd/endpoint_for_lb/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test Service creation with correct naming and labels</li> <li>Test Endpoints creation with correct IPs and ports</li> <li>Test update when LoadBalancer IP changes</li> <li>Test deletion and cleanup</li> <li>Test multiple external IPs</li> </ul>"},{"location":"prd/endpoint_for_lb/#integration-tests","title":"Integration Tests","text":"<ol> <li>Create LoadBalancer service</li> <li>Verify <code>-ext</code> Service and Endpoints are created</li> <li>Verify DNS resolves to external IP</li> <li>Update LoadBalancer (trigger IP change)</li> <li>Verify Endpoints updated with new IP</li> <li>Delete LoadBalancer</li> <li>Verify <code>-ext</code> resources cleaned up</li> </ol>"},{"location":"prd/endpoint_for_lb/#e2e-tests","title":"E2E Tests","text":"<ul> <li>Deploy Kamaji with multi-tenant setup</li> <li>Verify etcd DataStore works with <code>-ext</code> endpoint</li> <li>Verify TenantControlPlane can access etcd</li> <li>Simulate IP change and verify automatic reconciliation</li> </ul>"},{"location":"prd/endpoint_for_lb/#implementation-status","title":"Implementation Status","text":""},{"location":"prd/endpoint_for_lb/#phase-1-core-implementation-complete","title":"Phase 1: Core Implementation \u2705 Complete","text":"<ul> <li> Create <code>external_endpoint.go</code> with manager logic</li> <li> Integrate into <code>service_controller.go</code></li> <li> Add RBAC permissions for endpoints</li> <li> Documentation in <code>docs/tutorial-ip-and-lb.md</code></li> <li> Documentation in <code>docs/architecture-networking.md</code></li> </ul>"},{"location":"prd/endpoint_for_lb/#phase-2-deployment-testing-complete","title":"Phase 2: Deployment &amp; Testing \u2705 Complete","text":"<ul> <li> Deploy to staging environment (v0.1.34-dev1)</li> <li> Verified 10+ LoadBalancer services automatically got external endpoints</li> <li> DNS resolution tested and working</li> </ul>"},{"location":"prd/endpoint_for_lb/#test-results-2025-11-19","title":"Test Results (2025-11-19)","text":"<pre><code># All LoadBalancer services automatically got -ext endpoints:\n$ kubectl get endpoints -A --selector=kube-dc.com/managed-by=service-lb-controller\nNAMESPACE     NAME                                     ENDPOINTS\nshalb-dev     etcd-lb-ext                              168.119.17.51:2379\nshalb-dev     kamaji-demo-cp-ext                       168.119.17.59:6443\nshalb-dev     debug-net-lb-ext                         168.119.17.51:80,168.119.17.51:443\nshalb-envoy   cluster-a-cp-ext                         168.119.17.53:6443\nshalb-envoy   etcd-lb-ext                              168.119.17.55:2379,168.119.17.55:6443\n...\n\n# DNS resolution verified:\n$ kubectl run -it --rm debug --image=busybox -- nslookup etcd-lb-ext.shalb-dev.svc.cluster.local\nName:   etcd-lb-ext.shalb-dev.svc.cluster.local\nAddress: 168.119.17.51\n</code></pre>"},{"location":"prd/endpoint_for_lb/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>This feature is fully backwards compatible: - Existing LoadBalancer services continue to work unchanged - External endpoints are additive (new resources only) - No breaking changes to existing APIs or configurations - Users can opt-out by deleting the <code>-ext</code> resources (controller will recreate, but won't affect original service)</p>"},{"location":"prd/endpoint_for_lb/#success-metrics","title":"Success Metrics","text":"<ul> <li>Automation rate: 100% of LoadBalancer services have external endpoints</li> <li>Manual interventions: Reduce IP update operations to zero</li> <li>Reconciliation time: External endpoints updated within 10 seconds of IP change</li> <li>Error rate: &lt; 0.1% endpoint sync failures</li> </ul>"},{"location":"prd/endpoint_for_lb/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Configurable naming: Annotation to customize <code>-ext</code> suffix</li> <li>Selective enablement: Annotation to opt-in/opt-out per service</li> <li>External DNS integration: Automatically create DNS records</li> <li>Metrics: Prometheus metrics for endpoint sync operations</li> <li>Webhook validation: Prevent manual modification of managed resources</li> <li>Kamaji DataStore CRD enhancement: Add <code>externalNetworkType</code> field to Kamaji DataStore CRD to allow users to specify which external network type (<code>cloud</code> or <code>public</code>) to use when connecting via external endpoints. This ensures proper network routing and IP allocation matching the infrastructure requirements</li> </ol>"},{"location":"prd/endpoint_for_lb/#related-files","title":"Related Files","text":"<ul> <li><code>internal/service_lb/external_endpoint.go</code> - External endpoint manager implementation</li> <li><code>internal/controller/core/service_controller.go</code> - Service controller integration</li> <li><code>docs/tutorial-ip-and-lb.md</code> - User documentation</li> <li><code>docs/architecture-networking.md</code> - Architecture documentation</li> </ul>"},{"location":"prd/endpoint_for_lb/#related-documentation","title":"Related Documentation","text":"<ul> <li>Kamaji Multi-Tenant Architecture: <code>/examples/kamaji-capi/mt/README.md</code></li> <li>Service LoadBalancer Architecture: <code>/docs/prd/svc_lb_architecture.md</code></li> <li>EIP Management: <code>/internal/eip/</code></li> </ul>"},{"location":"prd/etcd-dns-resolution-timeout/","title":"Etcd DNS Resolution Timeout Issue in Kamaji TenantControlPlanes","text":""},{"location":"prd/etcd-dns-resolution-timeout/#problem","title":"Problem","text":"<p>New clusters created via cs-marketplace-partner-kubedc experience control plane CrashLoopBackOff with error:</p> <pre><code>grpc: addrConn.createTransport failed to connect ... dial tcp: lookup {etcd-endpoint}.svc.cluster.local: operation was canceled\n</code></pre>"},{"location":"prd/etcd-dns-resolution-timeout/#root-cause","title":"Root Cause","text":"<ul> <li>kube-apiserver's gRPC client creates 100+ parallel connections to etcd during startup</li> <li>Each connection triggers a DNS lookup for the etcd-lb-ext service</li> <li>DNS lookups timeout before the gRPC dial timeout (typically 20s default)</li> <li>Results in \"operation was canceled\" errors</li> </ul>"},{"location":"prd/etcd-dns-resolution-timeout/#diagnosis-steps","title":"Diagnosis Steps","text":"<ol> <li> <p>Check apiserver logs:    <pre><code>kubectl logs -n {namespace} -l kamaji.clastix.io/name={tcp-name} -c kube-apiserver | grep -i \"lookup\\|canceled\"\n</code></pre></p> </li> <li> <p>Verify DNS works from test pod:    <pre><code>kubectl run test --image=busybox -n {namespace} --rm -it -- nslookup {etcd-endpoint}\n</code></pre></p> </li> <li> <p>Verify TCP connectivity:    <pre><code>kubectl run test --image=busybox -n {namespace} --rm -it -- nc -zv {etcd-ip} {port}\n</code></pre></p> </li> </ol>"},{"location":"prd/etcd-dns-resolution-timeout/#resolution","title":"Resolution","text":"<p>Usually self-resolves after several pod restarts as DNS cache warms up. To expedite:</p> <pre><code>kubectl delete pod -n {namespace} -l kamaji.clastix.io/name={tcp-name}\n</code></pre>"},{"location":"prd/etcd-dns-resolution-timeout/#important-notes","title":"Important Notes","text":"<ul> <li>Do NOT use IP address directly in DataStore endpoint - TLS certificates are issued for DNS names</li> <li>The etcd-lb-ext service is a headless service pointing to LoadBalancer IP</li> <li>Port 32380+ are used for dedicated datastores (auto-allocated)</li> </ul>"},{"location":"prd/etcd-dns-resolution-timeout/#potential-improvements","title":"Potential Improvements","text":"<ol> <li>Add startup probe with longer failureThreshold in TenantControlPlane</li> <li>Consider pre-warming DNS cache before control plane creation</li> <li>Investigate Kamaji options for etcd client dial timeout configuration</li> </ol>"},{"location":"prd/etcd-dns-resolution-timeout/#related-components","title":"Related Components","text":"<ul> <li><code>kube-dc-k8-manager</code>: Creates KdcClusterDatastore and Kamaji DataStore resources</li> <li><code>cs-marketplace-partner-kubedc</code>: Backend that triggers cluster creation with dedicated datastores</li> <li>Kamaji: Manages TenantControlPlane pods that connect to etcd</li> </ul>"},{"location":"prd/gateway_sharding/","title":"PRD: Gateway Sharding for Scale","text":""},{"location":"prd/gateway_sharding/#overview","title":"Overview","text":"<p>This document outlines enhancement options for scaling the Gateway infrastructure to support thousands of tenants and services in Kube-DC.</p>"},{"location":"prd/gateway_sharding/#problem-statement","title":"Problem Statement","text":"<p>The current implementation creates a per-service HTTPS listener on a single shared Gateway. This approach has scaling limitations:</p> Constraint Limit Impact Envoy listeners per Gateway ~100-1000 Hard limit on services with <code>expose-route: https</code> Gateway memory Proportional to listeners Memory pressure with many listeners Configuration reload time Increases with listeners Slower route updates Single point of failure 1 Gateway All tenants affected by Gateway issues"},{"location":"prd/gateway_sharding/#current-architecture","title":"Current Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Single Gateway \"eg\"                               \u2502\n\u2502                 (envoy-gateway-system)                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Listeners:                                                          \u2502\n\u2502  - http :80 (shared)                                                \u2502\n\u2502  - tls-passthrough :443 (wildcard)                                  \u2502\n\u2502  - https-svc1-ns1 :443 (per-service)  \u2190\u2500\u2510                          \u2502\n\u2502  - https-svc2-ns1 :443 (per-service)    \u2502 N listeners              \u2502\n\u2502  - https-svc3-ns2 :443 (per-service)    \u2502 (scales linearly)        \u2502\n\u2502  - ...                                \u2190\u2500\u2518                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc               \u25bc               \u25bc\n         Org A (N svcs)  Org B (N svcs)  Org C (N svcs)\n</code></pre>"},{"location":"prd/gateway_sharding/#proposed-solutions","title":"Proposed Solutions","text":""},{"location":"prd/gateway_sharding/#option-1-wildcard-listeners-per-organization","title":"Option 1: Wildcard Listeners per Organization","text":"<p>Concept: Replace per-service listeners with wildcard listeners per organization.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Single Gateway \"eg\"                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Listeners:                                                          \u2502\n\u2502  - http :80                                                         \u2502\n\u2502  - tls-passthrough :443                                             \u2502\n\u2502  - https-org-a :443 (*.org-a.kube-dc.com)  \u2190\u2500\u2510                     \u2502\n\u2502  - https-org-b :443 (*.org-b.kube-dc.com)    \u2502 M listeners         \u2502\n\u2502  - https-org-c :443 (*.org-c.kube-dc.com)  \u2190\u2500\u2518 (M = # of orgs)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Benefits: - Reduces listeners from N (services) to M (organizations) - Single wildcard certificate per organization - SNI routing handles service selection within listener</p> <p>Requirements: - Wildcard certificate issuance per organization - DNS wildcard records per organization - Organization controller creates listener when org is created</p> <p>Implementation Complexity: Medium</p>"},{"location":"prd/gateway_sharding/#option-2-gateway-per-organization","title":"Option 2: Gateway per Organization","text":"<p>Concept: Each organization gets its own dedicated Gateway instance.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Gateway \"gw-org-a\" \u2502  \u2502  Gateway \"gw-org-b\" \u2502  \u2502  Gateway \"gw-org-c\" \u2502\n\u2502  IP: 88.99.29.250   \u2502  \u2502  IP: 88.99.29.251   \u2502  \u2502  IP: 88.99.29.252   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  *.org-a.kube-dc.com\u2502  \u2502  *.org-b.kube-dc.com\u2502  \u2502  *.org-c.kube-dc.com\u2502\n\u2502  - All org A routes \u2502  \u2502  - All org B routes \u2502  \u2502  - All org C routes \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Benefits: - Complete isolation between organizations - No listener limits per org (own Gateway) - Independent scaling and upgrades - Failure isolation</p> <p>Requirements: - EIP allocation per organization Gateway - GatewayClass configuration for multi-Gateway - Organization controller provisions Gateway</p> <p>Implementation Complexity: High</p>"},{"location":"prd/gateway_sharding/#option-3-hash-based-gateway-sharding","title":"Option 3: Hash-Based Gateway Sharding","text":"<p>Concept: Distribute services across N Gateway instances using consistent hashing.</p> <pre><code>                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502   Hash Function  \u2502\n                         \u2502  hash(hostname)  \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                   \u25bc                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Gateway \"gw-0\"    \u2502 \u2502   Gateway \"gw-1\"    \u2502 \u2502   Gateway \"gw-2\"    \u2502\n\u2502   hash % 3 == 0     \u2502 \u2502   hash % 3 == 1     \u2502 \u2502   hash % 3 == 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Benefits: - Even distribution across Gateways - Scales horizontally by adding Gateways - No org-specific infrastructure</p> <p>Requirements: - DNS-based load balancing or L4 load balancer - Consistent hashing for stable assignments - Gateway selector in route controller</p> <p>Implementation Complexity: High</p>"},{"location":"prd/gateway_sharding/#option-4-hybrid-approach-recommended","title":"Option 4: Hybrid Approach (Recommended)","text":"<p>Concept: Combine wildcard listeners with optional Gateway sharding.</p> <p>Phase 1 (Quick Win): - Convert per-service listeners to per-organization wildcard listeners - Single Gateway, wildcard cert per org - Reduces N services \u2192 M orgs listeners</p> <p>Phase 2 (Scale Out): - Add Gateway per org for large organizations - Small orgs share \"default\" Gateway - Org annotation controls Gateway assignment</p> <pre><code># Organization with dedicated Gateway\napiVersion: kube-dc.com/v1\nkind: Organization\nmetadata:\n  name: large-enterprise\nspec:\n  gateway:\n    dedicated: true  # Gets its own Gateway\n\n# Small org uses shared Gateway\napiVersion: kube-dc.com/v1\nkind: Organization\nmetadata:\n  name: small-startup\nspec:\n  gateway:\n    dedicated: false  # Uses shared Gateway (default)\n</code></pre>"},{"location":"prd/gateway_sharding/#comparison-matrix","title":"Comparison Matrix","text":"Aspect Current Option 1 Option 2 Option 3 Option 4 Max Services ~1000 ~100K Unlimited Unlimited Unlimited Isolation None Partial Full None Configurable IP Usage 1 1 N (per org) K (shards) 1 + N (large orgs) Complexity Low Medium High High Medium Cert Management Per-service Per-org wildcard Per-org wildcard Per-service Per-org wildcard Failure Domain All tenants All tenants Per org Per shard Configurable"},{"location":"prd/gateway_sharding/#recommended-approach","title":"Recommended Approach","text":""},{"location":"prd/gateway_sharding/#short-term-option-1","title":"Short Term (Option 1)","text":"<ol> <li>Implement wildcard listeners per organization</li> <li>Use wildcard certificates (<code>*.{org}.{base_domain}</code>)</li> <li>SNI routing within listener for service selection</li> <li>Keep single Gateway</li> </ol>"},{"location":"prd/gateway_sharding/#medium-term-option-4-phase-2","title":"Medium Term (Option 4 - Phase 2)","text":"<ol> <li>Add <code>spec.gateway.dedicated</code> to Organization CRD</li> <li>Organization controller provisions Gateway for dedicated orgs</li> <li>Small orgs continue using shared Gateway</li> <li>Automatic EIP allocation for dedicated Gateways</li> </ol>"},{"location":"prd/gateway_sharding/#implementation-details","title":"Implementation Details","text":""},{"location":"prd/gateway_sharding/#wildcard-listener-creation-option-1","title":"Wildcard Listener Creation (Option 1)","text":"<pre><code>// In organization controller\nfunc (r *OrganizationReconciler) syncGatewayListener(ctx context.Context, org *Organization) error {\n    listenerName := fmt.Sprintf(\"https-%s\", org.Name)\n    hostname := fmt.Sprintf(\"*.%s.%s\", org.Name, baseDomain)\n\n    // Create wildcard certificate\n    cert := &amp;certmanagerv1.Certificate{\n        Spec: certmanagerv1.CertificateSpec{\n            SecretName: fmt.Sprintf(\"%s-wildcard-tls\", org.Name),\n            DNSNames:   []string{hostname},\n            IssuerRef:  cmmeta.ObjectReference{Name: \"letsencrypt\", Kind: \"ClusterIssuer\"},\n        },\n    }\n\n    // Patch Gateway with new listener\n    return patchGatewayAddListener(ctx, listenerName, hostname, cert.Spec.SecretName)\n}\n</code></pre>"},{"location":"prd/gateway_sharding/#service-route-changes","title":"Service Route Changes","text":"<pre><code>// In service_lb/gateway_route.go\nfunc (m *GatewayRouteManager) syncHTTPSRoute(ctx context.Context, hostname string, port int32) error {\n    // With wildcard listeners, we DON'T create per-service listeners\n    // Just create HTTPRoute pointing to org's wildcard listener\n\n    org := m.getOrganization()\n    listenerName := fmt.Sprintf(\"https-%s\", org)\n\n    // Create HTTPRoute (no listener creation needed)\n    return m.syncHTTPSHTTPRoute(ctx, hostname, port, listenerName)\n}\n</code></pre>"},{"location":"prd/gateway_sharding/#dns-requirements","title":"DNS Requirements","text":""},{"location":"prd/gateway_sharding/#current-per-service","title":"Current (Per-Service)","text":"<pre><code>svc1-ns1.stage.kube-dc.com  \u2192 Gateway IP\nsvc2-ns1.stage.kube-dc.com  \u2192 Gateway IP\nsvc3-ns2.stage.kube-dc.com  \u2192 Gateway IP\n</code></pre>"},{"location":"prd/gateway_sharding/#with-wildcard-per-organization","title":"With Wildcard (Per-Organization)","text":"<pre><code>*.org-a.stage.kube-dc.com   \u2192 Gateway IP (wildcard record)\n*.org-b.stage.kube-dc.com   \u2192 Gateway IP (wildcard record)\n</code></pre>"},{"location":"prd/gateway_sharding/#certificate-requirements","title":"Certificate Requirements","text":""},{"location":"prd/gateway_sharding/#current","title":"Current","text":"<ul> <li>Per-service certificate via HTTP-01 challenge</li> <li>Certificate per hostname</li> </ul>"},{"location":"prd/gateway_sharding/#with-wildcard","title":"With Wildcard","text":"<ul> <li>Per-organization wildcard certificate</li> <li>Requires DNS-01 challenge (not HTTP-01)</li> <li>Issuer must support DNS-01 (e.g., with Cloudflare, Route53, etc.)</li> </ul> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-dns\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@kube-dc.com\n    privateKeySecretRef:\n      name: letsencrypt-dns-key\n    solvers:\n    - dns01:\n        cloudflare:\n          email: admin@kube-dc.com\n          apiKeySecretRef:\n            name: cloudflare-api-key\n            key: api-key\n</code></pre>"},{"location":"prd/gateway_sharding/#migration-path","title":"Migration Path","text":""},{"location":"prd/gateway_sharding/#phase-1-parallel-operation","title":"Phase 1: Parallel Operation","text":"<ol> <li>Keep existing per-service listeners working</li> <li>Add wildcard listener for new organizations</li> <li>New services use wildcard listener</li> </ol>"},{"location":"prd/gateway_sharding/#phase-2-migration","title":"Phase 2: Migration","text":"<ol> <li>Migrate existing services to wildcard listeners</li> <li>Update HTTPRoutes to point to org listener</li> <li>Remove old per-service listeners</li> </ol>"},{"location":"prd/gateway_sharding/#phase-3-cleanup","title":"Phase 3: Cleanup","text":"<ol> <li>Remove per-service listener code path</li> <li>Update documentation</li> <li>Deprecate old annotation behavior</li> </ol>"},{"location":"prd/gateway_sharding/#metrics-monitoring","title":"Metrics &amp; Monitoring","text":""},{"location":"prd/gateway_sharding/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ul> <li>Listeners per Gateway</li> <li>Gateway memory usage</li> <li>Route update latency</li> <li>Certificate expiration by org</li> </ul>"},{"location":"prd/gateway_sharding/#alerts","title":"Alerts","text":"<ul> <li>Gateway listener count &gt; 80% limit</li> <li>Certificate renewal failures</li> <li>Gateway pod restarts</li> </ul>"},{"location":"prd/gateway_sharding/#timeline-estimate","title":"Timeline Estimate","text":"Phase Effort Duration Option 1 (Wildcard Listeners) Medium 2-3 weeks Option 4 Phase 2 (Dedicated Gateways) High 4-6 weeks Migration of existing services Medium 2-3 weeks"},{"location":"prd/gateway_sharding/#decision-required","title":"Decision Required","text":"<ul> <li> Approve Option 1 (Wildcard Listeners) for immediate scale improvement</li> <li> Approve Option 4 (Hybrid) for long-term architecture</li> <li> DNS-01 solver configuration (requires DNS provider API access)</li> <li> Organization subdomain structure (<code>{service}.{org}.{domain}</code> vs <code>{service}-{org}.{domain}</code>)</li> </ul>"},{"location":"prd/gateway_sharding/#references","title":"References","text":"<ul> <li>Envoy Gateway Scaling Guide</li> <li>Gateway API Multi-Gateway</li> <li>cert-manager DNS-01 Solvers</li> </ul>"},{"location":"prd/hierarchical-quota-integration/","title":"PRD: Hierarchical Resource Quota Integration for Kube-DC","text":""},{"location":"prd/hierarchical-quota-integration/#status-frozen","title":"Status: Frozen","text":""},{"location":"prd/hierarchical-quota-integration/#branch-pricing","title":"Branch: <code>pricing</code>","text":""},{"location":"prd/hierarchical-quota-integration/#priority-high","title":"Priority: High","text":""},{"location":"prd/hierarchical-quota-integration/#last-updated-2026-02-07","title":"Last Updated: 2026-02-07","text":""},{"location":"prd/hierarchical-quota-integration/#1-overview","title":"1. Overview","text":""},{"location":"prd/hierarchical-quota-integration/#11-problem-statement","title":"1.1 Problem Statement","text":"<p>Kube-DC currently has a billing system with Stripe integration that defines subscription plans (Dev Pool, Pro Pool, Scale Pool) with resource limits (CPU, memory, storage, IPv4), but no actual Kubernetes-level enforcement of these limits. The <code>usage</code> field in the subscription data is hardcoded to <code>{ used: 0, limit: N }</code> (mock data). Organizations can exceed their plan limits without any restriction.</p> <p>Additionally, the existing <code>OrganizationProjectsLimit</code> (default: 3) is a simple integer count \u2014 it limits the number of projects but not the aggregate resource consumption across projects.</p>"},{"location":"prd/hierarchical-quota-integration/#12-goal","title":"1.2 Goal","text":"<p>Integrate Hierarchical Namespace Controller (HNC) with <code>HierarchicalResourceQuota</code> (HRQ) to enforce organization-level resource quotas that aggregate across all project namespaces. When a user selects a billing plan, a corresponding HRQ is created/updated on the organization namespace, and Kubernetes natively enforces the limits across all child project namespaces.</p>"},{"location":"prd/hierarchical-quota-integration/#13-success-criteria","title":"1.3 Success Criteria","text":"<ul> <li>Plan selection via UI creates a real <code>HierarchicalResourceQuota</code> on the organization namespace</li> <li>Resource consumption across all projects in an organization is capped by the HRQ</li> <li>Users see real-time quota usage in the Billing UI (not mock data)</li> <li>Plan upgrades/downgrades dynamically update the HRQ</li> <li>Turbo add-ons correctly increment HRQ limits</li> <li>Existing organizations are migrated seamlessly</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#2-current-architecture-analysis","title":"2. Current Architecture Analysis","text":""},{"location":"prd/hierarchical-quota-integration/#21-namespace-hierarchy-already-exists","title":"2.1 Namespace Hierarchy (Already Exists)","text":"<pre><code>Organization Namespace: shalb              \u2190 Organization CR lives here\n  \u251c\u2500\u2500 Project Namespace: shalb-demo        \u2190 Created by Project controller\n  \u251c\u2500\u2500 Project Namespace: shalb-dev         \u2190 Created by Project controller\n  \u2514\u2500\u2500 Project Namespace: shalb-envoy       \u2190 Created by Project controller\n</code></pre> <p>Key files: - <code>api/kube-dc.com/v1/organization_types.go</code> \u2014 Organization CRD (namespace = org name) - <code>api/kube-dc.com/v1/project_types.go</code> \u2014 Project CRD (namespace = org name, creates child ns <code>{org}-{project}</code>) - <code>internal/project/helpers.go:94</code> \u2014 <code>ProjectNamespaceName()</code> returns <code>p.Namespace + \"-\" + p.Name</code> - <code>internal/project/res_namespace.go</code> \u2014 Creates project namespace with annotation <code>kube-dc.com/project: {org}/{project}</code></p>"},{"location":"prd/hierarchical-quota-integration/#22-existing-billing-system","title":"2.2 Existing Billing System","text":"<p>Backend (Node.js): - <code>ui/backend/controllers/billing/subscriptionController.js</code> \u2014 Stripe integration, plan definitions, organization annotation management - <code>ui/backend/controllers/billing/billingController.js</code> \u2014 Billing API proxy - <code>ui/backend/routes/billing.js</code> \u2014 Route definitions</p> <p>Frontend (React/PatternFly): - <code>ui/frontend/src/app/ManageOrganization/Billing/Billing.tsx</code> \u2014 Main billing UI (1133 lines) - <code>ui/frontend/src/app/ManageOrganization/Billing/SubscribePlanModal.tsx</code> \u2014 Plan selection modal - <code>ui/frontend/src/app/ManageOrganization/Billing/api.ts</code> \u2014 Billing API client - <code>ui/frontend/src/app/ManageOrganization/Billing/types.ts</code> \u2014 TypeScript types and plan definitions</p> <p>Current Plans:</p> Plan CPU Memory Storage Object Storage IPv4 Price Dev Pool 4 vCPU 8 GB 60 GB 20 GB 1 \u20ac19/mo Pro Pool 8 vCPU 24 GB 160 GB 100 GB 1 \u20ac49/mo Scale Pool 16 vCPU 56 GB 320 GB 500 GB 3 \u20ac99/mo <p>Turbo Add-ons:</p> Add-on CPU Memory Price Turbo x1 +2 vCPU +4 GB \u20ac9/mo Turbo x2 +4 vCPU +8 GB \u20ac16/mo <p>Subscription data is stored as Organization annotations: <pre><code>billing.kube-dc.com/subscription: active\nbilling.kube-dc.com/plan-id: pro-pool\nbilling.kube-dc.com/plan-name: Pro Pool\nbilling.kube-dc.com/stripe-subscription-id: sub_xxx\nbilling.kube-dc.com/addons: [{\"addonId\":\"turbo-x1\",\"quantity\":1}]\n</code></pre></p>"},{"location":"prd/hierarchical-quota-integration/#23-existing-limits","title":"2.3 Existing Limits","text":"<ul> <li><code>MasterConfigSpec.OrganizationProjectsLimit</code> (default: 3) \u2014 Checked in <code>CheckOrganizationLimits()</code> in <code>internal/controller/kube-dc.com/organization_controller.go:229</code></li> <li>Project controller requeues with 30s delay when limit exceeded (<code>project_controller.go:148</code>)</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#24-gap-analysis","title":"2.4 Gap Analysis","text":"Capability Current State Target State Plan resource limits Defined in code, not enforced Enforced via HRQ Usage tracking Mock data (all zeros) Real K8s ResourceQuota aggregation Cross-project aggregation None HRQ aggregates across child namespaces Enforcement None (advisory only) Hard limits at pod scheduling time Plan change propagation Annotations only HRQ spec update Add-on resource addition Annotations only HRQ spec update"},{"location":"prd/hierarchical-quota-integration/#3-hnc-integration-design","title":"3. HNC Integration Design","text":""},{"location":"prd/hierarchical-quota-integration/#31-hnc-component-pfnethierarchical-namespaces","title":"3.1 HNC Component: pfnet/hierarchical-namespaces","text":"<p>Repository: https://github.com/pfnet/hierarchical-namespaces Latest Release: <code>v1.1.0-pfnet.10</code> Image: <code>ghcr.io/pfnet/hierarchical-namespaces/controller:v1.1.0-pfnet.10</code> HRQ CRD API: <code>hnc.x-k8s.io/v1alpha2</code></p> <p>HNC provides: 1. Namespace hierarchy \u2014 Parent-child relationships between namespaces 2. Policy propagation \u2014 RBAC, NetworkPolicies, Secrets propagated to children 3. HierarchicalResourceQuota (HRQ) \u2014 Aggregate quota across a namespace subtree</p>"},{"location":"prd/hierarchical-quota-integration/#32-namespace-hierarchy-mapping","title":"3.2 Namespace Hierarchy Mapping","text":"<p>HNC requires setting parent-child relationships on namespaces. The Kube-DC hierarchy maps naturally:</p> <pre><code>shalb (Organization namespace)          \u2190 HRQ \"plan-quota\" applied here\n\u251c\u2500\u2500 shalb-demo (Project namespace)      \u2190 child of shalb\n\u251c\u2500\u2500 shalb-dev (Project namespace)       \u2190 child of shalb\n\u2514\u2500\u2500 shalb-envoy (Project namespace)     \u2190 child of shalb\n</code></pre> <p>HNC hierarchy is set via annotations/labels on namespaces: <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: shalb-demo\n  labels:\n    shalb.tree.hnc.x-k8s.io/depth: \"1\"    # Set by HNC controller\n  annotations:\n    hnc.x-k8s.io/subnamespace-of: shalb    # OR use HierarchyConfiguration\n</code></pre></p> <p>OR via HierarchyConfiguration: <pre><code>apiVersion: hnc.x-k8s.io/v1alpha2\nkind: HierarchyConfiguration\nmetadata:\n  name: hierarchy\n  namespace: shalb-demo\nspec:\n  parent: shalb\n</code></pre></p>"},{"location":"prd/hierarchical-quota-integration/#33-hierarchicalresourcequota-hrq","title":"3.3 HierarchicalResourceQuota (HRQ)","text":"<p>Applied on the organization namespace, limits are enforced across ALL child project namespaces:</p> <pre><code>apiVersion: hnc.x-k8s.io/v1alpha2\nkind: HierarchicalResourceQuota\nmetadata:\n  name: plan-quota\n  namespace: shalb                    # Organization namespace\n  labels:\n    billing.kube-dc.com/auto-managed: \"true\"\n    billing.kube-dc.com/plan-id: \"pro-pool\"\nspec:\n  hard:\n    requests.cpu: \"8\"                 # From plan: 8 vCPU\n    requests.memory: \"24Gi\"           # From plan: 24 GB\n    limits.cpu: \"16\"                  # 2x burst for pro-pool (see \u00a73.6)\n    limits.memory: \"48Gi\"             # 2x burst for pro-pool\n    requests.storage: \"160Gi\"         # From plan: 160 GB storage\n    pods: \"200\"                       # Per-plan: 100/200/500\n    services.loadbalancers: \"5\"       # Per-plan: 3/5/10\n</code></pre> <p>HRQ Status (populated by HNC controller): <pre><code>status:\n  hard:\n    requests.cpu: \"8\"\n    requests.memory: \"24Gi\"\n    ...\n  used:\n    requests.cpu: \"2500m\"             # Aggregated across all child namespaces\n    requests.memory: \"6Gi\"\n    ...\n</code></pre></p>"},{"location":"prd/hierarchical-quota-integration/#34-limitrange-required-companion-to-hrq","title":"3.4 LimitRange \u2014 Required Companion to HRQ","text":""},{"location":"prd/hierarchical-quota-integration/#341-why-limitrange-is-mandatory","title":"3.4.1 Why LimitRange is Mandatory","text":"<p>Critical Kubernetes behavior: When a <code>ResourceQuota</code> (or HRQ, which creates internal <code>ResourceQuota</code> objects named <code>hrq.hnc.x-k8s.io</code> in each child namespace) constrains <code>cpu</code> or <code>memory</code>, Kubernetes rejects any pod that does not explicitly set <code>requests</code> or <code>limits</code> for those resources:</p> <p>For <code>cpu</code> and <code>memory</code> resources, ResourceQuotas enforce that every (new) pod in that namespace sets a limit for that resource. If you don't, the control plane may reject admission for that Pod. \u2014 Kubernetes Documentation</p> <p>This means: - Once HRQ is applied to an organization namespace, all project namespaces get internal ResourceQuotas - Any pod (container, KubeVirt VM, cloud-shell job, VPC DNS pod) created without explicit <code>resources.requests.cpu</code> / <code>resources.requests.memory</code> will be rejected by the API server - LimitRange solves this by providing default <code>requests</code> and <code>limits</code> for containers that don't specify them</p>"},{"location":"prd/hierarchical-quota-integration/#342-current-state-no-limitrange-exists","title":"3.4.2 Current State \u2014 No LimitRange Exists","text":"<p>The kube-dc codebase has zero LimitRange resources. Key workloads that currently omit explicit resource requests:</p> Workload File Sets requests/limits? KubeVirt VMs <code>AddNewVMModal.tsx:377-380</code> Sets <code>domain.cpu.cores</code> and <code>domain.memory.guest</code> but NOT pod-level <code>resources.requests</code> (KubeVirt auto-generates these from domain spec) Cloud-shell SSH jobs <code>cloudshell-job.tmpl.yaml</code> Likely no resource requests VPC DNS pods <code>res_vpc_dns.go</code> Created by Kube-OVN, may lack requests User-deployed pods User YAML No guarantee of resource requests <p>KubeVirt note: KubeVirt's <code>virt-controller</code> translates <code>domain.cpu.cores</code> and <code>domain.memory.guest</code> into pod-level <code>resources.requests</code> and <code>resources.limits</code> automatically. So KubeVirt VMs should work with ResourceQuota, but this must be verified.</p>"},{"location":"prd/hierarchical-quota-integration/#343-limitrange-design","title":"3.4.3 LimitRange Design","text":"<p>Create a <code>LimitRange</code> in the organization namespace and use HNC propagation to automatically copy it to all project namespaces.</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-resource-limits\n  namespace: shalb                    # Organization namespace\n  labels:\n    billing.kube-dc.com/auto-managed: \"true\"\nspec:\n  limits:\n    # Default requests/limits for Containers\n    - type: Container\n      default:                        # Default limits (if not specified)\n        cpu: 500m\n        memory: 512Mi\n      defaultRequest:                 # Default requests (if not specified)\n        cpu: 100m\n        memory: 128Mi\n      max:                            # Max per container (prevents single container from consuming entire quota)\n        cpu: \"8\"\n        memory: 32Gi\n      min:                            # Minimum per container\n        cpu: 10m\n        memory: 16Mi\n    # Default requests/limits for Pods\n    - type: Pod\n      max:                            # Max per pod (all containers combined)\n        cpu: \"16\"\n        memory: 64Gi\n    # PVC size limits\n    - type: PersistentVolumeClaim\n      max:\n        storage: 500Gi\n      min:\n        storage: 1Gi\n</code></pre> <p>How this works with HRQ:</p> <pre><code>1. User creates pod WITHOUT resources.requests/limits\n2. LimitRange admission controller applies defaults (cpu: 100m, memory: 128Mi)\n3. HRQ admission controller checks aggregated usage against quota\n4. Pod is admitted if within quota, rejected if over quota\n\nOrder of admission controllers: LimitRange \u2192 ResourceQuota/HRQ\n(LimitRange runs BEFORE ResourceQuota, so defaults are applied first)\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#344-hnc-propagation-of-limitrange","title":"3.4.4 HNC Propagation of LimitRange","text":"<p>HNC can propagate LimitRange objects from parent namespace to all child namespaces. This is the recommended approach because:</p> <ol> <li>Single source of truth \u2014 Define once in org namespace, propagated to all projects</li> <li>Automatic updates \u2014 Change the LimitRange in org namespace \u2192 HNC updates all children</li> <li>No controller code needed \u2014 HNC handles propagation natively</li> <li>Immutable in children \u2014 HNC admission webhook prevents modification of propagated copies</li> </ol> <p>Configure HNC to propagate LimitRange:</p> <pre><code>apiVersion: hnc.x-k8s.io/v1alpha2\nkind: HNCConfiguration\nmetadata:\n  name: config\nspec:\n  resources:\n    - resource: limitranges\n      mode: Propagate              # Propagate from org \u2192 project namespaces\n</code></pre> <p>After this, a LimitRange created in <code>shalb</code> (org namespace) will be automatically copied to <code>shalb-demo</code>, <code>shalb-dev</code>, <code>shalb-envoy</code> (all project namespaces).</p>"},{"location":"prd/hierarchical-quota-integration/#345-limitrange-as-standalone-kubernetes-object-hybrid-approach","title":"3.4.5 LimitRange as Standalone Kubernetes Object (Hybrid Approach)","text":"<p>LimitRange is a standard Kubernetes object \u2014 no need to embed it into the Organization CRD. Instead, the controller auto-creates a LimitRange with plan-based defaults, and the org admin can override it by creating their own.</p> <p>Design: controller auto-creation + user override</p> <ol> <li>When a billing plan is activated, the Organization controller checks if a LimitRange named <code>default-resource-limits</code> exists in the org namespace</li> <li>If no LimitRange exists \u2192 controller creates one with plan-based defaults and labels it <code>billing.kube-dc.com/auto-managed: \"true\"</code></li> <li>If a user-created LimitRange exists (without the <code>auto-managed</code> label) \u2192 controller does NOT overwrite it, user owns the resource policy</li> <li>If the auto-managed LimitRange exists and the plan changes \u2192 controller updates it with new plan defaults</li> <li>HNC propagates the LimitRange (regardless of who created it) to all project namespaces</li> </ol> <p>Ownership detection via labels:</p> <pre><code># Controller-managed (auto-created, will be updated on plan change)\nlabels:\n  billing.kube-dc.com/auto-managed: \"true\"\n  billing.kube-dc.com/plan-id: \"pro-pool\"\n\n# User-managed (controller will NOT touch this)\n# No billing.kube-dc.com/auto-managed label\n</code></pre> <p>Benefits of this approach: - No CRD changes \u2014 Organization spec stays clean, no API bloat - Standard Kubernetes \u2014 org admins who know K8s can use native LimitRange directly - Auto-defaults for everyone \u2014 users who don't know/care get sensible defaults from their plan - User can customize \u2014 just create/edit a LimitRange in the org namespace, controller backs off - Future-proof \u2014 any new LimitRange features work immediately without CRD updates - Separation of concerns \u2014 billing/identity (Organization CRD) separate from resource policy (LimitRange)</p> <p>Edge cases:</p> <ul> <li>User edits auto-managed LimitRange: Controller will overwrite it back to plan defaults on the next reconciliation (the <code>auto-managed</code> label is still present). This is enforced behavior. The controller should emit a Kubernetes Event (via <code>EventRecorder</code>) when it detects and overwrites user changes to an auto-managed resource, so the action is auditable and visible in <code>kubectl describe limitrange</code>.</li> <li>User removes the <code>auto-managed</code> label: Controller treats it as user-managed and stops updating it. This is acceptable \u2014 the HRQ still enforces aggregate quota. The user effectively \"detached\" the LimitRange from plan auto-updates. This is a valid power-user action, not a bug.</li> </ul> <p>Plan-based auto-defaults (used when controller creates the LimitRange):</p> Plan Default Request CPU Default Request Memory Default Limit CPU Default Limit Memory Max CPU Max Memory Dev Pool (4 CPU) 100m 128Mi 500m 512Mi 2 4Gi Pro Pool (8 CPU) 250m 256Mi 500m 512Mi 4 12Gi Scale Pool (16 CPU) 500m 512Mi 1 1Gi 8 32Gi <p>Rationale: Smaller plans need lower <code>max</code> to prevent a single container from consuming the entire quota. Container defaults should be reasonable for typical workloads \u2014 not too low (causes constant OOMKill) or too high (wastes quota).</p> <p>Intentional: LimitRange max &lt; HRQ burst limits. For Dev Pool, HRQ allows 12 CPU burst (3x), but LimitRange caps a single container at 2 CPU and a single pod at 4 CPU. This means a user needs at least 3 pods (4+4+4) to utilize the full burst capacity. This is by design \u2014 it prevents a single runaway pod from starving all other workloads in the organization. Users with \"single heavy task\" workloads (e.g., a large build job) are limited to the per-pod max, even if the HRQ has headroom. If this is too restrictive for a specific org, the org-admin can override the LimitRange (see edge cases above).</p>"},{"location":"prd/hierarchical-quota-integration/#346-limitrange-propagation-via-hnc","title":"3.4.6 LimitRange Propagation via HNC","text":"<p>HNC propagation is the recommended approach:</p> <ol> <li>Create LimitRange in org namespace \u2192 HNC auto-copies to all project namespaces</li> <li>Single source of truth \u2014 change once, propagated everywhere</li> <li>Immutable in children \u2014 HNC admission webhook prevents users from modifying propagated copies</li> <li>Automatic cleanup \u2014 delete from org namespace, HNC removes all copies</li> <li>Works regardless of who created it \u2014 both controller-managed and user-managed LimitRanges are propagated</li> </ol> <p>Alternative (not recommended): Create LimitRange directly in each project namespace via Project controller. This requires: - Extra controller code per project - Synchronization when org defaults change (must update all projects) - More RBAC surface area</p> <p>Recommendation: Use HNC propagation. Fall back to direct creation only if HNC propagation proves problematic.</p>"},{"location":"prd/hierarchical-quota-integration/#35-per-project-sub-quotas-org-admin-managed","title":"3.5 Per-Project Sub-Quotas (Org-Admin Managed)","text":"<p>HRQ enforces the aggregate limit across all projects. But org-admins also need to limit individual projects \u2014 e.g., \"dev project gets max 2 CPU, production project gets 6 CPU\" within an 8 CPU org total.</p>"},{"location":"prd/hierarchical-quota-integration/#351-design-standard-kubernetes-resourcequota","title":"3.5.1 Design: Standard Kubernetes ResourceQuota","text":"<p>Per-project quotas use standard Kubernetes <code>ResourceQuota</code> \u2014 no custom CRDs. This works because:</p> <ol> <li>HRQ creates internal ResourceQuota objects in child namespaces (managed by HNC, names prefixed <code>hrq-*</code>)</li> <li>Org-admin creates additional ResourceQuota in the project namespace</li> <li>Kubernetes applies all ResourceQuotas in a namespace \u2014 the most restrictive limit wins per resource</li> <li>HRQ still enforces the org-level cap (sum of all projects can't exceed org total)</li> </ol> <pre><code>Organization: acme-corp (HRQ: 8 CPU total)\n  \u251c\u2500\u2500 acme-corp-dev        ResourceQuota: 2 CPU    \u2190 org-admin sets this\n  \u251c\u2500\u2500 acme-corp-staging    ResourceQuota: 2 CPU    \u2190 org-admin sets this\n  \u2514\u2500\u2500 acme-corp-prod       (no ResourceQuota)      \u2190 gets remainder up to HRQ limit\n</code></pre> <p>In this example: - <code>acme-corp-dev</code> can use at most 2 CPU (ResourceQuota caps it) - <code>acme-corp-prod</code> can use up to 8 CPU minus whatever dev+staging consume (HRQ caps it) - Total across all projects can never exceed 8 CPU (HRQ enforces)</p>"},{"location":"prd/hierarchical-quota-integration/#352-rbac-org-admin-can-manage-resourcequota-in-project-namespaces","title":"3.5.2 RBAC: Org-Admin Can Manage ResourceQuota in Project Namespaces","text":"<p>Org-admin needs write access to <code>resourcequotas</code> in project namespaces. This is added to the existing org-admin Role:</p> <pre><code># Added to the org-admin Role in each project namespace\n- apiGroups: [\"\"]\n  resources: [\"resourcequotas\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre> <p>Constraints: - Org-admin can set per-project ResourceQuota (limits individual projects) - Org-admin cannot modify HRQ-managed internal ResourceQuota (names prefixed <code>hrq-*</code>, managed by HNC) - Org-admin cannot set per-project limits higher than the org HRQ (Kubernetes enforces: the most restrictive wins) - HNC admission webhook blocks modification of HRQ-internal ResourceQuota objects</p>"},{"location":"prd/hierarchical-quota-integration/#353-example-per-project-resourcequota","title":"3.5.3 Example: Per-Project ResourceQuota","text":"<pre><code># Created by org-admin in the project namespace\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: project-quota\n  namespace: acme-corp-dev           # project namespace\nspec:\n  hard:\n    requests.cpu: \"2\"\n    requests.memory: \"4Gi\"\n    limits.cpu: \"4\"\n    limits.memory: \"8Gi\"\n    requests.storage: \"20Gi\"\n    pods: \"50\"\n</code></pre> <p>This coexists with the HRQ-managed internal ResourceQuota. The effective limit per resource is <code>min(project-quota, hrq-internal-quota)</code>.</p>"},{"location":"prd/hierarchical-quota-integration/#354-ui-integration-optional","title":"3.5.4 UI Integration (Optional)","text":"<p>The console can expose per-project quota management under Project Settings \u2192 Resource Limits:</p> <ul> <li>Show current project usage vs project quota vs org HRQ remaining</li> <li>Form to set per-project limits (creates/updates ResourceQuota via backend API)</li> <li>Validation: warn if sum of all project quotas exceeds org HRQ total</li> <li>Backend endpoint: <code>PUT /api/projects/:namespace/quota</code> \u2014 creates ResourceQuota using SA token</li> </ul> <p>Note: UI for per-project quota management is needed but late-stage (post-MVP). For MVP, org-admins use <code>kubectl</code> directly since this is standard Kubernetes. The UI page should be implemented after the core HRQ + LimitRange + subscription lifecycle is stable.</p>"},{"location":"prd/hierarchical-quota-integration/#355-interaction-with-subscription-lifecycle","title":"3.5.5 Interaction with Subscription Lifecycle","text":"Subscription Status Org HRQ Per-Project ResourceQuota <code>active</code> Full plan limits Org-admin managed, respected <code>suspended</code> Minimal (0.5 CPU) Still exists but effectively capped by HRQ <code>canceled</code> Minimal (0.5 CPU) Still exists, workloads scaled to 0 anyway Re-subscribe Full plan limits restored Per-project quotas still in place, unmodified <p>Per-project ResourceQuotas are not touched by the subscription lifecycle \u2014 they are org-admin's responsibility. Only the org-level HRQ changes with subscription status.</p>"},{"location":"prd/hierarchical-quota-integration/#36-plan-to-hrq-resource-mapping","title":"3.6 Plan-to-HRQ Resource Mapping","text":"<p>Burst ratios vary by plan tier \u2014 dev workloads are bursty and low-risk, production workloads need predictability:</p> Plan Burst Ratio (limits/requests) Reasoning Dev Pool 3x Dev/sandbox workloads are bursty, low overcommit risk Pro Pool 2x Balanced burst for general workloads Scale Pool 1.5x Production workloads need predictability, less overcommit <pre><code>// Burst ratio per plan tier\nconst BURST_RATIOS = {\n    'dev-pool': 3,\n    'pro-pool': 2,\n    'scale-pool': 1.5,\n};\n\n// Mapping function: Plan resources \u2192 HRQ spec.hard\nfunction planToHRQ(plan, addons = []) {\n    let totalCpu = plan.resources.cpu;\n    let totalMemory = plan.resources.memory;\n\n    for (const addon of addons) {\n        const addonDef = TURBO_ADDONS.find(a =&gt; a.id === addon.addonId);\n        if (addonDef) {\n            totalCpu += addonDef.resources.cpu * (addon.quantity || 1);\n            totalMemory += addonDef.resources.memory * (addon.quantity || 1);\n        }\n    }\n\n    const burst = BURST_RATIOS[plan.id] || 2;\n\n    return {\n        'requests.cpu': `${totalCpu}`,\n        'requests.memory': `${totalMemory}Gi`,\n        'limits.cpu': `${totalCpu * burst}`,             // per-plan burst\n        'limits.memory': `${totalMemory * burst}Gi`,     // per-plan burst\n        'requests.storage': `${plan.resources.storage}Gi`,\n        'pods': plan.id === 'dev-pool' ? '100' : plan.id === 'pro-pool' ? '200' : '500',\n        'services.loadbalancers': plan.id === 'dev-pool' ? '3' : plan.id === 'pro-pool' ? '5' : '10',\n    };\n}\n</code></pre> <p>Resulting HRQ values per plan:</p> Dev Pool (3x burst) Pro Pool (2x burst) Scale Pool (1.5x burst) <code>requests.cpu</code> 4 8 16 <code>limits.cpu</code> 12 16 24 <code>requests.memory</code> 8Gi 24Gi 64Gi <code>limits.memory</code> 24Gi 48Gi 96Gi <code>requests.storage</code> 60Gi 160Gi 500Gi <code>pods</code> 100 200 500 <code>services.loadbalancers</code> 3 5 10"},{"location":"prd/hierarchical-quota-integration/#4-implementation-plan","title":"4. Implementation Plan","text":""},{"location":"prd/hierarchical-quota-integration/#phase-1-hnc-installation-configuration","title":"Phase 1: HNC Installation &amp; Configuration","text":""},{"location":"prd/hierarchical-quota-integration/#411-install-hnc-controller","title":"4.1.1 Install HNC Controller","text":"<p>Add HNC deployment to the Kube-DC installer (<code>installer/kube-dc/templates/</code>):</p> <pre><code># Install HNC controller\nkubectl apply -f https://github.com/pfnet/hierarchical-namespaces/releases/download/v1.1.0-pfnet.10/default.yaml\n\n# Install HRQ component (separate manifest)\nkubectl apply -f https://github.com/pfnet/hierarchical-namespaces/releases/download/v1.1.0-pfnet.10/hrq.yaml\n</code></pre> <p>Helm chart integration: - Add HNC manifests to <code>charts/kube-dc/templates/</code> or as a dependency - Configure HNC via <code>HNCConfiguration</code> resource to propagate relevant resource types</p>"},{"location":"prd/hierarchical-quota-integration/#412-hnc-configuration","title":"4.1.2 HNC Configuration","text":"<pre><code>apiVersion: hnc.x-k8s.io/v1alpha2\nkind: HNCConfiguration\nmetadata:\n  name: config\nspec:\n  resources:\n    # Propagate RBAC (HNC default)\n    - resource: roles.rbac.authorization.k8s.io\n      mode: Propagate\n    - resource: rolebindings.rbac.authorization.k8s.io\n      mode: Propagate\n    # Do NOT propagate Kube-DC CRDs (managed by controllers)\n    - resource: projects.kube-dc.com\n      mode: Ignore\n    - resource: organizations.kube-dc.com\n      mode: Ignore\n    # Propagate secrets for shared credentials\n    - resource: secrets\n      mode: Propagate\n    # Propagate LimitRange from org namespace to project namespaces\n    # CRITICAL: Required for HRQ enforcement \u2014 without LimitRange, pods\n    # without explicit resource requests will be REJECTED by ResourceQuota\n    - resource: limitranges\n      mode: Propagate\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#413-rbac-for-hnc-resources","title":"4.1.3 RBAC for HNC Resources","text":"<p>The kube-dc-manager service account needs permissions to manage HNC resources:</p> <pre><code># Add to existing ClusterRole or create new one\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kube-dc-hnc-manager\nrules:\n  - apiGroups: [\"hnc.x-k8s.io\"]\n    resources: [\"hierarchicalresourcequotas\", \"hierarchyconfigurations\", \"subnamespaceanchors\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"hnc.x-k8s.io\"]\n    resources: [\"hierarchicalresourcequotas/status\"]\n    verbs: [\"get\"]\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#phase-2-controller-changes-go","title":"Phase 2: Controller Changes (Go)","text":""},{"location":"prd/hierarchical-quota-integration/#421-project-controller-set-namespace-parent","title":"4.2.1 Project Controller \u2014 Set Namespace Parent","text":"<p>File: <code>internal/project/res_namespace.go</code></p> <p>When creating a project namespace, set the HNC parent relationship:</p> <pre><code>func NewProjectNamespace(cli client.Client, project *kubedccomv1.Project, l *logr.Logger) objmgr.KubeDcResource[*corev1.Namespace] {\n    genObject := &amp;corev1.Namespace{\n        ObjectMeta: v1.ObjectMeta{\n            Name: ProjectNamespaceName(project),\n            Annotations: map[string]string{\n                kubedccomv1.ProjectNamespaceAnnotationKey: nsProjAnnotation(project),\n            },\n            Labels: map[string]string{\n                // HNC will set tree labels automatically once hierarchy is configured\n            },\n        },\n    }\n    // ... existing diff function ...\n}\n</code></pre> <p>New file: <code>internal/project/res_hierarchy.go</code></p> <pre><code>package project\n\n// Create/update HierarchyConfiguration to set parent namespace\nfunc NewProjectHierarchy(cli client.Client, project *kubedccomv1.Project, l *logr.Logger) objmgr.KubeDcResource[*hncv1alpha2.HierarchyConfiguration] {\n    genObject := &amp;hncv1alpha2.HierarchyConfiguration{\n        ObjectMeta: v1.ObjectMeta{\n            Name:      \"hierarchy\",  // HNC requires this exact name\n            Namespace: ProjectNamespaceName(project),\n        },\n        Spec: hncv1alpha2.HierarchyConfigurationSpec{\n            Parent: project.Organization().Namespace, // org namespace as parent\n        },\n    }\n    // ...\n}\n</code></pre> <p>Integration in <code>internal/project/project.go</code> Sync(): <pre><code>// After namespace creation, before VPC creation:\nlog.Info(\"create or update HNC Hierarchy\")\nhierarchy := NewProjectHierarchy(cli, p, log)\nerr = hierarchy.Sync(ctx)\nprojectStatusChanged = projectStatusChanged || hierarchy.StatusChanged()\nif err != nil {\n    return err, projectStatusChanged\n}\n</code></pre></p>"},{"location":"prd/hierarchical-quota-integration/#422-organization-controller-manage-hrq","title":"4.2.2 Organization Controller \u2014 Manage HRQ","text":"<p>New file: <code>internal/organization/res_hrq.go</code></p> <pre><code>package organization\n\nimport (\n    hncv1alpha2 \"sigs.k8s.io/hierarchical-namespaces/api/v1alpha2\"\n    \"k8s.io/apimachinery/pkg/api/resource\"\n)\n\nconst (\n    HRQName           = \"plan-quota\"\n    HRQManagedLabel   = \"billing.kube-dc.com/managed\"\n    HRQPlanIdLabel    = \"billing.kube-dc.com/plan-id\"\n)\n\n// PlanResources holds computed resource limits for an organization\ntype PlanResources struct {\n    RequestsCPU       resource.Quantity\n    RequestsMemory    resource.Quantity\n    LimitsCPU         resource.Quantity\n    LimitsMemory      resource.Quantity\n    RequestsStorage   resource.Quantity\n    Pods              resource.Quantity\n    ServicesLB        resource.Quantity\n}\n\nfunc NewOrganizationHRQ(cli client.Client, org *kubedccomv1.Organization, \n    planResources *PlanResources, log *logr.Logger) objmgr.KubeDcResource[*hncv1alpha2.HierarchicalResourceQuota] {\n\n    if planResources == nil {\n        return nil // No plan = no HRQ\n    }\n\n    genObject := &amp;hncv1alpha2.HierarchicalResourceQuota{\n        ObjectMeta: v1.ObjectMeta{\n            Name:      HRQName,\n            Namespace: org.Namespace,\n            Labels: map[string]string{\n                HRQManagedLabel: \"true\",\n            },\n        },\n        Spec: hncv1alpha2.HierarchicalResourceQuotaSpec{\n            Hard: corev1.ResourceList{\n                corev1.ResourceRequestsCPU:    planResources.RequestsCPU,\n                corev1.ResourceRequestsMemory: planResources.RequestsMemory,\n                corev1.ResourceLimitsCPU:      planResources.LimitsCPU,\n                corev1.ResourceLimitsMemory:   planResources.LimitsMemory,\n                corev1.ResourceRequestsStorage: planResources.RequestsStorage,\n                corev1.ResourcePods:           planResources.Pods,\n                \"services.loadbalancers\":      planResources.ServicesLB,\n            },\n        },\n    }\n    // ...\n}\n</code></pre> <p>Reading plan from Organization annotations:</p> <pre><code>func GetPlanResourcesFromAnnotations(org *kubedccomv1.Organization) *PlanResources {\n    annotations := org.GetAnnotations()\n    if annotations == nil {\n        return nil\n    }\n\n    planId := annotations[\"billing.kube-dc.com/plan-id\"]\n    status := annotations[\"billing.kube-dc.com/subscription\"]\n\n    // Active statuses that should have full HRQ (see \u00a77.2 state machine)\n    if planId == \"\" || (status != \"active\" &amp;&amp; status != \"trialing\" &amp;&amp; status != \"canceling\") {\n        return nil\n    }\n\n    // Plan definitions (should match subscriptionController.js SUBSCRIPTION_PLANS)\n    plans := map[string]PlanResources{\n        \"dev-pool\": {\n            RequestsCPU:     resource.MustParse(\"4\"),\n            RequestsMemory:  resource.MustParse(\"8Gi\"),\n            LimitsCPU:       resource.MustParse(\"12\"),   // 3x burst (see \u00a73.6)\n            LimitsMemory:    resource.MustParse(\"24Gi\"), // 3x burst\n            RequestsStorage: resource.MustParse(\"60Gi\"),\n            Pods:            resource.MustParse(\"100\"),\n            ServicesLB:      resource.MustParse(\"3\"),\n        },\n        \"pro-pool\": { /* ... */ },\n        \"scale-pool\": { /* ... */ },\n    }\n\n    base, ok := plans[planId]\n    if !ok {\n        return nil\n    }\n\n    // Apply add-ons\n    addonsJSON := annotations[\"billing.kube-dc.com/addons\"]\n    // Parse and add addon resources to base...\n\n    return &amp;base\n}\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#423-organization-controller-watch-annotation-changes","title":"4.2.3 Organization Controller \u2014 Watch Annotation Changes","text":"<p>File: <code>internal/controller/kube-dc.com/organization_controller.go</code></p> <p>Update the predicate to trigger reconciliation on annotation changes (billing events):</p> <pre><code>func getPredicateFuncOrg() predicate.Funcs {\n    return predicate.Funcs{\n        UpdateFunc: func(e event.UpdateEvent) bool {\n            oldObj := e.ObjectOld.(*kubedccomv1.Organization)\n            newObj := e.ObjectNew.(*kubedccomv1.Organization)\n\n            // Existing spec/finalizer/deletion checks...\n\n            // NEW: Check if billing annotations changed\n            oldPlanId := oldObj.GetAnnotations()[\"billing.kube-dc.com/plan-id\"]\n            newPlanId := newObj.GetAnnotations()[\"billing.kube-dc.com/plan-id\"]\n            oldAddons := oldObj.GetAnnotations()[\"billing.kube-dc.com/addons\"]\n            newAddons := newObj.GetAnnotations()[\"billing.kube-dc.com/addons\"]\n            oldStatus := oldObj.GetAnnotations()[\"billing.kube-dc.com/subscription\"]\n            newStatus := newObj.GetAnnotations()[\"billing.kube-dc.com/subscription\"]\n\n            if oldPlanId != newPlanId || oldAddons != newAddons || oldStatus != newStatus {\n                return true // Reconcile to update HRQ\n            }\n\n            // ... existing checks\n        },\n    }\n}\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#424-organization-controller-manage-limitrange","title":"4.2.4 Organization Controller \u2014 Manage LimitRange","text":"<p>New file: <code>internal/organization/res_limitrange.go</code></p> <p>The LimitRange is created in the org namespace. HNC propagates it to all child project namespaces automatically.</p> <pre><code>package organization\n\nimport (\n    corev1 \"k8s.io/api/core/v1\"\n    \"k8s.io/apimachinery/pkg/api/resource\"\n)\n\nconst (\n    LimitRangeName = \"default-resource-limits\"\n)\n\n// LimitRangeDefaults holds default/max resource values per plan\ntype LimitRangeDefaults struct {\n    DefaultCPU        resource.Quantity\n    DefaultMemory     resource.Quantity\n    DefaultRequestCPU resource.Quantity\n    DefaultRequestMem resource.Quantity\n    MaxCPU            resource.Quantity\n    MaxMemory         resource.Quantity\n    MinCPU            resource.Quantity\n    MinMemory         resource.Quantity\n    MaxPodCPU         resource.Quantity\n    MaxPodMemory      resource.Quantity\n    MaxPVCStorage     resource.Quantity\n    MinPVCStorage     resource.Quantity\n}\n\nfunc GetLimitRangeDefaultsForPlan(planId string) *LimitRangeDefaults {\n    defaults := map[string]LimitRangeDefaults{\n        \"dev-pool\": {\n            DefaultCPU:        resource.MustParse(\"500m\"),\n            DefaultMemory:     resource.MustParse(\"512Mi\"),\n            DefaultRequestCPU: resource.MustParse(\"100m\"),\n            DefaultRequestMem: resource.MustParse(\"128Mi\"),\n            MaxCPU:            resource.MustParse(\"2\"),\n            MaxMemory:         resource.MustParse(\"4Gi\"),\n            MinCPU:            resource.MustParse(\"10m\"),\n            MinMemory:         resource.MustParse(\"16Mi\"),\n            MaxPodCPU:         resource.MustParse(\"4\"),\n            MaxPodMemory:      resource.MustParse(\"8Gi\"),\n            MaxPVCStorage:     resource.MustParse(\"60Gi\"),\n            MinPVCStorage:     resource.MustParse(\"1Gi\"),\n        },\n        \"pro-pool\": {\n            DefaultCPU:        resource.MustParse(\"500m\"),\n            DefaultMemory:     resource.MustParse(\"512Mi\"),\n            DefaultRequestCPU: resource.MustParse(\"250m\"),\n            DefaultRequestMem: resource.MustParse(\"256Mi\"),\n            MaxCPU:            resource.MustParse(\"4\"),\n            MaxMemory:         resource.MustParse(\"12Gi\"),\n            MinCPU:            resource.MustParse(\"10m\"),\n            MinMemory:         resource.MustParse(\"16Mi\"),\n            MaxPodCPU:         resource.MustParse(\"8\"),\n            MaxPodMemory:      resource.MustParse(\"24Gi\"),\n            MaxPVCStorage:     resource.MustParse(\"160Gi\"),\n            MinPVCStorage:     resource.MustParse(\"1Gi\"),\n        },\n        \"scale-pool\": {\n            DefaultCPU:        resource.MustParse(\"1\"),\n            DefaultMemory:     resource.MustParse(\"1Gi\"),\n            DefaultRequestCPU: resource.MustParse(\"500m\"),\n            DefaultRequestMem: resource.MustParse(\"512Mi\"),\n            MaxCPU:            resource.MustParse(\"8\"),\n            MaxMemory:         resource.MustParse(\"32Gi\"),\n            MinCPU:            resource.MustParse(\"10m\"),\n            MinMemory:         resource.MustParse(\"16Mi\"),\n            MaxPodCPU:         resource.MustParse(\"16\"),\n            MaxPodMemory:      resource.MustParse(\"56Gi\"),\n            MaxPVCStorage:     resource.MustParse(\"320Gi\"),\n            MinPVCStorage:     resource.MustParse(\"1Gi\"),\n        },\n    }\n    d, ok := defaults[planId]\n    if !ok {\n        return nil\n    }\n    return &amp;d\n}\n\nfunc NewOrganizationLimitRange(cli client.Client, org *kubedccomv1.Organization,\n    lrDefaults *LimitRangeDefaults, log *logr.Logger) objmgr.KubeDcResource[*corev1.LimitRange] {\n\n    if lrDefaults == nil {\n        return nil\n    }\n\n    genObject := &amp;corev1.LimitRange{\n        ObjectMeta: v1.ObjectMeta{\n            Name:      LimitRangeName,\n            Namespace: org.Namespace,\n            Labels: map[string]string{\n                HRQManagedLabel: \"true\",\n            },\n        },\n        Spec: corev1.LimitRangeSpec{\n            Limits: []corev1.LimitRangeItem{\n                {\n                    Type: corev1.LimitTypeContainer,\n                    Default: corev1.ResourceList{\n                        corev1.ResourceCPU:    lrDefaults.DefaultCPU,\n                        corev1.ResourceMemory: lrDefaults.DefaultMemory,\n                    },\n                    DefaultRequest: corev1.ResourceList{\n                        corev1.ResourceCPU:    lrDefaults.DefaultRequestCPU,\n                        corev1.ResourceMemory: lrDefaults.DefaultRequestMem,\n                    },\n                    Max: corev1.ResourceList{\n                        corev1.ResourceCPU:    lrDefaults.MaxCPU,\n                        corev1.ResourceMemory: lrDefaults.MaxMemory,\n                    },\n                    Min: corev1.ResourceList{\n                        corev1.ResourceCPU:    lrDefaults.MinCPU,\n                        corev1.ResourceMemory: lrDefaults.MinMemory,\n                    },\n                },\n                {\n                    Type: corev1.LimitTypePod,\n                    Max: corev1.ResourceList{\n                        corev1.ResourceCPU:    lrDefaults.MaxPodCPU,\n                        corev1.ResourceMemory: lrDefaults.MaxPodMemory,\n                    },\n                },\n                {\n                    Type: corev1.LimitTypePersistentVolumeClaim,\n                    Max: corev1.ResourceList{\n                        corev1.ResourceStorage: lrDefaults.MaxPVCStorage,\n                    },\n                    Min: corev1.ResourceList{\n                        corev1.ResourceStorage: lrDefaults.MinPVCStorage,\n                    },\n                },\n            },\n        },\n    }\n    // ...\n}\n</code></pre> <p>Update <code>organization.Sync()</code> in <code>internal/organization/organization.go</code>:</p> <pre><code>// After existing sync steps, add HRQ and LimitRange sync:\nlog.V(5).Info(\"Sync hierarchical resource quota...\")\nplanResources := GetPlanResourcesFromAnnotations(org)\nif planResources != nil {\n    hrqRes := NewOrganizationHRQ(client, org, planResources, log)\n    err = hrqRes.Sync(ctx)\n    orgStatusChanged = orgStatusChanged || hrqRes.StatusChanged()\n    if err != nil {\n        return err, orgStatusChanged\n    }\n}\n\n// LimitRange MUST be synced alongside HRQ \u2014 without it, pods without\n// explicit resource requests will be rejected by the ResourceQuota\n// that HRQ creates internally in each child namespace.\n// The LimitRange is created in the org namespace and HNC propagates\n// it to all project namespaces automatically.\nlog.V(5).Info(\"Sync LimitRange for default resource requests...\")\nplanId := org.GetAnnotations()[\"billing.kube-dc.com/plan-id\"]\nlrDefaults := GetLimitRangeDefaultsForPlan(planId)\nif lrDefaults != nil {\n    lrRes := NewOrganizationLimitRange(client, org, lrDefaults, log)\n    err = lrRes.Sync(ctx)\n    orgStatusChanged = orgStatusChanged || lrRes.StatusChanged()\n    if err != nil {\n        return err, orgStatusChanged\n    }\n}\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#phase-3-backend-changes-nodejs","title":"Phase 3: Backend Changes (Node.js)","text":""},{"location":"prd/hierarchical-quota-integration/#431-subscription-controller-createupdate-hrq-on-plan-change","title":"4.3.1 Subscription Controller \u2014 Create/Update HRQ on Plan Change","text":"<p>File: <code>ui/backend/controllers/billing/subscriptionController.js</code></p> <p>After updating Organization annotations (in <code>updateOrganizationSubscription</code>), the Go controller will automatically reconcile and create/update the HRQ. No direct HRQ manipulation needed from the backend \u2014 the Go controller watches annotation changes.</p> <p>However, we need to update the usage endpoint to read real HRQ status:</p> <pre><code>/**\n * Get real quota usage from HierarchicalResourceQuota status\n */\nasync function getOrganizationQuotaUsage(organization, saToken) {\n    try {\n        const hrqUrl = `https://${global.k8sUrl}/apis/hnc.x-k8s.io/v1alpha2/namespaces/${organization}/hierarchicalresourcequotas/plan-quota`;\n        const response = await fetch(hrqUrl, {\n            headers: {\n                'Accept': 'application/json',\n                'Authorization': `Bearer ${saToken}`,\n            },\n            agent: httpsAgent,\n        });\n\n        if (!response.ok) {\n            return null; // HRQ not found (no plan)\n        }\n\n        const hrq = await response.json();\n        const hard = hrq.status?.hard || {};\n        const used = hrq.status?.used || {};\n\n        return {\n            cpu: {\n                used: parseCpuValue(used['requests.cpu'] || '0'),\n                limit: parseCpuValue(hard['requests.cpu'] || '0'),\n            },\n            memory: {\n                used: parseMemoryValue(used['requests.memory'] || '0'),\n                limit: parseMemoryValue(hard['requests.memory'] || '0'),\n            },\n            storage: {\n                used: parseMemoryValue(used['requests.storage'] || '0'),\n                limit: parseMemoryValue(hard['requests.storage'] || '0'),\n            },\n            pods: {\n                used: parseInt(used['pods'] || '0'),\n                limit: parseInt(hard['pods'] || '0'),\n            },\n        };\n    } catch (error) {\n        logger.error('Error fetching HRQ usage:', error.message);\n        return null;\n    }\n}\n</code></pre> <p>Replace mock usage in <code>getOrganizationSubscriptionData()</code>:</p> <pre><code>// BEFORE (mock):\nusage: plan ? {\n    cpu: { used: 0, limit: totalCpu },\n    memory: { used: 0, limit: totalMemory },\n    storage: { used: 0, limit: totalStorage },\n    pods: { used: 0, limit: 200 },\n} : null,\n\n// AFTER (real):\nusage: plan ? await getOrganizationQuotaUsage(organization, saToken) : null,\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#phase-4-frontend-changes-reacttypescript","title":"Phase 4: Frontend Changes (React/TypeScript)","text":""},{"location":"prd/hierarchical-quota-integration/#441-billing-overview-show-real-usage","title":"4.4.1 Billing Overview \u2014 Show Real Usage","text":"<p>File: <code>ui/frontend/src/app/ManageOrganization/Billing/Billing.tsx</code></p> <p>The <code>QuotaUsage</code> type already exists in <code>types.ts</code> and matches the HRQ data shape. The overview tab already renders usage bars. The primary change is that the backend now returns real data instead of zeros.</p> <p>Additional UI improvements: - Show \"Quota enforcement: Active\" badge when HRQ exists - Warning alerts when usage exceeds 80% of limits - Error alerts when pods fail scheduling due to quota</p>"},{"location":"prd/hierarchical-quota-integration/#442-plan-selection-show-quota-impact","title":"4.4.2 Plan Selection \u2014 Show Quota Impact","text":"<p>File: <code>ui/frontend/src/app/ManageOrganization/Billing/SubscribePlanModal.tsx</code></p> <p>When changing plans, show: - Current usage vs new plan limits - Warning if downgrade would exceed new limits - Confirmation dialog for downgrades that would restrict resources</p>"},{"location":"prd/hierarchical-quota-integration/#phase-5-subscription-hrq-lifecycle-permission-model","title":"Phase 5: Subscription \u2192 HRQ Lifecycle &amp; Permission Model","text":""},{"location":"prd/hierarchical-quota-integration/#451-end-to-end-flow-who-changes-what","title":"4.5.1 End-to-End Flow: Who Changes What","text":"<p>Users cannot modify HRQ or LimitRange directly \u2014 they have no RBAC permissions for these resources. All quota changes flow through a two-stage indirect reconciliation:</p> <pre><code>User / Stripe\n     \u2502\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Stage 1: Node.js Backend                           \u2502\n\u2502  (runs as kube-dc-backend ServiceAccount)           \u2502\n\u2502                                                     \u2502\n\u2502  Authenticates via:                                 \u2502\n\u2502    \u2022 User JWT token (UI actions)                    \u2502\n\u2502    \u2022 Stripe webhook signature (Stripe events)       \u2502\n\u2502                                                     \u2502\n\u2502  Action: PATCH Organization annotations             \u2502\n\u2502    billing.kube-dc.com/plan-id: \"pro-pool\"          \u2502\n\u2502    billing.kube-dc.com/subscription: \"active\"       \u2502\n\u2502    billing.kube-dc.com/addons: [...]                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502 annotation change triggers reconcile\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Stage 2: Go Organization Controller                \u2502\n\u2502  (runs as kube-dc-manager ServiceAccount)           \u2502\n\u2502                                                     \u2502\n\u2502  Watches: Organization annotation changes           \u2502\n\u2502                                                     \u2502\n\u2502  Action: Create/Update/Delete HRQ + LimitRange      \u2502\n\u2502    based on billing.kube-dc.com/plan-id annotation  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why this design: - Security: Users never touch HRQ/LimitRange directly. Only privileged ServiceAccounts can. - Stripe is the source of truth: Payment confirmation (Stripe webhook) \u2192 annotations \u2192 HRQ. No payment = no quota. - Idempotent: The Go controller reconciles to desired state on every annotation change. If the backend crashes mid-update, the next reconcile fixes it. - Auditable: All changes flow through Organization annotations with timestamps, visible in <code>kubectl describe organization</code>.</p>"},{"location":"prd/hierarchical-quota-integration/#452-subscription-flows-in-detail","title":"4.5.2 Subscription Flows in Detail","text":"<p>Flow A: New Subscription (user initiates via UI)</p> <pre><code>1. User clicks \"Subscribe to Pro Pool\" in UI\n2. Frontend POST /api/billing/organization-subscription { planId: \"pro-pool\" }\n3. Backend verifies JWT \u2192 isOrgAdmin(token)\n4. Backend creates Stripe Checkout Session (redirect to Stripe payment page)\n5. User completes payment on Stripe\n6. Frontend calls POST /api/billing/verify-checkout { sessionId }\n7. Backend retrieves Stripe session, verifies payment/trial status\n8. Backend PATCHes Organization annotations (using SA token):\n     billing.kube-dc.com/subscription: \"active\"\n     billing.kube-dc.com/plan-id: \"pro-pool\"\n     billing.kube-dc.com/stripe-subscription-id: \"sub_...\"\n9. Go controller sees annotation change \u2192 reconciles:\n     \u2192 Creates HierarchicalResourceQuota (plan-quota) in org namespace\n     \u2192 Creates LimitRange (default-resource-limits) in org namespace\n     \u2192 HNC propagates LimitRange to all project namespaces\n</code></pre> <p>Flow B: Plan Change (upgrade/downgrade)</p> <pre><code>1. User clicks \"Change to Scale Pool\" in UI\n2. Frontend PUT /api/billing/organization-subscription { planId: \"scale-pool\" }\n3. Backend verifies JWT \u2192 isOrgAdmin(token)\n4. Backend updates Stripe subscription (proration applied)\n5. Backend PATCHes Organization annotations:\n     billing.kube-dc.com/plan-id: \"scale-pool\"    \u2190 changed\n6. Go controller sees plan-id changed \u2192 reconciles:\n     \u2192 Updates HRQ spec.hard with new plan limits\n     \u2192 Updates auto-managed LimitRange with new plan defaults\n       (or skips if user has their own LimitRange)\n</code></pre> <p>Flow C: Cancellation</p> <pre><code>1. User clicks \"Cancel Subscription\" in UI\n2. Frontend DELETE /api/billing/organization-subscription\n3. Backend verifies JWT \u2192 isOrgAdmin(token)\n4. Backend sets Stripe subscription to cancel_at_period_end: true\n5. Backend PATCHes Organization annotations:\n     billing.kube-dc.com/subscription: \"canceling\"   \u2190 not \"canceled\" yet\n     billing.kube-dc.com/plan-id: \"pro-pool\"          \u2190 KEPT until period end\n6. Go controller sees status=\"canceling\" \u2192 HRQ REMAINS ACTIVE until period end\n   (user still has quota for the rest of the paid period)\n\n--- At billing period end (Stripe fires webhook) ---\n\n7. Stripe sends customer.subscription.deleted webhook\n8. Backend verifies webhook signature (STRIPE_WEBHOOK_SECRET)\n9. Backend PATCHes Organization annotations (using SA token):\n     billing.kube-dc.com/subscription: \"suspended\"    \u2190 grace period starts (see \u00a77.2)\n     billing.kube-dc.com/suspended-at: \"2026-02-07T...\" \u2190 timestamp for grace tracking\n     billing.kube-dc.com/plan-id: \"pro-pool\"           \u2190 KEPT (for restore reference)\n10. Go controller sees status=\"suspended\" \u2192 reconciles:\n      \u2192 Sets HRQ to minimal suspended quota (0.5 CPU, 1Gi)\n      \u2192 Keeps LimitRange (system pods still need defaults)\n\n--- After 7-day grace period (cron/controller timer) ---\n\n11. Controller checks suspended-at timestamp, 7 days passed \u2192 transitions:\n      billing.kube-dc.com/subscription: \"canceled\"\n      billing.kube-dc.com/plan-id: \"\"                  \u2190 cleared\n      billing.kube-dc.com/addons: []                   \u2190 cleared\n12. Go controller sees status=\"canceled\" \u2192 reconciles:\n      \u2192 Keeps minimal HRQ (system pods)\n      \u2192 Suspends all user workloads (scale to 0, halt VMs)\n</code></pre> <p>Flow D: Stripe Webhook Events (server-to-server, no user involved)</p> <pre><code>Stripe \u2192 POST /api/billing/webhook (raw body + Stripe-Signature header)\nBackend verifies: stripe.webhooks.constructEvent(body, sig, STRIPE_WEBHOOK_SECRET)\nBackend uses: ServiceAccount token (NOT user JWT \u2014 no user is present)\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#453-webhook-events-to-handle","title":"4.5.3 Webhook Events to Handle","text":"<p>File: <code>ui/backend/controllers/billing/subscriptionController.js</code> (webhook handler)</p> <p>Note: The webhook endpoint already exists at <code>POST /api/billing/webhook</code> (line ~963 in <code>subscriptionController.js</code>). It currently handles <code>customer.subscription.updated</code>, <code>customer.subscription.deleted</code>, and <code>invoice.payment_failed</code>. The raw body is preserved for Stripe signature verification (JSON parsing is skipped for this route in <code>app.js</code>).</p> <p>CRITICAL: <code>checkout.session.completed</code> is currently not handled in the webhook \u2014 subscription activation relies solely on the frontend calling <code>POST /api/billing/verify-checkout</code> after Stripe redirect. This is the #1 cause of \"I paid but didn't get my upgrade\" support tickets \u2014 browsers crash, tabs close, mobile networks drop. Implementing <code>checkout.session.completed</code> in the webhook is a must-have for Phase 1, not a fallback. The webhook is the only reliable source of truth for payment completion. The frontend verify-checkout should become a secondary fast-path, not the primary activation mechanism.</p> Stripe Event Backend Action Annotation Change Go Controller Effect <code>checkout.session.completed</code> Activate subscription (must-have) <code>plan-id</code>, <code>subscription: active</code> Create HRQ + LimitRange <code>customer.subscription.updated</code> Sync status/trial (exists) <code>status</code>, <code>is-trial</code>, <code>trial-end-date</code> Update HRQ if plan changed <code>customer.subscription.deleted</code> Start grace period (exists) <code>subscription: suspended</code>, <code>suspended-at</code> timestamp Set HRQ to minimal suspended quota <code>invoice.payment_failed</code> Log warning (exists) <code>subscription: past_due</code> (optional) Keep HRQ (grace period) <code>customer.subscription.trial_will_end</code> Send notification (to add) No annotation change No HRQ change <p>On cancellation, the HRQ should NOT be deleted immediately (see \u00a77.2 for full lifecycle). Instead: 1. Mark subscription as <code>canceling</code> (existing behavior \u2014 keeps <code>plan-id</code>, full HRQ) 2. HRQ stays active for the remaining paid period 3. At period end (<code>customer.subscription.deleted</code>), set <code>suspended</code> \u2192 minimal HRQ (7-day grace) 4. After grace period, transition to <code>canceled</code> \u2192 workloads scaled to 0, data preserved</p>"},{"location":"prd/hierarchical-quota-integration/#454-add-on-flow-turbo-x1x2","title":"4.5.4 Add-on Flow (Turbo x1/x2)","text":"<pre><code>1. User clicks \"Add Turbo x1\" in UI\n2. Frontend POST /api/billing/organization-subscription/addons { addonId: \"turbo-x1\" }\n3. Backend verifies JWT \u2192 isOrgAdmin(token)\n4. Backend adds item to Stripe subscription\n5. Backend PATCHes Organization annotations:\n     billing.kube-dc.com/addons: '[{\"addonId\":\"turbo-x1\",\"quantity\":1,...}]'\n6. Go controller sees addons changed \u2192 reconciles:\n     \u2192 Reads base plan + addons from annotations\n     \u2192 Recalculates total resources (plan.cpu + addon.cpu * quantity)\n     \u2192 Updates HRQ spec.hard with new totals\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#455-permission-model-rbac","title":"4.5.5 Permission Model (RBAC)","text":"Actor Identity Can Read Organization Can Patch Annotations Can Manage HRQ Can Manage LimitRange User (org-admin) JWT from Dex/OIDC \u2705 \u274c (via backend only) \u274c \u274c (in project ns; can create in org ns) User (member) JWT from Dex/OIDC \u2705 \u274c \u274c \u274c UI Backend <code>kube-dc-backend</code> SA \u2705 \u2705 \u274c \u274c Go Controller <code>kube-dc-manager</code> SA \u2705 \u2705 (status only) \u2705 \u2705 Stripe Webhook Webhook signature N/A (uses Backend SA) \u2705 (via Backend) \u274c \u274c <p>Key RBAC rules to add for HRQ integration:</p> <pre><code># kube-dc-manager ServiceAccount \u2014 needs HRQ + LimitRange management\n- apiGroups: [\"hnc.x-k8s.io\"]\n  resources: [\"hierarchicalresourcequotas\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"hnc.x-k8s.io\"]\n  resources: [\"hierarchyconfigurations\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"limitranges\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n\n# kube-dc-backend ServiceAccount \u2014 reads HRQ status for usage display\n- apiGroups: [\"hnc.x-k8s.io\"]\n  resources: [\"hierarchicalresourcequotas\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>User RBAC (org-admin) \u2014 explicitly DENY HRQ writes:</p> <p>Users in org-admin group get a Role/RoleBinding in the org namespace that allows managing Projects, VMs, etc. but the Role must NOT include <code>hierarchicalresourcequotas</code> write permissions. HRQ is managed exclusively by the Go controller.</p> <p>For LimitRange: org-admin can create/update LimitRange in the org namespace (this is the \"user override\" path from section 3.4.5). They cannot modify HNC-propagated copies in project namespaces (HNC webhook blocks this).</p>"},{"location":"prd/hierarchical-quota-integration/#5-migration-strategy","title":"5. Migration Strategy","text":""},{"location":"prd/hierarchical-quota-integration/#51-existing-organizations","title":"5.1 Existing Organizations","text":"<p>For organizations that already have projects:</p> <ol> <li>Install HNC \u2014 HNC controller starts watching namespaces</li> <li>Set hierarchy \u2014 Run migration script or controller reconciliation sets parent on all existing project namespaces</li> <li>Create HRQ \u2014 Organization controller creates HRQ based on existing billing annotations</li> <li>Verify \u2014 Check that <code>kubectl hns tree &lt;org-namespace&gt;</code> shows correct hierarchy</li> </ol> <p>Migration script: <pre><code>#!/bin/bash\n# For each organization namespace\nfor org in $(kubectl get organizations -A -o jsonpath='{.items[*].metadata.namespace}'); do\n    # For each project in the organization\n    for proj_ns in $(kubectl get namespaces -l \"kube-dc.com/project\" -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\\n' | grep \"^${org}-\"); do\n        echo \"Setting parent of ${proj_ns} to ${org}\"\n        kubectl hns set ${proj_ns} --parent ${org}\n    done\ndone\n</code></pre></p>"},{"location":"prd/hierarchical-quota-integration/#52-new-organizations","title":"5.2 New Organizations","text":"<p>New organizations get the hierarchy configured automatically: 1. Organization controller creates org namespace (existing) 2. Project controller creates project namespace (existing) 3. NEW: Project controller sets HNC parent on project namespace 4. NEW: Organization controller creates HRQ + LimitRange when billing plan is activated 5. NEW: HNC propagates LimitRange from org namespace \u2192 all project namespaces</p>"},{"location":"prd/hierarchical-quota-integration/#6-resource-mapping-details","title":"6. Resource Mapping Details","text":""},{"location":"prd/hierarchical-quota-integration/#61-hrq-resource-types","title":"6.1 HRQ Resource Types","text":"HRQ Resource Plan Field Notes <code>requests.cpu</code> <code>resources.cpu</code> Direct mapping (vCPU = Kubernetes CPU) <code>requests.memory</code> <code>resources.memory</code> In GiB \u2192 <code>{N}Gi</code> <code>limits.cpu</code> <code>resources.cpu * burst</code> Burst ratio: 3x (dev), 2x (pro), 1.5x (scale) <code>limits.memory</code> <code>resources.memory * burst</code> Same per-plan burst ratio <code>requests.storage</code> <code>resources.storage</code> PVC storage in GiB <code>pods</code> Computed 100 for dev, 200 for pro, 500 for scale <code>services.loadbalancers</code> Computed 3 for dev, 5 for pro, 10 for scale (LB count, not IPv4) <p>Note: Public IPv4 (EIP) quota is enforced separately via the EIP controller \u2014 not via HRQ. See \u00a76.4.</p>"},{"location":"prd/hierarchical-quota-integration/#62-object-storage-rook-ceph-s3-quotas","title":"6.2 Object Storage (Rook-Ceph S3 Quotas)","text":"<p>Object storage (<code>resources.objectStorage</code>) is not enforceable via Kubernetes ResourceQuota/HRQ. However, Rook-Ceph provides native CRD-based quota enforcement via <code>CephObjectStoreUser</code>, which fits the same controller-driven pattern as HRQ.</p>"},{"location":"prd/hierarchical-quota-integration/#621-how-it-works","title":"6.2.1 How It Works","text":"<p>Rook-Ceph's <code>CephObjectStoreUser</code> CRD has built-in <code>spec.quotas</code>:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStoreUser\nmetadata:\n  name: acme-corp                      # one per organization\n  namespace: rook-ceph                 # Rook-Ceph operator namespace\nspec:\n  store: my-store                      # CephObjectStore name\n  displayName: \"acme-corp\"\n  quotas:\n    maxBuckets: 10                     # max number of buckets\n    maxSize: 20G                       # total size across all buckets (plan: objectStorage)\n    maxObjects: 100000                 # max object count (optional safeguard)\n  capabilities:\n    user: \"read\"\n    bucket: \"*\"\n</code></pre> <p>Ceph RGW (RADOS Gateway) enforces these quotas server-side \u2014 S3 PUT requests that exceed the quota are rejected with <code>403 QuotaExceeded</code>.</p>"},{"location":"prd/hierarchical-quota-integration/#622-integration-pattern-same-as-hrq","title":"6.2.2 Integration Pattern (Same as HRQ)","text":"<pre><code>Plan activated \u2192 controller creates/updates CephObjectStoreUser with plan quotas\nPlan changed  \u2192 controller updates maxSize to new plan's objectStorage value\nPlan canceled \u2192 controller sets maxSize to 0 (or deletes user)\n</code></pre> <p>Controller logic in <code>organization.Sync()</code> (alongside HRQ + LimitRange):</p> <pre><code>// Sync Ceph S3 user quota alongside HRQ\nlog.V(5).Info(\"Sync object storage quota...\")\nif planResources != nil {\n    s3Quota := NewOrganizationS3Quota(client, org, planResources.ObjectStorage, log)\n    err = s3Quota.Sync(ctx)\n    // ...\n}\n</code></pre> <p>Plan-to-S3 quota mapping:</p> Plan <code>maxSize</code> <code>maxBuckets</code> <code>maxObjects</code> Dev Pool 20Gi 5 100,000 Pro Pool 100Gi 20 1,000,000 Scale Pool 500Gi 50 10,000,000"},{"location":"prd/hierarchical-quota-integration/#623-per-bucket-quotas-via-objectbucketclaim","title":"6.2.3 Per-Bucket Quotas via ObjectBucketClaim","text":"<p>Users create buckets via <code>ObjectBucketClaim</code> (OBC), which also supports per-bucket quotas:</p> <pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: my-bucket\n  namespace: acme-corp-dev             # project namespace\nspec:\n  generateBucketName: acme-corp-dev-\n  storageClassName: rook-ceph-bucket\n  additionalConfig:\n    maxSize: \"5G\"                      # per-bucket limit (optional, user-configurable)\n    maxObjects: \"50000\"\n</code></pre> <p>The user-level quota (<code>CephObjectStoreUser.spec.quotas.maxSize</code>) enforces the org-wide total regardless of per-bucket settings \u2014 same relationship as HRQ (org total) vs ResourceQuota (per-project).</p>"},{"location":"prd/hierarchical-quota-integration/#624-rbac-credential-management","title":"6.2.4 RBAC &amp; Credential Management","text":"<ul> <li><code>kube-dc-manager</code> SA creates/updates <code>CephObjectStoreUser</code> in the <code>rook-ceph</code> namespace (needs RBAC for <code>cephobjectstoreusers</code> CRD)</li> <li>Rook operator auto-creates a Secret with S3 credentials (AccessKey, SecretKey) named <code>rook-ceph-object-user-{store}-{user}</code></li> <li>Controller copies/references the credentials Secret to the org namespace so projects can access S3</li> <li>Users create <code>ObjectBucketClaim</code> in project namespaces (standard RBAC, already works with Rook)</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#625-subscription-lifecycle","title":"6.2.5 Subscription Lifecycle","text":"Status S3 Quota Action <code>active</code> <code>maxSize</code> = plan's <code>objectStorage</code> value <code>suspended</code> <code>maxSize</code> = <code>0</code> (block new uploads, existing data preserved) <code>canceled</code> <code>maxSize</code> = <code>0</code>, data preserved during retention period Re-subscribe <code>maxSize</code> restored to new plan value <p>Confirmed feasible: Rook-Ceph <code>CephObjectStoreUser</code> quotas are enforced server-side by Ceph RGW. The same annotation-driven controller pattern (Stripe \u2192 annotations \u2192 Go controller \u2192 CRD update) works for S3 quotas. Implementation can be done alongside or after HRQ integration.</p>"},{"location":"prd/hierarchical-quota-integration/#63-kubevirt-vm-resources","title":"6.3 KubeVirt VM Resources","text":"<p>VMs created via KubeVirt consume CPU/memory from the same quota as pods. The <code>requests.cpu</code> and <code>requests.memory</code> in the HRQ naturally cover both containers and VMs, since KubeVirt VMs run as pods with <code>resources.requests</code> set.</p> <p>Important: Ensure VMs have proper <code>resources.requests</code> set, or they won't be counted against the quota. The Kube-DC VM templates should enforce this.</p>"},{"location":"prd/hierarchical-quota-integration/#64-public-ipv4-eip-quota-enforcement","title":"6.4 Public IPv4 (EIP) Quota Enforcement","text":"<p>Public IPv4 addresses are allocated via Kube-DC's <code>EIp</code> CRD. EIPs with <code>spec.externalNetworkType: public</code> (label <code>network.kube-dc.com/external-network-type: public</code>) consume real public IPs from the <code>ext-public</code> subnet. These are a scarce, paid resource and must be quota-controlled per organization.</p>"},{"location":"prd/hierarchical-quota-integration/#641-why-hrq-cant-track-eips","title":"6.4.1 Why HRQ Can\u2019t Track EIPs","text":"<p>HRQ/ResourceQuota only tracks standard Kubernetes resources (<code>pods</code>, <code>services.loadbalancers</code>, <code>requests.cpu</code>, etc.). <code>EIp</code> is a Kube-DC custom CRD \u2014 Kubernetes quota admission doesn't know about it. We need controller-side enforcement.</p>"},{"location":"prd/hierarchical-quota-integration/#642-design-eip-controller-quota-check","title":"6.4.2 Design: EIP Controller Quota Check","text":"<p>Before creating a new public EIP, the EIP controller counts existing public EIPs across all project namespaces for the organization and compares against the plan's <code>ipv4</code> limit.</p> <pre><code>// In EIP controller Reconcile(), before creating OvnEip:\nfunc checkPublicEIPQuota(ctx context.Context, cli client.Client, projectNamespace string) error {\n    // 1. Get organization name from project namespace\n    orgName := getOrgFromProjectNamespace(projectNamespace) // e.g., \"shalb\" from \"shalb-demo\"\n\n    // 2. Get organization's plan ipv4 limit from annotations\n    org := &amp;kubedccomv1.Organization{}\n    cli.Get(ctx, types.NamespacedName{Name: orgName, Namespace: orgName}, org)\n    planId := org.GetAnnotations()[\"billing.kube-dc.com/plan-id\"]\n    plan := GetPlanResources(planId)  // plan.IPv4 = 1 (dev), 1 (pro), 3 (scale)\n\n    // 3. Count public EIPs across ALL project namespaces for this org\n    eipList := &amp;kubedccomv1.EIpList{}\n    cli.List(ctx, eipList, client.MatchingLabels{\n        \"network.kube-dc.com/external-network-type\": \"public\",\n    })\n    // Filter to only EIPs in namespaces belonging to this org\n    count := 0\n    for _, eip := range eipList.Items {\n        if strings.HasPrefix(eip.Namespace, orgName+\"-\") || eip.Namespace == orgName {\n            count++\n        }\n    }\n\n    // 4. Reject if over quota\n    if count &gt;= plan.IPv4 {\n        return fmt.Errorf(\"public EIP quota exceeded: %d/%d for organization %s\", count, plan.IPv4, orgName)\n    }\n    return nil\n}\n</code></pre> <p>Race condition mitigation: If multiple EIPs are created simultaneously (e.g., <code>kubectl apply -f 5-eips.yaml</code>), parallel reconcile threads could each see <code>count=0</code> and all pass the check. Mitigation: the EIP controller uses a single worker queue per organization (controller-runtime's default concurrency is 1 per controller). If higher concurrency is configured, add a post-creation recount \u2014 after creating the OvnEip, re-list public EIPs and if <code>count &gt; limit</code>, delete the just-created EIP and return an error (compensating transaction). In practice, EIP creation is rare and slow (OVN provisioning), so the default single-worker queue is sufficient for MVP.</p>"},{"location":"prd/hierarchical-quota-integration/#643-plan-to-eip-quota-mapping","title":"6.4.3 Plan-to-EIP Quota Mapping","text":"Plan <code>resources.ipv4</code> Max Public EIPs Notes Dev Pool 1 1 1 public IP (default-gw SNAT + shared LB) Pro Pool 1 1 1 public IP Scale Pool 3 3 3 public IPs (dedicated LBs, multiple services)"},{"location":"prd/hierarchical-quota-integration/#644-what-counts-as-a-public-eip","title":"6.4.4 What Counts as a Public EIP","text":"<p>Only EIPs with <code>spec.externalNetworkType: public</code> count against the quota. EIPs with <code>externalNetworkType: cloud</code> use internal cloud IPs from <code>ext-cloud</code> subnet and are not quota-controlled (they're free internal addresses).</p> <pre><code>EIP label: network.kube-dc.com/external-network-type: public  \u2192 COUNTS against ipv4 quota\nEIP label: network.kube-dc.com/external-network-type: cloud   \u2192 does NOT count\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#645-enforcement-points","title":"6.4.5 Enforcement Points","text":"Action Enforcement Project creation (auto-creates <code>default-gw</code> EIP) EIP controller checks quota before creating public EIP User creates additional EIP via kubectl/UI EIP controller checks quota Service type=LoadBalancer (may allocate EIP) Service controller / EIP controller checks quota Plan downgrade (e.g., 3 \u2192 1 IPv4) Existing EIPs not deleted \u2014 but new ones blocked until under limit"},{"location":"prd/hierarchical-quota-integration/#646-subscription-lifecycle","title":"6.4.6 Subscription Lifecycle","text":"Status EIP Quota Action <code>active</code> Enforce plan's <code>ipv4</code> limit <code>suspended</code> Block new public EIPs (limit = 0), keep existing <code>canceled</code> Block new public EIPs, existing released when workloads scaled down No plan No quota enforcement (or limit = 0 if billing is mandatory)"},{"location":"prd/hierarchical-quota-integration/#647-usage-reporting","title":"6.4.7 Usage Reporting","text":"<p>The backend should expose public EIP count alongside HRQ usage:</p> <pre><code>// In getOrganizationQuotaUsage():\nconst eipUrl = `https://${global.k8sUrl}/apis/kube-dc.com/v1/eips?labelSelector=network.kube-dc.com/external-network-type=public`;\n// Filter by org namespaces, count results\nreturn {\n    // ... existing HRQ usage\n    ipv4: {\n        used: publicEipCount,\n        limit: plan.resources.ipv4,\n    },\n};\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#65-kube-dc-system-resources","title":"6.5 Kube-DC System Resources","text":"<p>Resources consumed by Kube-DC infrastructure in project namespaces count against the quota. The overhead must be explicitly accounted for in <code>plan_resources.go</code> so users get the full advertised resources.</p> <p>System pod footprint per project namespace:</p> System Pod CPU Request Memory Request Notes VpcDns (CoreDNS) 100m 128Mi Created by Kube-OVN per project <p>Per-project overhead: ~100m CPU, ~128Mi memory.</p> <p>Recommended implementation in <code>plan_resources.go</code>: Add a fixed overhead per project to the HRQ, based on <code>OrganizationProjectsLimit</code> (default: 3):</p> <pre><code>overhead_cpu    = projects_limit * 100m   \u2192 300m for 3 projects\noverhead_memory = projects_limit * 128Mi  \u2192 384Mi for 3 projects\n</code></pre> <p>So if the plan advertises 4 CPU, the HRQ <code>requests.cpu</code> should be <code>4.3</code> (= 4 + 0.3 overhead). This prevents the scenario where a user sees \"4 CPU\" in the UI but can only schedule 3.7 CPU of their own workloads. The UI should display the user-available amount (plan value), not the raw HRQ value.</p>"},{"location":"prd/hierarchical-quota-integration/#7-edge-cases-considerations","title":"7. Edge Cases &amp; Considerations","text":""},{"location":"prd/hierarchical-quota-integration/#71-quota-exceeded-on-plan-downgrade","title":"7.1 Quota Exceeded on Plan Downgrade","text":"<p>If a user downgrades from Scale Pool (16 CPU) to Dev Pool (4 CPU) but is using 10 CPU: - HRQ updates immediately \u2014 new limit is 4 CPU - Existing pods are NOT evicted \u2014 Kubernetes quotas only block new pod creation - New pods will fail until usage drops below the new limit - UI must warn before downgrade if current usage exceeds new plan</p>"},{"location":"prd/hierarchical-quota-integration/#72-subscription-expiration-post-cancellation-lifecycle","title":"7.2 Subscription Expiration &amp; Post-Cancellation Lifecycle","text":"<p>When a subscription ends (trial expires, cancellation at period end, or payment failure), we need a phased wind-down \u2014 not an instant shutdown. The goal: give users time to re-subscribe or export data, while preventing free-riding.</p> <p>Key Kubernetes constraint: HRQ only blocks new pod creation. Setting HRQ to 0 does NOT evict running pods \u2014 they keep running until they crash/restart, at which point they can't come back.</p>"},{"location":"prd/hierarchical-quota-integration/#phase-1-warning-before-expiration","title":"Phase 1: Warning (before expiration)","text":"<p>No HRQ changes. Notifications only.</p> Trigger Action 7 days before period end Email + in-app notification: \"Your subscription expires in 7 days\" 3 days before Email + persistent UI banner 1 day before Email + UI banner: \"Last chance to renew\" Trial: <code>customer.subscription.trial_will_end</code> Stripe fires this 3 days before trial ends"},{"location":"prd/hierarchical-quota-integration/#phase-2-grace-period-7-days-after-expiration","title":"Phase 2: Grace Period (7 days after expiration)","text":"<p>Annotation: <code>billing.kube-dc.com/subscription: suspended</code></p> <p>The Go controller sets HRQ to a minimal \"suspended\" quota instead of deleting it:</p> <pre><code># Suspended quota \u2014 enough for system pods only\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: HierarchicalResourceQuota\nmetadata:\n  name: plan-quota\n  namespace: acme-corp\n  labels:\n    billing.kube-dc.com/auto-managed: \"true\"\n    billing.kube-dc.com/plan-id: \"suspended\"\nspec:\n  hard:\n    requests.cpu: \"500m\"          # ~150m per project for VpcDns + buffer\n    requests.memory: \"1Gi\"        # ~256Mi per project for VpcDns + buffer\n    limits.cpu: \"1\"\n    limits.memory: \"2Gi\"\n    requests.storage: \"0\"         # block new PVCs\n    pods: \"10\"                    # only system pods\n    services.loadbalancers: \"0\"   # block new LBs\n</code></pre> <p>What happens to running workloads: - Existing pods keep running (Kubernetes doesn't evict on quota change) - If a pod crashes/restarts and there's quota headroom \u2192 it comes back - If quota is exhausted \u2192 crashed pods can't restart, user workloads degrade naturally - System pods (VpcDns) have priority because they're small (~50m CPU, ~64Mi memory each) - New user workloads blocked (quota too small for anything meaningful) - New PVCs and LoadBalancers blocked</p> <p>User experience: - Console shows \"Subscription expired \u2014 renew to restore full access\" banner - Billing page shows \"Suspended\" status with one-click re-subscribe - Users can still view resources, check logs, export data - Users cannot create new VMs, deployments, or PVCs</p> <p>Controller logic in <code>organization.Sync()</code>:</p> <pre><code>subscriptionStatus := org.GetAnnotations()[\"billing.kube-dc.com/subscription\"]\n\nswitch subscriptionStatus {\ncase \"active\", \"trialing\", \"canceling\":\n    // Active plan \u2014 full HRQ from plan resources\n    planResources := GetPlanResourcesFromAnnotations(org)\n    // ... create/update HRQ with full limits\ncase \"suspended\":\n    // Grace period \u2014 minimal quota for system pods only\n    hrqRes := NewSuspendedHRQ(client, org, log)\n    // ... create/update HRQ with suspended limits\ncase \"canceled\":\n    // Past grace period \u2014 scale down + minimal quota\n    hrqRes := NewSuspendedHRQ(client, org, log)\n    // ... also trigger workload suspension (Phase 3)\ndefault:\n    // No subscription \u2014 no HRQ (no quota enforcement)\n}\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#phase-3-workload-suspension-after-grace-period-day-8","title":"Phase 3: Workload Suspension (after grace period, day 8+)","text":"<p>Annotation: <code>billing.kube-dc.com/subscription: canceled</code></p> <p>The Stripe webhook fires <code>customer.subscription.deleted</code> \u2192 backend sets status to <code>canceled</code>. If the user hasn't re-subscribed within the grace period, the controller actively suspends workloads:</p> Resource Action Reversible? Deployments Scale replicas to 0, save original in annotation \u2705 Restore on re-subscribe StatefulSets Scale replicas to 0, save original in annotation \u2705 Restore on re-subscribe VirtualMachines Set <code>runStrategy: Halted</code> \u2705 Set back to <code>Always</code> on re-subscribe CronJobs Set <code>suspend: true</code> \u2705 Set back to <code>false</code> on re-subscribe PVCs Keep (data preserved) \u2705 Data intact Secrets, ConfigMaps Keep \u2705 Intact Services, Ingress Keep (but no backends) \u2705 Intact <p>Implementation: New function <code>SuspendOrgWorkloads(ctx, cli, orgNamespace)</code> that: 1. Lists all project namespaces for the org 2. For each project namespace, scales down all Deployments/StatefulSets, halts VMs 3. Stores original replica counts in annotations (e.g., <code>billing.kube-dc.com/pre-suspend-replicas: \"3\"</code>) 4. On re-subscribe, <code>RestoreOrgWorkloads()</code> reads annotations and restores everything</p> <p>HRQ stays at \"suspended\" quota \u2014 system pods (VpcDns) keep running for network integrity.</p>"},{"location":"prd/hierarchical-quota-integration/#phase-4-data-retention-day-30-configurable","title":"Phase 4: Data Retention (day 30+, configurable)","text":"<p>After 30 days of <code>canceled</code> status with no re-subscription: - Send final \"Data deletion warning\" email - After 60 days: mark org for cleanup - After 90 days: delete project namespaces and all data (PVCs, secrets, etc.)</p> <p>Design decision: Data retention period should be configurable per deployment. Cloud providers typically use 30-90 days. Self-hosted Kube-DC may have different policies.</p>"},{"location":"prd/hierarchical-quota-integration/#summary-subscription-status-hrq-state","title":"Summary: Subscription Status \u2192 HRQ State","text":"<code>subscription</code> annotation HRQ State Workloads User Can... <code>active</code> / <code>trialing</code> Full plan quota Running normally Create, scale, everything <code>canceling</code> Full plan quota (paid period) Running normally Everything (until period end) <code>suspended</code> (grace, 7d) Minimal quota (0.5 CPU, 1Gi) Running but can't scale/restart View, export, re-subscribe <code>canceled</code> (post-grace) Minimal quota Scaled to 0 by controller View data, re-subscribe <code>\"\"</code> (no plan ever) No HRQ (no enforcement) Running freely Everything (no billing)"},{"location":"prd/hierarchical-quota-integration/#stripe-annotation-state-machine","title":"Stripe \u2192 Annotation State Machine","text":"<pre><code>checkout.session.completed    \u2192 \"active\"\nsubscription.updated (active) \u2192 \"active\"\nsubscription.updated (trial)  \u2192 \"trialing\"\nuser cancels (period end)     \u2192 \"canceling\"\nperiod ends / trial expires   \u2192 \"suspended\"  \u2190 7-day grace starts\ngrace period ends (cron/timer)\u2192 \"canceled\"   \u2190 workloads suspended\nuser re-subscribes at any point \u2192 \"active\"   \u2190 full restore\n</code></pre> <p>Note on <code>suspended</code> vs <code>canceled</code> transition: Stripe fires <code>customer.subscription.deleted</code> at period end. The backend should set <code>suspended</code> (not <code>canceled</code>) and record <code>billing.kube-dc.com/suspended-at</code> timestamp. A separate cron job or controller timer checks if 7 days have passed since <code>suspended-at</code> and transitions to <code>canceled</code> + triggers workload suspension.</p>"},{"location":"prd/hierarchical-quota-integration/#73-hnc-controller-unavailability","title":"7.3 HNC Controller Unavailability","text":"<p>If HNC controller is down: - HRQ enforcement stops (no admission webhook) - Existing pods continue running - New pods may be created without quota checks - Mitigation: HNC should be a critical system component with proper HA</p>"},{"location":"prd/hierarchical-quota-integration/#74-race-conditions","title":"7.4 Race Conditions","text":"<ul> <li>Plan change + simultaneous pod creation: HNC admission webhook serializes checks</li> <li>Multiple addon additions: Backend should serialize Organization annotation updates</li> <li>Organization deletion with active HRQ: Project controller deletes hierarchy first, then org controller cleans up HRQ</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#75-hnc-and-kube-ovn-interaction","title":"7.5 HNC and Kube-OVN Interaction","text":"<p>HNC propagates resources from parent to child namespaces. Ensure Kube-OVN CRDs are set to <code>Ignore</code> mode in HNC configuration to prevent unwanted propagation of VPCs, Subnets, etc.</p>"},{"location":"prd/hierarchical-quota-integration/#8-api-changes","title":"8. API Changes","text":""},{"location":"prd/hierarchical-quota-integration/#81-new-backend-endpoints","title":"8.1 New Backend Endpoints","text":"<pre><code>GET  /api/billing/quota-usage          \u2192 Real-time HRQ usage from K8s API\nGET  /api/billing/quota-status         \u2192 HRQ existence and health check\nPOST /api/billing/simulate-downgrade   \u2192 Check if downgrade is safe (usage &lt; new limits)\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#82-organization-crd-changes-optional","title":"8.2 Organization CRD Changes (Optional)","text":"<p>Consider adding quota status to Organization CRD status:</p> <pre><code>type OrganizationStatus struct {\n    Ready      bool               `json:\"ready\"`\n    Conditions []metav1.Condition `json:\"conditions,omitempty\"`\n    // NEW: Quota status\n    QuotaEnforced bool            `json:\"quotaEnforced,omitempty\"`\n    QuotaPlanId   string          `json:\"quotaPlanId,omitempty\"`\n}\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#9-testing-plan","title":"9. Testing Plan","text":""},{"location":"prd/hierarchical-quota-integration/#91-unit-tests","title":"9.1 Unit Tests","text":"<ul> <li>Plan-to-HRQ resource mapping</li> <li>Addon calculation logic</li> <li>Annotation parsing for plan resources</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#92-e2e-tests","title":"9.2 E2E Tests","text":"<p>Add to <code>tests/e2e/</code>:</p> <ol> <li>HRQ Creation Test:</li> <li>Create organization with billing plan annotation</li> <li>Verify HRQ is created with correct limits</li> <li> <p>Create project, verify namespace is child of org namespace</p> </li> <li> <p>Quota Enforcement Test:</p> </li> <li>Create organization with Dev Pool plan (4 CPU)</li> <li>Create pods consuming close to 4 CPU across projects</li> <li> <p>Verify next pod fails with quota exceeded error</p> </li> <li> <p>Plan Upgrade Test:</p> </li> <li>Start with Dev Pool (4 CPU), upgrade to Pro Pool (8 CPU)</li> <li>Verify HRQ limits are updated</li> <li> <p>Verify previously blocked pods can now be created</p> </li> <li> <p>Plan Cancellation Test:</p> </li> <li>Cancel subscription</li> <li>Verify HRQ is removed or set to minimal limits</li> <li>Verify new pod creation is blocked</li> </ol>"},{"location":"prd/hierarchical-quota-integration/#93-manual-testing","title":"9.3 Manual Testing","text":"<ul> <li>Stripe test mode checkout \u2192 verify HRQ creation</li> <li>Turbo addon purchase \u2192 verify HRQ update</li> <li>UI quota usage display with real metrics</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#10-deployment-sequence","title":"10. Deployment Sequence","text":""},{"location":"prd/hierarchical-quota-integration/#101-phase-rollout","title":"10.1 Phase Rollout","text":"<ol> <li>Phase 1 (Week 1-2): Install HNC, configure hierarchy for existing namespaces, no enforcement</li> <li>Phase 2 (Week 3-4): Add Go controller changes, create HRQ on plan selection, migration script</li> <li>Phase 3 (Week 5): Update backend to read real HRQ usage, update UI</li> <li>Phase 4 (Week 6): End-to-end testing, Stripe webhook integration</li> <li>Phase 5 (Week 7): Production rollout with monitoring</li> </ol>"},{"location":"prd/hierarchical-quota-integration/#102-rollback-plan","title":"10.2 Rollback Plan","text":"<ul> <li>HRQ can be deleted without affecting running workloads</li> <li>LimitRange deletion does not affect running pods (only applies at admission time)</li> <li>HNC hierarchy can be removed by unsetting parent annotations</li> <li>Organization annotations remain unchanged (billing data safe)</li> <li>Backend falls back to mock usage if HRQ not found</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#11-files-to-createmodify","title":"11. Files to Create/Modify","text":""},{"location":"prd/hierarchical-quota-integration/#new-files","title":"New Files","text":"File Purpose <code>internal/project/res_hierarchy.go</code> HierarchyConfiguration resource for project namespace <code>internal/organization/res_hrq.go</code> HierarchicalResourceQuota resource management <code>internal/organization/res_limitrange.go</code> LimitRange auto-creation with plan defaults, user override detection (\u00a73.4.5) <code>internal/organization/res_s3_quota.go</code> Rook-Ceph CephObjectStoreUser quota management (\u00a76.2) <code>internal/organization/plan_resources.go</code> Plan-to-resource mapping, burst ratios, addon calculation (\u00a73.6) <code>internal/organization/workload_suspend.go</code> SuspendOrgWorkloads / RestoreOrgWorkloads for post-cancellation lifecycle (\u00a77.2) <code>charts/kube-dc/templates/hnc-install.yaml</code> HNC installation manifests <code>charts/kube-dc/templates/hnc-config.yaml</code> HNC configuration (propagation rules for LimitRange, RBAC, Secrets) <code>charts/kube-dc/templates/hnc-rbac.yaml</code> RBAC for kube-dc-manager to manage HNC + LimitRange + CephObjectStoreUser <code>tests/e2e/quota_test.go</code> E2E tests for quota enforcement <code>hack/migrate-hierarchy.sh</code> Migration script for existing organizations <code>examples/organization/04-org-defaults.yaml</code> Example: Standalone LimitRange + HRQ reference (hybrid approach)"},{"location":"prd/hierarchical-quota-integration/#modified-files","title":"Modified Files","text":"File Change <code>internal/project/project.go</code> Add HierarchyConfiguration sync step after namespace creation <code>internal/organization/organization.go</code> Add HRQ + LimitRange + S3 quota sync steps, read plan from annotations <code>internal/controller/kube-dc.com/organization_controller.go</code> Watch billing annotation changes (<code>plan-id</code>, <code>addons</code>, <code>subscription</code>), trigger reconcile <code>internal/controller/kube-dc.com/eip_controller.go</code> Add public EIP quota check before OvnEip creation (\u00a76.4) <code>ui/backend/controllers/billing/subscriptionController.js</code> Read real HRQ usage, add <code>checkout.session.completed</code> webhook handler, set <code>suspended</code> on deletion <code>ui/frontend/src/app/ManageOrganization/Billing/Billing.tsx</code> Show real usage, quota alerts, suspension banner <code>ui/frontend/src/app/ManageOrganization/Billing/SubscribePlanModal.tsx</code> Downgrade warning with current usage <code>ui/frontend/src/app/ManageOrganization/Billing/types.ts</code> Add quota status types <code>go.mod</code> Add HNC API + Rook-Ceph API dependencies <code>Makefile</code> Add HNC CRD generation targets"},{"location":"prd/hierarchical-quota-integration/#go-dependencies-to-add","title":"Go Dependencies to Add","text":"<pre><code>sigs.k8s.io/hierarchical-namespaces/api v1.1.0-pfnet.10\ngithub.com/rook/rook/pkg/apis                        # For CephObjectStoreUser CRD (\u00a76.2)\n</code></pre>"},{"location":"prd/hierarchical-quota-integration/#12-monitoring-observability","title":"12. Monitoring &amp; Observability","text":""},{"location":"prd/hierarchical-quota-integration/#121-metrics","title":"12.1 Metrics","text":"<ul> <li><code>kube_dc_hrq_usage_ratio</code> \u2014 Current usage / limit per resource type per organization</li> <li><code>kube_dc_hrq_sync_errors</code> \u2014 Count of HRQ sync failures</li> <li><code>kube_dc_plan_changes_total</code> \u2014 Count of plan changes (upgrade/downgrade/cancel)</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#122-alerts","title":"12.2 Alerts","text":"<ul> <li>Organization usage &gt; 80% of HRQ limit \u2192 Warning notification</li> <li>Organization usage &gt; 95% of HRQ limit \u2192 Critical notification</li> <li>HRQ sync failure \u2192 Controller error alert</li> <li>HNC controller down \u2192 System alert</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#123-logging","title":"12.3 Logging","text":"<ul> <li>HRQ create/update/delete events in organization controller</li> <li>Quota exceeded events surfaced to UI</li> <li>Plan change audit trail in Organization annotations (already exists via <code>billing.kube-dc.com/subscribed-at</code>)</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#13-security-considerations","title":"13. Security Considerations","text":""},{"location":"prd/hierarchical-quota-integration/#131-authentication-layers","title":"13.1 Authentication Layers","text":"Entry Point Authentication Method Identity UI API endpoints JWT token (Dex/OIDC) User (org-admin or member) Stripe webhook <code>Stripe-Signature</code> header verified via <code>STRIPE_WEBHOOK_SECRET</code> Stripe server Go controller In-cluster ServiceAccount (<code>kube-dc-manager</code>) Controller process Backend K8s calls ServiceAccount token (<code>/var/run/secrets/.../token</code>) Backend pod"},{"location":"prd/hierarchical-quota-integration/#132-rbac-separation","title":"13.2 RBAC Separation","text":"<ul> <li>HRQ is controller-managed only: Users have no RBAC verbs on <code>hierarchicalresourcequotas</code>. Only <code>kube-dc-manager</code> SA can create/update/delete HRQ.</li> <li>Billing annotations are backend-managed: The <code>kube-dc-backend</code> SA can PATCH Organization metadata.annotations. Users cannot \u2014 their JWT-based RBAC does not include Organization write permissions on billing annotations.</li> <li>LimitRange in org namespace: org-admin users can create/update LimitRange in the org namespace (standard K8s RBAC). This is intentional \u2014 it's the \"user override\" path. However, they cannot modify HNC-propagated copies in project namespaces (HNC admission webhook blocks it).</li> <li>HNC hierarchy: Only <code>kube-dc-manager</code> SA can create/update <code>HierarchyConfiguration</code> objects. Users cannot re-parent namespaces.</li> <li>Admission webhooks: HNC webhook must be highly available. If webhook is down, Kubernetes API server rejects HNC resource mutations (fail-closed by default).</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#133-stripe-webhook-security","title":"13.3 Stripe Webhook Security","text":"<ul> <li>Webhook endpoint (<code>POST /api/billing/webhook</code>) verifies every event using <code>stripe.webhooks.constructEvent(body, sig, STRIPE_WEBHOOK_SECRET)</code></li> <li>Raw body is preserved (Express JSON parsing is skipped for this route) \u2014 required for signature verification</li> <li><code>STRIPE_WEBHOOK_SECRET</code> is stored as a Kubernetes Secret, mounted as env var in the backend pod</li> <li>Webhook handler uses ServiceAccount token (not user JWT) because there is no user context in server-to-server calls</li> <li>Failed signature verification returns 400 immediately \u2014 no annotation changes</li> </ul>"},{"location":"prd/hierarchical-quota-integration/#134-threat-mitigations","title":"13.4 Threat Mitigations","text":"Threat Mitigation User directly creates HRQ to get more quota RBAC: no write verbs on HRQ for user roles User patches billing annotations to fake a plan RBAC: users cannot PATCH Organization annotations Forged Stripe webhook to activate a free plan Stripe signature verification rejects forged events User deletes LimitRange in project namespace HNC admission webhook blocks deletion of propagated objects User modifies HNC hierarchy to escape quota RBAC: no write verbs on HierarchyConfiguration for user roles Backend SA token leaked Limit SA scope to only Organization PATCH + HRQ read; rotate regularly"},{"location":"prd/hierarchical-quota-integration/#14-open-questions","title":"14. Open Questions","text":"<ol> <li>Free tier limits: Resolved (see \u00a77.2): Organizations without a plan get no HRQ (no enforcement). After subscription expires, HRQ transitions to a minimal \"suspended\" quota (0.5 CPU, 1Gi) that allows only system pods (VpcDns). After grace period, workloads are actively scaled to 0.</li> <li>Grace period on cancellation: Resolved (see \u00a77.2): 4-phase lifecycle: <code>canceling</code> (paid period, full quota) \u2192 <code>suspended</code> (7-day grace, minimal quota) \u2192 <code>canceled</code> (workloads scaled to 0, data preserved) \u2192 cleanup (90 days, configurable).</li> <li>Per-project sub-quotas: Resolved (see \u00a73.5): Org-admin can create standard Kubernetes <code>ResourceQuota</code> in project namespaces. Coexists with HRQ \u2014 most restrictive limit wins. No custom CRDs needed. RBAC grants org-admin <code>resourcequotas</code> write in project namespaces. UI integration optional for MVP.</li> <li>Object storage enforcement: Resolved (see \u00a76.2): Rook-Ceph <code>CephObjectStoreUser</code> CRD has native <code>spec.quotas</code> (maxSize, maxBuckets, maxObjects). Ceph RGW enforces server-side. Same controller pattern as HRQ \u2014 plan annotation \u2192 Go controller \u2192 update CephObjectStoreUser quota. Implementation can be done alongside or after HRQ.</li> <li>IPv4 tracking: Resolved (see \u00a76.4): Public EIPs (<code>spec.externalNetworkType: public</code>, label <code>network.kube-dc.com/external-network-type: public</code>) are quota-controlled via EIP controller-side check \u2014 count public EIPs across org's project namespaces, compare against plan's <code>ipv4</code> limit. Cloud EIPs are not counted. No custom quota resource needed \u2014 enforced in Go controller before OvnEip creation.</li> <li>Burst ratios: Resolved (see \u00a73.6): Per-plan burst ratios: Dev Pool 3x (bursty dev workloads), Pro Pool 2x (balanced), Scale Pool 1.5x (production predictability). Configured via <code>BURST_RATIOS</code> map in planToHRQ mapping.</li> </ol>"},{"location":"prd/hierarchical-quota-integration/#15-references","title":"15. References","text":"<ul> <li>pfnet/hierarchical-namespaces \u2014 Active HNC fork</li> <li>HNC Concepts Guide</li> <li>HierarchicalResourceQuota API</li> <li>Kubernetes ResourceQuota</li> <li>Stripe Billing Webhooks</li> </ul>"},{"location":"prd/infrastructure-requirements/","title":"Kube-DC Infrastructure Requirements","text":"<p>This document outlines the resource requirements for deploying and operating a Kube-DC platform.</p>"},{"location":"prd/infrastructure-requirements/#architecture-overview","title":"Architecture Overview","text":""},{"location":"prd/infrastructure-requirements/#management-platform-provider-infrastructure","title":"Management Platform (Provider Infrastructure)","text":"<p>The Kube-DC management platform runs on 3 VMs that host: - Kubernetes control plane (RKE2/K3s) - All Kube-DC platform components (controllers, UI, monitoring) - Kamaji controller for managing tenant control planes - Tenant control plane pods (scaled on-demand)</p> <p>Key Principle: Tenant control planes run as pods on the management cluster and scale flexibly by adding more VMs to the management cluster as tenant count grows.</p>"},{"location":"prd/infrastructure-requirements/#tenant-infrastructure-customer-infrastructure","title":"Tenant Infrastructure (Customer Infrastructure)","text":"<ul> <li>Worker VMs: Run on customer's CloudSigma account</li> <li>Storage (Disks): Provisioned on customer's CloudSigma account via CSI driver</li> <li>Networking: Customer's CloudSigma network resources (IPs, VLANs)</li> </ul> <p>Key Principle: Customer workloads and data remain on customer infrastructure. Only the control plane is managed centrally.</p>"},{"location":"prd/infrastructure-requirements/#management-cluster-base-requirements","title":"Management Cluster Base Requirements","text":""},{"location":"prd/infrastructure-requirements/#production-setup-3-vm-architecture","title":"Production Setup: 3-VM Architecture","text":"VM Role Nb VMs CPU Cores CPU (GHz) RAM (GB) DISK (GB) Purpose Control Plane + Tenant CP Host 3 8 16-24 64 200+ K8s control plane, platform components, tenant control planes <p>Scaling Model: - Start with 3 VMs for HA and base capacity - Add VMs horizontally as tenant cluster count grows - Each additional VM can host ~10-20 tenant control planes - No distinction between \"master\" and \"worker\" - all nodes can host tenant control planes</p>"},{"location":"prd/infrastructure-requirements/#management-cluster-components-shared-infrastructure","title":"Management Cluster Components (Shared Infrastructure)","text":"<p>These components run once in the management cluster and serve all tenant clusters.</p>"},{"location":"prd/infrastructure-requirements/#core-platform-components","title":"Core Platform Components","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes Kube-DC Manager 1 0.5 1000 0.5 - - Pod Core controller managing CRDs UI Frontend 1 0.2 400 0.5 - - Pod React application UI Backend 1 0.5 1000 1 - - Pod Node.js API server Keycloak 1 0.25 500 0.5 - - Pod Identity and access management Keycloak PostgreSQL 1 0.25 500 0.25 5 - Pod Keycloak database Kamaji Controller 1 0.5 1000 0.25 - - Pod Tenant control plane manager Kamaji etcd 3 0.5 1000 0.5 - 10 StatefulSet Shared etcd for all tenant control planes (default) Cert-Manager 1 0.2 400 0.25 - - Pod Certificate automation Envoy Gateway 2 0.5 1000 0.5 - - Deployment Ingress controller noVNC 1 0.1 200 0.25 - - Pod VM console access <p>Subtotal (Core Platform): ~5 CPU cores, ~5 GB RAM, ~15 GB storage</p>"},{"location":"prd/infrastructure-requirements/#monitoring-stack-shared","title":"Monitoring Stack (Shared)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes Prometheus 1 1 2000 2 - 20 StatefulSet Metrics storage and alerting Prometheus Operator 1 0.5 1000 0.25 - - Pod Prometheus lifecycle manager Alertmanager 1 0.5 1000 0.25 - - Pod Alert routing and notification Grafana 1 0.5 1000 0.5 - - Pod Visualization dashboards Loki Backend 1 0.5 1000 1 - - Pod Log aggregation backend Loki Read 1 0.5 1000 0.5 - - Pod Log query service Loki Write 1 0.5 1000 0.5 - - Pod Log ingestion service Loki Minio 1 0.5 1000 0.25 - 10 StatefulSet Object storage for logs (2 drives) Grafana Alloy 1+ 0.2 400 0.25 - - DaemonSet Metrics/logs collector (per node) Node Exporter 1+ 0.1 200 0.06 - - DaemonSet Node metrics (per node) Kube State Metrics 1 0.2 400 0.12 - - Pod Kubernetes metrics <p>Subtotal (Monitoring): ~5 CPU cores, ~6 GB RAM, ~30 GB storage</p>"},{"location":"prd/infrastructure-requirements/#networking-stack-shared","title":"Networking Stack (Shared)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes Kube-OVN Controller 1 1 2000 1 - - Pod OVN network controller Kube-OVN CNI 1+ 0.5 1000 0.5 - - DaemonSet CNI plugin (per node) OVN Central 3 0.5 1000 1 - - StatefulSet OVN database cluster Multus CNI 1+ 0.1 200 0.25 - - DaemonSet Multi-network support (per node) <p>Subtotal (Networking): ~4 CPU cores, ~5 GB RAM</p>"},{"location":"prd/infrastructure-requirements/#virtualization-stack-shared","title":"Virtualization Stack (Shared)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes KubeVirt Operator 1 0.5 1000 0.5 - - Pod VM lifecycle manager KubeVirt Handler 1+ 0.2 400 0.25 - - DaemonSet VM runtime (per node) CDI Operator 1 0.5 1000 0.5 - - Pod DataVolume manager CDI Controller 1 0.2 400 0.25 - - Pod Import/upload controller <p>Subtotal (Virtualization): ~2 CPU cores, ~2 GB RAM</p>"},{"location":"prd/infrastructure-requirements/#cluster-api-stack-shared","title":"Cluster API Stack (Shared)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes CAPI Controller 1 0.5 1000 0.5 - - Pod Cluster API core CAPI KubeVirt Provider 1 0.2 400 0.25 - - Pod KubeVirt infrastructure provider CAPI K3s Provider 1 0.2 400 0.25 - - Pod K3s bootstrap/control plane Sveltos 1 0.2 400 0.25 - - Pod Addon deployment controller Kyverno 3 0.2 400 0.25 - - Deployment Policy engine <p>Subtotal (Cluster API): ~2 CPU cores, ~2 GB RAM</p>"},{"location":"prd/infrastructure-requirements/#management-cluster-total-base-infrastructure","title":"Management Cluster Total (Base Infrastructure)","text":"Category CPU Cores RAM (GB) Storage (GB) Core Platform ~5 ~5 ~15 Monitoring ~5 ~6 ~30 Networking ~4 ~5 - Virtualization ~2 ~2 - Cluster API ~2 ~2 - TOTAL ~18 ~20 ~45 <p>Note: This excludes: - OS and system processes overhead (~2 CPU, ~4 GB RAM) - etcd for Kubernetes control plane (~1 CPU, ~2 GB RAM, ~20 GB storage) - RKE2/K3s components (~2 CPU, ~4 GB RAM)</p> <p>Recommended Management Cluster Minimum: 24 CPU cores, 32 GB RAM, 100 GB storage per node</p>"},{"location":"prd/infrastructure-requirements/#per-tenant-cluster-components-multiply-by-number-of-clusters","title":"Per-Tenant Cluster Components (Multiply by Number of Clusters)","text":"<p>These components are deployed for EACH tenant Kubernetes cluster created via KdcCluster.</p>"},{"location":"prd/infrastructure-requirements/#control-plane-kamaji-tenantcontrolplane","title":"Control Plane (Kamaji TenantControlPlane)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes kube-apiserver 2 0.5 1000 1 - - Pod Kubernetes API server (HA) kube-controller-manager 2 0.5 1000 0.5 - - Pod K8s controller manager kube-scheduler 2 0.2 400 0.25 - - Pod K8s scheduler <p>Subtotal per Cluster (Control Plane with shared etcd): ~2.4 CPU cores, ~3.5 GB RAM</p>"},{"location":"prd/infrastructure-requirements/#dedicated-etcd-optional-per-cluster","title":"Dedicated etcd (Optional, per Cluster)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes etcd 3 0.5 1000 0.5 - 10 StatefulSet Dedicated etcd cluster (if not using shared) <p>Subtotal (Dedicated etcd): ~1.5 CPU cores, ~1.5 GB RAM, ~30 GB storage</p>"},{"location":"prd/infrastructure-requirements/#worker-nodes-configurable-per-cluster","title":"Worker Nodes (Configurable per Cluster)","text":"<p>Worker nodes are provisioned as VMs via Cluster API. Resources depend on tenant requirements.</p>"},{"location":"prd/infrastructure-requirements/#example-worker-configurations","title":"Example Worker Configurations","text":"<p>Minimal Worker: - CPU: 1 core (2000 MHz) - RAM: 1 GB - Disk: 20 GB</p> <p>Standard Worker: - CPU: 2 cores (4000 MHz) - RAM: 4 GB - Disk: 40 GB</p> <p>High-Memory Worker: - CPU: 4 cores (8000 MHz) - RAM: 16 GB - Disk: 80 GB</p>"},{"location":"prd/infrastructure-requirements/#cloudsigma-specific-components-per-cluster-using-cloudsigma","title":"CloudSigma-Specific Components (Per Cluster using CloudSigma)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes CloudSigma CCM 1 0.2 400 0.1 - - DaemonSet Cloud controller manager (on control plane) CloudSigma CSI Controller 1 0.5 1000 0.25 - - Deployment Storage provisioner controller CloudSigma CSI Node 1+ 0.5 1000 0.25 - - DaemonSet CSI node plugin (per worker) <p>Subtotal (CloudSigma integration per cluster): ~1.2 CPU cores, ~0.6 GB RAM</p>"},{"location":"prd/infrastructure-requirements/#cninetworking-per-cluster-if-using-cilium","title":"CNI/Networking (Per Cluster - if using Cilium)","text":"Component Nb Instances CPU (cores) CPU (MHz) RAM per Instance (GB) DISK per Instance (GB) Unmounted Drives (GB) Type Notes Cilium Agent 1+ 0.1 200 0.5 - - DaemonSet CNI agent (per node) Cilium Operator 1 0.1 200 0.12 - - Deployment Cilium operator <p>Subtotal (Cilium per cluster): ~0.2 CPU cores (+ per node), ~0.62 GB RAM</p>"},{"location":"prd/infrastructure-requirements/#per-tenant-cluster-total","title":"Per-Tenant Cluster Total","text":""},{"location":"prd/infrastructure-requirements/#configuration-1-minimal-shared-etcd-2-minimal-workers","title":"Configuration 1: Minimal (Shared etcd, 2 minimal workers)","text":"Component CPU Cores RAM (GB) Storage (GB) Control Plane (2 replicas) 2.4 3.5 - Workers (2x minimal) 2 2 40 CloudSigma Integration 1.2 0.6 - Cilium CNI 0.4 1.1 - TOTAL 6 7.2 40"},{"location":"prd/infrastructure-requirements/#configuration-2-standard-shared-etcd-3-standard-workers","title":"Configuration 2: Standard (Shared etcd, 3 standard workers)","text":"Component CPU Cores RAM (GB) Storage (GB) Control Plane (2 replicas) 2.4 3.5 - Workers (3x standard) 6 12 120 CloudSigma Integration 1.7 1.35 - Cilium CNI 0.5 1.62 - TOTAL 10.6 18.47 120"},{"location":"prd/infrastructure-requirements/#configuration-3-production-dedicated-etcd-3-standard-workers","title":"Configuration 3: Production (Dedicated etcd, 3 standard workers)","text":"Component CPU Cores RAM (GB) Storage (GB) Control Plane (2 replicas) 2.4 3.5 - Dedicated etcd (3 replicas) 1.5 1.5 30 Workers (3x standard) 6 12 120 CloudSigma Integration 1.7 1.35 - Cilium CNI 0.5 1.62 - TOTAL 12.1 19.97 150"},{"location":"prd/infrastructure-requirements/#example-deployment-scenarios","title":"Example Deployment Scenarios","text":""},{"location":"prd/infrastructure-requirements/#scenario-1-small-platform-5-tenant-clusters","title":"Scenario 1: Small Platform (5 Tenant Clusters)","text":"Infrastructure CPU Cores RAM (GB) Storage (GB) Management Cluster Base 18 20 45 5x Tenant Clusters (Standard) 53 92.35 600 OS/System Overhead (3 nodes) 6 12 60 TOTAL 77 124.35 705 <p>Recommended Infrastructure: - 3x Master/Worker nodes: 32 cores, 48 GB RAM, 300 GB SSD each</p>"},{"location":"prd/infrastructure-requirements/#scenario-2-medium-platform-20-tenant-clusters","title":"Scenario 2: Medium Platform (20 Tenant Clusters)","text":"Infrastructure CPU Cores RAM (GB) Storage (GB) Management Cluster Base 18 20 45 20x Tenant Clusters (Standard) 212 369.4 2400 OS/System Overhead (6 nodes) 12 24 120 TOTAL 242 413.4 2565 <p>Recommended Infrastructure: - 3x Master nodes: 32 cores, 64 GB RAM, 500 GB SSD each - 6x Worker nodes: 48 cores, 96 GB RAM, 1 TB SSD each</p>"},{"location":"prd/infrastructure-requirements/#scenario-3-large-platform-50-tenant-clusters","title":"Scenario 3: Large Platform (50 Tenant Clusters)","text":"Infrastructure CPU Cores RAM (GB) Storage (GB) Management Cluster Base 18 20 45 50x Tenant Clusters (Standard) 530 923.5 6000 OS/System Overhead (12 nodes) 24 48 240 TOTAL 572 991.5 6285 <p>Recommended Infrastructure: - 3x Master nodes: 64 cores, 128 GB RAM, 1 TB SSD each - 12x Worker nodes: 64 cores, 128 GB RAM, 2 TB SSD each</p>"},{"location":"prd/infrastructure-requirements/#storage-requirements-breakdown","title":"Storage Requirements Breakdown","text":""},{"location":"prd/infrastructure-requirements/#management-cluster-persistent-storage","title":"Management Cluster Persistent Storage","text":"Volume Type Size per Instance Instances Total Purpose Kamaji etcd 10 GB 3 30 GB Shared etcd datastore Prometheus 20 GB 1 20 GB Metrics storage (configurable: 365d retention) Loki Minio 10 GB 1 10 GB Log storage (7d retention) Keycloak PostgreSQL 5 GB 1 5 GB User database TOTAL - - 65 GB -"},{"location":"prd/infrastructure-requirements/#per-tenant-cluster-storage","title":"Per-Tenant Cluster Storage","text":"Volume Type Size per Instance Instances Total Purpose Dedicated etcd (optional) 10 GB 3 30 GB Per-cluster etcd Worker OS Disk 20-80 GB N Variable OS and container images Tenant PVCs Variable Variable Variable Application storage via CSI"},{"location":"prd/infrastructure-requirements/#network-requirements","title":"Network Requirements","text":""},{"location":"prd/infrastructure-requirements/#bandwidth","title":"Bandwidth","text":"<ul> <li>Management Cluster Internal: 10 Gbps recommended (node-to-node)</li> <li>External Access: 1 Gbps minimum for API/UI</li> <li>Storage Network (if separate): 10 Gbps recommended</li> </ul>"},{"location":"prd/infrastructure-requirements/#ip-address-requirements","title":"IP Address Requirements","text":""},{"location":"prd/infrastructure-requirements/#management-cluster-kube-ovn","title":"Management Cluster (Kube-OVN)","text":"<ul> <li>Pod CIDR: /16 recommended (65,536 IPs)</li> <li>Service CIDR: /16 recommended (65,536 IPs)</li> <li>Per Project VPC: /16 (configurable)</li> </ul>"},{"location":"prd/infrastructure-requirements/#per-tenant-cluster","title":"Per-Tenant Cluster","text":"<ul> <li>Pod CIDR: /16 (65,536 IPs per cluster)</li> <li>Service CIDR: /16 (65,536 IPs per cluster)</li> </ul>"},{"location":"prd/infrastructure-requirements/#external-ips","title":"External IPs","text":"<ul> <li>Management cluster: 1-5 public IPs (UI, API, monitoring)</li> <li>Per tenant cluster: Variable (depends on LoadBalancer services)</li> </ul>"},{"location":"prd/infrastructure-requirements/#notes-and-recommendations","title":"Notes and Recommendations","text":""},{"location":"prd/infrastructure-requirements/#scaling-considerations","title":"Scaling Considerations","text":"<ol> <li>Horizontal Scaling:</li> <li>Add worker nodes to management cluster as tenant cluster count grows</li> <li>Management cluster workers can be scaled independently</li> <li> <p>Each worker node can host ~10-20 tenant control plane pods</p> </li> <li> <p>Vertical Scaling:</p> </li> <li>Increase worker node resources for larger tenant worker VMs</li> <li> <p>Monitoring stack may need scaling with high pod/metric count</p> </li> <li> <p>Storage Scaling:</p> </li> <li>Prometheus retention and storage grow with cluster count</li> <li>Consider external object storage for Loki at scale</li> <li>Plan for ~50 GB additional storage per 10 tenant clusters</li> </ol>"},{"location":"prd/infrastructure-requirements/#resource-overhead","title":"Resource Overhead","text":"<ul> <li>Kubernetes System: ~10-15% CPU/RAM overhead</li> <li>VM Runtime (KubeVirt): ~10-20% overhead per VM</li> <li>Network Overhead: ~5-10% CPU for OVN/CNI</li> </ul>"},{"location":"prd/infrastructure-requirements/#high-availability","title":"High Availability","text":"<ul> <li>Management Cluster: 3+ master nodes, 3+ worker nodes</li> <li>etcd: Always use 3 or 5 replicas (odd number)</li> <li>Control Planes: 2+ replicas per tenant cluster</li> <li>Monitoring: Consider external storage for production</li> </ul>"},{"location":"prd/infrastructure-requirements/#cost-optimization","title":"Cost Optimization","text":"<ol> <li>Use shared etcd for non-critical tenant clusters</li> <li>Right-size worker VMs based on actual workload</li> <li>Enable cluster autoscaling for variable workloads</li> <li>Use lower retention for Prometheus/Loki in dev environments</li> </ol>"},{"location":"prd/infrastructure-requirements/#actual-production-cluster-analysis","title":"Actual Production Cluster Analysis","text":""},{"location":"prd/infrastructure-requirements/#current-deployment-stagekube-dccom","title":"Current Deployment (stage.kube-dc.com)","text":"<p>Management Cluster Hardware: - Node Count: 2 nodes (kube-dc-master-1, kube-dc-worker-1) - Per Node: 8 CPU cores, 64 GB RAM, Debian 12 - Total Capacity: 16 cores, 128 GB RAM</p> <p>Resource Utilization (Live): - kube-dc-master-1: 40% CPU (3.2 cores), 49% RAM (31 GB) - kube-dc-worker-1: 33% CPU (2.7 cores), 64% RAM (41 GB) - Total Used: ~6 cores (37%), ~72 GB RAM (56%)</p> <p>Tenant Clusters Hosted: 7 active KdcClusters</p> Namespace Cluster Name Control Plane Replicas Worker Replicas Datastore Type cloudsigma-example-3415fbd3 banja 1 2 banja-etcd (dedicated) cloudsigma-example-3415fbd3 xxx 1 1 banja-etcd (shared) cloudsigma-tan-dovan-90e02d2d c1 1 3 c1-etcd (dedicated) shalb-demo demo-cluster 1 1 demo-cluster-etcd (dedicated) shalb-demo k3s-cluster 2 1 k3s-cluster-etcd (dedicated) shalb-demo my-cluster 2 2 my-cluster-etcd (dedicated) shalb-envoy three-cluster 2 1 three-cluster-etcd (dedicated)"},{"location":"prd/infrastructure-requirements/#tenant-cluster-resource-consumption-analysis","title":"Tenant Cluster Resource Consumption Analysis","text":""},{"location":"prd/infrastructure-requirements/#example-banja-and-xxx-clusters-cloudsigma-example-3415fbd3","title":"Example: \"banja\" and \"xxx\" Clusters (cloudsigma-example-3415fbd3)","text":"<p>Control Plane Pods (Management Cluster):</p> Component Pod Count CPU Request Memory Request Actual Location banja-cp 1 None None kube-dc-worker-1 banja-etcd 1 None None kube-dc-worker-1 xxx-cp 1 None None kube-dc-worker-1 csccm-banja (CCM) 1 10m 64Mi kube-dc-worker-1 csccm-xxx (CCM) 1 10m 64Mi kube-dc-worker-1 <p>Worker VMs (Customer CloudSigma Account): - Banja: 2 workers (2 cores, 4GB each) = 4 cores, 8GB total - xxx: 1 worker (2 cores, 4GB) = 2 cores, 4GB total</p> <p>Key Observations:</p> <ol> <li>No Resource Limits: Most tenant control plane pods have no CPU/memory requests, relying on cluster overcommit</li> <li>Shared etcd: xxx cluster uses banja's etcd datastore (cost optimization)</li> <li>Lightweight Control Planes: Each tenant CP consumes ~0.5-1 GB RAM in practice</li> <li>Management Overhead: CloudSigma CCM pods: 10m CPU, 64Mi RAM per cluster</li> <li>Worker VMs on Customer Infrastructure: Worker nodes NOT visible in management cluster metrics</li> </ol>"},{"location":"prd/infrastructure-requirements/#platform-components-resource-usage","title":"Platform Components Resource Usage","text":"<p>Core Platform (kube-dc namespace):</p> Component CPU Request Memory Request Notes kube-dc-manager None None Core controller kube-dc-backend None None API server kube-dc-frontend None None React UI kube-dc-k8-manager 10m 64Mi Tenant cluster controller <p>Monitoring Stack (monitoring namespace):</p> Component CPU Request Memory Request Storage Prometheus None 1Gi 20Gi Grafana None None - Alertmanager None 128Mi - Loki Backend None None - Loki Write None 256Mi - Loki Read None 256Mi - Loki Minio 100m 128Mi 10Gi Node Exporter (per node) None 32Mi - <p>Shared Infrastructure (kamaji-system namespace):</p> Component CPU Request Memory Request Storage Kamaji Controller 100m 20Mi - Kamaji etcd (3 replicas) None None 30Gi (3x10Gi) CAPI Kamaji Controller 10m 64Mi -"},{"location":"prd/infrastructure-requirements/#actual-vs-theoretical-requirements","title":"Actual vs Theoretical Requirements","text":"<p>Current Cluster Efficiency: - 7 tenant clusters running on 16 cores, 128 GB RAM - Average per cluster: ~0.9 cores, ~10 GB RAM (management overhead only) - Platform base: ~4 cores, ~20 GB RAM - Remaining capacity: ~6 cores, ~36 GB RAM for additional tenants</p> <p>Scaling Projection: - Current 2-node cluster can support ~10-15 tenant clusters before CPU/RAM constraints - With 3-node architecture (24 cores, 192 GB): ~20-30 tenant clusters - Adding 1 node per 10 tenant clusters is recommended for stability</p> <p>Resource Optimization Observed: - Resource requests not set on most components \u2192 cluster overcommit strategy - Shared etcd reduces storage requirements (xxx uses banja's etcd) - Control plane memory footprint lower than theoretical (0.5-1GB vs 1-2GB estimated)</p>"},{"location":"prd/infrastructure-requirements/#storage-usage-persistent-volumes","title":"Storage Usage (Persistent Volumes)","text":"<p>Management Cluster Storage: - Kamaji shared etcd: 30 GB (3x 10GB PVCs) - Prometheus: 20 GB - Loki/Minio: 10 GB - Keycloak PostgreSQL: ~5 GB - Per tenant etcd (dedicated): 10-30 GB (1-3 replicas)</p> <p>Total Storage in Use: ~500-600 GB across all tenant etcd instances + monitoring</p>"},{"location":"prd/infrastructure-requirements/#cloudsigma-platform-capacity-planning","title":"CloudSigma Platform Capacity Planning","text":""},{"location":"prd/infrastructure-requirements/#proposed-hardware-configuration","title":"Proposed Hardware Configuration","text":"<p>Management Cluster Infrastructure:</p> VM Role Nb VMs CPU Cores CPU (GHz) RAM (GB) Disk (GB) Backup (GB) Purpose Master-1 1 8 16 32 200 300 K8s control plane + tenant CPs Master-2 1 8 16 32 200 300 K8s control plane + tenant CPs Master-3 1 8 16 32 200 300 K8s control plane + tenant CPs Worker-1 1 16 32 64 250 300 Tenant CPs + monitoring Worker-2 1 16 32 64 250 300 Tenant CPs + monitoring Management VM 1 4 16 32 200 300 Optional dedicated management TOTAL 6 60 128 256 1300 1800 - <p>Configuration Notes: - All 5 nodes (3 masters + 2 workers) can host tenant control planes - Management VM can be used for backups, monitoring, or additional capacity - Total usable for tenant CPs: 52 cores, 224 GB RAM (after K8s overhead)</p>"},{"location":"prd/infrastructure-requirements/#per-tenant-cluster-resource-consumption-cloudsigma","title":"Per-Tenant Cluster Resource Consumption (CloudSigma)","text":""},{"location":"prd/infrastructure-requirements/#components-per-tenant-cluster","title":"Components per Tenant Cluster:","text":"<p>1. Tenant Control Plane Pod (on Management Cluster) - kube-apiserver: ~200-300Mi RAM, 50-100m CPU - kube-controller-manager: ~150-200Mi RAM, 25-50m CPU - kube-scheduler: ~50-100Mi RAM, 10-25m CPU - Total per TCP: ~500Mi RAM, ~100m CPU</p> <p>2. CloudSigma Cloud Controller Manager (per cluster) - csccm pod: 64Mi RAM, 10m CPU (observed) - Total: 64Mi RAM, 10m CPU</p> <p>3. etcd Datastore (50% 1-node / 50% 3-node)</p> <p>1-node etcd (50% of clusters): - etcd pod: 512Mi RAM, 100m CPU, 10Gi storage - Total: 512Mi RAM, 100m CPU, 10Gi</p> <p>3-node etcd (50% of clusters): - etcd pods (3x): 1536Mi RAM (512Mi\u00d73), 300m CPU (100m\u00d73), 30Gi storage (10Gi\u00d73) - Total: 1536Mi RAM, 300m CPU, 30Gi</p>"},{"location":"prd/infrastructure-requirements/#average-per-cluster-mixed-etcd-topology","title":"Average per Cluster (Mixed etcd Topology)","text":"Component CPU (cores) RAM (GB) Storage (GB) Notes Control Plane Pod 0.1 0.5 - 3-in-1 pod CloudSigma CCM 0.01 0.06 - Per cluster etcd (1-node) 0.1 0.5 10 50% of clusters etcd (3-node) 0.3 1.5 30 50% of clusters Average 0.21 1.06 20 Weighted avg Conservative 0.3 1.5 20 With overhead"},{"location":"prd/infrastructure-requirements/#capacity-calculation","title":"Capacity Calculation","text":""},{"location":"prd/infrastructure-requirements/#total-available-resources-after-k8s-overhead-15","title":"Total Available Resources (after K8s overhead ~15%)","text":"Resource Total K8s Overhead (15%) Available Reserved for Platform (20%) Usable for Tenants CPU Cores 60 9 51 10 41 cores RAM (GB) 256 38 218 44 174 GB Storage (GB) 1300 - 1300 100 1200 GB"},{"location":"prd/infrastructure-requirements/#capacity-by-constraint","title":"Capacity by Constraint","text":"<p>CPU Constraint: - Usable CPU: 41 cores - Per cluster: 0.3 cores - Capacity: ~136 clusters</p> <p>Memory Constraint: - Usable RAM: 174 GB - Per cluster: 1.5 GB - Capacity: ~116 clusters</p> <p>Storage Constraint: - Usable Storage: 1200 GB - Per cluster: 20 GB (average) - Capacity: ~60 clusters</p> <p>Bottleneck: Storage (most restrictive)</p>"},{"location":"prd/infrastructure-requirements/#recommended-capacity-50-60-tenant-clusters","title":"Recommended Capacity: 50-60 Tenant Clusters","text":""},{"location":"prd/infrastructure-requirements/#breakdown-by-etcd-topology-5050-split","title":"Breakdown by etcd Topology (50/50 split)","text":"Cluster Type Count etcd Nodes etcd Storage TCP + CCM Resources 1-node etcd 25-30 25-30 250-300 GB 25-30 \u00d7 (0.11 cores, 0.56 GB) 3-node etcd 25-30 75-90 750-900 GB 25-30 \u00d7 (0.11 cores, 0.56 GB) TOTAL 50-60 100-120 1000-1200 GB 5.5-6.6 cores, 28-34 GB"},{"location":"prd/infrastructure-requirements/#resource-distribution-across-nodes","title":"Resource Distribution Across Nodes","text":"<p>Assuming 60 tenant clusters evenly distributed:</p> Node Role TCP Pods etcd Pods CPU Used RAM Used Storage Used Master-1 K8s CP + TCP host 12 20 ~6 cores ~20 GB ~200 GB Master-2 K8s CP + TCP host 12 20 ~6 cores ~20 GB ~200 GB Master-3 K8s CP + TCP host 12 20 ~6 cores ~20 GB ~200 GB Worker-1 TCP + Monitoring 12 20 ~7 cores ~25 GB ~250 GB Worker-2 TCP + Monitoring 12 20 ~7 cores ~25 GB ~250 GB TOTAL - 60 100 32 cores 110 GB 1100 GB"},{"location":"prd/infrastructure-requirements/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"prd/infrastructure-requirements/#to-increase-capacity-beyond-60-clusters","title":"To Increase Capacity Beyond 60 Clusters:","text":"<p>Option 1: Add Storage - Expand disk on Worker-1 and Worker-2 to 500 GB each - New Capacity: ~100 clusters (limited by CPU/RAM)</p> <p>Option 2: Add Worker Nodes - Add 2 more workers: 32 cores, 128 GB RAM, 500 GB disk each - New Capacity: ~150 clusters</p> <p>Option 3: Use Shared etcd (Kamaji default) - 3-node shared etcd for all clusters instead of dedicated - Saves: ~950 GB storage, ~20 cores, ~80 GB RAM - New Capacity: 200+ clusters (limited by control plane density)</p> <p>Option 4: External etcd Storage - Move etcd storage to separate storage cluster or cloud object storage - Management cluster only hosts control plane pods - New Capacity: 300+ clusters (Kamaji scaling limit per management cluster)</p>"},{"location":"prd/infrastructure-requirements/#kamaji-scaling-best-practices","title":"Kamaji Scaling Best Practices","text":"<p>From Kamaji Official Documentation:</p> <ol> <li>Control Plane Density: 50-100 tenant control planes per management node recommended</li> <li>etcd Sizing: </li> <li>Shared etcd: 3-node cluster can handle 100+ tenant clusters</li> <li>Dedicated etcd: 1 or 3 nodes per cluster based on SLA requirements</li> <li>Resource Overcommit: Kamaji leverages Kubernetes QoS, safe to run without strict resource limits</li> <li>High Availability: Distribute TCP pods across nodes using pod anti-affinity</li> </ol>"},{"location":"prd/infrastructure-requirements/#production-recommendations","title":"Production Recommendations","text":"<p>Conservative (50 clusters): - Current hardware configuration as-is - 25 clusters with 1-node etcd (dev/test) - 25 clusters with 3-node etcd (production) - Leaves 20% headroom for growth</p> <p>Standard (60 clusters): - Current hardware configuration - 30 clusters with 1-node etcd - 30 clusters with 3-node etcd - Uses ~85% of available storage</p> <p>Aggressive (100+ clusters): - Add storage expansion or additional worker nodes - Use shared etcd for majority of clusters - Reserve dedicated etcd for critical production clusters only</p>"},{"location":"prd/infrastructure-requirements/#customer-infrastructure-not-counted-in-management-cluster","title":"Customer Infrastructure (Not Counted in Management Cluster)","text":"<p>Per Tenant Cluster Workers (CloudSigma): - Worker VMs: Run on customer's CloudSigma account - CSI Driver storage: Customer's CloudSigma disks - Networking: Customer's CloudSigma IPs/VLANs</p> <p>Example Customer Workload: - 3 workers \u00d7 (4 cores, 8 GB) = 12 cores, 24 GB on customer side - 100 GB storage per worker via CSI = 300 GB on customer CloudSigma</p> <p>Management cluster only hosts control plane - customer workloads NOT included in above calculations.</p> <p>Last Updated: January 2025 Document Version: 2.1 - Added CloudSigma capacity planning with Kamaji scaling guidelines</p>"},{"location":"prd/kube-dc-cli/","title":"Kube-DC CLI - Browser-Based Authentication Tool","text":""},{"location":"prd/kube-dc-cli/#problem-statement","title":"Problem Statement","text":"<p>The current kubeconfig setup for Kube-DC users is cumbersome and requires multiple manual steps:</p> <ol> <li>Copy-paste shell script from UI</li> <li>Download and execute bash scripts</li> <li>Manually enter environment variables</li> <li>Source activation scripts</li> <li>Enter username/password in terminal when tokens expire</li> </ol> <p>This approach has several issues:</p>"},{"location":"prd/kube-dc-cli/#current-pain-points","title":"Current Pain Points","text":"Issue Impact Shell-based scripts Not cross-platform (bash/zsh only) Password entry in terminal Security risk (history, shoulder surfing) Manual refresh prompts Poor UX when tokens expire Complex setup process High barrier to entry for new users No kubectx/kubens compatibility Conflicts with popular tools Overwrites existing kubeconfig Risk of losing other cluster configs"},{"location":"prd/kube-dc-cli/#user-expectations-cloud-cli-patterns","title":"User Expectations (Cloud CLI Patterns)","text":"<p>Users are accustomed to cloud provider CLIs that provide seamless authentication:</p> <ul> <li>AWS CLI: <code>aws sso login</code> \u2192 browser OAuth \u2192 seamless for weeks</li> <li>GCloud: <code>gcloud auth login</code> \u2192 browser OAuth \u2192 auto-refresh</li> <li>DigitalOcean: <code>doctl auth init</code> \u2192 browser OAuth \u2192 token cached</li> <li>Azure: <code>az login</code> \u2192 browser OAuth \u2192 device code flow option</li> </ul> <p>Kube-DC should follow these established patterns.</p>"},{"location":"prd/kube-dc-cli/#requirements","title":"Requirements","text":""},{"location":"prd/kube-dc-cli/#functional-requirements","title":"Functional Requirements","text":"ID Requirement Priority FR-1 Browser-based OAuth login (PKCE flow) High FR-2 Automatic token refresh using refresh_token High FR-3 Exec credential plugin for kubectl integration High FR-4 Support multiple organizations/realms High FR-5 Preserve existing kubeconfig entries High FR-6 kubectx/kubens compatibility High FR-7 Namespace switching based on JWT claims Medium FR-8 Context switching between projects Medium FR-9 Offline token validation (JWT expiry check) Medium FR-10 Device code flow for headless environments Low"},{"location":"prd/kube-dc-cli/#non-functional-requirements","title":"Non-Functional Requirements","text":"ID Requirement Priority NFR-1 Single binary, no external dependencies High NFR-2 Cross-platform (Linux, macOS, Windows) High NFR-3 Credential storage with 0600 permissions High NFR-4 &lt; 20MB binary size Medium NFR-5 &lt; 100ms startup time for credential command Medium NFR-6 Works without internet for cached tokens Medium"},{"location":"prd/kube-dc-cli/#solution-design","title":"Solution Design","text":""},{"location":"prd/kube-dc-cli/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              User Workflow                                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                              \u2502\n\u2502   $ kube-dc login --server https://api.kube-dc.cloud                        \u2502\n\u2502         \u2502                                                                    \u2502\n\u2502         \u25bc                                                                    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502   \u2502 Start local \u2502\u2500\u2500\u2500\u2500\u25ba\u2502 Open browser\u2502\u2500\u2500\u2500\u2500\u25ba\u2502 Keycloak login page     \u2502       \u2502\n\u2502   \u2502 HTTP server \u2502     \u2502 to auth URL \u2502     \u2502 (organization realm)    \u2502       \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502         \u25b2                                              \u2502                     \u2502\n\u2502         \u2502              OAuth callback                  \u2502                     \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502                                                                              \u2502\n\u2502   Tokens cached to ~/.kube-dc/credentials/                                   \u2502\n\u2502   Kubeconfig updated in ~/.kube/config                                       \u2502\n\u2502                                                                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                              \u2502\n\u2502   $ kubectl get pods    \u25c4\u2500\u2500 Uses exec credential plugin                     \u2502\n\u2502         \u2502                                                                    \u2502\n\u2502         \u25bc                                                                    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502   \u2502  kubectl exec: kube-dc credential --server https://api...       \u2502       \u2502\n\u2502   \u2502                      \u2502                                          \u2502       \u2502\n\u2502   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502       \u2502\n\u2502   \u2502     \u25bc                                 \u25bc                         \u2502       \u2502\n\u2502   \u2502  Token valid?                    Token expired?                 \u2502       \u2502\n\u2502   \u2502     \u2502                                 \u2502                         \u2502       \u2502\n\u2502   \u2502     \u25bc                                 \u25bc                         \u2502       \u2502\n\u2502   \u2502  Return token               Use refresh_token                   \u2502       \u2502\n\u2502   \u2502                                       \u2502                         \u2502       \u2502\n\u2502   \u2502                                       \u25bc                         \u2502       \u2502\n\u2502   \u2502                             New tokens cached                   \u2502       \u2502\n\u2502   \u2502                             Return access_token                 \u2502       \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/kube-dc-cli/#directory-structure","title":"Directory Structure","text":"<pre><code>cli/                                    # Source code location\n\u251c\u2500\u2500 cmd/\n\u2502   \u2514\u2500\u2500 kube-dc/\n\u2502       \u2514\u2500\u2500 main.go                     # Entry point\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 auth/\n\u2502   \u2502   \u251c\u2500\u2500 oauth.go                    # Browser-based OAuth flow\n\u2502   \u2502   \u251c\u2500\u2500 pkce.go                     # PKCE code challenge\n\u2502   \u2502   \u251c\u2500\u2500 callback.go                 # Local HTTP callback server\n\u2502   \u2502   \u251c\u2500\u2500 token.go                    # Token refresh logic\n\u2502   \u2502   \u2514\u2500\u2500 device.go                   # Device code flow (optional)\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u251c\u2500\u2500 config.go                   # CLI config management\n\u2502   \u2502   \u2514\u2500\u2500 credentials.go              # Token storage\n\u2502   \u251c\u2500\u2500 kubeconfig/\n\u2502   \u2502   \u251c\u2500\u2500 manager.go                  # Kubeconfig read/write\n\u2502   \u2502   \u251c\u2500\u2500 merge.go                    # Merge with existing config\n\u2502   \u2502   \u2514\u2500\u2500 context.go                  # Context management\n\u2502   \u2514\u2500\u2500 jwt/\n\u2502       \u2514\u2500\u2500 claims.go                   # JWT parsing for namespaces\n\u251c\u2500\u2500 pkg/\n\u2502   \u2514\u2500\u2500 credential/\n\u2502       \u2514\u2500\u2500 exec.go                     # Exec credential plugin\n\u251c\u2500\u2500 go.mod\n\u2514\u2500\u2500 go.sum\n\n~/.kube-dc/                             # User data location\n\u251c\u2500\u2500 config.yaml                         # CLI configuration\n\u2514\u2500\u2500 credentials/\n    \u2514\u2500\u2500 {server-hash}.json              # Cached tokens per server\n\n~/.kube/config                          # Standard kubeconfig (modified)\n</code></pre>"},{"location":"prd/kube-dc-cli/#kubeconfig-integration-strategy","title":"Kubeconfig Integration Strategy","text":"<p>Key Principle: Never overwrite or remove non-kube-dc entries.</p> <pre><code># ~/.kube/config - Example after kube-dc login\n\napiVersion: v1\nkind: Config\n\n# Existing clusters preserved\nclusters:\n- name: production-aws              # \u2190 Existing, untouched\n  cluster:\n    server: https://k8s.example.com\n- name: minikube                    # \u2190 Existing, untouched\n  cluster:\n    server: https://192.168.49.2:8443\n- name: kube-dc-cloud               # \u2190 Added by kube-dc CLI\n  cluster:\n    server: https://api.kube-dc.cloud:6443\n    certificate-authority-data: LS0tLS1...\n\n# Existing users preserved\nusers:\n- name: aws-user                    # \u2190 Existing, untouched\n  user:\n    exec:\n      command: aws\n      args: [\"eks\", \"get-token\", ...]\n- name: minikube                    # \u2190 Existing, untouched\n  user:\n    client-certificate: ~/.minikube/...\n- name: kube-dc@shalb               # \u2190 Added by kube-dc CLI\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1\n      command: kube-dc\n      args: [\"credential\", \"--server\", \"https://api.kube-dc.cloud\"]\n      interactiveMode: IfAvailable\n\n# Existing contexts preserved\ncontexts:\n- name: production                  # \u2190 Existing, untouched\n  context:\n    cluster: production-aws\n    user: aws-user\n- name: minikube                    # \u2190 Existing, untouched\n  context:\n    cluster: minikube\n    user: minikube\n- name: kube-dc/shalb/demo          # \u2190 Added by kube-dc CLI\n  context:\n    cluster: kube-dc-cloud\n    user: kube-dc@shalb\n    namespace: shalb-demo\n- name: kube-dc/shalb/dev           # \u2190 Added by kube-dc CLI\n  context:\n    cluster: kube-dc-cloud\n    user: kube-dc@shalb\n    namespace: shalb-dev\n\ncurrent-context: kube-dc/shalb/demo  # \u2190 Updated by kube-dc use\n</code></pre>"},{"location":"prd/kube-dc-cli/#kubectxkubens-compatibility","title":"kubectx/kubens Compatibility","text":"<p>Context Naming Convention: <code>kube-dc/{org}/{project}</code></p> <p>This allows: <pre><code># Works with kubectx\n$ kubectx\nminikube\nproduction\nkube-dc/shalb/demo      \u2190 Kube-DC contexts clearly identified\nkube-dc/shalb/dev\nkube-dc/shalb/envoy\n\n$ kubectx kube-dc/shalb/demo\nSwitched to context \"kube-dc/shalb/demo\".\n\n# Works with kubens\n$ kubens\nshalb-demo              \u2190 Current namespace\nshalb-dev\nshalb-envoy\n\n$ kubens shalb-dev\nContext \"kube-dc/shalb/demo\" modified.\nActive namespace is \"shalb-dev\".\n</code></pre></p> <p>Kube-DC CLI namespace switching (uses JWT claims for validation): <pre><code>$ kube-dc ns\nAvailable namespaces (from your access token):\n  1) shalb-demo (current)\n  2) shalb-dev\n  3) shalb-envoy\n\n$ kube-dc ns shalb-dev\nSwitched to namespace \"shalb-dev\"\n</code></pre></p>"},{"location":"prd/kube-dc-cli/#command-reference","title":"Command Reference","text":""},{"location":"prd/kube-dc-cli/#kube-dc-login","title":"<code>kube-dc login</code>","text":"<p>Authenticate with a Kube-DC server using browser-based OAuth.</p> <pre><code># Interactive - prompts for server if not provided\n$ kube-dc login\n\n# Explicit server\n$ kube-dc login --server https://api.kube-dc.cloud\n\n# With organization (realm) specified\n$ kube-dc login --server https://api.kube-dc.cloud --org shalb\n\n# Device code flow for headless/SSH environments\n$ kube-dc login --server https://api.kube-dc.cloud --device-code\n</code></pre> <p>Flow: 1. Start local HTTP server on random available port 2. Generate PKCE code verifier and challenge 3. Build authorization URL with parameters:    - <code>client_id=kube-dc</code>    - <code>redirect_uri=http://localhost:{port}/callback</code>    - <code>response_type=code</code>    - <code>scope=openid</code>    - <code>code_challenge={challenge}</code>    - <code>code_challenge_method=S256</code> 4. Open browser to authorization URL 5. Wait for callback with authorization code 6. Exchange code for tokens 7. Parse JWT to extract organization and available projects 8. Cache tokens to <code>~/.kube-dc/credentials/</code> 9. Update <code>~/.kube/config</code> with new contexts</p>"},{"location":"prd/kube-dc-cli/#kube-dc-logout","title":"<code>kube-dc logout</code>","text":"<p>Remove cached credentials and kubeconfig entries.</p> <pre><code># Logout from current server\n$ kube-dc logout\n\n# Logout from specific server\n$ kube-dc logout --server https://api.kube-dc.cloud\n\n# Logout from all servers\n$ kube-dc logout --all\n</code></pre>"},{"location":"prd/kube-dc-cli/#kube-dc-use","title":"<code>kube-dc use</code>","text":"<p>Switch between organizations and projects.</p> <pre><code># Switch to specific project\n$ kube-dc use shalb/demo\nSwitched to context \"kube-dc/shalb/demo\" (namespace: shalb-demo)\n\n# Interactive selection\n$ kube-dc use\nAvailable contexts:\n  1) kube-dc/shalb/demo\n  2) kube-dc/shalb/dev\n  3) kube-dc/shalb/envoy\nSelect [1-3]: 2\nSwitched to context \"kube-dc/shalb/dev\"\n</code></pre>"},{"location":"prd/kube-dc-cli/#kube-dc-ns","title":"<code>kube-dc ns</code>","text":"<p>Switch or list namespaces (from JWT claims).</p> <pre><code># List available namespaces\n$ kube-dc ns\nAvailable namespaces:\n  shalb-demo (current)\n  shalb-dev\n  shalb-envoy\n\n# Switch namespace\n$ kube-dc ns shalb-dev\nSwitched to namespace \"shalb-dev\"\n</code></pre>"},{"location":"prd/kube-dc-cli/#kube-dc-config","title":"<code>kube-dc config</code>","text":"<p>Manage CLI configuration.</p> <pre><code># Show current configuration\n$ kube-dc config show\n\n# List all kube-dc contexts\n$ kube-dc config get-contexts\n\n# Set default server\n$ kube-dc config set-default-server https://api.kube-dc.cloud\n</code></pre>"},{"location":"prd/kube-dc-cli/#kube-dc-credential","title":"<code>kube-dc credential</code>","text":"<p>Exec credential plugin for kubectl (not typically called directly).</p> <pre><code># Returns ExecCredential JSON for kubectl\n$ kube-dc credential --server https://api.kube-dc.cloud\n{\n  \"apiVersion\": \"client.authentication.k8s.io/v1\",\n  \"kind\": \"ExecCredential\",\n  \"status\": {\n    \"token\": \"eyJhbGciOiJSUzI1NiIs...\",\n    \"expirationTimestamp\": \"2026-01-26T12:30:00Z\"\n  }\n}\n</code></pre>"},{"location":"prd/kube-dc-cli/#kube-dc-version","title":"<code>kube-dc version</code>","text":"<p>Show version information.</p> <pre><code>$ kube-dc version\nkube-dc CLI v1.0.0\n  Git commit: abc1234\n  Built: 2026-01-26T10:00:00Z\n  Go version: go1.22.0\n</code></pre>"},{"location":"prd/kube-dc-cli/#token-lifecycle","title":"Token Lifecycle","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           Token Lifecycle                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                                \u2502\n\u2502  \u2502  Login   \u2502 \u2500\u2500\u25ba access_token (15 min) + refresh_token (30 days)           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                                \u2502\n\u2502       \u2502                                                                      \u2502\n\u2502       \u25bc                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502                    kubectl command executed                          \u2502   \u2502\n\u2502  \u2502                              \u2502                                       \u2502   \u2502\n\u2502  \u2502                              \u25bc                                       \u2502   \u2502\n\u2502  \u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502   \u2502\n\u2502  \u2502                  \u2502 kube-dc credential    \u2502                           \u2502   \u2502\n\u2502  \u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502   \u2502\n\u2502  \u2502                              \u2502                                       \u2502   \u2502\n\u2502  \u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502   \u2502\n\u2502  \u2502           \u25bc                  \u25bc                  \u25bc                    \u2502   \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502   \u2502\n\u2502  \u2502   \u2502 access_token \u2502  \u2502 access_token \u2502  \u2502 refresh_token    \u2502          \u2502   \u2502\n\u2502  \u2502   \u2502 valid        \u2502  \u2502 expired      \u2502  \u2502 also expired     \u2502          \u2502   \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502\n\u2502  \u2502          \u2502                 \u2502                   \u2502                     \u2502   \u2502\n\u2502  \u2502          \u25bc                 \u25bc                   \u25bc                     \u2502   \u2502\n\u2502  \u2502   Return token      Refresh token       Error + prompt:              \u2502   \u2502\n\u2502  \u2502   immediately       via Keycloak        \"Run: kube-dc login\"         \u2502   \u2502\n\u2502  \u2502                           \u2502                                          \u2502   \u2502\n\u2502  \u2502                           \u25bc                                          \u2502   \u2502\n\u2502  \u2502                    Cache new tokens                                  \u2502   \u2502\n\u2502  \u2502                    Return access_token                               \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                              \u2502\n\u2502  Timeline:                                                                   \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\n\u2502  \u2502 0          15min        30min        ...        30 days                 \u2502\u2502\n\u2502  \u2502 \u2502           \u2502            \u2502                        \u2502                     \u2502\u2502\n\u2502  \u2502 Login   Refresh     Refresh                   Re-login                  \u2502\u2502\n\u2502  \u2502         (auto)      (auto)                    required                   \u2502\u2502\n\u2502  \u2502                                                                          \u2502\u2502\n\u2502  \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Seamless kubectl usage \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502\u25c4\u2500\u2500 Browser login \u2500\u2500\u25ba   \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/kube-dc-cli/#credential-storage","title":"Credential Storage","text":""},{"location":"prd/kube-dc-cli/#file-format","title":"File Format","text":"<pre><code>// ~/.kube-dc/credentials/api-kube-dc-cloud.json\n{\n  \"server\": \"https://api.kube-dc.cloud\",\n  \"keycloak_url\": \"https://login.kube-dc.cloud\",\n  \"realm\": \"shalb\",\n  \"client_id\": \"kube-dc\",\n  \"access_token\": \"eyJhbGciOiJSUzI1NiIs...\",\n  \"refresh_token\": \"eyJhbGciOiJSUzI1NiIs...\",\n  \"access_token_expiry\": \"2026-01-26T12:00:00Z\",\n  \"refresh_token_expiry\": \"2026-02-25T11:41:00Z\",\n  \"id_token\": \"eyJhbGciOiJSUzI1NiIs...\",\n  \"user\": {\n    \"email\": \"voa@shalb.com\",\n    \"org\": \"shalb\",\n    \"groups\": [\"org-admin\"],\n    \"namespaces\": [\"shalb-demo\", \"shalb-dev\", \"shalb-envoy\"]\n  },\n  \"created_at\": \"2026-01-26T11:41:00Z\",\n  \"updated_at\": \"2026-01-26T11:45:00Z\"\n}\n</code></pre>"},{"location":"prd/kube-dc-cli/#security","title":"Security","text":"<ul> <li>File permissions: <code>0600</code> (owner read/write only)</li> <li>Directory permissions: <code>0700</code></li> <li>Tokens never logged or displayed</li> <li>Optional: Keychain/Credential Manager integration (future)</li> </ul>"},{"location":"prd/kube-dc-cli/#cli-configuration","title":"CLI Configuration","text":"<pre><code># ~/.kube-dc/config.yaml\ndefault_server: https://api.kube-dc.cloud\n\nservers:\n  - url: https://api.kube-dc.cloud\n    keycloak_url: https://login.kube-dc.cloud\n    ca_cert: |\n      -----BEGIN CERTIFICATE-----\n      ...\n      -----END CERTIFICATE-----\n\npreferences:\n  auto_open_browser: true\n  credential_cache_ttl: 120  # seconds before re-checking\n</code></pre>"},{"location":"prd/kube-dc-cli/#oauth-flow-details","title":"OAuth Flow Details","text":""},{"location":"prd/kube-dc-cli/#pkce-proof-key-for-code-exchange","title":"PKCE (Proof Key for Code Exchange)","text":"<p>Required for public clients to prevent authorization code interception.</p> <pre><code>// Generate code verifier (43-128 characters)\nverifier := generateRandomString(64)\n\n// Generate code challenge (SHA256 hash, base64url encoded)\nhash := sha256.Sum256([]byte(verifier))\nchallenge := base64.RawURLEncoding.EncodeToString(hash[:])\n</code></pre>"},{"location":"prd/kube-dc-cli/#authorization-request","title":"Authorization Request","text":"<pre><code>GET https://login.kube-dc.cloud/realms/{org}/protocol/openid-connect/auth?\n  client_id=kube-dc&amp;\n  redirect_uri=http://localhost:8400/callback&amp;\n  response_type=code&amp;\n  scope=openid&amp;\n  state={random_state}&amp;\n  code_challenge={challenge}&amp;\n  code_challenge_method=S256\n</code></pre>"},{"location":"prd/kube-dc-cli/#token-exchange","title":"Token Exchange","text":"<pre><code>POST https://login.kube-dc.cloud/realms/{org}/protocol/openid-connect/token\nContent-Type: application/x-www-form-urlencoded\n\ngrant_type=authorization_code&amp;\nclient_id=kube-dc&amp;\ncode={authorization_code}&amp;\nredirect_uri=http://localhost:8400/callback&amp;\ncode_verifier={verifier}\n</code></pre>"},{"location":"prd/kube-dc-cli/#token-refresh","title":"Token Refresh","text":"<pre><code>POST https://login.kube-dc.cloud/realms/{org}/protocol/openid-connect/token\nContent-Type: application/x-www-form-urlencoded\n\ngrant_type=refresh_token&amp;\nclient_id=kube-dc&amp;\nrefresh_token={refresh_token}\n</code></pre>"},{"location":"prd/kube-dc-cli/#discovery-flow","title":"Discovery Flow","text":"<p>The CLI needs to discover Keycloak URL and CA certificate from the API server.</p>"},{"location":"prd/kube-dc-cli/#option-1-well-known-endpoint-recommended","title":"Option 1: Well-Known Endpoint (Recommended)","text":"<pre><code># API server exposes discovery endpoint\nGET https://api.kube-dc.cloud/.well-known/kube-dc-config\n\n{\n  \"keycloak_url\": \"https://login.kube-dc.cloud\",\n  \"client_id\": \"kube-dc\",\n  \"ca_certificate\": \"LS0tLS1...\",\n  \"realms\": [\"shalb\", \"acme\", \"demo\"]\n}\n</code></pre>"},{"location":"prd/kube-dc-cli/#option-2-oidc-discovery-from-api-server","title":"Option 2: OIDC Discovery from API Server","text":"<pre><code># Use Kubernetes OIDC discovery\nGET https://api.kube-dc.cloud/.well-known/openid-configuration\n</code></pre>"},{"location":"prd/kube-dc-cli/#option-3-manual-configuration","title":"Option 3: Manual Configuration","text":"<pre><code>$ kube-dc login --server https://api.kube-dc.cloud \\\n    --keycloak-url https://login.kube-dc.cloud \\\n    --org shalb\n</code></pre>"},{"location":"prd/kube-dc-cli/#implementation-plan","title":"Implementation Plan","text":""},{"location":"prd/kube-dc-cli/#phase-1-core-authentication-week-1-2","title":"Phase 1: Core Authentication (Week 1-2)","text":"<ol> <li>Project setup</li> <li>Initialize Go module in <code>cli/</code></li> <li>Set up cobra for CLI commands</li> <li> <p>Configure goreleaser for builds</p> </li> <li> <p>OAuth implementation</p> </li> <li>PKCE code generation</li> <li>Local callback server</li> <li>Browser opening (cross-platform)</li> <li> <p>Token exchange</p> </li> <li> <p>Credential storage</p> </li> <li>Secure file storage</li> <li>Token caching</li> <li>Expiry checking</li> </ol> <p>Deliverables: - <code>kube-dc login</code> working - <code>kube-dc logout</code> working - <code>kube-dc credential</code> working</p>"},{"location":"prd/kube-dc-cli/#phase-2-kubeconfig-integration-week-2-3","title":"Phase 2: Kubeconfig Integration (Week 2-3)","text":"<ol> <li>Kubeconfig management</li> <li>Parse existing kubeconfig</li> <li>Merge kube-dc entries</li> <li> <p>Preserve non-kube-dc entries</p> </li> <li> <p>Context management</p> </li> <li>Create contexts from JWT namespaces</li> <li>Context naming convention</li> <li> <p>Context switching</p> </li> <li> <p>kubectx compatibility testing</p> </li> <li>Test with kubectx</li> <li>Test with kubens</li> <li>Fix any conflicts</li> </ol> <p>Deliverables: - <code>kube-dc use</code> working - <code>kube-dc ns</code> working - <code>kube-dc config</code> working</p>"},{"location":"prd/kube-dc-cli/#phase-3-polish-distribution-week-3-4","title":"Phase 3: Polish &amp; Distribution (Week 3-4)","text":"<ol> <li>Discovery endpoint</li> <li>Implement <code>/.well-known/kube-dc-config</code> in manager</li> <li> <p>CLI auto-discovery</p> </li> <li> <p>Cross-platform testing</p> </li> <li>Linux (amd64, arm64)</li> <li>macOS (amd64, arm64)</li> <li> <p>Windows (amd64)</p> </li> <li> <p>Distribution</p> </li> <li>GitHub releases</li> <li>Homebrew tap</li> <li>APT/YUM repositories (optional)</li> <li>Installation script</li> </ol> <p>Deliverables: - Multi-platform binaries - Homebrew formula - Installation documentation</p>"},{"location":"prd/kube-dc-cli/#phase-4-advanced-features-future","title":"Phase 4: Advanced Features (Future)","text":"<ol> <li>Device code flow for headless environments</li> <li>Keychain/Credential Manager integration</li> <li>Token introspection and validation</li> <li>Offline mode improvements</li> </ol>"},{"location":"prd/kube-dc-cli/#testing-strategy","title":"Testing Strategy","text":""},{"location":"prd/kube-dc-cli/#unit-tests","title":"Unit Tests","text":"<ul> <li>PKCE generation</li> <li>JWT parsing</li> <li>Kubeconfig merging</li> <li>Token refresh logic</li> </ul>"},{"location":"prd/kube-dc-cli/#integration-tests","title":"Integration Tests","text":"<ul> <li>Full OAuth flow against test Keycloak</li> <li>Kubeconfig manipulation</li> <li>Credential caching</li> </ul>"},{"location":"prd/kube-dc-cli/#e2e-tests","title":"E2E Tests","text":"<ul> <li>Login \u2192 kubectl command \u2192 token refresh</li> <li>Multiple contexts</li> <li>kubectx/kubens compatibility</li> </ul>"},{"location":"prd/kube-dc-cli/#implementation-status","title":"Implementation Status","text":""},{"location":"prd/kube-dc-cli/#completed-v022-january-2026","title":"\u2705 Completed (v0.2.2 - January 2026)","text":"Feature Status Notes <code>kube-dc login</code> \u2705 Done Browser OAuth with PKCE, <code>--domain</code>/<code>--org</code> flags <code>kube-dc logout</code> \u2705 Done Clear credentials, optional context removal <code>kube-dc use</code> \u2705 Done List and switch contexts <code>kube-dc ns</code> \u2705 Done List and switch namespaces <code>kube-dc config show</code> \u2705 Done Show context, credentials, token status <code>kube-dc config get-contexts</code> \u2705 Done List kube-dc contexts <code>kube-dc credential</code> \u2705 Done kubectl exec plugin with auto-refresh <code>kube-dc version</code> \u2705 Done Version info Token caching \u2705 Done <code>~/.kube-dc/credentials/</code> with 0600 perms Kubeconfig merging \u2705 Done Preserves existing entries JWT namespace extraction \u2705 Done From token claims Automatic token refresh \u2705 Done Via credential plugin, Keycloak as source of truth Offline access tokens \u2705 Done 30-day refresh token expiry (industry standard) GoReleaser setup \u2705 Done Multi-platform builds via GitHub Actions CA certificate support \u2705 Done <code>--ca-cert</code> flag for self-hosted"},{"location":"prd/kube-dc-cli/#planned","title":"\ud83d\udccb Planned","text":"Feature Priority Notes Auto-discovery Medium <code>/.well-known/kube-dc-config</code> endpoint Device code flow Medium For SSH/headless environments Homebrew formula Low macOS distribution"},{"location":"prd/kube-dc-cli/#success-criteria","title":"Success Criteria","text":"<ul> <li> User can login with single <code>kube-dc login</code> command</li> <li> Browser opens automatically for authentication</li> <li> Tokens are refreshed automatically (no password prompts for 30 days)</li> <li> Existing kubeconfig entries are preserved</li> <li> kubectx and kubens work correctly</li> <li> Works on Linux, macOS, and Windows</li> <li> Binary size &lt; 20MB (~5MB actual)</li> <li> Credential command responds in &lt; 100ms</li> <li> Clear error messages with correct login command format</li> </ul>"},{"location":"prd/kube-dc-cli/#migration-path","title":"Migration Path","text":""},{"location":"prd/kube-dc-cli/#from-shell-scripts","title":"From Shell Scripts","text":"<p>Users with existing <code>~/.kube-dc/{org}-{project}/</code> directories:</p> <ol> <li>Install <code>kube-dc</code> CLI</li> <li>Run <code>kube-dc login</code> (creates new credentials)</li> <li>Old directories can be removed manually</li> </ol>"},{"location":"prd/kube-dc-cli/#deprecation-timeline","title":"Deprecation Timeline","text":"Phase Action v1.0 CLI released, shell scripts still available v1.1 Shell scripts deprecated (warning in UI) v1.2 Shell scripts removed from documentation v2.0 Shell scripts removed from repository"},{"location":"prd/kube-dc-cli/#dependencies","title":"Dependencies","text":""},{"location":"prd/kube-dc-cli/#go-libraries","title":"Go Libraries","text":"<pre><code>require (\n    github.com/spf13/cobra v1.8.0           // CLI framework\n    github.com/pkg/browser v0.0.0-20210911075715-681adbf594b8  // Open browser\n    golang.org/x/oauth2 v0.16.0             // OAuth2 client\n    gopkg.in/yaml.v3 v3.0.1                 // YAML parsing\n    github.com/golang-jwt/jwt/v5 v5.2.0     // JWT parsing\n)\n</code></pre>"},{"location":"prd/kube-dc-cli/#external-dependencies","title":"External Dependencies","text":"<ul> <li>Keycloak server with <code>kube-dc</code> client configured</li> <li>Well-known endpoint on API server (optional)</li> </ul>"},{"location":"prd/kube-dc-cli/#open-questions","title":"Open Questions","text":"<ol> <li>Realm discovery: How does user know which organization/realm to use?</li> <li> <p>Proposal: Discovery endpoint lists available realms, or user specifies</p> </li> <li> <p>Multiple organizations: Can user be logged into multiple orgs simultaneously?</p> </li> <li> <p>Proposal: Yes, separate credential files per server/realm</p> </li> <li> <p>CA certificate distribution: How to securely distribute cluster CA?</p> </li> <li> <p>Proposal: Include in discovery endpoint, verify against well-known CA</p> </li> <li> <p>Offline token validation: Should we validate JWT signature locally?</p> </li> <li>Proposal: Check expiry only (faster), server validates signature</li> </ol>"},{"location":"prd/kube-dc-cli/#release-process","title":"Release Process","text":""},{"location":"prd/kube-dc-cli/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Update version in code if needed</li> <li>Commit and push to <code>shalb/kube-dc</code> main branch</li> <li>Sync workflow automatically pushes CLI changes to <code>kube-dc/kube-dc-public</code></li> <li>Create tag on <code>kube-dc-public</code>:    <pre><code>cd kube-dc-public\ngit pull origin main\ngit tag v0.2.3  # increment version\ngit push origin v0.2.3\n</code></pre></li> <li>GoReleaser workflow automatically builds and publishes binaries</li> </ol>"},{"location":"prd/kube-dc-cli/#release-artifacts","title":"Release Artifacts","text":"<p>GoReleaser produces binaries for: - <code>kube-dc_linux_amd64</code> - <code>kube-dc_linux_arm64</code> - <code>kube-dc_darwin_amd64</code> - <code>kube-dc_darwin_arm64</code> - <code>kube-dc_windows_amd64.exe</code></p> <p>Binaries are published to: https://github.com/kube-dc/kube-dc-public/releases</p>"},{"location":"prd/kube-dc-cli/#version-history","title":"Version History","text":"Version Date Changes v0.2.2 2026-01-26 Offline access tokens (30-day sessions), fix error messages v0.2.1 2026-01-26 Raw binary releases, remove homebrew v0.2.0 2026-01-26 Initial release with browser OAuth, auto-refresh"},{"location":"prd/kube-dc-cli/#references","title":"References","text":"<ul> <li>Kubernetes Client Authentication</li> <li>OAuth 2.0 PKCE</li> <li>Keycloak OIDC</li> <li>kubelogin (kubectl oidc-login)</li> <li>AWS CLI SSO</li> <li>GCloud Auth</li> </ul>"},{"location":"prd/kube-dc-cloud-deployment/","title":"Kube-DC Cloud Production Deployment","text":""},{"location":"prd/kube-dc-cloud-deployment/#overview","title":"Overview","text":"<p>Production deployment of Kube-DC platform on 4 bare metal servers in Amsterdam datacenter.</p> <p>Domain: <code>kube-dc.cloud</code> Location: Amsterdam (ams1) Provider: Bare metal servers</p>"},{"location":"prd/kube-dc-cloud-deployment/#infrastructure","title":"Infrastructure","text":""},{"location":"prd/kube-dc-cloud-deployment/#architecture","title":"Architecture","text":"<p>3-Node HA Cluster - All masters run workloads (no dedicated workers)</p> <ul> <li>Control plane distributed across 3 nodes for high availability</li> <li>Masters untainted to allow scheduling workloads</li> <li>etcd quorum maintained with 3 nodes</li> </ul>"},{"location":"prd/kube-dc-cloud-deployment/#hosts","title":"Hosts","text":"Role Hostname IP Interface Master 1 (primary) ams1-blade179-8 213.111.154.233 enp94s0f0np0 Master 2 ams1-blade184-5 213.111.154.229 enp94s0f0np0 Master 3 ams1-blade58-2 213.111.154.223 enp8s0f0 Worker ams1-blade187-2 213.111.154.234 eno1"},{"location":"prd/kube-dc-cloud-deployment/#network-requirements","title":"Network Requirements","text":"<ul> <li>Pod CIDR: 10.100.0.0/16</li> <li>Service CIDR: 10.101.0.0/16</li> <li>Join CIDR: 172.30.0.0/22</li> <li>Cloud Network: VLAN 884, 100.65.0.0/16 (managed by Kube-OVN)</li> <li>K8s Internal: VLAN 885, 192.168.0.0/18</li> </ul>"},{"location":"prd/kube-dc-cloud-deployment/#k8s-internal-network-vlan-885","title":"K8s Internal Network (VLAN 885)","text":"Node Public IP Private IP (VLAN 885) Master 1 213.111.154.233 192.168.0.2 Master 2 213.111.154.229 192.168.0.3 Master 3 213.111.154.223 192.168.0.4 Worker 1 213.111.154.234 192.168.0.10 <p>Note: VLAN 885 is used for Kubernetes internal communication (etcd, kubelet, etc.). VLAN 884 is reserved for Kube-OVN cloud network (do not use for K8s internal).</p>"},{"location":"prd/kube-dc-cloud-deployment/#deployment-phases","title":"Deployment Phases","text":""},{"location":"prd/kube-dc-cloud-deployment/#phase-1-host-preparation","title":"Phase 1: Host Preparation \u2705","text":"<ul> <li> SSH key deployment</li> <li> OS verification (Ubuntu 24.04.3 LTS on all 4 nodes)</li> <li> Kernel upgraded to 6.8.0-90-generic on all nodes</li> <li> Sysctl tuning (/etc/sysctl.d/99-kube-dc.conf on all nodes)</li> <li> nf_conntrack module configured (max=1000000)</li> <li> DNS configured on all nodes (systemd-resolved disabled, static /etc/resolv.conf)</li> <li> System packages installed (unzip, iptables, curl, wget)</li> </ul>"},{"location":"prd/kube-dc-cloud-deployment/#phase-2-network-setup","title":"Phase 2: Network Setup \u2705","text":"<ul> <li> VLAN 885 configured for K8s internal traffic (192.168.0.0/18)</li> <li> Netplan configured with VLAN 885 on all nodes</li> <li> Inter-node connectivity verified on private network</li> <li> Configure DNS wildcard record (<code>*.kube-dc.cloud</code> \u2192 213.111.154.233)</li> <li> VLAN 884 reserved for Kube-OVN cloud network (100.65.0.0/16)</li> </ul> <p>Note: VLAN 885 managed by netplan, VLAN 884 auto-managed by Kube-OVN v1.15.0 Important: DNS wildcard must be configured before cdev installation (required for ingress/certs)</p>"},{"location":"prd/kube-dc-cloud-deployment/#phase-3-kubernetes-installation","title":"Phase 3: Kubernetes Installation \u2705","text":"<ul> <li> RKE2 v1.35.0+rke2r1 installed on all nodes</li> <li> Master 1 initialized (192.168.0.2)</li> <li> Master 2 joined (192.168.0.3)</li> <li> Master 3 joined (192.168.0.4)</li> <li> Worker 1 joined (192.168.0.10)</li> <li> HA cluster verified (3 control-plane + 1 worker)</li> <li> Kubeconfig copied to bastion (~/.kube/cloud_kubeconfig)</li> <li> Untaint masters to allow workloads (after CNI install)</li> </ul> <p>Cluster uses private IPs (VLAN 885) for internal traffic, public IPs for external access.</p>"},{"location":"prd/kube-dc-cloud-deployment/#phase-4-kube-dc-installation","title":"Phase 4: Kube-DC Installation \u2b1c","text":"<ul> <li> Create project.yaml configuration</li> <li> Create stack.yaml configuration</li> <li> Run <code>cdev apply</code></li> <li> Verify all components healthy</li> </ul>"},{"location":"prd/kube-dc-cloud-deployment/#phase-5-post-installation","title":"Phase 5: Post-Installation \u2b1c","text":"<ul> <li> Verify Keycloak access</li> <li> Create initial organization</li> <li> Test VM deployment</li> <li> Test external network connectivity</li> </ul>"},{"location":"prd/kube-dc-cloud-deployment/#configuration-files","title":"Configuration Files","text":""},{"location":"prd/kube-dc-cloud-deployment/#stackyaml-template","title":"stack.yaml Template","text":"<pre><code>name: cluster\ntemplate: https://github.com/kube-dc/kube-dc-public//installer/kube-dc/templates/kube-dc?ref=main\nkind: Stack\nbackend: default\nvariables:\n  debug: \"true\"\n  kubeconfig: /root/.kube/config\n\n  cluster_config:\n    pod_cidr: \"10.100.0.0/16\"\n    svc_cidr: \"10.101.0.0/16\"\n    join_cidr: \"172.30.0.0/22\"\n    cluster_dns: \"10.101.0.11\"\n    default_external_network:\n      nodes_list:\n        - kube-dc-master-1\n        - kube-dc-master-2\n        - kube-dc-master-3\n        - kube-dc-worker-1\n      name: ext-public\n      vlan_id: \"TBD\"\n      interface: \"TBD\"\n      cidr: \"TBD\"\n      gateway: \"TBD\"\n      mtu: \"1400\"\n\n  node_external_ip: 213.111.154.233\n  email: \"admin@kube-dc.cloud\"\n  domain: \"kube-dc.cloud\"\n  install_terraform: true\n\n  create_default:\n    organization:\n      name: default\n      description: \"Default Organization\"\n      email: \"admin@kube-dc.cloud\"\n    project:\n      name: demo\n      cidr_block: \"10.1.0.0/16\"\n\n  versions:\n    kube_dc: \"v0.1.35\"\n</code></pre>"},{"location":"prd/kube-dc-cloud-deployment/#success-criteria","title":"Success Criteria","text":"<ol> <li>\u2705 All 4 nodes joined to cluster (3 masters HA + 1 worker)</li> <li>\u2705 Kube-OVN networking operational</li> <li>\u2705 KubeVirt VMs can be created</li> <li>\u2705 External IPs assignable to workloads</li> <li>\u2705 UI accessible at <code>console.kube-dc.cloud</code></li> <li>\u2705 Keycloak SSO working</li> </ol>"},{"location":"prd/kube-dc-cloud-deployment/#dependencies","title":"Dependencies","text":"<ul> <li>External subnet allocation from provider</li> <li>VLAN configuration on physical switch</li> <li>DNS wildcard record: <code>*.kube-dc.cloud \u2192 213.111.154.233</code></li> </ul>"},{"location":"prd/kube-dc-cloud-deployment/#reference","title":"Reference","text":"<ul> <li>Quickstart Hetzner Guide</li> <li>Architecture Overview</li> <li>Installer Design</li> </ul>"},{"location":"prd/kube-dc-cloud-network-design/","title":"Kube-DC Cloud Network Design","text":""},{"location":"prd/kube-dc-cloud-network-design/#overview","title":"Overview","text":"<p>Network architecture for kube-dc.cloud using Kube-OVN v1.15.0 underlay networking features.</p>"},{"location":"prd/kube-dc-cloud-network-design/#kube-ovn-v1150-key-features","title":"Kube-OVN v1.15.0 Key Features","text":"Feature PR Benefit Node Selectors for ProviderNetwork #5518 Use labels instead of per-node config Shared VLAN across ProviderNetworks #5471 Multiple networks on same VLAN Auto create VLAN sub-interfaces #5966 No manual VLAN interface creation Auto move VLAN to OVS bridges #5949 Automatic OVS integration <p>Key insight: With v1.15.0, Kube-OVN auto-creates VLAN interfaces - no need to configure them in netplan!</p>"},{"location":"prd/kube-dc-cloud-network-design/#physical-interface-mapping","title":"Physical Interface Mapping","text":"Node Hostname Primary Interface Public IP Master 1 ams1-blade179-8 <code>enp94s0f0np0</code> 213.111.154.233 Master 2 ams1-blade184-5 <code>enp94s0f0np0</code> 213.111.154.229 Master 3 ams1-blade58-2 <code>enp8s0f0</code> 213.111.154.223 Worker ams1-blade187-2 <code>eno1</code> 213.111.154.234"},{"location":"prd/kube-dc-cloud-network-design/#interface-name-differences","title":"Interface Name Differences","text":"<p>Three different interface naming patterns: - <code>enp94s0f0np0</code> - Intel/Mellanox NICs (Master 1, 2) - <code>enp8s0f0</code> - Standard PCIe NIC (Master 3) - <code>eno1</code> - Onboard NIC (Worker)</p>"},{"location":"prd/kube-dc-cloud-network-design/#network-architecture","title":"Network Architecture","text":""},{"location":"prd/kube-dc-cloud-network-design/#vlan-layout","title":"VLAN Layout","text":"VLAN Network Purpose Managed By 885 192.168.0.0/18 K8s Internal (etcd, kubelet) Netplan 884 100.65.0.0/16 Cloud Network (VMs, Pods) Kube-OVN"},{"location":"prd/kube-dc-cloud-network-design/#k8s-internal-network-vlan-885","title":"K8s Internal Network (VLAN 885)","text":"Node Public IP Private IP Master 1 213.111.154.233 192.168.0.2 Master 2 213.111.154.229 192.168.0.3 Master 3 213.111.154.223 192.168.0.4 Worker 1 213.111.154.234 192.168.0.10"},{"location":"prd/kube-dc-cloud-network-design/#network-diagram","title":"Network Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Physical Switch                          \u2502\n\u2502         VLAN 885 (K8s Internal)  \u2502  VLAN 884 (Cloud Network)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n              \u2502               \u2502               \u2502               \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n         \u2502Master 1 \u2502     \u2502Master 2 \u2502     \u2502Master 3 \u2502     \u2502 Worker  \u2502\n         \u2502.233     \u2502     \u2502.229     \u2502     \u2502.223     \u2502     \u2502.234     \u2502\n         \u2502192.168. \u2502     \u2502192.168. \u2502     \u2502192.168. \u2502     \u2502192.168. \u2502\n         \u2502  0.2    \u2502     \u2502  0.3    \u2502     \u2502  0.4    \u2502     \u2502  0.10   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n              \u2502               \u2502               \u2502               \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                               \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502   VLAN 885      \u2502            \u2502   Kube-OVN      \u2502\n           \u2502  K8s Internal   \u2502            \u2502 ProviderNetwork \u2502\n           \u2502 192.168.0.0/18  \u2502            \u2502  VLAN 884       \u2502\n           \u2502  (via netplan)  \u2502            \u2502 100.65.0.0/16   \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/kube-dc-cloud-network-design/#netplan-configuration","title":"Netplan Configuration","text":""},{"location":"prd/kube-dc-cloud-network-design/#host-netplan-configuration","title":"Host Netplan Configuration","text":"<p>Each node has: - Public IP on physical interface - VLAN 885 for K8s internal traffic (configured in netplan) - VLAN 884 for cloud network (managed by Kube-OVN, NOT in netplan)</p> <p>Master 1 (<code>/etc/netplan/01-netcfg.yaml</code>): <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp94s0f0np0:\n      addresses: [213.111.154.233/23]\n      routes:\n        - to: default\n          via: 213.111.154.1\n  vlans:\n    vlan885:\n      id: 885\n      link: enp94s0f0np0\n      addresses: [192.168.0.2/18]\n</code></pre></p> <p>Master 2: <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp94s0f0np0:\n      addresses: [213.111.154.229/23]\n      routes:\n        - to: default\n          via: 213.111.154.1\n  vlans:\n    vlan885:\n      id: 885\n      link: enp94s0f0np0\n      addresses: [192.168.0.3/18]\n</code></pre></p> <p>Master 3: <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp8s0f0:\n      addresses: [213.111.154.223/23]\n      routes:\n        - to: default\n          via: 213.111.154.1\n  vlans:\n    vlan885:\n      id: 885\n      link: enp8s0f0\n      addresses: [192.168.0.4/18]\n</code></pre></p> <p>Worker: <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    eno1:\n      addresses: [213.111.154.234/23]\n      routes:\n        - to: default\n          via: 213.111.154.1\n  vlans:\n    vlan885:\n      id: 885\n      link: eno1\n      addresses: [192.168.0.10/18]\n</code></pre></p> <p>Note: VLAN 884 is NOT configured in netplan - Kube-OVN manages it automatically.</p>"},{"location":"prd/kube-dc-cloud-network-design/#kube-ovn-providernetwork-configuration","title":"Kube-OVN ProviderNetwork Configuration","text":""},{"location":"prd/kube-dc-cloud-network-design/#using-custominterfaces-for-different-nic-names","title":"Using customInterfaces for Different NIC Names","text":"<p>The cluster has three different interface naming patterns - Kube-OVN handles this with <code>customInterfaces</code>:</p> <pre><code>apiVersion: kubeovn.io/v1\nkind: ProviderNetwork\nmetadata:\n  name: ext-cloud\nspec:\n  # Default interface for most nodes (Master 1, 2)\n  defaultInterface: enp94s0f0np0\n\n  # Custom interfaces for nodes with different NIC names\n  customInterfaces:\n    - interface: enp8s0f0\n      nodes:\n        - ams1-blade58-2      # Master 3\n    - interface: eno1\n      nodes:\n        - ams1-blade187-2     # Worker\n\n  # NEW in v1.15.0: Auto-create VLAN subinterfaces\n  # When combined with customInterfaces, creates VLAN interfaces\n  # on each node's specific NIC (e.g., enp94s0f0np0.884, enp8s0f0.884, eno1.884)\n  autoCreateVlanSubinterfaces: true\n\n  # NEW in v1.15.0: Preserve/migrate existing VLAN configs to OVS\n  # Transfers IPs and routes from kernel VLAN interfaces to OVS ports\n  preserveVlanInterfaces: true\n\n  # Alternative: Use node selectors (v1.15.0 feature)\n  # nodeSelector:\n  #   matchLabels:\n  #     node-role.kubernetes.io/control-plane: \"\"\n</code></pre>"},{"location":"prd/kube-dc-cloud-network-design/#how-custominterfaces-autocreatevlansubinterfaces-works","title":"How customInterfaces + autoCreateVlanSubinterfaces Works","text":"<p>When both are configured, Kube-OVN:</p> <ol> <li>Resolves interface per node:</li> <li>Master 1, 2 \u2192 <code>enp94s0f0np0</code> (defaultInterface)</li> <li>Master 3 \u2192 <code>enp8s0f0</code> (from customInterfaces)</li> <li> <p>Worker \u2192 <code>eno1</code> (from customInterfaces)</p> </li> <li> <p>Auto-creates VLAN subinterfaces on each node:</p> </li> <li>Master 1, 2 \u2192 <code>enp94s0f0np0.884</code></li> <li>Master 3 \u2192 <code>enp8s0f0.884</code></li> <li> <p>Worker \u2192 <code>eno1.884</code></p> </li> <li> <p>Moves VLAN interfaces to OVS bridges with <code>preserveVlanInterfaces: true</code>:</p> </li> <li>Transfers IPs from kernel VLAN interface to OVS internal port</li> <li>Preserves routing table entries</li> <li>Eliminates duplicate IP/route issues</li> </ol>"},{"location":"prd/kube-dc-cloud-network-design/#vlan-configuration","title":"VLAN Configuration","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Vlan\nmetadata:\n  name: vlan884\nspec:\n  id: 884\n  provider: ext-cloud\n</code></pre>"},{"location":"prd/kube-dc-cloud-network-design/#subnet-configuration","title":"Subnet Configuration","text":"<pre><code>apiVersion: kubeovn.io/v1\nkind: Subnet\nmetadata:\n  name: ext-cloud\nspec:\n  protocol: IPv4\n  cidrBlock: 100.65.0.0/16\n  gateway: 100.65.0.1\n  vlan: vlan884\n  excludeIps:\n    - 100.65.0.1..100.65.0.10  # Reserved for infrastructure\n</code></pre>"},{"location":"prd/kube-dc-cloud-network-design/#configuration-approaches-comparison","title":"Configuration Approaches Comparison","text":""},{"location":"prd/kube-dc-cloud-network-design/#option-a-manual-vlan-in-netplan-traditional","title":"Option A: Manual VLAN in Netplan (Traditional)","text":"<pre><code># Netplan creates VLAN interface\nvlans:\n  enp94s0f0np0.884:\n    id: 884\n    link: enp94s0f0np0\n    addresses:\n      - 100.65.0.2/16\n</code></pre> <p>Pros: Full control, works with any Kube-OVN version Cons: Must configure on each node, different interface names = different configs</p>"},{"location":"prd/kube-dc-cloud-network-design/#option-b-kube-ovn-auto-vlan-v1150-recommended","title":"Option B: Kube-OVN Auto-VLAN (v1.15.0) \u2705 Recommended","text":"<pre><code># ProviderNetwork with customInterfaces + auto VLAN features\nspec:\n  defaultInterface: enp94s0f0np0\n  customInterfaces:\n    - interface: enp8s0f0\n      nodes: [ams1-blade58-2]\n    - interface: eno1\n      nodes: [ams1-blade187-2]\n  autoCreateVlanSubinterfaces: true   # Creates VLAN interfaces automatically\n  preserveVlanInterfaces: true        # Migrates IPs/routes to OVS\n</code></pre> <p>Pros:  - Single configuration point (ProviderNetwork CRD) - Handles different NIC names elegantly via <code>customInterfaces</code> - Auto creates VLAN sub-interfaces on the correct NIC per node - Auto integrates with OVS bridges - Properly migrates IPs and routes (no duplicates) - Works across nodes with different hardware</p> <p>Cons: Requires Kube-OVN v1.15.0+</p>"},{"location":"prd/kube-dc-cloud-network-design/#interface-resolution-flow","title":"Interface Resolution Flow","text":"<pre><code>ProviderNetwork.spec.customInterfaces lookup:\n  \u251c\u2500\u2500 Node: ams1-blade179-8 \u2192 Not in customInterfaces \u2192 use defaultInterface: enp94s0f0np0\n  \u251c\u2500\u2500 Node: ams1-blade184-5 \u2192 Not in customInterfaces \u2192 use defaultInterface: enp94s0f0np0  \n  \u251c\u2500\u2500 Node: ams1-blade58-2  \u2192 Found in customInterfaces \u2192 use: enp8s0f0\n  \u2514\u2500\u2500 Node: ams1-blade187-2 \u2192 Found in customInterfaces \u2192 use: eno1\n\nWith autoCreateVlanSubinterfaces=true + Vlan.spec.id=884:\n  \u251c\u2500\u2500 ams1-blade179-8: Creates enp94s0f0np0.884\n  \u251c\u2500\u2500 ams1-blade184-5: Creates enp94s0f0np0.884\n  \u251c\u2500\u2500 ams1-blade58-2:  Creates enp8s0f0.884\n  \u2514\u2500\u2500 ams1-blade187-2: Creates eno1.884\n</code></pre>"},{"location":"prd/kube-dc-cloud-network-design/#implementation-steps","title":"Implementation Steps","text":""},{"location":"prd/kube-dc-cloud-network-design/#phase-1-prepare-hosts","title":"Phase 1: Prepare Hosts","text":"<ol> <li>Verify netplan has base interface configured (public IP only)</li> <li>Remove any manual VLAN configurations from netplan</li> <li>Ensure switch ports are configured for VLAN 884 trunking</li> </ol>"},{"location":"prd/kube-dc-cloud-network-design/#phase-2-deploy-kube-ovn-v1150","title":"Phase 2: Deploy Kube-OVN v1.15.0","text":"<ol> <li>Install Kube-OVN with underlay support enabled</li> <li>Apply ProviderNetwork with customInterfaces</li> <li>Apply Vlan and Subnet resources</li> </ol>"},{"location":"prd/kube-dc-cloud-network-design/#phase-3-verify","title":"Phase 3: Verify","text":"<pre><code># Check ProviderNetwork status\nkubectl get provider-network ext-cloud -o yaml\n\n# Verify VLAN interfaces created by Kube-OVN\nssh root@&lt;node&gt; 'ip link show | grep 884'\n\n# Check OVS bridge integration\nssh root@&lt;node&gt; 'ovs-vsctl show'\n</code></pre>"},{"location":"prd/kube-dc-cloud-network-design/#switch-requirements","title":"Switch Requirements","text":"Requirement Status VLAN 884 configured \u2b1c Verify Trunk mode on all node ports \u2b1c Verify Native VLAN for management traffic \u2b1c Verify VLAN 884 allowed on trunk \u2b1c Verify"},{"location":"prd/kube-dc-cloud-network-design/#ip-allocation","title":"IP Allocation","text":""},{"location":"prd/kube-dc-cloud-network-design/#cloud-network-100650016","title":"Cloud Network (100.65.0.0/16)","text":"Range Purpose 100.65.0.1 Gateway 100.65.0.2-10 Reserved (infrastructure) 100.65.0.11-100.65.255.254 Available for EIPs"},{"location":"prd/kube-dc-cloud-network-design/#success-criteria","title":"Success Criteria","text":"<ul> <li> All 4 nodes show in ProviderNetwork <code>readyNodes</code></li> <li> VLAN 884 sub-interfaces created on all nodes</li> <li> OVS bridges configured correctly</li> <li> External API access working (issue fixed)</li> <li> Upgrade to Kube-OVN v1.15.0 with new VLAN auto-features</li> <li> Test <code>autoCreateVlanSubinterfaces</code> + <code>customInterfaces</code> combination</li> <li> Verify no duplicate IPs/routes after upgrade</li> </ul>"},{"location":"prd/kube-dc-cloud-network-design/#references","title":"References","text":"<ul> <li>Kube-OVN v1.15.0 Release</li> <li>Kube-OVN Underlay Docs</li> <li>ProviderNetwork CRD</li> </ul>"},{"location":"prd/kube-dc-cloud-storage-design/","title":"Kube-DC Cloud Storage Design","text":""},{"location":"prd/kube-dc-cloud-storage-design/#overview","title":"Overview","text":"<p>Storage architecture for kube-dc.cloud production deployment. Phased approach starting with minimal S3 backup storage, expandable to full replicated Ceph cluster.</p>"},{"location":"prd/kube-dc-cloud-storage-design/#infrastructure-summary","title":"Infrastructure Summary","text":""},{"location":"prd/kube-dc-cloud-storage-design/#available-storage","title":"Available Storage","text":"Host Role Disk Size Current Use Available ams1-blade179-8 Master 1 nvme0n1 1.7T OS (/) ~1.6T ams1-blade184-5 Master 2 nvme0n1 1.7T OS (/) ~1.6T ams1-blade58-2 Master 3 sda 931G OS (/) ~900G nvme0n1 1.8T /home 1.8T (can repurpose) ams1-blade187-2 Worker sda 20T OS (/) ~19T <p>Total Capacity: ~25T across 4 nodes</p>"},{"location":"prd/kube-dc-cloud-storage-design/#storage-components","title":"Storage Components","text":""},{"location":"prd/kube-dc-cloud-storage-design/#1-local-path-provisioner-primary","title":"1. local-path Provisioner (Primary)","text":"<p>Default storage class for general workloads.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: local-path-config\n  namespace: local-path-storage\ndata:\n  config.json: |\n    {\n      \"nodePathMap\": [\n        {\n          \"node\": \"DEFAULT_PATH_FOR_NON_LISTED_NODES\",\n          \"paths\": [\"/opt/local-path-provisioner\"]\n        }\n      ]\n    }\n</code></pre> <p>Use cases: - Prometheus/Loki data - Keycloak PostgreSQL - etcd (Kamaji) - General PVCs</p>"},{"location":"prd/kube-dc-cloud-storage-design/#2-rook-ceph-s3-object-storage","title":"2. Rook Ceph (S3 Object Storage)","text":"<p>For S3-compatible backup storage and future RWX volumes.</p>"},{"location":"prd/kube-dc-cloud-storage-design/#phased-deployment","title":"Phased Deployment","text":""},{"location":"prd/kube-dc-cloud-storage-design/#phase-1-local-path-only","title":"Phase 1: local-path Only \u2705","text":"<p>Timeline: Day 1</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Kubernetes Cluster                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Master 1   \u2502   Master 2   \u2502   Master 3   \u2502   Worker   \u2502\n\u2502   1.7T NVMe  \u2502   1.7T NVMe  \u2502   931G SSD   \u2502   20T HDD  \u2502\n\u2502              \u2502              \u2502   +1.8T NVMe \u2502            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  local-path-provisioner (default StorageClass)          \u2502\n\u2502  Path: /opt/local-path-provisioner                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Deliverables: - local-path-provisioner deployed - StorageClass <code>local-path</code> as default - Platform PVCs working (Prometheus, Loki, Keycloak, etcd)</p>"},{"location":"prd/kube-dc-cloud-storage-design/#phase-2-rook-ceph-s3-on-worker","title":"Phase 2: Rook Ceph S3 on Worker \u2b1c","text":"<p>Timeline: Week 1-2</p> <p>Single-node Ceph cluster on worker for S3 backup storage.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Kubernetes Cluster                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Master 1   \u2502   Master 2   \u2502   Master 3   \u2502   Worker   \u2502\n\u2502              \u2502              \u2502              \u2502            \u2502\n\u2502              \u2502              \u2502              \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502              \u2502              \u2502              \u2502  \u2502 MON  \u2502  \u2502\n\u2502              \u2502              \u2502              \u2502  \u2502 MGR  \u2502  \u2502\n\u2502              \u2502              \u2502              \u2502  \u2502 OSD  \u2502  \u2502\n\u2502              \u2502              \u2502              \u2502  \u2502 RGW  \u2502  \u2502\n\u2502              \u2502              \u2502              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502              \u2502              \u2502              \u2502   ~10T     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                   \u2193\n                                            S3 API (backup)\n</code></pre> <p>Configuration:</p> <pre><code># Rook Ceph Cluster - Phase 2\napiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v19.2.1\n  dataDirHostPath: /var/lib/rook\n\n  mon:\n    count: 1  # Single mon for Phase 2\n    allowMultiplePerNode: true\n\n  mgr:\n    count: 1\n    modules:\n      - name: rook\n        enabled: true\n\n  storage:\n    storageClassDeviceSets:\n      - name: s3-backup\n        count: 1\n        portable: false\n        placement:\n          nodeAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n              nodeSelectorTerms:\n                - matchExpressions:\n                    - key: kubernetes.io/hostname\n                      operator: In\n                      values:\n                        - ams1-blade187-2\n        volumeClaimTemplates:\n          - metadata:\n              name: data\n            spec:\n              resources:\n                requests:\n                  storage: 10Ti\n              storageClassName: local-path\n              volumeMode: Filesystem\n              accessModes:\n                - ReadWriteOnce\n</code></pre> <p>Object Store (S3):</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStore\nmetadata:\n  name: backup-store\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    replicated:\n      size: 1  # No replication\n  dataPool:\n    replicated:\n      size: 1  # No replication\n  gateway:\n    port: 80\n    instances: 1\n</code></pre> <p>Deliverables: - Rook Ceph operator deployed - Single OSD on worker node (~10T) - S3 endpoint: <code>s3.kube-dc.cloud</code> - Backup bucket for platform backups</p>"},{"location":"prd/kube-dc-cloud-storage-design/#phase-3-ha-ceph-cluster","title":"Phase 3: HA Ceph Cluster \u2b1c","text":"<p>Timeline: When needed for production storage</p> <p>Extend to 3+ OSDs across nodes for replication.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Kubernetes Cluster                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Master 1   \u2502   Master 2   \u2502   Master 3   \u2502   Worker   \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502 MON  \u2502   \u2502   \u2502 MON  \u2502   \u2502   \u2502 MON  \u2502   \u2502  \u2502 OSD  \u2502  \u2502\n\u2502   \u2502 OSD  \u2502   \u2502   \u2502 OSD  \u2502   \u2502   \u2502 OSD  \u2502   \u2502  \u2502 (lg) \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502    ~500G     \u2502    ~500G     \u2502    ~500G     \u2502   ~10T     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2193              \u2193              \u2193            \u2193\n                    Replicated Storage (size=2)\n</code></pre> <p>Changes: - Add OSDs on master nodes (using local-path PVCs) - Increase MON count to 3 - Update pools to <code>size: 2</code> - Enable CephFS for RWX volumes</p> <p>Configuration Update:</p> <pre><code>storage:\n  storageClassDeviceSets:\n    - name: ceph-osd-masters\n      count: 3  # One per master\n      portable: false\n      placement:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: node-role.kubernetes.io/control-plane\n                    operator: Exists\n      volumeClaimTemplates:\n        - metadata:\n            name: data\n          spec:\n            resources:\n              requests:\n                storage: 500Gi\n            storageClassName: local-path\n            volumeMode: Filesystem\n            accessModes:\n              - ReadWriteOnce\n    - name: ceph-osd-worker\n      count: 1\n      portable: false\n      placement:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: kubernetes.io/hostname\n                    operator: In\n                    values:\n                      - ams1-blade187-2\n      volumeClaimTemplates:\n        - metadata:\n            name: data\n          spec:\n            resources:\n              requests:\n                storage: 10Ti\n            storageClassName: local-path\n            volumeMode: Filesystem\n            accessModes:\n              - ReadWriteOnce\n</code></pre>"},{"location":"prd/kube-dc-cloud-storage-design/#storage-classes","title":"Storage Classes","text":"Name Provisioner Use Case Phase <code>local-path</code> rancher.io/local-path Default, platform PVCs 1 <code>ceph-block</code> rook-ceph.rbd.csi.ceph.com Block storage (RWO) 3 <code>ceph-filesystem</code> rook-ceph.cephfs.csi.ceph.com Shared storage (RWX) 3 <code>ceph-bucket</code> rook-ceph.ceph.rook.io/bucket S3 object storage 2"},{"location":"prd/kube-dc-cloud-storage-design/#resource-requirements","title":"Resource Requirements","text":""},{"location":"prd/kube-dc-cloud-storage-design/#rook-ceph-components","title":"Rook Ceph Components","text":"Component Count CPU RAM Phase rook-operator 1 500m 256Mi 2 MON 1\u21923 100m 384Mi 2\u21923 MGR 1 500m 2Gi 2 OSD 1\u21924 300m 1Gi 2\u21923 RGW 1 100m 256Mi 2 MDS 1 500m 2Gi 3 <p>Phase 2 Total: ~1.5 CPU, ~4Gi RAM Phase 3 Total: ~4 CPU, ~10Gi RAM</p>"},{"location":"prd/kube-dc-cloud-storage-design/#backup-strategy","title":"Backup Strategy","text":""},{"location":"prd/kube-dc-cloud-storage-design/#platform-backups-phase-2","title":"Platform Backups (Phase 2)","text":"Data Backup Target Frequency etcd (RKE2) S3: <code>backup-store</code> Daily Prometheus S3: <code>backup-store</code> Weekly Keycloak DB S3: <code>backup-store</code> Daily Loki S3: <code>backup-store</code> Weekly"},{"location":"prd/kube-dc-cloud-storage-design/#s3-backup-bucket","title":"S3 Backup Bucket","text":"<pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: platform-backups\n  namespace: kube-system\nspec:\n  bucketName: platform-backups\n  storageClassName: ceph-bucket\n</code></pre>"},{"location":"prd/kube-dc-cloud-storage-design/#limitations-trade-offs","title":"Limitations &amp; Trade-offs","text":""},{"location":"prd/kube-dc-cloud-storage-design/#phase-2-single-node-ceph","title":"Phase 2 (Single Node Ceph)","text":"Aspect Status Mitigation Durability \u26a0\ufe0f Worker node failure = S3 data loss Acceptable for backups only Performance \u26a0\ufe0f Single OSD Sufficient for backup workload HA \u274c No MON quorum Upgrade to Phase 3 when needed"},{"location":"prd/kube-dc-cloud-storage-design/#using-local-path-for-ceph-osds","title":"Using local-path for Ceph OSDs","text":"Aspect Impact Performance ~10-20% overhead vs raw device Simplicity \u2705 No disk repartitioning needed Flexibility \u2705 Easy to resize/move"},{"location":"prd/kube-dc-cloud-storage-design/#success-criteria","title":"Success Criteria","text":""},{"location":"prd/kube-dc-cloud-storage-design/#phase-1","title":"Phase 1","text":"<ul> <li> local-path-provisioner deployed</li> <li> All platform PVCs bound and healthy</li> </ul>"},{"location":"prd/kube-dc-cloud-storage-design/#phase-2","title":"Phase 2","text":"<ul> <li> Rook Ceph operator running</li> <li> Single OSD healthy on worker</li> <li> S3 endpoint accessible</li> <li> Backup job running successfully</li> </ul>"},{"location":"prd/kube-dc-cloud-storage-design/#phase-3","title":"Phase 3","text":"<ul> <li> 3+ OSDs across nodes</li> <li> 3 MONs with quorum</li> <li> Pool replication size=2</li> <li> CephFS available for RWX</li> </ul>"},{"location":"prd/kube-dc-cloud-storage-design/#references","title":"References","text":"<ul> <li>Rook Ceph Documentation</li> <li>local-path-provisioner</li> <li>Ceph Object Storage</li> </ul>"},{"location":"prd/multi-network-eip-support/","title":"PRD: Multi-Network EIP Support","text":""},{"location":"prd/multi-network-eip-support/#overview","title":"Overview","text":"<p>This document describes the architecture and capabilities for projects to use External IPs (EIPs) from multiple external networks (e.g., <code>public</code> and <code>cloud</code>) simultaneously.</p>"},{"location":"prd/multi-network-eip-support/#problem-statement","title":"Problem Statement","text":"<p>Organizations often need workloads to use different types of external IPs: - Public IPs: Real IPv4 addresses accessible from the internet (limited, expensive) - Cloud IPs: Internal cloud network IPs for inter-project communication (abundant, cost-effective)</p> <p>Currently, a project's <code>egressNetworkType</code> determines which external network is used for: 1. Default gateway SNAT (outbound traffic) 2. Default EIP allocation</p> <p>Question: Can a project mix IPs from both networks?</p>"},{"location":"prd/multi-network-eip-support/#current-implementation-analysis","title":"Current Implementation Analysis","text":""},{"location":"prd/multi-network-eip-support/#architecture","title":"Architecture","text":"<pre><code>                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502                    Project VPC                      \u2502\n                     \u2502                                                     \u2502\n                     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n                     \u2502  \u2502   Subnet    \u2502     \u2502      OvnSnatRule        \u2502   \u2502\n                     \u2502  \u2502 10.x.x.0/24 \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 V4IpCidr: 10.x.x.0/24   \u2502   \u2502\n                     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 OvnEip: vpc-ext-{net}   \u2502   \u2502\n                     \u2502                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n                     \u2502                                  \u2502                 \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                        \u2502\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502                             \u25bc                             \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502      ext-public       \u2502     \u2502       OvnEip          \u2502     \u2502      ext-cloud      \u2502\n              \u2502   91.224.11.0/24      \u2502\u25c0\u2500\u2500\u2500\u2500\u2502   (allocated from     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   100.65.0.0/16     \u2502\n              \u2502      VLAN 883         \u2502     \u2502    external subnet)   \u2502     \u2502      VLAN 884       \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/multi-network-eip-support/#kube-ovn-resources","title":"Kube-OVN Resources","text":"Resource Purpose Key Fields <code>OvnEip</code> External IP allocation <code>ExternalSubnet</code>, <code>V4Ip</code>, <code>Type</code> <code>OvnSnatRule</code> SNAT for outbound traffic <code>OvnEip</code>, <code>VpcSubnet</code>, <code>V4IpCidr</code> <code>OvnFipRule</code> Floating IP for inbound traffic <code>OvnEip</code>, <code>V4Ip</code> (internal)"},{"location":"prd/multi-network-eip-support/#kube-dc-resources","title":"Kube-DC Resources","text":"Resource Purpose Network Selection <code>Project</code> Project definition <code>spec.egressNetworkType</code> (immutable) <code>EIp</code> EIP abstraction <code>spec.externalNetworkType</code> <code>FIp</code> Floating IP <code>spec.externalNetworkType</code>"},{"location":"prd/multi-network-eip-support/#current-capabilities","title":"Current Capabilities","text":""},{"location":"prd/multi-network-eip-support/#supported-different-network-types-per-resource","title":"\u2705 Supported: Different Network Types per Resource","text":"<p>Each resource can independently specify its network type:</p> <pre><code># Project with cloud SNAT (default egress)\napiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: my-project\n  namespace: my-org\nspec:\n  cidrBlock: 10.100.0.0/24\n  egressNetworkType: cloud  # Default SNAT uses cloud network\n---\n# FIP using public network (different from project default)\napiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: web-server-fip\n  namespace: my-org-my-project\nspec:\n  vmTarget:\n    vmName: web-server\n  externalNetworkType: public  # This FIP uses public network\n---\n# Another FIP using cloud network\napiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: internal-service-fip\n  namespace: my-org-my-project\nspec:\n  vmTarget:\n    vmName: internal-service\n  externalNetworkType: cloud  # This FIP uses cloud network\n</code></pre>"},{"location":"prd/multi-network-eip-support/#supported-multiple-eips-per-project","title":"\u2705 Supported: Multiple EIPs per Project","text":"<p>Projects can have multiple EIPs from different networks:</p> <pre><code># Default gateway EIP (created automatically based on project.spec.egressNetworkType)\napiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: default-gw\n  namespace: my-org-my-project\nspec:\n  externalNetworkType: cloud\n---\n# Additional EIP for load balancer (public network)\napiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: lb-public\n  namespace: my-org-my-project\nspec:\n  externalNetworkType: public\n</code></pre>"},{"location":"prd/multi-network-eip-support/#limitation-single-default-snat-network","title":"\u26a0\ufe0f Limitation: Single Default SNAT Network","text":"<p>The default SNAT rule uses one network type (from <code>project.spec.egressNetworkType</code>). This is immutable after project creation.</p> <p>Workaround: Create additional SNAT rules for specific source CIDRs if needed (advanced use case).</p>"},{"location":"prd/multi-network-eip-support/#resource-flow","title":"Resource Flow","text":""},{"location":"prd/multi-network-eip-support/#eip-allocation-flow","title":"EIP Allocation Flow","text":"<pre><code>User creates EIp with externalNetworkType=public\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     EIP Controller              \u2502\n\u2502  1. Find ext-public subnet      \u2502\n\u2502  2. Find free OvnEip or create  \u2502\n\u2502  3. Label OvnEip with EIp ref   \u2502\n\u2502  4. Update EIp status           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\nOvnEip allocated from ext-public: 91.224.11.x\n</code></pre>"},{"location":"prd/multi-network-eip-support/#fip-allocation-flow","title":"FIP Allocation Flow","text":"<pre><code>User creates FIp with externalNetworkType=public\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     FIP Controller              \u2502\n\u2502  1. Resolve VM target IP        \u2502\n\u2502  2. Create/find EIp (public)    \u2502\n\u2502  3. Create OvnFipRule           \u2502\n\u2502  4. Update FIp status           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\nVM internal IP \u2190\u2192 Public external IP (bidirectional NAT)\n</code></pre>"},{"location":"prd/multi-network-eip-support/#configuration","title":"Configuration","text":""},{"location":"prd/multi-network-eip-support/#external-networks-setup","title":"External Networks Setup","text":"<p>Two external networks configured on the same ProviderNetwork:</p> <pre><code># VLAN for public network (real IPv4)\napiVersion: kubeovn.io/v1\nkind: Vlan\nmetadata:\n  name: vlan883\nspec:\n  id: 883\n  provider: ext-cloud\n\n---\n# Public subnet\napiVersion: kubeovn.io/v1\nkind: Subnet\nmetadata:\n  name: ext-public\n  labels:\n    network.kube-dc.com/external-network-type: public\nspec:\n  cidrBlock: 91.224.11.0/24\n  gateway: 91.224.11.254\n  vlan: vlan883\n  excludeIps:\n    - 91.224.11.1..91.224.11.3\n    - 91.224.11.254\n\n---\n# VLAN for cloud network (internal)\napiVersion: kubeovn.io/v1\nkind: Vlan\nmetadata:\n  name: vlan884\nspec:\n  id: 884\n  provider: ext-cloud\n\n---\n# Cloud subnet\napiVersion: kubeovn.io/v1\nkind: Subnet\nmetadata:\n  name: ext-cloud\n  labels:\n    network.kube-dc.com/external-network-type: cloud\nspec:\n  cidrBlock: 100.65.0.0/16\n  gateway: 100.65.0.1\n  vlan: vlan884\n</code></pre>"},{"location":"prd/multi-network-eip-support/#master-config-defaults","title":"Master Config Defaults","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dc-master-config\ndata:\n  config: |\n    default_gw_network_type: cloud\n    default_eip_network_type: cloud\n    default_fip_network_type: public\n    default_svc_lb_network_type: public\n</code></pre>"},{"location":"prd/multi-network-eip-support/#use-cases","title":"Use Cases","text":""},{"location":"prd/multi-network-eip-support/#use-case-1-web-application-with-public-frontend","title":"Use Case 1: Web Application with Public Frontend","text":"<p>Scenario: Project needs public IP for web server, cloud IP for backend services.</p> <pre><code># Project uses cloud for default SNAT (backend services)\napiVersion: kube-dc.com/v1\nkind: Project\nspec:\n  egressNetworkType: cloud\n\n---\n# Public FIP for web server VM\napiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: web-fip\nspec:\n  vmTarget:\n    vmName: web-server\n  externalNetworkType: public\n</code></pre> <p>Result: - Web server: Accessible via public IP (91.224.11.x) - Backend services: Egress via cloud IP (100.65.x.x)</p>"},{"location":"prd/multi-network-eip-support/#use-case-2-development-vs-production-ips","title":"Use Case 2: Development vs Production IPs","text":"<p>Scenario: Use cloud IPs for development, public IPs for production workloads.</p> <pre><code># Dev FIP (cloud network)\napiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: dev-app-fip\nspec:\n  vmTarget:\n    vmName: dev-app\n  externalNetworkType: cloud\n\n---\n# Prod FIP (public network)\napiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: prod-app-fip\nspec:\n  vmTarget:\n    vmName: prod-app\n  externalNetworkType: public\n</code></pre>"},{"location":"prd/multi-network-eip-support/#use-case-3-loadbalancer-service-with-public-ip","title":"Use Case 3: LoadBalancer Service with Public IP","text":"<p>Scenario: Kubernetes service in tenant cluster needs public LoadBalancer IP.</p> <pre><code># Service with public LoadBalancer\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-lb\n  annotations:\n    service.nlb.kube-dc.com/external-network-type: public\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 443\n</code></pre>"},{"location":"prd/multi-network-eip-support/#verification","title":"Verification","text":""},{"location":"prd/multi-network-eip-support/#test-public-network-eip-allocation","title":"Test: Public Network EIP Allocation","text":"<pre><code># Create project with public egress\nkubectl apply -f - &lt;&lt;EOF\napiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: public-test\n  namespace: test\nspec:\n  cidrBlock: 10.50.0.0/24\n  egressNetworkType: public\nEOF\n\n# Verify EIP from public network\nkubectl get eip -n test-public-test\n# NAME         EXTERNAL IP   READY\n# default-gw   91.224.11.5   true\n\n# Verify SNAT rule\nkubectl get ovn-snat-rules | grep public-test\n# test-public-test   test-public-test   91.224.11.4   10.50.0.0/24   true\n</code></pre>"},{"location":"prd/multi-network-eip-support/#test-mixed-network-fips-in-same-project","title":"Test: Mixed Network FIPs in Same Project","text":"<pre><code># Create FIP with public network\nkubectl apply -f - &lt;&lt;EOF\napiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: public-vm-fip\n  namespace: test-public-test\nspec:\n  ipAddress: 10.50.0.100\n  externalNetworkType: public\nEOF\n\n# Create FIP with cloud network (different from project default)\nkubectl apply -f - &lt;&lt;EOF\napiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: cloud-vm-fip\n  namespace: test-public-test\nspec:\n  ipAddress: 10.50.0.101\n  externalNetworkType: cloud\nEOF\n\n# Verify both FIPs allocated from correct networks\nkubectl get fip -n test-public-test\n# NAME             TARGET IP    EXTERNAL IP     READY\n# public-vm-fip    10.50.0.100  91.224.11.x     true\n# cloud-vm-fip     10.50.0.101  100.65.x.x      true\n</code></pre>"},{"location":"prd/multi-network-eip-support/#test-results-2026-01-20","title":"Test Results (2026-01-20)","text":""},{"location":"prd/multi-network-eip-support/#test-environment","title":"Test Environment","text":"<ul> <li>Cluster: kube-dc.cloud</li> <li>Networks: ext-cloud (100.65.0.0/16, VLAN 884), ext-public (91.224.11.0/24, VLAN 883)</li> </ul>"},{"location":"prd/multi-network-eip-support/#test-public-network-project","title":"Test: Public Network Project","text":"<pre><code># Created project with public egress\nkubectl apply -f - &lt;&lt;EOF\napiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: public-test\n  namespace: test\nspec:\n  cidrBlock: 10.50.0.0/24\n  egressNetworkType: public\nEOF\n\n# Results:\n# - EIP: 91.224.11.5 (from ext-public) \u2705\n# - SNAT: 91.224.11.4 \u2192 10.50.0.0/24 \u2705\n</code></pre>"},{"location":"prd/multi-network-eip-support/#test-mixed-network-fips","title":"Test: Mixed Network FIPs","text":"<pre><code># Created FIPs with different network types in same project\n# Public FIP:\nkubectl get ovn-fip test-public-test-test-public-fip\n# V4EIP: 91.224.11.6, V4IP: 10.50.0.100, READY: true \u2705\n\n# Cloud FIP:\nkubectl get ovn-fip test-public-test-test-cloud-fip  \n# V4EIP: 100.65.0.112, V4IP: 10.50.0.101, READY: true \u2705\n</code></pre>"},{"location":"prd/multi-network-eip-support/#known-issue-fip-status-sync","title":"Known Issue: FIP Status Sync","text":"<p>The FIP controller creates the OvnFip correctly but doesn't always sync the external IP back to the FIp resource status. The OvnFip is functional but <code>fip.status.externalIP</code> may be empty.</p> <p>Workaround: Check <code>ovn-fip</code> resources directly for actual external IP assignment.</p>"},{"location":"prd/multi-network-eip-support/#summary","title":"Summary","text":"Feature Supported Notes Project default SNAT from specific network \u2705 Via <code>project.spec.egressNetworkType</code> EIP from different network than project default \u2705 Via <code>eip.spec.externalNetworkType</code> FIP from different network than project default \u2705 Via <code>fip.spec.externalNetworkType</code> LoadBalancer from different network \u2705 Via service annotation Multiple FIPs from different networks in same project \u2705 Each FIP specifies its network Multiple SNAT rules for different source CIDRs \u26a0\ufe0f Requires manual OvnSnatRule creation Changing project default network after creation \u274c <code>egressNetworkType</code> is immutable FIP status sync \u26a0\ufe0f OvnFip works, but FIp status may not reflect external IP"},{"location":"prd/multi-network-eip-support/#references","title":"References","text":"<ul> <li>Kube-OVN v1.15.0 Release Notes</li> <li>Kube-OVN EIP/SNAT Documentation</li> <li>Kube-DC EIP Controller: <code>internal/eip/</code></li> <li>Kube-DC FIP Controller: <code>internal/fip/</code></li> </ul>"},{"location":"prd/oidc-dynamic-reload-issues/","title":"PRD: OIDC Dynamic Reload - Issues and Solutions","text":""},{"location":"prd/oidc-dynamic-reload-issues/#overview","title":"Overview","text":"<p>This document describes the issues encountered during the implementation of OIDC dynamic reload for new organization creation in Kube-DC, along with proposed solutions.</p>"},{"location":"prd/oidc-dynamic-reload-issues/#background","title":"Background","text":"<p>When a new organization is created in Kube-DC: 1. A new Keycloak realm is created for the organization 2. The OIDC configuration is updated in <code>auth-conf.yaml</code> ConfigMap 3. The DaemonSet syncs the config to all control plane nodes 4. The Kubernetes API server needs to reload the OIDC configuration 5. Only then can tokens from the new realm be accepted</p> <p>Problem: There is a timing window between org creation and OIDC config reload where tokens are rejected with 401 Unauthorized.</p>"},{"location":"prd/oidc-dynamic-reload-issues/#issues-identified","title":"Issues Identified","text":""},{"location":"prd/oidc-dynamic-reload-issues/#issue-1-cors-error-on-apiregisterverify-token","title":"Issue 1: CORS Error on <code>/api/register/verify-token</code>","text":"<p>Symptoms: <pre><code>Access to fetch at 'https://backend.kube-dc.cloud/api/register/verify-token' \nfrom origin 'https://console.kube-dc.cloud' has been blocked by CORS policy: \nNo 'Access-Control-Allow-Origin' header is present on the requested resource.\n</code></pre></p> <p>Root Cause: The 504 Gateway Timeout response from the ingress does not include CORS headers. When the backend times out, the ingress returns a 504 without forwarding CORS headers from the backend.</p> <p>Solution Options: 1. Add CORS headers at the ingress level for error responses 2. Reduce the verify-token timeout to stay within ingress timeout limits 3. Move the polling logic to the frontend (client-side polling)</p>"},{"location":"prd/oidc-dynamic-reload-issues/#issue-2-504-gateway-timeout-on-apiregisterverify-token","title":"Issue 2: 504 Gateway Timeout on <code>/api/register/verify-token</code>","text":"<p>Symptoms: <pre><code>POST https://backend.kube-dc.cloud/api/register/verify-token\nnet::ERR_FAILED 504 (Gateway Timeout)\n</code></pre></p> <p>Root Cause: The verify-token endpoint polls K8s API for up to 30 seconds (15 attempts \u00d7 2s delay). This exceeds the default ingress timeout (typically 60s, but can be lower).</p> <p>Current Implementation: <pre><code>const maxAttempts = 15; // 15 attempts\nconst delayMs = 2000;   // 2 seconds between attempts\n// Total time: up to 30 seconds\n</code></pre></p> <p>Solution Options: 1. Reduce polling time: Lower maxAttempts or delayMs to stay within timeout 2. Async polling: Return immediately and let frontend poll for status 3. Increase ingress timeout: Configure higher timeout for this specific endpoint 4. WebSocket: Use WebSocket for real-time status updates</p>"},{"location":"prd/oidc-dynamic-reload-issues/#issue-3-login-screen-still-appears-after-go-to-dashboard","title":"Issue 3: Login Screen Still Appears After \"Go to Dashboard\"","text":"<p>Symptoms: User clicks \"Go to Dashboard\" after org creation but is redirected to login screen instead of entering dashboard directly.</p> <p>Root Cause Analysis:</p> <p>The current flow in <code>AppLayout.tsx</code>: <pre><code>const handleGoToDashboard = () =&gt; {\n  const orgToken = localStorage.getItem('org-token');\n  if (orgToken) {\n    localStorage.setItem('token', orgToken);\n  }\n  localStorage.setItem('organization', createdOrgName);\n  window.location.href = `${window.location.origin}/manage-organization/projects?realm=${createdOrgName}`;\n};\n</code></pre></p> <p>Problems: 1. The <code>token</code> is set in localStorage but the app's auth context may not recognize it 2. The app may use <code>oidc-client-ts</code> which checks for session state, not just localStorage token 3. Navigation to <code>/manage-organization/projects</code> triggers route guards that check auth state 4. The auth state check may redirect to login because OIDC session is not established</p> <p>Solution Options: 1. Set auth context directly: Update the OIDC auth context with the new token 2. Use programmatic login: Trigger OIDC login flow with the org realm 3. Token injection: Properly inject token into auth provider's state 4. Session storage: Also set token in session storage if auth library uses it</p>"},{"location":"prd/oidc-dynamic-reload-issues/#issue-4-oidc-config-reload-timing","title":"Issue 4: OIDC Config Reload Timing","text":"<p>Symptoms: Even with controller-side waiting (10s), the API server may not have reloaded by the time the user clicks \"Go to Dashboard\".</p> <p>Root Cause: 1. Controller waits 10s for OIDC reload (may not be enough) 2. DaemonSet sync has its own timing (checks every few seconds) 3. API server file watcher has debounce/delay 4. Multiple control plane nodes need to reload</p> <p>Current Flow: <pre><code>1. Org created in controller\n2. Auth config updated in ConfigMap\n3. Controller waits 10s for OIDC reload\n4. DaemonSet syncs config to nodes (every 5s check)\n5. API server detects file change\n6. API server reloads OIDC config\n7. Token becomes valid\n</code></pre></p> <p>Timing Breakdown: - ConfigMap update: immediate - DaemonSet poll interval: 5s - File write to node: &lt; 1s - API server file watcher: 1-5s debounce - OIDC config reload: 1-2s - Total worst case: ~15s</p>"},{"location":"prd/oidc-dynamic-reload-issues/#proposed-solutions","title":"Proposed Solutions","text":""},{"location":"prd/oidc-dynamic-reload-issues/#solution-a-frontend-side-polling-recommended","title":"Solution A: Frontend-Side Polling (Recommended)","text":"<p>Instead of backend polling, move the verification to frontend:</p> <p>Frontend Changes: <pre><code>// After org creation, show \"Preparing your workspace...\" with spinner\n// Poll a lightweight endpoint that checks token validity\nconst verifyOrgReady = async (token: string, org: string): Promise&lt;boolean&gt; =&gt; {\n  try {\n    const response = await fetch(`${backendURL}/api/manage-organization/projects`, {\n      headers: { 'Authorization': `Bearer ${token}` }\n    });\n    return response.status !== 401;\n  } catch {\n    return false;\n  }\n};\n\n// Poll until ready or timeout\nconst pollUntilReady = async (token: string, org: string, maxAttempts = 15): Promise&lt;boolean&gt; =&gt; {\n  for (let i = 0; i &lt; maxAttempts; i++) {\n    if (await verifyOrgReady(token, org)) return true;\n    await sleep(2000);\n  }\n  return false;\n};\n</code></pre></p> <p>Benefits: - No ingress timeout issues - User sees progress - Can be cancelled by user - No CORS issues (same-origin)</p>"},{"location":"prd/oidc-dynamic-reload-issues/#solution-b-increase-controller-wait-time","title":"Solution B: Increase Controller Wait Time","text":"<p>Increase the OIDC reload wait time in the controller to ensure config is loaded before returning.</p> <p>Changes: <pre><code>// organization.go\nif err := kubeAuthCli.WaitForOIDCConfigReload(ctx, 30*time.Second); err != nil {\n    log.V(5).Info(\"OIDC reload verification failed\", \"error\", err)\n}\n</code></pre></p> <p>Risks: - Backend registration timeout (currently 30s) - User waits longer during org creation - May still not be enough on slow clusters</p>"},{"location":"prd/oidc-dynamic-reload-issues/#solution-c-fix-authentication-flow","title":"Solution C: Fix Authentication Flow","text":"<p>Properly integrate org-token with OIDC auth library:</p> <p>Changes: <pre><code>// AppLayout.tsx\nconst handleGoToDashboard = async () =&gt; {\n  const orgToken = localStorage.getItem('org-token');\n  if (orgToken) {\n    // Decode token to get expiration and user info\n    const decoded = decodeJWT(orgToken);\n\n    // Create a proper User object for oidc-client-ts\n    const user = new User({\n      access_token: orgToken,\n      token_type: 'Bearer',\n      profile: decoded,\n      expires_at: decoded.exp,\n    });\n\n    // Store in auth manager\n    await auth.storeUser(user);\n\n    // Set localStorage for API calls\n    localStorage.setItem('token', orgToken);\n  }\n\n  localStorage.setItem('organization', createdOrgName);\n\n  // Use React Router navigation instead of full page reload\n  history.push(`/manage-organization/projects`);\n};\n</code></pre></p>"},{"location":"prd/oidc-dynamic-reload-issues/#recommended-implementation-plan","title":"Recommended Implementation Plan","text":""},{"location":"prd/oidc-dynamic-reload-issues/#phase-1-quick-fix-immediate","title":"Phase 1: Quick Fix (Immediate)","text":"<ol> <li>Remove backend verify-token endpoint (causes timeout/CORS issues)</li> <li>Increase controller OIDC wait to 20s</li> <li>Add frontend polling after \"Go to Dashboard\" click:</li> <li>Show \"Connecting to your workspace...\" spinner</li> <li>Retry API calls on 401 (already implemented)</li> <li>Max 30s total wait</li> </ol>"},{"location":"prd/oidc-dynamic-reload-issues/#phase-2-proper-fix-short-term","title":"Phase 2: Proper Fix (Short-term)","text":"<ol> <li>Fix authentication flow:</li> <li>Store org-token properly in auth context</li> <li>Use SPA history navigation, not full page reload</li> <li> <p>Ensure auth guards recognize the new token</p> </li> <li> <p>Add visual feedback:</p> </li> <li>Progress indicator during org creation</li> <li>Clear error messages if timeout occurs</li> <li>\"Click to retry\" option on failure</li> </ol>"},{"location":"prd/oidc-dynamic-reload-issues/#phase-3-robust-solution-long-term","title":"Phase 3: Robust Solution (Long-term)","text":"<ol> <li>WebSocket status updates:</li> <li>Real-time OIDC reload status from controller</li> <li>Frontend subscribes to org creation events</li> <li> <p>No polling needed</p> </li> <li> <p>API server readiness check:</p> </li> <li>Controller queries API server metrics</li> <li>Confirms new issuer is in loaded config</li> <li>Only then marks org as ready</li> </ol>"},{"location":"prd/oidc-dynamic-reload-issues/#testing-checklist","title":"Testing Checklist","text":"<ul> <li> Create new org \u2192 enters dashboard without login prompt</li> <li> Create new org \u2192 no 401 errors in console</li> <li> Create new org \u2192 projects page loads immediately</li> <li> Timeout scenario \u2192 graceful error message</li> <li> Refresh after org creation \u2192 still logged in</li> <li> Multiple orgs \u2192 can switch between them</li> </ul>"},{"location":"prd/oidc-dynamic-reload-issues/#files-affected","title":"Files Affected","text":""},{"location":"prd/oidc-dynamic-reload-issues/#backend","title":"Backend","text":"<ul> <li><code>ui/backend/controllers/registration/index.js</code> - Remove or fix verify-token endpoint</li> </ul>"},{"location":"prd/oidc-dynamic-reload-issues/#frontend","title":"Frontend","text":"<ul> <li><code>ui/frontend/src/app/Registration/SetupOrgPage.tsx</code> - Add polling logic</li> <li><code>ui/frontend/src/app/AppLayout/AppLayout.tsx</code> - Fix handleGoToDashboard</li> <li><code>ui/frontend/src/app/utils/fetchWithRetry.ts</code> - Already implemented</li> </ul>"},{"location":"prd/oidc-dynamic-reload-issues/#controller","title":"Controller","text":"<ul> <li><code>internal/organization/organization.go</code> - Adjust OIDC wait timeout</li> <li><code>internal/organization/client_kube_auth.go</code> - WaitForOIDCConfigReload function</li> </ul>"},{"location":"prd/oidc-dynamic-reload-issues/#references","title":"References","text":"<ul> <li>Kubernetes OIDC Authentication: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens</li> <li>K8s 1.30 Structured Authentication Config: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-authentication-configuration</li> <li>oidc-client-ts: https://github.com/authts/oidc-client-ts</li> </ul>"},{"location":"prd/organization_standard_groups/","title":"PRD: Organization Standard Groups and Role Distribution","text":""},{"location":"prd/organization_standard_groups/#problem-statement","title":"Problem Statement","text":"<p>When approving join requests, the UI offers group options (<code>developer</code>, <code>project-manager</code>, <code>user</code>) that don't exist in the organization's Keycloak realm. Currently, only <code>org-admin</code> group is created during organization setup.</p> <p>Current State: - Organization controller creates only <code>org-admin</code> group in Keycloak realm - RoleBinding maps <code>&lt;org&gt;:org-admin</code> to Kubernetes RBAC in org namespace - No standard user-level groups exist for assigning regular users - Join request approval fails silently when trying to add users to non-existent groups</p>"},{"location":"prd/organization_standard_groups/#design-approach-hybrid-model","title":"Design Approach: Hybrid Model","text":"<p>This PRD implements a hybrid approach combining automatic standard groups with fine-grained OrganizationGroup CRD control:</p> <ol> <li>Standard Groups (<code>org-admin</code>, <code>user</code>) - Created automatically during organization setup, provide org-wide basic access</li> <li>Elevated Access (<code>developer</code>, <code>project-manager</code>) - Granted via OrganizationGroup CRD for per-project granular control</li> </ol>"},{"location":"prd/organization_standard_groups/#requirements","title":"Requirements","text":""},{"location":"prd/organization_standard_groups/#1-standard-keycloak-groups-organization-realm","title":"1. Standard Keycloak Groups (Organization Realm)","text":"<p>Create during organization creation:</p> Group Name Purpose Created By <code>org-admin</code> Full organization management Organization controller (exists) <code>user</code> Read-only access to all projects Organization controller (new) <p>Note: <code>developer</code> and <code>project-manager</code> groups are created on-demand via OrganizationGroup CRD for per-project access control.</p>"},{"location":"prd/organization_standard_groups/#2-kubernetes-roles","title":"2. Kubernetes Roles","text":""},{"location":"prd/organization_standard_groups/#organization-namespace-roles","title":"Organization Namespace Roles","text":"Role Name Permissions Created By <code>{org}-admin</code> Full access to org resources Organization controller (exists) <code>{org}-user</code> Read-only org access Organization controller (new)"},{"location":"prd/organization_standard_groups/#project-namespace-roles-templates","title":"Project Namespace Roles (Templates)","text":"<p>Role templates stored in <code>kube-dc</code> namespace, copied to each project namespace:</p> Role Name Permissions Created By <code>admin</code> Full project access Project controller (exists) <code>developer</code> VMs, containers, pods, services: full CRUD Project controller (new) <code>project-manager</code> VMs, containers, pods, services: get, list, watch Project controller (new) <code>user</code> VMs, containers, pods, services: get, list Project controller (new)"},{"location":"prd/organization_standard_groups/#3-rolebindings","title":"3. RoleBindings","text":""},{"location":"prd/organization_standard_groups/#organization-namespace","title":"Organization Namespace","text":"RoleBinding Subject Role <code>org-admin</code> <code>{org}:org-admin</code> <code>{org}-admin</code> (exists) <code>user</code> <code>{org}:user</code> <code>{org}-user</code> (new)"},{"location":"prd/organization_standard_groups/#project-namespace","title":"Project Namespace","text":"RoleBinding Subject Role Created By <code>org-admin</code> <code>{org}:org-admin</code> <code>admin</code> Project controller (exists) <code>user</code> <code>{org}:user</code> <code>user</code> Project controller (new) <p>Note: <code>developer</code> and <code>project-manager</code> RoleBindings are created by OrganizationGroup controller when admin assigns users to specific projects.</p>"},{"location":"prd/organization_standard_groups/#implementation-details","title":"Implementation Details","text":""},{"location":"prd/organization_standard_groups/#files-modified","title":"Files Modified","text":"<ol> <li><code>internal/organization/helpers.go</code> \u2705 DONE</li> <li>Added <code>DefaultKeycloakUserGroup</code> and <code>DefaultKeycloakUserRole</code> constants</li> <li> <p>Added <code>generateUserRoleName()</code> helper function</p> </li> <li> <p><code>internal/organization/client_keycloak.go</code> \u2705 DONE</p> </li> <li> <p>Added <code>user</code> group creation in <code>Create()</code> method</p> </li> <li> <p><code>internal/organization/res_kube_role.go</code> \u2705 DONE</p> </li> <li> <p>Added <code>NewRealmUserRole()</code> function for <code>{org}-user</code> role</p> </li> <li> <p><code>internal/organization/res_kube_role_binding.go</code> \u2705 DONE</p> </li> <li> <p>Added <code>NewRealmUserRoleBinding()</code> function</p> </li> <li> <p><code>internal/organization/organization.go</code> \u2705 DONE</p> </li> <li> <p>Added calls to sync user role and rolebinding in <code>Sync()</code></p> </li> <li> <p><code>internal/project/helpers.go</code> \u2705 DONE</p> </li> <li> <p>Added role template constants and loader functions</p> </li> <li> <p><code>internal/project/res_role.go</code> \u2705 DONE</p> </li> <li> <p>Added <code>NewProjectDeveloperRole()</code>, <code>NewProjectManagerRole()</code>, <code>NewProjectUserRole()</code></p> </li> <li> <p><code>internal/project/res_role_binding.go</code> \u2705 DONE</p> </li> <li> <p>Added <code>NewProjectUserRoleBinding()</code> for automatic <code>user</code> group binding</p> </li> <li> <p><code>internal/project/project.go</code> \u2705 DONE</p> </li> <li> <p>Added Sync and Delete calls for new roles and role bindings</p> </li> <li> <p><code>api/kube-dc.com/v1/types.go</code> \u2705 DONE</p> <ul> <li>Added role template name constants</li> </ul> </li> <li> <p><code>charts/kube-dc/templates/default-project-admin-role.yaml</code> \u2705 DONE</p> <ul> <li>Contains all 4 role templates: <code>admin</code>, <code>developer</code>, <code>project-manager</code>, <code>user</code></li> </ul> </li> </ol>"},{"location":"prd/organization_standard_groups/#controller-flow","title":"Controller Flow","text":"<pre><code>Organization Created\n    \u251c\u2500\u2500 Create Keycloak Realm\n    \u2502   \u2514\u2500\u2500 Create Groups: org-admin, user\n    \u251c\u2500\u2500 Create Org Namespace\n    \u2502   \u251c\u2500\u2500 Create Roles: {org}-admin, {org}-user\n    \u2502   \u2514\u2500\u2500 Create RoleBindings: org-admin \u2192 {org}-admin, user \u2192 {org}-user\n    \u2514\u2500\u2500 Done\n\nProject Created\n    \u251c\u2500\u2500 Create Project Namespace\n    \u2502   \u251c\u2500\u2500 Create Roles from templates: admin, developer, project-manager, user\n    \u2502   \u2514\u2500\u2500 Create RoleBindings: \n    \u2502       \u251c\u2500\u2500 {org}:org-admin \u2192 admin (exists)\n    \u2502       \u2514\u2500\u2500 {org}:user \u2192 user (new)\n    \u2514\u2500\u2500 Done\n\nOrganizationGroup Created (for elevated access)\n    \u251c\u2500\u2500 Create Keycloak Group (e.g., \"my-developers\")\n    \u251c\u2500\u2500 For each project in spec.permissions:\n    \u2502   \u2514\u2500\u2500 Create RoleBinding: {org}:my-developers \u2192 developer\n    \u2514\u2500\u2500 Done\n</code></pre>"},{"location":"prd/organization_standard_groups/#role-permissions-matrix","title":"Role Permissions Matrix","text":""},{"location":"prd/organization_standard_groups/#organization-namespace_1","title":"Organization Namespace","text":"Resource org-admin user organizations get, list, patch, update, watch get projects full CRUD get, list organizationgroups full CRUD -"},{"location":"prd/organization_standard_groups/#project-namespace_1","title":"Project Namespace","text":"Resource admin developer project-manager user virtualmachines full CRUD full CRUD get, list, watch get, list virtualmachineinstances full CRUD full CRUD get, list, watch get, list pods full CRUD full CRUD get, list, watch get, list pods/log get get get get services full CRUD full CRUD get, list, watch get, list deployments full CRUD full CRUD get, list, watch get, list secrets full CRUD full CRUD get, list - configmaps full CRUD full CRUD get, list get, list"},{"location":"prd/organization_standard_groups/#user-approval-flow","title":"User Approval Flow","text":"<pre><code>User requests to join organization\n    \u2193\nOrg admin sees join request in UI\n    \u2193\nAdmin selects group: \"user\" (default) or \"org-admin\"\n    \u2193\nBackend adds user to Keycloak group\n    \u2193\nUser gets automatic read-only access to all projects\n    \u2193\n(Optional) Admin creates OrganizationGroup for elevated per-project access\n</code></pre>"},{"location":"prd/organization_standard_groups/#success-criteria","title":"Success Criteria","text":"<ol> <li>New organizations have <code>org-admin</code> and <code>user</code> groups in Keycloak</li> <li>Join request approval adds users to <code>user</code> group by default</li> <li>Users in <code>user</code> group have read-only access to all projects</li> <li>Project creation includes <code>developer</code>, <code>project-manager</code>, <code>user</code> role templates</li> <li>OrganizationGroup CRD can grant elevated access per-project</li> </ol>"},{"location":"prd/organization_standard_groups/#out-of-scope","title":"Out of Scope","text":"<ul> <li>Migration of existing organizations (recreate after approval)</li> <li>Custom group creation via UI (future feature)</li> <li>Per-project group overrides outside OrganizationGroup CRD</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/","title":"PRD: OVN Database Connection Stability Improvements","text":""},{"location":"prd/ovn_database_stability_improvements/#status","title":"Status","text":"<ul> <li>Status: Proposed</li> <li>Created: 2026-01-12</li> <li>Priority: High</li> <li>Impact: Platform Stability, Service Availability</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#executive-summary","title":"Executive Summary","text":"<p>The kube-dc-manager controller experiences severe OVN database connection instability, resulting in 2.6+ million disconnections over 158 days of uptime. This causes: - 20+ minute service reconciliation deadlocks - LoadBalancer services stuck in <code>&lt;pending&gt;</code> state - Mass reconnection storms during network hiccups - Unpredictable service external IP assignment delays</p> <p>This PRD outlines comprehensive improvements to OVN database client connectivity, probe intervals, and reconnection strategies to achieve production-grade stability.</p>"},{"location":"prd/ovn_database_stability_improvements/#problem-statement","title":"Problem Statement","text":""},{"location":"prd/ovn_database_stability_improvements/#current-issues","title":"Current Issues","text":"<p>1. Extreme Connection Churn <pre><code>OVN Database Metrics (158 days uptime):\n- Disconnections: 2,682,477 (avg 16,964/day)\n- Active Connections: 13 established, 12 TIME_WAIT\n- Pattern: Constant disconnect/reconnect cycle\n</code></pre></p> <p>2. Service Reconciliation Deadlocks - Services remain <code>&lt;pending&gt;</code> for 20+ minutes during connection issues - Example: <code>jump-cp</code> and <code>jump-etcd-etcd-lb</code> stuck from 14:35 to 14:56 (21 minutes) - 32 services complete simultaneously when connection recovers (queued backlog)</p> <p>3. Controller Behavior - Before fix: Parallel deadlock (5 concurrent reconciliations block on libovsdb internal mutexes) - After SafeOvnClient fix: Serial bottleneck (one stuck operation blocks all others via operation mutex) - After timeout fix: Operations timeout after 30s, but still experience delays during reconnection</p>"},{"location":"prd/ovn_database_stability_improvements/#impact-on-production","title":"Impact on Production","text":"<ul> <li>Service Availability: LoadBalancer services cannot get external IPs during connection issues</li> <li>Unpredictability: No SLA for service provisioning time (0-30+ minutes)</li> <li>Cascading Failures: Connection loss triggers mass reconnection storm</li> <li>Resource Waste: Controller CPU spikes during reconnection attempts</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"prd/ovn_database_stability_improvements/#1-aggressive-server-side-probe-intervals-critical","title":"1. Aggressive Server-Side Probe Intervals \u26a0\ufe0f CRITICAL","text":"<p>Current Configuration: <pre><code>OVN_LEADER_PROBE_INTERVAL: 5 seconds\nOVN_NORTHD_PROBE_INTERVAL: 5000ms (5 seconds)\nPROBE_INTERVAL: 180000ms (3 minutes)\n</code></pre></p> <p>Problem: - Industry standard: 30-60 seconds for production environments - Any GC pause, network jitter, or temporary CPU spike \u2192 disconnection - Too aggressive for distributed systems with network latency</p> <p>Reference: Mirantis/OpenStack production deployments use 60-second inactivity probes</p>"},{"location":"prd/ovn_database_stability_improvements/#2-no-client-side-inactivity-monitoring-critical","title":"2. No Client-Side Inactivity Monitoring \u26a0\ufe0f CRITICAL","text":"<p>Current State: <pre><code>// kube-dc-manager uses kube-ovn v1.14.4\novsClient, err := ovs.NewOvnNbClient(\n    kubedccomv1.ConfigGlobal.MasterConfig.OvnDbIps,\n    ovnNbTimeout,           // 2 seconds\n    ovsDbConTimeout,        // 2 seconds  \n    ovsDbInactivityTimeout, // 2 seconds\n    maxRetry,               // 2 attempts\n)\n</code></pre></p> <p>Problems: - No proactive Echo requests to keep connections alive - No awareness of stale connections until server kills them - No graceful reconnection strategy (instant retry, thundering herd)</p> <p>What's Missing: <pre><code>// libovsdb client options NOT used:\nclient.WithInactivityCheck(30*time.Second, nil, nil)  // \u274c Not implemented\nclient.WithReconnect(60*time.Second, backoff)         // \u274c Not implemented\n</code></pre></p>"},{"location":"prd/ovn_database_stability_improvements/#3-kube-ovn-newovnnbclient-limitations","title":"3. kube-ovn NewOvnNbClient Limitations","text":"<p>Current Implementation (<code>kube-ovn v1.14.4/pkg/ovs/ovn.go</code>): <pre><code>func NewOvnNbClient(ovnNbAddr string, ovnNbTimeout, ovsDbConTimeout, \n                    ovsDbInactivityTimeout, maxRetry int) (*OVNNbClient, error) {\n    // ...\n    nbClient, err = ovsclient.NewOvsDbClient(\n        ovsclient.NBDB,\n        ovnNbAddr,\n        dbModel,\n        monitors,\n        ovsDbConTimeout,        // Used for initial connection timeout\n        ovsDbInactivityTimeout, // Passed to underlying client\n    )\n    // ...\n    // NO WithReconnect() option\n    // NO WithInactivityCheck() option\n    // Simple retry loop with 2-second sleep (no backoff)\n}\n</code></pre></p> <p>Limitations: - Does not expose libovsdb's <code>WithReconnect()</code> option - Does not expose libovsdb's <code>WithInactivityCheck()</code> option - No exponential backoff on reconnection - Fixed 2-second sleep between retries (can cause thundering herd)</p>"},{"location":"prd/ovn_database_stability_improvements/#4-controller-mutex-architecture","title":"4. Controller Mutex Architecture","text":"<p>Evolution:</p> <p>V1 - Original (Parallel Deadlock): <pre><code>// Multiple reconciliations share singleton client\n// libovsdb internal mutexes cause contention \u2192 DEADLOCK\n</code></pre></p> <p>V2 - SafeOvnClient (Serial Bottleneck): <pre><code>// Operation mutex serializes ALL OVN calls\n// One stuck operation blocks everything \u2192 20min hang\n</code></pre></p> <p>V3 - Timeout Wrapper (Current): <pre><code>// 30-second timeout on mutex acquisition\n// Stuck operations timeout and reset client\n// BUT: Reconnection still takes time, services still delayed\n</code></pre></p> <p>Still Missing: - Proactive connection health monitoring - Graceful degradation during reconnection - Connection pool per operation type (optional optimization)</p>"},{"location":"prd/ovn_database_stability_improvements/#proposed-solutions","title":"Proposed Solutions","text":""},{"location":"prd/ovn_database_stability_improvements/#phase-1-immediate-configuration-changes-zero-code","title":"Phase 1: Immediate Configuration Changes (Zero Code) \ud83d\ude80","text":"<p>1.1 Increase OVN Database Probe Intervals</p> <p>Change: <pre><code># Edit: kubectl edit deployment -n kube-system ovn-central\nenv:\n- name: OVN_LEADER_PROBE_INTERVAL\n  value: \"60\"  # Was: 5 (12x increase)\n\n- name: OVN_NORTHD_PROBE_INTERVAL\n  value: \"60000\"  # Was: 5000 (12x increase)\n\n- name: PROBE_INTERVAL\n  value: \"300000\"  # Was: 180000 (1.67x increase to 5 minutes)\n</code></pre></p> <p>Expected Impact: - \u2705 Reduce disconnections by 80-90% (from 17K/day to &lt;2K/day) - \u2705 Prevent transient network issues from causing mass reconnections - \u2705 Reduce OVN database CPU during connection churn - \u2705 More tolerance for GC pauses and temporary load spikes</p> <p>Risk: Very low - aligns with industry best practices</p> <p>Rollback: Simple <code>kubectl rollout undo</code> if issues occur</p> <p>1.2 Configure OVN Database Connection Inactivity Probe</p> <p>Change: <pre><code>kubectl exec -n kube-system ovn-central-* -- \\\n  ovn-nbctl set-connection ptcp:6641 -- \\\n  set connection . inactivity_probe=60000\n</code></pre></p> <p>Expected Impact: - \u2705 Server-side 60-second inactivity timeout (matches PROBE_INTERVAL) - \u2705 Consistent behavior across all connection types - \u2705 Prevents premature connection kills</p> <p>Risk: Low - standard OVN configuration</p>"},{"location":"prd/ovn_database_stability_improvements/#phase-2-client-side-improvements-code-changes","title":"Phase 2: Client-Side Improvements (Code Changes) \ud83d\udd27","text":"<p>2.1 Extend kube-ovn Client with libovsdb Options</p> <p>Option A: Wrapper Around NewOvnNbClient (Recommended)</p> <p>Create enhanced client initialization in <code>internal/service_lb/ovn_client.go</code>:</p> <pre><code>import (\n    \"github.com/cenkalti/backoff/v4\"\n    \"github.com/ovn-org/libovsdb/client\"\n)\n\nfunc createEnhancedOvnClient(addr string, timeout int) (*ovs.OVNNbClient, error) {\n    // 1. Create base client using kube-ovn\n    baseClient, err := ovs.NewOvnNbClient(\n        addr,\n        timeout,\n        ovsDbConTimeout,\n        ovsDbInactivityTimeout,\n        maxRetry,\n    )\n    if err != nil {\n        return nil, err\n    }\n\n    // 2. Access underlying libovsdb client\n    // Note: May require reflection or type assertion depending on kube-ovn internals\n    underlyingClient := baseClient.Client // Access internal client field\n\n    // 3. Configure reconnection strategy\n    reconnectBackoff := backoff.NewExponentialBackOff()\n    reconnectBackoff.InitialInterval = 1 * time.Second\n    reconnectBackoff.MaxInterval = 30 * time.Second\n    reconnectBackoff.MaxElapsedTime = 2 * time.Minute\n    reconnectBackoff.Multiplier = 2.0\n\n    // Apply options to underlying client\n    underlyingClient.SetOption(client.WithReconnect(60*time.Second, reconnectBackoff))\n    underlyingClient.SetOption(client.WithInactivityCheck(\n        30*time.Second,  // Send Echo every 30s\n        nil,             // Default success handler\n        func(err error) {\n            klog.Warningf(\"OVN client inactivity check failed: %v\", err)\n        },\n    ))\n\n    return baseClient, nil\n}\n</code></pre> <p>Option B: Fork kube-ovn NewOvnNbClient (Alternative)</p> <p>Copy and modify <code>NewOvnNbClient</code> to include reconnection options:</p> <pre><code>func NewEnhancedOvnNbClient(ovnNbAddr string, ovnNbTimeout, ovsDbConTimeout, \n                            ovsDbInactivityTimeout, maxRetry int) (*OVNNbClient, error) {\n    dbModel, err := ovnnb.FullDatabaseModel()\n    if err != nil {\n        return nil, err\n    }\n\n    // ... setup indexes and monitors (same as kube-ovn) ...\n\n    // Configure reconnection strategy\n    reconnectBackoff := backoff.NewExponentialBackOff()\n    reconnectBackoff.InitialInterval = 1 * time.Second\n    reconnectBackoff.MaxInterval = 30 * time.Second\n    reconnectBackoff.MaxElapsedTime = 2 * time.Minute\n\n    // Create client with enhanced options\n    nbClient, err = ovsclient.NewOvsDbClient(\n        ovsclient.NBDB,\n        ovnNbAddr,\n        dbModel,\n        monitors,\n        ovsDbConTimeout,\n        ovsDbInactivityTimeout,\n        // NEW: Add reconnection options\n        client.WithReconnect(60*time.Second, reconnectBackoff),\n        client.WithInactivityCheck(30*time.Second, nil, nil),\n    )\n\n    // ... rest of implementation ...\n}\n</code></pre> <p>Recommended Approach: Option A (wrapper) for minimal kube-ovn dependency changes</p> <p>2.2 Update GetOvnClient Implementation</p> <p>File: <code>internal/service_lb/ovn_client.go</code></p> <p>Change: <pre><code>func GetOvnClient(ctx context.Context, cli client.Client) (*SafeOvnClient, error) {\n    ovnClientMu.RLock()\n    if globalOvnClient != nil {\n        ovnClientMu.RUnlock()\n        return &amp;SafeOvnClient{\n            OVNNbClient: globalOvnClient,\n            opMu:        &amp;ovnOpMu,\n        }, nil\n    }\n    ovnClientMu.RUnlock()\n\n    ovnClientMu.Lock()\n    defer ovnClientMu.Unlock()\n\n    if globalOvnClient != nil {\n        return &amp;SafeOvnClient{\n            OVNNbClient: globalOvnClient,\n            opMu:        &amp;ovnOpMu,\n        }, nil\n    }\n\n    if err := kubedccomv1.ConfigGlobal.ReadConfig(ctx, cli); err != nil {\n        return nil, err\n    }\n\n    // NEW: Use enhanced client creation\n    ovsClient, err := createEnhancedOvnClient(\n        kubedccomv1.ConfigGlobal.MasterConfig.OvnDbIps,\n        ovnNbTimeout,\n    )\n    if err != nil {\n        return nil, err\n    }\n\n    globalOvnClient = ovsClient\n    return &amp;SafeOvnClient{\n        OVNNbClient: globalOvnClient,\n        opMu:        &amp;ovnOpMu,\n    }, nil\n}\n</code></pre></p> <p>Benefits: - \u2705 Proactive connection health monitoring (Echo requests every 30s) - \u2705 Automatic reconnection with exponential backoff - \u2705 Graceful handling of temporary network issues - \u2705 Reduced thundering herd during mass reconnections</p>"},{"location":"prd/ovn_database_stability_improvements/#phase-3-monitoring-observability","title":"Phase 3: Monitoring &amp; Observability \ud83d\udcca","text":"<p>3.1 Add Prometheus Metrics</p> <pre><code>import (\n    \"github.com/prometheus/client_golang/prometheus\"\n)\n\nvar (\n    ovnConnectionsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"kube_dc_ovn_connections_total\",\n            Help: \"Total OVN database connections by state\",\n        },\n        []string{\"state\"}, // connected, disconnected, reconnecting\n    )\n\n    ovnOperationDuration = prometheus.NewHistogramVec(\n        prometheus.HistogramOpts{\n            Name: \"kube_dc_ovn_operation_duration_seconds\",\n            Help: \"OVN operation duration in seconds\",\n            Buckets: []float64{.001, .005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10, 30},\n        },\n        []string{\"operation\"}, // GetLogicalRouter, ListLoadBalancers, etc.\n    )\n\n    ovnMutexWaitDuration = prometheus.NewHistogram(\n        prometheus.HistogramOpts{\n            Name: \"kube_dc_ovn_mutex_wait_duration_seconds\",\n            Help: \"Time spent waiting for OVN operation mutex\",\n            Buckets: []float64{.001, .01, .1, 1, 5, 10, 30},\n        },\n    )\n)\n\nfunc init() {\n    prometheus.MustRegister(ovnConnectionsTotal)\n    prometheus.MustRegister(ovnOperationDuration)\n    prometheus.MustRegister(ovnMutexWaitDuration)\n}\n</code></pre> <p>Use in tryLockWithTimeout: <pre><code>func (c *SafeOvnClient) tryLockWithTimeout() error {\n    start := time.Now()\n    lockCh := make(chan struct{})\n    go func() {\n        c.opMu.Lock()\n        close(lockCh)\n    }()\n\n    select {\n    case &lt;-lockCh:\n        ovnMutexWaitDuration.Observe(time.Since(start).Seconds())\n        return nil\n    case &lt;-time.After(ovnOpMutexTimeout):\n        ovnMutexWaitDuration.Observe(ovnOpMutexTimeout.Seconds())\n        ovnConnectionsTotal.WithLabelValues(\"timeout\").Inc()\n        ResetOvnClient()\n        return fmt.Errorf(\"OVN operation mutex timeout after %v\", ovnOpMutexTimeout)\n    }\n}\n</code></pre></p> <p>Grafana Dashboard Metrics: - OVN connection state over time - Operation duration percentiles (p50, p95, p99) - Mutex wait time distribution - Reconnection events per hour - Service reconciliation success rate</p> <p>3.2 Enhanced Logging</p> <pre><code>// Add structured logging for connection events\nklog.InfoS(\"OVN client connecting\", \n    \"address\", ovnNbAddr,\n    \"timeout\", ovnNbTimeout,\n    \"retry\", attempt)\n\nklog.InfoS(\"OVN client reconnecting\",\n    \"reason\", \"inactivity_timeout\",\n    \"elapsed\", timeSinceLastActivity)\n\nklog.WarningS(\"OVN operation slow\",\n    \"operation\", \"GetLogicalRouter\",\n    \"duration\", duration,\n    \"threshold\", 5*time.Second)\n</code></pre>"},{"location":"prd/ovn_database_stability_improvements/#implementation-plan","title":"Implementation Plan","text":""},{"location":"prd/ovn_database_stability_improvements/#timeline","title":"Timeline","text":"Phase Tasks Duration Priority Phase 1 Configuration changes (probe intervals) 1 day P0 - Critical Phase 2 Client-side improvements (code) 3-5 days P1 - High Phase 3 Monitoring &amp; metrics 2-3 days P2 - Medium <p>Total Estimated Effort: 6-9 days</p>"},{"location":"prd/ovn_database_stability_improvements/#phase-1-configuration-changes-day-1","title":"Phase 1: Configuration Changes (Day 1)","text":"<p>Tasks: 1. \u2705 Document current OVN configuration 2. \u2705 Backup ovn-central deployment YAML 3. \u2705 Apply probe interval changes 4. \u2705 Monitor disconnection rate for 24 hours 5. \u2705 Validate no service disruption</p> <p>Success Criteria: - Disconnection rate drops by &gt;80% - No increase in service reconciliation failures - No DEADLOCK alerts in controller logs</p>"},{"location":"prd/ovn_database_stability_improvements/#phase-2-client-improvements-days-2-6","title":"Phase 2: Client Improvements (Days 2-6)","text":"<p>Tasks: 1. Research kube-ovn client internals for extensibility 2. Implement <code>createEnhancedOvnClient()</code> wrapper 3. Add exponential backoff configuration 4. Add inactivity check with Echo requests 5. Update <code>GetOvnClient()</code> to use enhanced client 6. Unit tests for reconnection scenarios 7. Integration tests with simulated network failures 8. Deploy to staging environment 9. Monitor for 48 hours before production</p> <p>Success Criteria: - Client reconnects within 5 seconds of disconnection - No thundering herd during mass reconnections - Service reconciliation continues during brief network issues - Mutex timeout events drop to zero</p>"},{"location":"prd/ovn_database_stability_improvements/#phase-3-observability-days-7-9","title":"Phase 3: Observability (Days 7-9)","text":"<p>Tasks: 1. Add Prometheus metrics collection 2. Create Grafana dashboard 3. Set up alerts for abnormal connection patterns 4. Document troubleshooting runbook 5. Train team on new metrics</p> <p>Success Criteria: - Real-time visibility into OVN connection health - Alerts trigger before user-visible impact - &lt; 5 minute MTTR for connection issues</p>"},{"location":"prd/ovn_database_stability_improvements/#success-metrics","title":"Success Metrics","text":""},{"location":"prd/ovn_database_stability_improvements/#immediate-phase-1-configuration","title":"Immediate (Phase 1 - Configuration)","text":"Metric Before Target Daily Disconnections 16,964 &lt; 2,000 Service Provisioning P95 30+ minutes &lt; 5 minutes DEADLOCK Alerts/Day 2-5 0 Controller CPU (avg) Unknown Baseline + Monitor"},{"location":"prd/ovn_database_stability_improvements/#long-term-phase-23-code-monitoring","title":"Long-term (Phase 2+3 - Code + Monitoring)","text":"Metric Before Target Service Provisioning P95 30+ minutes &lt; 30 seconds Service Provisioning P99 Unknown &lt; 2 minutes Reconnection Time P95 Unknown &lt; 10 seconds MTTR for Connection Issues Unknown &lt; 5 minutes Connection Uptime SLA Unknown 99.9%"},{"location":"prd/ovn_database_stability_improvements/#risks-mitigation","title":"Risks &amp; Mitigation","text":""},{"location":"prd/ovn_database_stability_improvements/#risk-1-increased-probe-intervals-delay-failure-detection","title":"Risk 1: Increased Probe Intervals Delay Failure Detection","text":"<p>Risk: Longer probe intervals mean slower detection of actual failures</p> <p>Mitigation: - Client-side inactivity checks (30s) provide faster detection than server probes (60s) - Echo requests every 30s ensure liveness monitoring - Acceptable tradeoff for production stability</p>"},{"location":"prd/ovn_database_stability_improvements/#risk-2-kube-ovn-internal-changes-required","title":"Risk 2: kube-ovn Internal Changes Required","text":"<p>Risk: kube-ovn v1.14.4 may not expose necessary client APIs</p> <p>Mitigation: - Use reflection/type assertion to access internal client - Fork kube-ovn client code if necessary (Option B) - Contribute improvements upstream to kube-ovn project</p>"},{"location":"prd/ovn_database_stability_improvements/#risk-3-regression-during-rollout","title":"Risk 3: Regression During Rollout","text":"<p>Risk: Changes could introduce new stability issues</p> <p>Mitigation: - Staged rollout: Config \u2192 Staging \u2192 Production - Comprehensive monitoring during each phase - Quick rollback plan (kubectl rollout undo) - Feature flag for enhanced client (gradual enablement)</p>"},{"location":"prd/ovn_database_stability_improvements/#testing-strategy","title":"Testing Strategy","text":""},{"location":"prd/ovn_database_stability_improvements/#unit-tests","title":"Unit Tests","text":"<pre><code>func TestSafeOvnClient_ReconnectionBackoff(t *testing.T) {\n    // Test exponential backoff behavior\n}\n\nfunc TestSafeOvnClient_InactivityCheck(t *testing.T) {\n    // Test Echo request behavior\n}\n\nfunc TestSafeOvnClient_MutexTimeout(t *testing.T) {\n    // Test 30-second timeout enforcement\n}\n</code></pre>"},{"location":"prd/ovn_database_stability_improvements/#integration-tests","title":"Integration Tests","text":"<pre><code>func TestLoadBalancerService_DuringNetworkPartition(t *testing.T) {\n    // Simulate network partition\n    // Verify service eventually gets external IP\n    // Verify no deadlock\n}\n\nfunc TestLoadBalancerService_DuringOVNRestart(t *testing.T) {\n    // Restart OVN database\n    // Verify client reconnects\n    // Verify services continue reconciling\n}\n</code></pre>"},{"location":"prd/ovn_database_stability_improvements/#chaos-testing","title":"Chaos Testing","text":"<ul> <li>Randomly kill OVN database connections</li> <li>Inject network latency (100-500ms)</li> <li>Simulate OVN database CPU saturation</li> <li>Test with 100+ simultaneous service creations</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#documentation-requirements","title":"Documentation Requirements","text":"<ol> <li>Architecture Docs: Update OVN client connectivity diagram</li> <li>Operations Runbook: Troubleshooting guide for connection issues</li> <li>Metrics Guide: Prometheus metrics and Grafana dashboards</li> <li>Configuration Reference: All tunable parameters and defaults</li> <li>Migration Guide: Rollout procedure and rollback steps</li> </ol>"},{"location":"prd/ovn_database_stability_improvements/#dependencies","title":"Dependencies","text":""},{"location":"prd/ovn_database_stability_improvements/#external-dependencies","title":"External Dependencies","text":"<ul> <li>kube-ovn: v1.14.4 (current), potential upgrade to v1.15+ for better client APIs</li> <li>libovsdb: Indirect dependency via kube-ovn</li> <li>backoff library: <code>github.com/cenkalti/backoff/v4</code> (already in kube-ovn)</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#internal-dependencies","title":"Internal Dependencies","text":"<ul> <li>OVN database deployment configuration</li> <li>kube-dc-manager controller deployment</li> <li>Prometheus metrics infrastructure (if Phase 3)</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#future-enhancements","title":"Future Enhancements","text":""},{"location":"prd/ovn_database_stability_improvements/#post-implementation-improvements","title":"Post-Implementation Improvements","text":"<ol> <li>Connection Pooling: Separate pools for read vs write operations</li> <li>Circuit Breaker: Temporarily disable OVN operations during extended outages</li> <li>Degraded Mode: Continue serving existing services even when OVN unreachable</li> <li>Multi-Database HA: Support OVN database clustering (not just single-node)</li> <li>Local Caching: Cache read-heavy operations (logical routers, switches)</li> </ol>"},{"location":"prd/ovn_database_stability_improvements/#kube-ovn-upstream-contributions","title":"kube-ovn Upstream Contributions","text":"<ol> <li>PR to expose <code>WithReconnect()</code> and <code>WithInactivityCheck()</code> options</li> <li>PR to use exponential backoff in <code>NewOvnNbClient()</code></li> <li>Share production learnings and best practices</li> </ol>"},{"location":"prd/ovn_database_stability_improvements/#references","title":"References","text":""},{"location":"prd/ovn_database_stability_improvements/#documentation","title":"Documentation","text":"<ul> <li>libovsdb Client Options</li> <li>Mirantis OVS Timeouts Guide</li> <li>kube-ovn v1.14.4 Source</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#related-issues","title":"Related Issues","text":"<ul> <li>Original deadlock issue: Services stuck <code>&lt;pending&gt;</code> for 20+ minutes</li> <li>SafeOvnClient fix: Added operation mutex (commit <code>760fffa</code>)</li> <li>Timeout wrapper fix: Added 30s timeout on mutex (commit <code>f57c5c9</code>)</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#investigation-findings","title":"Investigation Findings","text":"<ul> <li>OVN Database: 2,682,477 disconnections over 158 days</li> <li>Probe Intervals: 5s (too aggressive, should be 60s)</li> <li>Client Options: No inactivity checking or reconnection strategy</li> <li>Pattern: Mass reconnection during network hiccups causes cascade</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#approval-sign-off","title":"Approval &amp; Sign-off","text":""},{"location":"prd/ovn_database_stability_improvements/#stakeholders","title":"Stakeholders","text":"<ul> <li>Engineering Lead: Review technical approach</li> <li>Platform Team: Review infrastructure changes</li> <li>SRE: Review monitoring and operations impact</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#approval-criteria","title":"Approval Criteria","text":"<ul> <li>\u2705 Technical design reviewed</li> <li>\u2705 Risk assessment completed</li> <li>\u2705 Rollback plan documented</li> <li>\u2705 Success metrics defined</li> <li>\u2705 Testing strategy approved</li> </ul>"},{"location":"prd/ovn_database_stability_improvements/#appendix","title":"Appendix","text":""},{"location":"prd/ovn_database_stability_improvements/#a-current-configuration-snapshot","title":"A. Current Configuration Snapshot","text":"<pre><code># OVN Central Environment\nkubectl get deployment -n kube-system ovn-central -o yaml | grep -A 3 \"env:\"\n\nenv:\n- name: ENABLE_SSL\n  value: \"false\"\n- name: NODE_IPS\n  value: 192.168.1.3\n- name: OVN_LEADER_PROBE_INTERVAL\n  value: \"5\"\n- name: OVN_NORTHD_PROBE_INTERVAL\n  value: \"5000\"\n- name: PROBE_INTERVAL\n  value: \"180000\"\n</code></pre>"},{"location":"prd/ovn_database_stability_improvements/#b-disconnection-analysis","title":"B. Disconnection Analysis","text":"<pre><code># OVN Cluster Status (2026-01-12)\nkubectl exec -n kube-system ovn-central-* -- \\\n  ovs-appctl -t /var/run/ovn/ovnnb_db.ctl cluster/status OVN_Northbound\n\nDisconnections: 2682477\nUptime: 158 days\nRate: ~16,964 disconnections/day\n</code></pre>"},{"location":"prd/ovn_database_stability_improvements/#c-service-reconciliation-timeline","title":"C. Service Reconciliation Timeline","text":"<pre><code>14:35:12 - Service jump-etcd-etcd-lb: reconciliation started\n14:35:12 - Creating LoadBalancer resource manager\n14:35:12 - [STUCK - OVN connection issue]\n...\n14:56:15 - Service jump-etcd-etcd-lb: reconciliation completed (21 minutes)\n14:56:15 - 32 services complete simultaneously (backlog flush)\n</code></pre> <p>Document Version: 1.0 Last Updated: 2026-01-12 Author: kube-dc Platform Team Review Date: TBD</p>"},{"location":"prd/rook-ceph-object-storage-service/","title":"PRD: Rook Ceph Object Storage Service for Kube-DC","text":""},{"location":"prd/rook-ceph-object-storage-service/#overview","title":"Overview","text":"<p>S3-compatible object storage as a managed service within Kube-DC. Users create buckets, manage access keys, and upload/download objects through the UI \u2014 similar to AWS S3 or DigitalOcean Spaces. Backed by Rook Ceph RGW (RADOS Gateway), with support for both local and remote Ceph clusters.</p>"},{"location":"prd/rook-ceph-object-storage-service/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Kube-DC UI                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Object       \u2502  \u2502 Bucket       \u2502  \u2502 File Browser             \u2502  \u2502\n\u2502  \u2502 Storage Tab  \u2502  \u2502 Management   \u2502  \u2502 (Upload/Download/Delete) \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                 \u2502                      \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                       UI Backend (Express.js)                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Bucket API   \u2502  \u2502 S3 Keys API  \u2502  \u2502 S3 Proxy (presigned URLs)\u2502  \u2502\n\u2502  \u2502 (K8s CRDs)   \u2502  \u2502 (K8s Secret) \u2502  \u2502 (AWS SDK \u2192 RGW)         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                 \u2502                      \u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Kubernetes API / S3 API                           \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Go Controller           \u2502    \u2502 Rook-Ceph                      \u2502  \u2502\n\u2502  \u2502 (kube-dc manager)       \u2502    \u2502                                \u2502  \u2502\n\u2502  \u2502                         \u2502    \u2502  CephObjectStore (my-store)    \u2502  \u2502\n\u2502  \u2502 Organization Reconciler \u2502\u2500\u2500\u2500\u25b6\u2502  CephObjectStoreUser (per-org) \u2502  \u2502\n\u2502  \u2502 - CephObjectStoreUser   \u2502    \u2502  ObjectBucketClaim (per-bucket)\u2502  \u2502\n\u2502  \u2502 - S3 Quota enforcement  \u2502    \u2502  RGW Gateway (S3 API)         \u2502  \u2502\n\u2502  \u2502 - Billing integration   \u2502    \u2502                                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 Ceph Cluster (local or remote)                              \u2502    \u2502\n\u2502  \u2502  MON \u2192 MGR \u2192 OSD(s) \u2192 RGW                                  \u2502    \u2502\n\u2502  \u2502  S3 Endpoint: s3.kube-dc.cloud (via Envoy Gateway)         \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#what-already-exists","title":"What Already Exists","text":""},{"location":"prd/rook-ceph-object-storage-service/#go-controller-kube-dc-manager","title":"Go Controller (kube-dc manager)","text":"<ul> <li><code>res_s3_quota.go</code>: Creates/updates <code>CephObjectStoreUser</code> per organization with quotas (<code>maxSize</code>, <code>maxBuckets</code>, <code>maxObjects</code>). Handles suspension (sets <code>maxSize=0</code>). Deletes user on org removal.</li> <li><code>plan_config.go</code>: <code>ObjectStorage</code> field per billing plan (dev=20GB, pro=100GB, scale=500GB). <code>MaxBuckets</code> auto-calculated. <code>ObjectStorageConfig</code> for Ceph namespace/store name.</li> <li><code>organization.go</code>: Syncs S3 quota during organization reconciliation. Handles suspended/canceled orgs.</li> </ul>"},{"location":"prd/rook-ceph-object-storage-service/#billing-plans-valuesyaml","title":"Billing Plans (values.yaml)","text":"<pre><code>dev-pool:   objectStorage: 20    # 20 GB, 5 buckets\npro-pool:   objectStorage: 100   # 100 GB, 20 buckets\nscale-pool: objectStorage: 500   # 500 GB, 50 buckets\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#examples","title":"Examples","text":"<ul> <li><code>ObjectBucketClaim</code> examples in <code>examples/organization/object-storage/</code></li> <li>S3 public policy JSON example</li> </ul>"},{"location":"prd/rook-ceph-object-storage-service/#implementation-status-2026-02-13","title":"Implementation Status (2026-02-13)","text":"<p>\u2705 Phase 1 \u2014 Rook Ceph Deployment &amp; S3 Endpoint: Complete - Rook Operator v1.19.1 deployed via Helm - CephCluster: 1 MON + 1 MGR + 1 OSD (500GB loop device, single-node) - CephObjectStore <code>my-store</code> with RGW gateway - StorageClass <code>ceph-bucket</code> for OBC provisioning - S3 endpoint exposed via Envoy Gateway with TLS (Let's Encrypt) - BackendTrafficPolicy for large uploads (no timeout) - Deployment doc: <code>docs/deploy-rook-ceph-object-storage.md</code></p> <p>\u2705 Phase 2 \u2014 UI Backend API: Complete - File: <code>ui/backend/controllers/objectStorageModule.js</code> - Bucket CRUD (create/delete/list via ObjectBucketClaim) - Bucket details with per-bucket credentials from OBC secrets - File browser API (list, upload via presigned URL, download, delete, create folder) - S3 key retrieval (org-level credentials from rook-ceph secret) - Key management: list all keys, generate new keys, revoke keys (via RGW Admin API) - Quota/usage API (RGW Admin API <code>GET /admin/user?stats=true</code> + <code>GET /admin/bucket?stats=true</code>) - Bucket policies: public-read / private toggle via S3 bucket policy - CORS auto-configuration for browser uploads</p> <p>\u2705 Phase 3 \u2014 UI Frontend: Complete - File: <code>ui/frontend/src/app/ManageWorkloads/Views/ObjectStorage/ObjectStorageView.tsx</code> - Object Storage sidebar tab with tree view (Overview, Buckets, Access Keys) - Overview: quota usage bars, endpoint info, bucket count, storage used - Buckets: table with expandable details, access toggle (Public/Private), Edit/Browse/Delete actions - File Browser (<code>BucketFileBrowser.tsx</code>): breadcrumb navigation, drag-and-drop upload, download, delete, create folder - Access Keys: primary credentials with reveal/copy, Key Management card (generate/revoke), CLI examples (AWS CLI, Python boto3, kubectl) - Sidebar tree: <code>ObjectStorageTreeViewImpl.tsx</code> with per-bucket entries - Billing integration: Object Storage usage bars on Billing Overview, Subscription, and Project pages</p> <p>\u2705 Go Controller Enhancements: - <code>res_s3_quota.go</code>: CephObjectStoreUser capabilities upgraded to <code>user=*</code>, <code>bucket=*</code> (enables key management via RGW Admin API) - Patch logic also updates capabilities on existing users (not just quotas) - Billing quota controller (<code>quotaController.js</code>): shared helpers <code>getOrgS3Context</code>, <code>sumBucketUsage</code>, <code>buildStorageResult</code> (DRY) - Project-level object storage usage uses org-level quota limits</p> <p>\ud83d\udd32 Phase 4 \u2014 Remote Ceph Cluster Support: Not started - External CephCluster mode - Multi-store configuration - Per-org store assignment</p> <p>\ud83d\udd32 Not Yet Implemented: - Remote Ceph cluster support - Virtual-hosted bucket access (<code>&lt;bucket&gt;.s3.example.com</code>) - Custom bucket policies (JSON editor \u2014 currently only public-read/private toggle) - Sub-user creation with restricted permissions (read-only sub-users)</p>"},{"location":"prd/rook-ceph-object-storage-service/#phased-implementation","title":"Phased Implementation","text":""},{"location":"prd/rook-ceph-object-storage-service/#phase-1-rook-ceph-deployment-s3-endpoint","title":"Phase 1: Rook Ceph Deployment &amp; S3 Endpoint","text":"<p>Goal: Working S3 endpoint accessible internally and externally.</p>"},{"location":"prd/rook-ceph-object-storage-service/#11-deploy-rook-ceph-operator","title":"1.1 Deploy Rook Ceph Operator","text":"<p>Add to installer template or Helm chart:</p> <pre><code># Rook Operator (namespace: rook-ceph)\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  chart:\n    spec:\n      chart: rook-ceph\n      version: v1.16.x\n      sourceRef:\n        kind: HelmRepository\n        name: rook-release\n  values:\n    crds:\n      enabled: true\n    resources:\n      requests:\n        cpu: 200m\n        memory: 256Mi\n      limits:\n        memory: 512Mi\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#12-deploy-cephcluster-single-node-worker","title":"1.2 Deploy CephCluster (Single-Node, Worker)","text":"<p>As defined in existing <code>kube-dc-cloud-storage-design.md</code> Phase 2: - Single MON, MGR, OSD on worker node (20T HDD) - <code>storageClassDeviceSets</code> using <code>local-path</code> - No replication (<code>size: 1</code>) \u2014 acceptable for initial deployment</p>"},{"location":"prd/rook-ceph-object-storage-service/#13-deploy-cephobjectstore","title":"1.3 Deploy CephObjectStore","text":"<pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStore\nmetadata:\n  name: my-store\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    replicated:\n      size: 1\n  dataPool:\n    replicated:\n      size: 1\n  preservePoolsOnDelete: true\n  gateway:\n    port: 80\n    securePort: 443\n    instances: 1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 256Mi\n      limits:\n        memory: 1Gi\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#14-deploy-storageclass-for-obc","title":"1.4 Deploy StorageClass for OBC","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ceph-bucket\nprovisioner: rook-ceph.ceph.rook.io/bucket\nreclaimPolicy: Delete\nparameters:\n  objectStoreName: my-store\n  objectStoreNamespace: rook-ceph\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#15-expose-s3-endpoint-via-envoy-gateway","title":"1.5 Expose S3 Endpoint via Envoy Gateway","text":"<pre><code># Internal Service already created by Rook: rook-ceph-rgw-my-store.rook-ceph\n# Expose externally via HTTPRoute\n\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: s3-endpoint\n  namespace: rook-ceph\nspec:\n  hostnames:\n    - s3.kube-dc.cloud\n  parentRefs:\n    - name: eg\n      namespace: envoy-gateway-system\n      sectionName: https-s3\n  rules:\n    - backendRefs:\n        - name: rook-ceph-rgw-my-store\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n---\n# BackendTrafficPolicy for large uploads\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: BackendTrafficPolicy\nmetadata:\n  name: s3-timeouts\n  namespace: rook-ceph\nspec:\n  targetRefs:\n    - group: gateway.networking.k8s.io\n      kind: HTTPRoute\n      name: s3-endpoint\n  timeout:\n    http:\n      requestTimeout: \"0s\"          # No timeout for large uploads\n      connectionIdleTimeout: 3600s\n      maxConnectionDuration: 7200s\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#16-dns","title":"1.6 DNS","text":"<ul> <li><code>s3.kube-dc.cloud</code> \u2192 Envoy Gateway LB IP</li> <li>Wildcard <code>*.s3.kube-dc.cloud</code> for virtual-hosted bucket access (optional, path-style first)</li> </ul> <p>Phase 1 Deliverables: - [x] Rook Ceph operator running - [x] CephCluster healthy (1 MON, 1 MGR, 1 OSD) - [x] CephObjectStore <code>my-store</code> with RGW gateway - [x] StorageClass <code>ceph-bucket</code> for OBC - [x] S3 endpoint exposed via Envoy Gateway with TLS - [x] BackendTrafficPolicy for large file uploads - [x] Existing org reconciler creates <code>CephObjectStoreUser</code> automatically</p>"},{"location":"prd/rook-ceph-object-storage-service/#phase-2-ui-backend-api","title":"Phase 2: UI Backend API","text":"<p>Goal: REST API for bucket and S3 key management.</p>"},{"location":"prd/rook-ceph-object-storage-service/#21-backend-routes","title":"2.1 Backend Routes","text":"<p>New file: <code>ui/backend/controllers/objectStorageController.js</code></p> <pre><code>GET    /api/object-storage/:namespace/buckets          # List buckets for project\nPOST   /api/object-storage/:namespace/buckets          # Create bucket (ObjectBucketClaim)\nDELETE /api/object-storage/:namespace/buckets/:name     # Delete bucket\nGET    /api/object-storage/:namespace/buckets/:name     # Get bucket details + usage\n\nGET    /api/object-storage/:namespace/keys              # Get S3 access keys\nPOST   /api/object-storage/:namespace/keys/regenerate   # Regenerate S3 keys\n\nGET    /api/object-storage/:namespace/quota             # Get org S3 quota + usage\nGET    /api/object-storage/:namespace/endpoint           # Get S3 endpoint URL\n\n# File browser (proxied via presigned URLs)\nGET    /api/object-storage/:namespace/buckets/:name/objects?prefix=&amp;delimiter=  # List objects\nPOST   /api/object-storage/:namespace/buckets/:name/upload-url                  # Get presigned upload URL\nGET    /api/object-storage/:namespace/buckets/:name/download-url/:key           # Get presigned download URL\nDELETE /api/object-storage/:namespace/buckets/:name/objects/:key                # Delete object\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#22-how-bucket-creation-works","title":"2.2 How Bucket Creation Works","text":"<p>When a user creates a bucket through the UI:</p> <ol> <li>Frontend \u2192 <code>POST /api/object-storage/:namespace/buckets</code> with <code>{ name: \"my-bucket\" }</code></li> <li>Backend creates an <code>ObjectBucketClaim</code> in the user's project namespace:    <pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: my-bucket\n  namespace: shalb-demo        # User's project namespace\n  labels:\n    kube-dc.com/organization: shalb\nspec:\n  bucketName: shalb-demo-my-bucket  # Prefixed with namespace for uniqueness\n  storageClassName: ceph-bucket\n</code></pre></li> <li>Rook OBC controller provisions the bucket in <code>my-store</code> using the org's <code>CephObjectStoreUser</code></li> <li>Rook creates a Secret + ConfigMap in the namespace with S3 credentials and endpoint</li> </ol>"},{"location":"prd/rook-ceph-object-storage-service/#23-s3-credential-model","title":"2.3 S3 Credential Model","text":"<p>Users have no access to <code>rook-ceph</code> namespace. Two credential paths exist:</p>"},{"location":"prd/rook-ceph-object-storage-service/#per-bucket-credentials-user-accessible-in-project-namespace","title":"Per-Bucket Credentials (user-accessible, in project namespace)","text":"<p>When an <code>ObjectBucketClaim</code> is created in a project namespace, Rook auto-creates:</p> <pre><code>Secret/&lt;bucket-name&gt;           \u2192 AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\nConfigMap/&lt;bucket-name&gt;        \u2192 BUCKET_HOST, BUCKET_PORT, BUCKET_NAME, BUCKET_REGION\n</code></pre> <p>These live in the user's project namespace (e.g., <code>shalb-demo</code>) and are directly accessible. Users can <code>kubectl get secret my-bucket -o yaml</code> in their own namespace.</p> <p>This is the primary credential path for end users.</p>"},{"location":"prd/rook-ceph-object-storage-service/#org-level-credentials-admin-only-proxied-via-backend-api","title":"Org-Level Credentials (admin-only, proxied via backend API)","text":"<p>The <code>CephObjectStoreUser</code> (one per org, created by Go controller) owns the master S3 credentials:</p> <pre><code>Secret: rook-ceph-object-user-my-store-&lt;orgname&gt;   (in rook-ceph namespace)\n  AccessKey: XXXX\n  SecretKey: XXXX\n  Endpoint: http://rook-ceph-rgw-my-store.rook-ceph:80\n</code></pre> <p>Users cannot access this directly. The UI backend reads it via service account and exposes it through the API:</p> <pre><code>GET /api/object-storage/:namespace/keys\n</code></pre> <p>The backend resolves <code>namespace \u2192 organization</code> (via namespace labels), then reads the org-level Secret from <code>rook-ceph</code>. This gives the user full account access (all buckets under their org's quota).</p>"},{"location":"prd/rook-ceph-object-storage-service/#credential-flow-summary","title":"Credential Flow Summary","text":"<pre><code>User creates bucket via UI/CLI\n  \u2192 ObjectBucketClaim in shalb-demo\n  \u2192 Rook creates Secret + ConfigMap in shalb-demo  \u2190 user can read this\n  \u2192 User gets per-bucket S3 keys from their own namespace\n\nUser requests \"Account Keys\" via UI\n  \u2192 Backend reads Secret from rook-ceph (service account)\n  \u2192 Returns org-level keys to user                  \u2190 proxied, not direct access\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#24-file-browser-presigned-url-proxy","title":"2.4 File Browser (Presigned URL Proxy)","text":"<p>For the file browser, the backend uses the org's S3 keys to generate presigned URLs:</p> <pre><code>const { S3Client, ListObjectsV2Command, PutObjectCommand, GetObjectCommand, DeleteObjectCommand } = require('@aws-sdk/client-s3');\nconst { getSignedUrl } = require('@aws-sdk/s3-request-presigner');\n\n// Create S3 client using org's credentials from K8s Secret\nconst s3Client = new S3Client({\n  endpoint: 'https://s3.kube-dc.cloud',\n  region: 'us-east-1',  // Ceph default\n  credentials: { accessKeyId, secretAccessKey },\n  forcePathStyle: true,  // Required for Ceph RGW\n});\n\n// List objects\nconst listObjects = async (bucket, prefix) =&gt; {\n  const cmd = new ListObjectsV2Command({\n    Bucket: bucket,\n    Prefix: prefix,\n    Delimiter: '/',\n  });\n  return s3Client.send(cmd);\n};\n\n// Generate presigned upload URL (frontend uploads directly to S3)\nconst getUploadUrl = async (bucket, key) =&gt; {\n  const cmd = new PutObjectCommand({ Bucket: bucket, Key: key });\n  return getSignedUrl(s3Client, cmd, { expiresIn: 3600 });\n};\n</code></pre> <p>Phase 2 Deliverables: - [x] Bucket CRUD API (<code>ObjectBucketClaim</code> management) - [x] S3 key retrieval API - [x] Key management API (list all keys, generate, revoke via RGW Admin API) - [x] Quota/usage API (RGW Admin API stats) - [x] File browser API (list, presigned upload/download, delete, create folder) - [x] Bucket policy API (public-read / private toggle) - [x] RBAC: users can only manage buckets in their project namespaces</p>"},{"location":"prd/rook-ceph-object-storage-service/#phase-3-ui-frontend","title":"Phase 3: UI Frontend","text":"<p>Goal: Object Storage tab in the sidebar with bucket management and file browser.</p>"},{"location":"prd/rook-ceph-object-storage-service/#31-sidebar-integration","title":"3.1 Sidebar Integration","text":"<p>Add \"Object Storage\" tab to <code>Sidebar.tsx</code>, alongside existing tabs (VM, System, Volumes, Networking):</p> <pre><code>Sidebar Tabs:\n  [VM] [Nodes] [System] [Volumes] [Networking] [Object Storage]\n</code></pre> <p>Tree view under Object Storage tab: <pre><code>\ud83d\udce6 Object Storage\n\u251c\u2500\u2500 \ud83d\udcca Overview (quota usage, endpoint info)\n\u251c\u2500\u2500 \ud83e\udea3 Buckets\n\u2502   \u251c\u2500\u2500 my-bucket (24 objects, 1.2 GB)\n\u2502   \u251c\u2500\u2500 backups (8 objects, 5.4 GB)\n\u2502   \u2514\u2500\u2500 + Create Bucket\n\u2514\u2500\u2500 \ud83d\udd11 Access Keys\n</code></pre></p>"},{"location":"prd/rook-ceph-object-storage-service/#32-views","title":"3.2 Views","text":"<p>Overview View (<code>ObjectStorageOverview.tsx</code>): - S3 endpoint URL (copyable): <code>s3.kube-dc.cloud</code> - Storage quota bar: <code>12.5 GB / 100 GB used</code> - Bucket count: <code>3 / 20 buckets</code> - Quick start guide with <code>aws s3</code> CLI examples</p> <p>Bucket List View (<code>BucketListView.tsx</code>): - Table: Name, Objects, Size, Created, Actions (Delete) - \"Create Bucket\" button \u2192 modal</p> <p>Bucket Detail / File Browser (<code>BucketFileBrowser.tsx</code>): - Breadcrumb navigation: <code>my-bucket / images / 2024 /</code> - File/folder list: Name, Size, Last Modified, Actions - Upload button (drag-and-drop zone) - Download button (presigned URL) - Delete button (with confirmation) - Create folder - Pagination for large directories</p> <p>Access Keys View (<code>AccessKeysView.tsx</code>): - Show Access Key ID (visible) - Show Secret Key (hidden, reveal on click) - Copy buttons - Regenerate keys button (with confirmation) - Connection examples:   <pre><code># AWS CLI\naws configure set aws_access_key_id XXXXX\naws configure set aws_secret_access_key XXXXX\naws --endpoint-url https://s3.kube-dc.cloud s3 ls\n\n# s3cmd\ns3cmd --configure --host=s3.kube-dc.cloud --host-bucket=s3.kube-dc.cloud\n\n# Python boto3\nimport boto3\ns3 = boto3.client('s3', endpoint_url='https://s3.kube-dc.cloud',\n                   aws_access_key_id='XXXXX', aws_secret_access_key='XXXXX')\n</code></pre></p> <p>Create Bucket Modal (<code>CreateBucketModal.tsx</code>): - Bucket name input (validated: lowercase, alphanumeric, hyphens) - Access policy: Private (default) / Public Read - Create button</p>"},{"location":"prd/rook-ceph-object-storage-service/#33-route-structure","title":"3.3 Route Structure","text":"<pre><code>/project/:projectName/object-storage                \u2192 Overview\n/project/:projectName/object-storage/buckets         \u2192 Bucket list\n/project/:projectName/object-storage/bucket/:name    \u2192 File browser\n/project/:projectName/object-storage/keys            \u2192 Access keys\n</code></pre> <p>Phase 3 Deliverables: - [x] Object Storage sidebar tab (<code>ObjectStorageTreeViewImpl.tsx</code>) - [x] Overview view with quota usage (RGW Admin API stats, quota bars) - [x] Bucket list view with create/delete, expandable details, access toggle - [x] File browser with upload/download/delete, folder creation, drag-and-drop - [x] Access keys view with CLI examples + Key Management (generate/revoke) - [x] Route integration in <code>MainContainer.tsx</code> - [x] Billing integration: Object Storage usage bars on Billing &amp; Project pages</p>"},{"location":"prd/rook-ceph-object-storage-service/#phase-4-remote-ceph-cluster-support","title":"Phase 4: Remote Ceph Cluster Support","text":"<p>Goal: Connect to external Ceph clusters (customer-owned or multi-site).</p>"},{"location":"prd/rook-ceph-object-storage-service/#41-cephcluster-external-mode","title":"4.1 CephCluster External Mode","text":"<p>Rook supports connecting to an external Ceph cluster without deploying MON/OSD/MGR locally:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: external-ceph\n  namespace: rook-ceph-external\nspec:\n  external:\n    enable: true\n  # No storage, mon, mgr specs needed\n</code></pre> <p>Requirements from the remote cluster: - Ceph monitor endpoints - Admin keyring (for user/bucket management) - RGW endpoint (for S3 access)</p>"},{"location":"prd/rook-ceph-object-storage-service/#42-import-script","title":"4.2 Import Script","text":"<p>Rook provides <code>create-external-cluster-resources.py</code> which exports the needed credentials from a source Ceph cluster:</p> <pre><code># On the remote Ceph cluster\npython3 create-external-cluster-resources.py \\\n  --rbd-data-pool-name &lt;pool&gt; \\\n  --rgw-endpoint &lt;rgw-host&gt;:&lt;port&gt; \\\n  --namespace rook-ceph-external \\\n  --format bash\n</code></pre> <p>This outputs Secrets and ConfigMaps that need to be applied to the management cluster.</p>"},{"location":"prd/rook-ceph-object-storage-service/#43-multi-store-configuration","title":"4.3 Multi-Store Configuration","text":"<p>Support multiple <code>CephObjectStore</code> instances pointing to different clusters:</p> <pre><code># values.yaml\nobjectStorageConfig:\n  stores:\n    - name: local-store\n      namespace: rook-ceph\n      endpoint: s3.kube-dc.cloud\n      default: true\n    - name: remote-store\n      namespace: rook-ceph-external\n      endpoint: s3-remote.kube-dc.cloud\n      region: eu-west-1\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#44-per-organization-store-assignment","title":"4.4 Per-Organization Store Assignment","text":"<p>Extend <code>PlanDefinitionYAML</code> to optionally specify which object store to use:</p> <pre><code>plans:\n  scale-pool:\n    objectStorage: 500\n    objectStoreName: remote-store   # Optional: specific store for this plan\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#45-ui-store-selection","title":"4.5 UI: Store Selection","text":"<p>If multiple stores are configured, show store selector in: - Bucket creation modal (which store to create bucket in) - Overview (per-store usage breakdown) - Access keys (per-store credentials)</p> <p>Phase 4 Deliverables: - [ ] External CephCluster support - [ ] Import script/workflow for remote cluster credentials - [ ] Multi-store configuration in billing plans - [ ] Per-org store assignment - [ ] UI store selector (when multiple stores exist)</p>"},{"location":"prd/rook-ceph-object-storage-service/#data-model","title":"Data Model","text":""},{"location":"prd/rook-ceph-object-storage-service/#kubernetes-resources-per-organization","title":"Kubernetes Resources (per organization)","text":"Resource Namespace Created By Purpose <code>CephObjectStoreUser/&lt;org&gt;</code> <code>rook-ceph</code> Go controller (auto) Org-level S3 user with quotas <code>Secret/rook-ceph-object-user-my-store-&lt;org&gt;</code> <code>rook-ceph</code> Rook (auto) S3 access key + secret key"},{"location":"prd/rook-ceph-object-storage-service/#kubernetes-resources-per-bucket","title":"Kubernetes Resources (per bucket)","text":"Resource Namespace Created By Purpose <code>ObjectBucketClaim/&lt;bucket&gt;</code> <code>&lt;org&gt;-&lt;project&gt;</code> UI Backend Bucket provisioning request <code>ObjectBucket/&lt;bucket&gt;</code> cluster-scoped Rook (auto) Actual bucket reference <code>Secret/bucket-&lt;bucket&gt;</code> <code>&lt;org&gt;-&lt;project&gt;</code> Rook (auto) Bucket-specific S3 credentials <code>ConfigMap/bucket-&lt;bucket&gt;</code> <code>&lt;org&gt;-&lt;project&gt;</code> Rook (auto) Bucket endpoint + name"},{"location":"prd/rook-ceph-object-storage-service/#quota-enforcement","title":"Quota Enforcement","text":"<pre><code>Organization (billing plan)\n  \u2514\u2500 CephObjectStoreUser (maxSize: 100G, maxBuckets: 20, maxObjects: 100000)\n      \u2514\u2500 Bucket 1 (ObjectBucketClaim in project-ns)\n      \u2514\u2500 Bucket 2 (ObjectBucketClaim in project-ns)\n      \u2514\u2500 ...\n</code></pre> <p>Quotas enforced at Ceph RGW level \u2014 no bypass possible even via direct S3 API.</p>"},{"location":"prd/rook-ceph-object-storage-service/#rbac","title":"RBAC","text":""},{"location":"prd/rook-ceph-object-storage-service/#user-permissions","title":"User Permissions","text":"<p>Users manage buckets via <code>ObjectBucketClaim</code> in their project namespace. Existing Kube-DC RBAC already scopes users to their project namespaces.</p> <p>Required ClusterRole addition for project members: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kube-dc-object-storage\nrules:\n  - apiGroups: [\"objectbucket.io\"]\n    resources: [\"objectbucketclaims\"]\n    verbs: [\"get\", \"list\", \"create\", \"delete\"]\n</code></pre></p>"},{"location":"prd/rook-ceph-object-storage-service/#backend-service-account","title":"Backend Service Account","text":"<p>The UI backend needs read access to the <code>CephObjectStoreUser</code> Secret in <code>rook-ceph</code> namespace for S3 key retrieval:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kube-dc-s3-keys-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\"]\n    resourceNames: []  # Filtered by org name in code\n  - apiGroups: [\"ceph.rook.io\"]\n    resources: [\"cephobjectstoreusers\"]\n    verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#s3-endpoint-configuration","title":"S3 Endpoint Configuration","text":""},{"location":"prd/rook-ceph-object-storage-service/#path-style-phase-1","title":"Path-Style (Phase 1)","text":"<pre><code>https://s3.kube-dc.cloud/&lt;bucket-name&gt;/&lt;key&gt;\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#virtual-hosted-phase-4-optional","title":"Virtual-Hosted (Phase 4, optional)","text":"<pre><code>https://&lt;bucket-name&gt;.s3.kube-dc.cloud/&lt;key&gt;\n</code></pre> <p>Requires wildcard DNS + TLS certificate.</p>"},{"location":"prd/rook-ceph-object-storage-service/#billing-integration","title":"Billing Integration","text":""},{"location":"prd/rook-ceph-object-storage-service/#current-plan-limits","title":"Current Plan Limits","text":"Plan Object Storage Max Buckets Price Dev Pool 20 GB 5 \u20ac19/mo Pro Pool 100 GB 20 \u20ac49/mo Scale Pool 500 GB 50 \u20ac99/mo"},{"location":"prd/rook-ceph-object-storage-service/#usage-tracking","title":"Usage Tracking","text":"<p>The UI backend queries Ceph RGW admin API (or <code>radosgw-admin</code> equivalent) for per-user usage:</p> <pre><code>GET /admin/user?uid=&lt;org-name&gt;&amp;stats=true\n</code></pre> <p>Returns: <pre><code>{\n  \"stats\": {\n    \"size\": 12884901888,\n    \"size_actual\": 12884901888,\n    \"num_objects\": 1542\n  }\n}\n</code></pre></p> <p>This is exposed in the UI Overview as quota usage bars.</p>"},{"location":"prd/rook-ceph-object-storage-service/#overage-enforcement","title":"Overage / Enforcement","text":"<p>Ceph RGW enforces quotas natively. When quota is exceeded: - New uploads return <code>HTTP 403 QuotaExceeded</code> - Existing objects remain accessible - UI shows warning banner</p>"},{"location":"prd/rook-ceph-object-storage-service/#security-considerations","title":"Security Considerations","text":"<ul> <li>S3 keys: Stored in Kubernetes Secrets, accessed only via authenticated backend API</li> <li>Bucket isolation: Each bucket is prefixed with <code>&lt;namespace&gt;-</code> to prevent name collisions</li> <li>Quota enforcement: At Ceph RGW level (server-side, cannot be bypassed)</li> <li>TLS: S3 endpoint exposed via HTTPS through Envoy Gateway</li> <li>RBAC: Users can only create OBCs in namespaces they have access to</li> <li>Public buckets: Optional, requires explicit bucket policy (S3 policy JSON)</li> </ul>"},{"location":"prd/rook-ceph-object-storage-service/#resource-requirements","title":"Resource Requirements","text":""},{"location":"prd/rook-ceph-object-storage-service/#phase-1-single-node-ceph","title":"Phase 1 (Single-Node Ceph)","text":"Component CPU Memory Rook Operator 200m 256Mi MON 100m 384Mi MGR 500m 2Gi OSD (1x) 300m 1Gi RGW 100m 256Mi Total 1.2 CPU ~4 Gi"},{"location":"prd/rook-ceph-object-storage-service/#phase-4-ha-remote","title":"Phase 4 (HA + Remote)","text":"Component CPU Memory Rook Operator 200m 256Mi MON (3x) 300m 1.2Gi MGR (2x) 1000m 4Gi OSD (4x) 1200m 4Gi RGW (2x) 200m 512Mi Total ~3 CPU ~10 Gi"},{"location":"prd/rook-ceph-object-storage-service/#testing-strategy","title":"Testing Strategy","text":""},{"location":"prd/rook-ceph-object-storage-service/#unit-tests","title":"Unit Tests","text":"<ul> <li>Go controller: <code>CephObjectStoreUser</code> creation/update/deletion</li> <li>Backend API: bucket CRUD, key management mocking</li> </ul>"},{"location":"prd/rook-ceph-object-storage-service/#integration-tests","title":"Integration Tests","text":"<ul> <li>Create org \u2192 verify <code>CephObjectStoreUser</code> created with correct quotas</li> <li>Create bucket via API \u2192 verify <code>ObjectBucketClaim</code> + Secret + ConfigMap</li> <li>Upload/download file via presigned URL</li> <li>Quota enforcement: upload beyond limit \u2192 verify rejection</li> <li>Delete bucket \u2192 verify cleanup</li> </ul>"},{"location":"prd/rook-ceph-object-storage-service/#e2e-tests","title":"E2E Tests","text":"<ul> <li>Full flow: create org \u2192 subscribe plan \u2192 create bucket \u2192 upload file \u2192 browse \u2192 delete</li> <li>Suspended org: verify uploads blocked (<code>maxSize=0</code>)</li> <li>Plan upgrade: verify quota increase reflected</li> </ul>"},{"location":"prd/rook-ceph-object-storage-service/#implementation-priority","title":"Implementation Priority","text":"<pre><code>Phase 1 (Week 1-2):  Rook Ceph deployment + S3 endpoint          \u2705 DONE\nPhase 2 (Week 2-3):  Backend API for bucket/key management        \u2705 DONE\nPhase 3 (Week 3-5):  UI frontend (Object Storage tab)             \u2705 DONE\nPhase 4 (Week 6+):   Remote Ceph cluster support                  \ud83d\udd32 NOT STARTED\n</code></pre>"},{"location":"prd/rook-ceph-object-storage-service/#remaining-work","title":"Remaining Work","text":"<ol> <li>Remote Ceph cluster support (Phase 4)</li> <li>Custom bucket policies (JSON editor for fine-grained access control)</li> <li>Sub-user creation with restricted permissions (read-only keys)</li> <li>Virtual-hosted bucket access (<code>&lt;bucket&gt;.s3.example.com</code>)</li> </ol>"},{"location":"prd/rook-ceph-object-storage-service/#files-to-create-modify","title":"Files to Create / Modify","text":""},{"location":"prd/rook-ceph-object-storage-service/#files-created","title":"Files Created","text":"File Purpose <code>ui/backend/controllers/objectStorageModule.js</code> Backend API: buckets, keys, files, quota, policies <code>ui/frontend/src/app/ManageWorkloads/Views/ObjectStorage/ObjectStorageView.tsx</code> All views: Overview, Buckets, Access Keys (single file) <code>ui/frontend/src/app/ManageWorkloads/Views/ObjectStorage/BucketFileBrowser.tsx</code> File browser with upload/download/delete/folders <code>ui/frontend/src/app/ManageWorkloads/Sidebar/ObjectStorageTreeViewImpl.tsx</code> Sidebar tree with per-bucket entries <code>docs/deploy-rook-ceph-object-storage.md</code> Deployment guide (generic domains)"},{"location":"prd/rook-ceph-object-storage-service/#files-modified","title":"Files Modified","text":"File Change <code>ui/frontend/src/app/ManageWorkloads/MainContainer.tsx</code> Added OBJECT_STORAGE view index + routes <code>ui/frontend/src/app/ManageWorkloads/Sidebar/Sidebar.tsx</code> Added Object Storage tab <code>ui/backend/server.js</code> Registered objectStorage routes <code>ui/backend/controllers/billing/quotaController.js</code> Added object storage usage to billing (DRY helpers) <code>ui/frontend/src/app/ManageOrganization/Billing/Billing.tsx</code> Added Object Storage usage bars <code>ui/frontend/src/app/ManageOrganization/Billing/types.ts</code> Added <code>ObjectStorageUsage</code> interface <code>ui/frontend/src/app/ManageWorkloads/Views/Main/MainView.tsx</code> Added Object Storage bar to project quotas, removed Pods bar <code>internal/organization/res_s3_quota.go</code> Upgraded capabilities <code>user=*</code>, patch includes capabilities <code>charts/kube-dc/templates/backend-sa.yaml</code> Added RBAC for CephObjectStoreUser + OBC read <code>charts/kube-dc/templates/default-project-admin-role.yaml</code> Added OBC permissions to all project roles"},{"location":"prd/rook-ceph-object-storage-service/#references","title":"References","text":"<ul> <li>Rook Ceph Documentation</li> <li>Rook CephObjectStore</li> <li>Rook ObjectBucketClaim</li> <li>Rook External Cluster</li> <li>Ceph RGW Admin API</li> <li>AWS SDK v3 for JS</li> <li>Existing design doc</li> </ul>"},{"location":"prd/secondary-external-network-fip/","title":"PRD: EIP Support on Secondary External Networks","text":""},{"location":"prd/secondary-external-network-fip/#status-implemented","title":"Status: \u2705 IMPLEMENTED","text":"<p>Completed: 2026-01-21</p>"},{"location":"prd/secondary-external-network-fip/#overview","title":"Overview","text":"<p>This document describes the implementation for enabling External IPs (EIPs) to work correctly when allocated from a secondary external network (different from the project's default egress network). This affects both FIPs and Service LoadBalancers.</p>"},{"location":"prd/secondary-external-network-fip/#problem-statement","title":"Problem Statement","text":"<p>When a project has <code>egressNetworkType: cloud</code>, FIPs or LoadBalancer services allocated from <code>ext-public</code> (secondary network) fail to respond. The DNAT works (traffic reaches the pod), but return traffic is dropped because OVN's SNAT only triggers when <code>outport</code> matches the NAT's gateway port.</p> <pre><code>VPC: test-jump (egressNetworkType: cloud)\n\u251c\u2500\u2500 Default Route: 0.0.0.0/0 \u2192 100.65.0.1 (ext-cloud)\n\u2514\u2500\u2500 FIP NAT: 91.224.11.10 \u2194 10.0.0.7 (on ext-public)\n\nTraffic Flow (Broken):\n1. Inbound: 91.224.11.10 \u2192 DNAT \u2192 10.0.0.7 \u2705\n2. Return: 10.0.0.7 \u2192 routing \u2192 outport = ext-cloud (default route)\n3. SNAT check: outport == ext-public? NO \u2192 SNAT skipped\n4. Traffic exits via ext-cloud with source 10.0.0.7 \u2192 DROPPED\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#solution-policy-based-routing","title":"Solution: Policy-Based Routing","text":"<p>Add source-based policy routes to override default routing for EIPs on secondary networks.</p> <pre><code>ovn-nbctl lr-policy-add test-jump 30000 \"ip4.src == 10.0.0.7\" reroute 91.224.11.1\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#implementation-details","title":"Implementation Details","text":""},{"location":"prd/secondary-external-network-fip/#files-modified","title":"Files Modified","text":"File Changes <code>internal/utils/policy_route.go</code> New file - PolicyRouteManager for VPC policy route operations <code>internal/controller/kube-dc.com/fip_controller.go</code> Added <code>syncPolicyRoute()</code> and <code>deletePolicyRoute()</code> <code>internal/service_lb/service_lb.go</code> Added <code>syncPolicyRoutes()</code> and <code>deletePolicyRoutes()</code>"},{"location":"prd/secondary-external-network-fip/#policy-route-manager-internalutilspolicy_routego","title":"Policy Route Manager (<code>internal/utils/policy_route.go</code>)","text":"<pre><code>const (\n    PolicyRoutePriorityFIP   int = 30000  // FIP policy routes\n    PolicyRoutePrioritySvcLB int = 30010  // SvcLB policy routes (different priority to avoid conflicts)\n)\n\ntype PolicyRouteManager struct {\n    cli      client.Client\n    vpcName  string\n    priority int\n}\n\n// Key methods:\nfunc (m *PolicyRouteManager) SyncPolicyRoute(ctx context.Context, internalIP, gateway string) error\nfunc (m *PolicyRouteManager) DeletePolicyRoute(ctx context.Context, internalIP string) error\nfunc (m *PolicyRouteManager) SyncPolicyRoutes(ctx context.Context, internalIPs []string, gateway string) error\nfunc (m *PolicyRouteManager) DeleteAllPolicyRoutes(ctx context.Context) error\n\n// Helper functions:\nfunc NeedsPolicyRoute(eipSubnetName, projectDefaultSubnetName string) bool\nfunc GetSubnetGateway(ctx context.Context, cli client.Client, subnetName string) (string, error)\nfunc GetOvnEipSubnet(ctx context.Context, cli client.Client, ovnEipName string) (string, error)\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#fip-controller-integration","title":"FIP Controller Integration","text":"<pre><code>// In reconcileSync(), after FIP is synced:\nif err := r.syncPolicyRoute(ctx, reconciledFIp, resourceEIp.Found().Status.OvnEipRef, targetIP, &amp;l); err != nil {\n    l.Error(err, \"Failed to sync policy route for FIP\")\n}\n\n// In reconcileDelete():\nif err := r.deletePolicyRoute(ctx, reconciledFIp, &amp;l); err != nil {\n    l.Error(err, \"Failed to delete policy route for FIP\")\n}\n\n// syncPolicyRoute checks if policy route is needed:\nfunc (r *FIpReconciler) syncPolicyRoute(...) error {\n    // Get project's default external subnet\n    defaultSubnet, _ := proc.GetDefaultExternalSubnet()\n\n    // Get EIP's actual subnet\n    eipSubnetName, _ := utils.GetOvnEipSubnet(ctx, r.Client, ovnEipName)\n\n    // Only add policy route if EIP is on secondary network\n    if !utils.NeedsPolicyRoute(eipSubnetName, defaultSubnet.Name) {\n        return nil\n    }\n\n    // Get gateway and create policy route\n    gateway, _ := utils.GetSubnetGateway(ctx, r.Client, eipSubnetName)\n    prm := utils.NewPolicyRouteManager(r.Client, vpcName, utils.PolicyRoutePriorityFIP)\n    return prm.SyncPolicyRoute(ctx, internalIP, gateway)\n}\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#svclb-controller-integration","title":"SvcLB Controller Integration","text":"<pre><code>// In LBResource.Sync(), after LB VIPs are synced:\nif err := r.syncPolicyRoutes(ctx, backends); err != nil {\n    r.LogDebug(\"Failed to sync policy routes: %v\", err)\n}\n\n// In LBResource.Delete():\nif err := r.deletePolicyRoutes(ctx); err != nil {\n    r.LogDebug(\"Failed to delete policy routes, continuing: %v\", err)\n}\n\n// syncPolicyRoutes handles multiple backend IPs:\nfunc (r *LBResource) syncPolicyRoutes(ctx context.Context, backends []string) error {\n    needsRoute, gateway, _ := r.checkPolicyRouteNeeded(ctx)\n    if !needsRoute {\n        return nil\n    }\n\n    prm := utils.NewPolicyRouteManager(r.Cli, vpcName, utils.PolicyRoutePrioritySvcLB)\n    return prm.SyncPolicyRoutes(ctx, backends, gateway)\n}\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#vpc-patching-with-retryhandler","title":"VPC Patching with RetryHandler","text":"<p>Policy routes are stored in <code>Vpc.Spec.PolicyRoutes</code> (Kubernetes CRD). Critical: Create RetryHandler BEFORE modifying the VPC to capture original state for proper patch diff:</p> <pre><code>vpc := &amp;kubeovn.Vpc{}\ncli.Get(ctx, client.ObjectKey{Name: m.vpcName}, vpc)\n\n// Create handler BEFORE modifications\nvpcUpd := objmgr.NewRetryHandler(m.cli, vpc).WithContext(ctx)\n\n// Now modify\nvpc.Spec.PolicyRoutes = append(vpc.Spec.PolicyRoutes, newRoute)\n\n// Patch with proper diff\nvpcUpd.WaitPatch()\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#state-persistence","title":"State Persistence","text":"<p>Policy routes are stored in Kubernetes VPC CRD (etcd), not directly in OVN DB:</p> <pre><code>Kubernetes VPC CRD \u2192 Kube-OVN Controller \u2192 OVN NB Database\n     (etcd)              (watches)           (ovsdb)\n</code></pre> <p>After any restart, kube-ovn rebuilds OVN state from Kubernetes CRDs automatically.</p>"},{"location":"prd/secondary-external-network-fip/#multiple-external-networks","title":"Multiple External Networks","text":"<p>The implementation is generic and supports any number of external networks. The gateway is dynamically retrieved from the EIP's actual subnet:</p> <pre><code>eipSubnetName, _ := GetOvnEipSubnet(ctx, cli, ovnEipName)  // e.g., \"ext-public-2\"\ngateway, _ := GetSubnetGateway(ctx, cli, eipSubnetName)     // e.g., \"92.0.0.1\"\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#test-results","title":"Test Results","text":"<pre><code># VPC policy routes after creating FIP and SvcLB with public EIPs:\n$ kubectl get vpc test-jump -o jsonpath='{.spec.policyRoutes}' | jq\n[\n  {\"action\": \"reroute\", \"match\": \"ip4.src == 10.0.0.7\",  \"nextHopIP\": \"91.224.11.1\", \"priority\": 30000},\n  {\"action\": \"reroute\", \"match\": \"ip4.src == 10.0.0.13\", \"nextHopIP\": \"91.224.11.1\", \"priority\": 30010},\n  {\"action\": \"reroute\", \"match\": \"ip4.src == 10.0.0.14\", \"nextHopIP\": \"91.224.11.1\", \"priority\": 30010}\n]\n\n# Test public SvcLB on cloud project:\n$ curl 91.224.11.13:80\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n...\n</code></pre>"},{"location":"prd/secondary-external-network-fip/#known-limitations","title":"Known Limitations","text":""},{"location":"prd/secondary-external-network-fip/#fip-and-cloud-network-service-sharing-same-pod-ip","title":"FIP and Cloud-Network Service Sharing Same Pod IP","text":"<p>A pod cannot simultaneously serve as: 1. A target for a public FIP 2. A backend for a cloud-network LoadBalancer service</p> <p>Root Cause: Policy routes are source-based (<code>ip4.src == &lt;internal-ip&gt;</code>), affecting ALL outbound traffic from that IP. A public FIP's policy route breaks cloud-network services using the same pod.</p> <p>Workarounds: 1. Use separate pods for FIP targets and cloud-service backends 2. Use consistent network types (all public or all cloud) 3. Remove conflicting FIP if cloud service access is required</p> <p>See tutorial-service-exposure.md for details.</p>"},{"location":"prd/secondary-external-network-fip/#priority-reference","title":"Priority Reference","text":"Priority Purpose 31000 Allow internal subnet traffic (kube-ovn default) 30010 SvcLB source-based reroute 30000 FIP source-based reroute Default Routing table lookup"},{"location":"prd/secondary-external-network-fip/#ovn-commands-reference","title":"OVN Commands Reference","text":"<pre><code># List policy routes\novn-nbctl lr-policy-list &lt;router&gt;\n\n# Add policy route manually (for testing)\novn-nbctl lr-policy-add &lt;router&gt; &lt;priority&gt; &lt;match&gt; reroute &lt;nexthop&gt;\n\n# Delete policy route\novn-nbctl lr-policy-del &lt;router&gt; &lt;priority&gt; [match]\n</code></pre>"},{"location":"prd/service_lb_sync_issue/","title":"Service LoadBalancer OVN Sync Issue","text":""},{"location":"prd/service_lb_sync_issue/#summary","title":"Summary","text":"<p>When kube-ovn-controller restarts (due to OVN database timeouts or other failures), the LoadBalancer VIP entries managed by <code>kube-dc-manager</code> are lost from the OVN Northbound database. This causes external services (like tenant cluster API servers) to become unreachable until <code>kube-dc-manager</code> is manually restarted.</p>"},{"location":"prd/service_lb_sync_issue/#issue-details","title":"Issue Details","text":""},{"location":"prd/service_lb_sync_issue/#observed-behavior","title":"Observed Behavior","text":"<p>Date: 2025-12-01</p> <p>Symptoms: - Tenant cluster control planes unreachable: <code>dial tcp 168.119.17.55:6443: connect: no route to host</code> - All EIP resources show <code>READY: true</code> - OVN EIP resources show <code>READY: true</code> - VMs are running correctly</p> <p>Timeline: 1. <code>kube-ovn-controller</code> experienced OVN database connection timeouts:    <pre><code>E1201 10:34:42.771547 controller.go:1021] OVN database echo timeout (4/5) after 60s\nE1201 10:35:57.772950 controller.go:1021] OVN database echo timeout (5/5) after 60s\nE1201 10:35:57.773181 klog.go:10] \"OVN database connection timeout after 5 attempts\"\n</code></pre> 2. <code>kube-ovn-controller</code> pod restarted at 10:35:58 3. After restart, OVN NB database was missing LoadBalancer entries for:    - <code>shalb-envoy-user-a-cp-tcp</code> (168.119.17.55:6443)    - <code>shalb-envoy-user-b-cp-tcp</code> (168.119.17.53:6443)    - <code>shalb-envoy-user-c-cp-tcp</code> (168.119.17.56:6443)    - <code>shalb-envoy-envoy-delta-controller-envoy-tcp</code> (168.119.17.54:443/10000/9901)</p>"},{"location":"prd/service_lb_sync_issue/#root-cause-analysis","title":"Root Cause Analysis","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     writes to      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  kube-dc-manager    \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 \u2502   OVN NB Database   \u2502\n\u2502  (service_lb.go)    \u2502                    \u2502   (LoadBalancers)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    \u2191\n                                                    \u2502 reconciles\n                                                    \u2502\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502 kube-ovn-controller \u2502\n                                           \u2502                     \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The Problem:</p> <ol> <li><code>kube-dc-manager</code> writes LoadBalancer VIPs directly to OVN NB database via <code>ovs.NbClient</code></li> <li><code>kube-ovn-controller</code> manages its own OVN resources but does NOT manage <code>kube-dc-manager</code>'s LBs</li> <li>When <code>kube-ovn-controller</code> restarts, it reconciles resources it knows about</li> <li><code>kube-dc-manager</code>'s LBs are NOT reconciled because:</li> <li>They are not tracked by any Kubernetes resource that <code>kube-ovn-controller</code> watches</li> <li><code>kube-dc-manager</code> only reconciles on Endpoints changes, not on OVN state changes</li> </ol>"},{"location":"prd/service_lb_sync_issue/#affected-code","title":"Affected Code","text":"<p><code>internal/service_lb/service_lb.go</code>:</p> <pre><code>func (r *LBResource) Sync(ctx context.Context) error {\n    // Builds VIP maps\n    vipListTcp := map[string]string{}\n    // ...\n\n    // Updates OVN LB directly\n    err = r.updateLbs(r.tcpLbName(), ovnnb.LoadBalancerProtocolTCP, vipListTcp)\n\n    // Attaches to router and switch\n    err = r.ovsCli.LogicalRouterUpdateLoadBalancers(r.projectRouter.Name, ...)\n    err = r.ovsCli.LogicalSwitchUpdateLoadBalancers(project.SubnetName(r.project), ...)\n}\n</code></pre> <p>The sync is triggered by: - Endpoint creation/update - Service creation/update</p> <p>NOT triggered by: - OVN database reconnection - kube-ovn-controller restart - OVN NB database state changes</p>"},{"location":"prd/service_lb_sync_issue/#solutions-considered","title":"Solutions Considered","text":"Option Description Pros Cons Status Periodic Reconciliation Full resync every N minutes Simple Downtime up to N min, wasteful \u274c Rejected OVN Connection Monitor Track connection state Immediate Requires OVS client mods \u274c Rejected K8s Annotation State Store LB state in annotations Declarative Complex implementation \u274c Rejected OVSDB Event Watch Subscribe to LB deletions Real-time Complex, stability concerns \u274c Rejected LB Watcher + Restart Detection Verify LBs periodically + detect restarts Minimal impact, targeted Slight delay \u2705 Implemented"},{"location":"prd/service_lb_sync_issue/#implemented-solution","title":"Implemented Solution","text":"<p>LB Watcher with Periodic Verification + kube-ovn-controller Restart Detection</p>"},{"location":"prd/service_lb_sync_issue/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            LBWatcher                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Periodic Verification      \u2502    \u2502  Restart Detection          \u2502    \u2502\n\u2502  \u2502  (every 2 min)              \u2502    \u2502  (poll every 30s)           \u2502    \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n\u2502  \u2502 1. List LoadBalancer svcs   \u2502    \u2502 1. Check pod restart count  \u2502    \u2502\n\u2502  \u2502 2. Check OVN LB exists      \u2502    \u2502 2. If changed: wait 30s     \u2502    \u2502\n\u2502  \u2502 3. If missing: trigger      \u2502    \u2502 3. Verify all LBs           \u2502    \u2502\n\u2502  \u2502    reconciliation           \u2502    \u2502                             \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                 \u2502                                \u2502                      \u2502\n\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502                                  \u25bc                                      \u2502\n\u2502                    Update annotation on Service                         \u2502\n\u2502                    kube-dc.com/lb-resync-timestamp                      \u2502\n\u2502                                  \u2502                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Service Controller         \u2502\n                    \u2502  (existing reconcile loop)  \u2502\n                    \u2502  - Detects annotation change\u2502\n                    \u2502  - Recreates OVN LB         \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"prd/service_lb_sync_issue/#files-modifiedcreated","title":"Files Modified/Created","text":"<ol> <li><code>internal/service_lb/lb_watcher.go</code> (New)</li> <li><code>LBWatcher</code> struct implementing <code>manager.Runnable</code></li> <li>Periodic verification every 2 minutes</li> <li>kube-ovn-controller pod restart detection (every 30s)</li> <li> <p>Triggers reconciliation via annotation update</p> </li> <li> <p><code>internal/controller/core/service_controller.go</code> (Modified)</p> </li> <li>Added <code>lbWatcher</code> field to <code>ServiceReconciler</code></li> <li>Added RBAC for pods: <code>+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch</code></li> <li>Added predicate for <code>kube-dc.com/lb-resync-timestamp</code> annotation</li> <li>Integrated <code>LBWatcher</code> startup via <code>mgr.Add()</code></li> </ol>"},{"location":"prd/service_lb_sync_issue/#performance-impact","title":"Performance Impact","text":"Metric Value Notes Periodic check interval 2 min Configurable via <code>LBVerificationInterval</code> Pod restart poll 30s Single API call OVN LB check ~10ms per svc Only for provisioned LB services Reconciliation trigger On-demand Only when LB actually missing"},{"location":"prd/service_lb_sync_issue/#test-results-2025-12-02","title":"Test Results (2025-12-02)","text":"<p>Test: Manual LB Deletion Recovery</p> <pre><code># Deleted OVN LB manually\n$ ovn-nbctl lb-del shalb-dev-etcd-lb-tcp\n\n# LB Watcher detected and recovered (within 2 min):\n12:46:14 [WARN] LB watcher: OVN LB missing for service shalb-dev/etcd-lb, triggering reconciliation\n12:46:14 [INFO] Update service-lb resync annotation changed, run reconciliation\n12:46:15 [INFO] Kind: ServiceLb, Message: Create Lb shalb-dev-etcd-lb-tcp\n12:46:15 [INFO] Kind: ServiceLb, Message: Add Vip Lb shalb-dev-etcd-lb-tcp, Vip 168.119.17.51:2379\n12:46:15 [INFO] LB watcher: triggered reconciliation for 1 services with missing LBs\n</code></pre> <p>Result: \u2705 LB automatically recreated within 2 minutes</p>"},{"location":"prd/service_lb_sync_issue/#verification-steps","title":"Verification Steps","text":"<p>After implementing fix, verify with:</p> <pre><code># 1. Check OVN LBs exist\nkubectl exec -n kube-system deployment/ovn-central -- \\\n  ovn-nbctl --no-leader-only lb-list | grep \"168.119\"\n\n# 2. Simulate failure - restart kube-ovn-controller\nkubectl rollout restart deployment/kube-ovn-controller -n kube-system\n\n# 3. Wait 2 minutes and verify LBs still exist\nsleep 120\nkubectl exec -n kube-system deployment/ovn-central -- \\\n  ovn-nbctl --no-leader-only lb-list | grep \"168.119\"\n\n# 4. Test connectivity\ncurl -k https://168.119.17.55:6443/version\n</code></pre>"},{"location":"prd/service_lb_sync_issue/#related-files","title":"Related Files","text":"<ul> <li><code>internal/service_lb/lb_watcher.go</code> - NEW LB Watcher implementation</li> <li><code>internal/service_lb/service_lb.go</code> - ServiceLB OVN sync logic</li> <li><code>internal/controller/core/service_controller.go</code> - Service controller (modified)</li> </ul>"},{"location":"prd/service_lb_sync_issue/#version","title":"Version","text":"<ul> <li>Implemented in: v0.1.34-dev4</li> <li>Date: 2025-12-02</li> </ul>"},{"location":"prd/svc_lb_architecture/","title":"Service LoadBalancer Architecture in Kube-DC","text":"<p>Status: \u2705 Documented &amp; Misconceptions Resolved Last Updated: 2025-12-02  </p>"},{"location":"prd/svc_lb_architecture/#summary","title":"Summary","text":"<p>Service LoadBalancers in kube-dc work without requiring VPC policy routes or NAT rules. The load balancer VIPs are directly accessible through the OVN router's external interface.</p>"},{"location":"prd/svc_lb_architecture/#how-it-works","title":"How It Works","text":""},{"location":"prd/svc_lb_architecture/#1-network-topology","title":"1. Network Topology","text":"<pre><code>External Network (168.119.17.48/28)\n         \u2193\nOVN Router Port: shalb-dev-ext-public\n  MAC: ae:d6:b9:e1:76:78\n  Network: 168.119.17.51/28\n         \u2193\nOVN Load Balancer VIPs (168.119.17.51-.63)\n         \u2193\nBackend Pods/VMs in Project VPC\n</code></pre>"},{"location":"prd/svc_lb_architecture/#2-eip-allocation-for-service-loadbalancers","title":"2. EIP Allocation for Service LoadBalancers","text":"<p>When a Service of type <code>LoadBalancer</code> is created:</p> <ol> <li>kube-dc Service Controller (<code>internal/controller/core/service_controller.go</code>):</li> <li>Detects the LoadBalancer service</li> <li> <p>Creates an <code>EIp</code> resource</p> </li> <li> <p>kube-dc EIP Controller (<code>internal/eip/ovn_eip_res.go</code>):</p> </li> <li>Creates an <code>OvnEip</code> resource</li> <li>Allocates an IP from the external subnet (e.g., <code>ext-public</code>)</li> <li> <p>Type: <code>lrp</code> (Logical Router Port)</p> </li> <li> <p>Kube-OVN (upstream controller):</p> </li> <li>Does NOT create NAT rules for <code>lrp</code> type EIPs when used with load balancers</li> <li>The IP becomes part of the router's external interface subnet</li> </ol>"},{"location":"prd/svc_lb_architecture/#3-load-balancer-vip-configuration","title":"3. Load Balancer VIP Configuration","text":"<p>kube-dc Service LB Controller (<code>internal/service_lb/service_lb.go</code>):</p> <pre><code>// Creates OVN load balancer\nvipKey := fmt.Sprintf(\"%s:%d\", r.ipAddress, port.Port)  // e.g., \"168.119.17.55:80\"\nbackends := \"10.1.0.40:31416,10.1.0.41:31416\"  // Pod/VM IPs:ports\n\n// Attaches to BOTH router and logical switch\novsCli.LogicalRouterUpdateLoadBalancers(r.projectRouter.Name, ...)  // shalb-dev\novsCli.LogicalSwitchUpdateLoadBalancers(project.SubnetName(r.project), ...)  // shalb-dev-default\n</code></pre>"},{"location":"prd/svc_lb_architecture/#4-traffic-flow","title":"4. Traffic Flow","text":""},{"location":"prd/svc_lb_architecture/#external-client-service","title":"External Client \u2192 Service","text":"<pre><code>1. Client sends packet to 168.119.17.55:80\n2. ARP resolution: Who has 168.119.17.55?\n3. OVN router responds: ae:d6:b9:e1:76:78 (my MAC)\n4. Packet arrives at router's external port\n5. OVN load balancer intercepts (VIP match)\n6. Packet DNAT'd to backend: 10.1.0.40:31416\n7. Response SNAT'd back through VIP\n8. Client receives response from 168.119.17.55:80\n</code></pre>"},{"location":"prd/svc_lb_architecture/#internal-pod-service-clusterip","title":"Internal Pod \u2192 Service (ClusterIP)","text":"<p>Normal kube-proxy/Cilium routing within the cluster.</p>"},{"location":"prd/svc_lb_architecture/#key-differences-from-fip","title":"Key Differences from FIP","text":"Feature Service LoadBalancer (lrp EIP) Floating IP (FIP) OvnEip Type <code>lrp</code> <code>lrp</code> NAT Rules None (uses LB VIP) <code>dnat_and_snat</code> Policy Routes Not needed Required for outbound Annotation <code>ovn.kubernetes.io/vpc_nat</code> NOT needed <code>ovn.kubernetes.io/vpc_nat: {vpc}-{fip-name}</code> required Use Case Inbound load balancing Bidirectional NAT to VM/Pod Status.nat <code>\"\"</code> (empty) <code>fip</code>"},{"location":"prd/svc_lb_architecture/#verification-commands","title":"Verification Commands","text":""},{"location":"prd/svc_lb_architecture/#check-service-loadbalancer","title":"Check Service LoadBalancer","text":"<pre><code># 1. Verify Service has external IP\nkubectl get svc -n {namespace} {service-name}\n\n# 2. Check EIP resource\nkubectl get eip -n {namespace} | grep slb-\n\n# 3. Verify OvnEip (no vpc_nat needed)\nEIP_NAME=$(kubectl get eip -n {namespace} {eip-name} -o jsonpath='{.status.ovnEIpRef}')\nkubectl get ovn-eip $EIP_NAME -o yaml\n\n# 4. Check OVN load balancer\nkubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lb-list | grep {namespace}\n\n# 5. Verify router attachment\nkubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lr-lb-list {namespace}\n\n# 6. Check ARP resolution (from gateway/bastion)\narp -n | grep {external-ip}\n# Should show router MAC: ae:d6:b9:e1:76:78 (for shalb-dev)\n\n# 7. Test connectivity\ncurl http://{external-ip}:{port}\n</code></pre>"},{"location":"prd/svc_lb_architecture/#check-vpc-router-configuration","title":"Check VPC Router Configuration","text":"<pre><code># Show router ports and subnet\nkubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl show {vpc-name}\n\n# Example output:\nrouter 9cae37d3-65ae-46e9-ad95-a0ebf58108d9 (shalb-dev)\n    port shalb-dev-ext-public\n        mac: \"ae:d6:b9:e1:76:78\"\n        networks: [\"168.119.17.51/28\"]  # \u2190 All EIPs in this range\n        gateway chassis: [...]\n</code></pre>"},{"location":"prd/svc_lb_architecture/#common-misconceptions-resolved","title":"Common Misconceptions (Resolved)","text":"<p>Note: As of 2025-12-02, the codebase has been cleaned up to remove sources of these misconceptions.</p>"},{"location":"prd/svc_lb_architecture/#misconception-1-policy-routes-required","title":"\u274c Misconception 1: Policy Routes Required","text":"<p>FALSE: Service LoadBalancers do NOT require VPC policy routes.</p> <p>Policy routes (like <code>ip4.src==168.119.17.X reroute 168.119.17.49</code>) are only needed for: - Outbound traffic from VMs/Pods with dedicated EIPs - FIP resources that need bidirectional NAT</p> <p>Service LoadBalancers use the router's external interface directly.</p> <p>Status: \u2705 No incorrect references found in codebase.</p>"},{"location":"prd/svc_lb_architecture/#misconception-2-vpc_nat-annotation-required","title":"\u274c Misconception 2: vpc_nat Annotation Required","text":"<p>FALSE: The <code>ovn.kubernetes.io/vpc_nat</code> annotation is NOT required for Service LoadBalancer OvnEips.</p> <p>This annotation is only needed for: - FIP resources (creates DNAT/SNAT rules) - Resources that need VPC-level NAT management</p> <p>Service LoadBalancers work through the OVN load balancer VIP mechanism.</p> <p>Status: \u2705 kube-dc controllers verified - no <code>vpc_nat</code> usage for Service LBs.</p>"},{"location":"prd/svc_lb_architecture/#misconception-3-kyverno-policy-is-mandatory","title":"\u274c Misconception 3: Kyverno Policy is Mandatory","text":"<p>FALSE: The Kyverno policies for Service LoadBalancers were NOT required.</p> <p>Status: \u2705 REMOVED on 2025-12-02: - Deleted <code>installer/kube-dc/templates/kube-dc/kyverno/mutate-tenant-svc-lb.yaml</code> - Deleted <code>installer/kube-dc/templates/kube-dc/kyverno/mutate-svc-lb-dep.yaml</code> - Removed unnecessary OVN annotations from <code>examples/capi-cluster/addons.yaml</code></p> <p>These policies set annotations (<code>ovn.kubernetes.io/vpc</code>, etc.) that were NOT read by kube-dc controllers.</p>"},{"location":"prd/svc_lb_architecture/#troubleshooting","title":"Troubleshooting","text":""},{"location":"prd/svc_lb_architecture/#service-not-accessible-externally","title":"Service Not Accessible Externally","text":"<ol> <li> <p>Check if EIP is allocated: <pre><code>kubectl get svc -n {namespace} {service-name}\n# STATUS should show EXTERNAL-IP\n</code></pre></p> </li> <li> <p>Verify OVN load balancer exists: <pre><code>kubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lb-list | grep {svc-name}\n</code></pre></p> </li> <li> <p>Check backend endpoints: <pre><code>kubectl get endpoints -n {namespace} {service-name}\n# Should list pod/VM IPs\n</code></pre></p> </li> <li> <p>Verify router attachment: <pre><code>kubectl exec -n kube-system ovn-central-xxx -- ovn-nbctl lr-lb-list {vpc-name}\n# Should list the load balancer\n</code></pre></p> </li> <li> <p>Test from within cluster first: <pre><code>kubectl run test --image=curlimages/curl --rm -i -n {namespace} \\\n  -- curl http://{external-ip}:{port}\n</code></pre></p> </li> <li> <p>Check ARP resolution (from external host): <pre><code># On gateway/bastion\narp -n | grep {external-ip}\n# Should show router MAC\n</code></pre></p> </li> </ol>"},{"location":"prd/svc_lb_architecture/#service-works-internally-but-not-externally","title":"Service Works Internally but Not Externally","text":"<p>Possible causes:</p> <ol> <li>Firewall rules on external gateway/firewall</li> <li>Network routing - external network may not route to EIP subnet</li> <li>Testing from wrong location - if testing from bastion/gateway that has the subnet assigned locally, connections will be routed locally</li> </ol> <p>Solution: - Test from a truly external client (your laptop, different server) - Check firewall rules on the physical network infrastructure - Verify routing table on the internet gateway</p>"},{"location":"prd/svc_lb_architecture/#slow-initial-connection","title":"Slow Initial Connection","text":"<p>If first connection fails but subsequent ones work: - ARP cache warming - first packet triggers ARP resolution - Wait 5-10 seconds and try again - Check ARP cache: <code>arp -n | grep {external-ip}</code></p>"},{"location":"prd/svc_lb_architecture/#code-references","title":"Code References","text":""},{"location":"prd/svc_lb_architecture/#service-lb-controller","title":"Service LB Controller","text":"<ul> <li>Main controller: <code>/home/voa/projects/kube-dc/internal/controller/core/service_controller.go</code></li> <li>Load balancer logic: <code>/home/voa/projects/kube-dc/internal/service_lb/service_lb.go</code></li> <li>EIP management: <code>/home/voa/projects/kube-dc/internal/service_lb/eip_res.go</code></li> </ul>"},{"location":"prd/svc_lb_architecture/#eip-controller","title":"EIP Controller","text":"<ul> <li>OvnEip creation: <code>/home/voa/projects/kube-dc/internal/eip/ovn_eip_res.go</code></li> <li>Lines 134-150: OvnEip resource generation (NO vpc_nat annotation needed)</li> </ul>"},{"location":"prd/svc_lb_architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Service LB Tutorial: <code>/home/voa/projects/kube-dc/docs/tutorial-ip-and-lb.md</code></li> <li>Networking Architecture: <code>/home/voa/projects/kube-dc/docs/architecture-networking.md</code></li> <li>FIP Resources: <code>/home/voa/projects/kube-dc/internal/fip/res_fip.go</code></li> </ul>"}]}