{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What Is Kube-DC?","text":"<p>Kube-DC is an advanced, enterprise-grade platform that transforms Kubernetes into a comprehensive Data Center solution supporting both virtual machines and containerized workloads. It provides organizations with a unified management interface for all their infrastructure needs, from multi-tenancy and virtualization to networking and billing.</p>"},{"location":"#overview","title":"Overview","text":"<p>Kube-DC bridges the gap between traditional virtualization and modern container orchestration, allowing teams to run both legacy workloads and cloud-native applications on the same platform. By leveraging Kubernetes as the foundation, Kube-DC inherits its robust ecosystem while extending functionality to support enterprise requirements.</p> <p></p>"},{"location":"#key-features-at-a-glance","title":"Key Features at a Glance","text":"<p>Kube-DC offers a comprehensive set of features designed for modern data center operations:</p> <ul> <li>Multi-Tenancy - Host multiple organizations with isolated environments and custom SSO integration</li> <li>Unified Workload Management - Run both VMs and containers on the same platform</li> <li>Advanced Networking - VPC per project, VLAN support, and software-defined networking</li> <li>Enterprise Virtualization - KubeVirt integration with GPU passthrough and live migration</li> <li>Infrastructure as Code - Kubernetes-native APIs with support for Terraform, Ansible, and more</li> <li>Integrated Billing - Track and allocate costs for all resources</li> <li>Managed Services Platform - Deploy databases, storage, and AI/ML infrastructure</li> </ul> <p>For detailed information about each feature, including capabilities and use cases, visit the Core Features page.</p>"},{"location":"#why-choose-kube-dc","title":"Why Choose Kube-DC?","text":""},{"location":"#for-enterprise-it","title":"For Enterprise IT","text":"<ul> <li>Run legacy VMs alongside modern containers</li> <li>Implement chargeback models for departmental resource usage</li> <li>Provide self-service infrastructure while maintaining governance</li> <li>Reduce operational costs by consolidating virtualization and container platforms</li> </ul>"},{"location":"#for-service-providers","title":"For Service Providers","text":"<ul> <li>Offer multi-tenant infrastructure with complete isolation</li> <li>Provide value-added services beyond basic IaaS</li> <li>Implement flexible billing based on actual resource usage</li> <li>Support diverse customer workloads on a single platform</li> </ul>"},{"location":"#for-devops-teams","title":"For DevOps Teams","text":"<ul> <li>Unify VM and container management workflows</li> <li>Implement infrastructure as code for all resources</li> <li>Integrate with existing CI/CD pipelines</li> <li>Enable developer self-service while maintaining control</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to explore Kube-DC? Check out our Quick Start guides to begin your journey.</p> <p>For a deeper understanding of the underlying architecture and concepts, visit the Architecture &amp; Concepts section.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Kube-DC is built with a focus on community collaboration. Visit our Community &amp; Support page to learn how to get involved, report issues, or seek assistance.</p>"},{"location":"CODEBASE_CONTEXT/","title":"Codebase Context for Kube-DC","text":"<p>This document summarizes the codebase structure of the Kube-DC project for future reference. It serves as a single source of truth to avoid repeating exploratory analysis.</p>"},{"location":"CODEBASE_CONTEXT/#top-level-organization","title":"Top-level Organization","text":"<pre><code>.github/             # GitHub workflows and issue templates\n.vscode/             # Editor settings\napi/                 # Kubernetes CRD API definitions for kube-dc.com\ncharts/              # Helm charts for deploying Kube-DC\ncmd/                 # Controller manager entry point (main.go)\ndocs/                # User-facing documentation and architecture guides\nexamples/            # Sample manifests and usage scenarios\nhack/                # Development and automation scripts\ninternal/            # Core libraries, controllers, and utilities\ninstaller/           # Installation manifests and scripts\nservices/            # Auxiliary Kubernetes services (DB, storage, etc.)\nui/                  # Web UI (frontend and backend)\n\nDockerfile           # Container image for controller manager\nDockerfile_manager   # Alternate Dockerfile for the manager image\n.dockerignore        # Files to ignore in Docker builds\n.gitignore           # Git ignore rules\n.golangci.yml        # GolangCI-Lint configuration\nMakefile             # Build and automation targets\nPROJECT              # Project metadata\nREADME.md            # Project overview and key features\ngo.mod, go.sum       # Go module definitions\npackage-lock.json    # Node/NPM dependency lock file for UI backend\nmkdocs.yml           # MkDocs configuration for documentation site\n</code></pre>"},{"location":"CODEBASE_CONTEXT/#detailed-directory-breakdown","title":"Detailed Directory Breakdown","text":""},{"location":"CODEBASE_CONTEXT/#cmd","title":"cmd/","text":"<p>Contains the <code>main.go</code> entry point for the Kube-DC controller manager that initializes and runs Kubernetes controllers.</p>"},{"location":"CODEBASE_CONTEXT/#internal","title":"internal/","text":"<p>Modular Go packages implementing business logic and controller patterns: - service_lb/: Load balancer and external IP management - organization/: Organization CRD and Keycloak integration - eip/, fip/: External/ floating IP resource controllers - project/, organizationgroup/: CRD controllers for multi-tenancy - client/, objmgr/, controller/, utils/: Core abstractions for resource management</p>"},{"location":"CODEBASE_CONTEXT/#ui-web-ui","title":"ui/ (Web UI)","text":"<p>The <code>ui</code> directory contains the Kube\u2011DC user interface, split into two subprojects:</p>"},{"location":"CODEBASE_CONTEXT/#frontend","title":"frontend/","text":"<p>React/TypeScript single\u2011page application scaffolded from PatternFly Seed: - Entry: <code>src/index.tsx</code> and <code>src/app/</code> for layout, routing, and components - Build: Webpack configs (<code>webpack.common.js</code>, <code>webpack.dev.js</code>, <code>webpack.prod.js</code>) and scripts in <code>package.json</code> - Assets &amp; manifests: <code>kubernetes/</code> holds deployment, service, and ingress YAML for UI - Dev tools: Jest tests, Storybook (<code>stories/</code>), ESLint/Prettier, bundle analyzer, and Surge deployment (<code>dr-surge.js</code>)</p> <p><pre><code>ui/frontend/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 index.tsx\n\u2502   \u2514\u2500\u2500 app/\n\u251c\u2500\u2500 kubernetes/\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 webpack.common.js\n\u2514\u2500\u2500 README.md\n</code></pre> \u3010F:ui/frontend/README.md\u2020L1-L6\u3011\u3010F:ui/frontend/package.json\u2020L9-L16\u3011\u3010F:ui/frontend/webpack.common.js\u2020L1-L7\u3011</p>"},{"location":"CODEBASE_CONTEXT/#backend","title":"backend/","text":"<p>Node.js/Express API server that provides UI endpoints and in\u2011cluster proxies: - Server: <code>app.js</code> sets up routes, CORS, body parsing, and WebSocket proxy for VNC - Controllers: <code>controllers/</code> contains modules for cloud-shell, VMs, volumes, network, projects, metrics, system functions, etc. - Proxy: HTTP and WebSocket proxy middleware to route VNC and other traffic via Kubernetes services - Kubernetes manifests: <code>kubernetes/</code> and <code>kubernetes_service/</code> directories for deployment YAML</p> <p><pre><code>ui/backend/\n\u251c\u2500\u2500 app.js\n\u251c\u2500\u2500 controllers/\n\u2502   \u251c\u2500\u2500 cloudShellModule.js\n\u2502   \u2514\u2500\u2500 volumeModule.js\n\u251c\u2500\u2500 kubernetes/\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 README.md\n</code></pre> \u3010F:ui/backend/app.js\u2020L1-L20\u3011\u3010F:ui/backend/controllers/cloudShellModule.js\u2020L1-L10\u3011</p>"},{"location":"CODEBASE_CONTEXT/#hack","title":"hack/","text":"<p>Utility scripts for: - cluster setup and bootstrap - UI code updates and build automation - integration tests and version management</p>"},{"location":"CODEBASE_CONTEXT/#charts","title":"charts/","text":"<p>Helm chart definitions to deploy Kube-DC components onto a Kubernetes cluster.</p>"},{"location":"CODEBASE_CONTEXT/#docs","title":"docs/","text":"<p>Markdown files for: - Tutorials (quickstart, kubeconfig, IP &amp; LB, VMs, user groups) - Architecture (networking, virtualization, multi-tenancy, overview) - Community and support guidelines</p>"},{"location":"CODEBASE_CONTEXT/#examples","title":"examples/","text":"<p>Sample manifests demonstrating cluster API integration, VM workloads, and organization/user configurations.</p>"},{"location":"CODEBASE_CONTEXT/#installer","title":"installer/","text":"<p>Installation scripts and YAML manifests for bootstrapping the control plane and CRDs.</p>"},{"location":"CODEBASE_CONTEXT/#services","title":"services/","text":"<p>Predefined Kubernetes objects for ancillary services such as database and storage provisioning.</p>"},{"location":"CODEBASE_CONTEXT/#go-controller-manager-architecture","title":"Go Controller Manager Architecture","text":""},{"location":"CODEBASE_CONTEXT/#cmdmaingo","title":"cmd/main.go","text":"<ul> <li>Registers schemes for Kubernetes core, kube-dc CRDs, OVN, and CNI types.</li> <li>Parses flags (metrics address, leader election, Keycloak debug, HTTP/2, config secret).</li> <li>Initializes controller-runtime Manager with metrics server, health/readiness probes, and webhook server.</li> <li>Sets global configuration (ConfigSecretName, KubeDcNamespace).</li> <li>Registers Reconcilers: OrganizationReconciler, ProjectReconciler, OrganizationGroupReconciler, EIpReconciler, FIpReconciler, ServiceReconciler.</li> </ul> <p><pre><code>// Add CRD schemes and plugins; initialize Manager and register controllers\nutilruntime.Must(clientgoscheme.AddToScheme(scheme))\nutilruntime.Must(kubedccomv1.AddToScheme(scheme))\n// ...\nmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{...})\n// ...\n(&amp;controller.OrganizationReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme(), Debug: debug}).SetupWithManager(mgr)\n(&amp;controller.ProjectReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme(), Debug: debug}).SetupWithManager(mgr)\n// ...\n(&amp;corecontroller.ServiceReconciler{Client: mgr.GetClient(), Scheme: mgr.GetScheme()}).SetupWithManager(mgr)\n</code></pre> \u3010F:cmd/main.go\u2020L54-L60\u3011\u3010F:cmd/main.go\u2020L169-L212\u3011</p>"},{"location":"CODEBASE_CONTEXT/#crd-types-and-schemas-apikube-dccomv1","title":"CRD Types and Schemas (api/kube-dc.com/v1)","text":"<ul> <li>Defines custom resources: Organization, Project, OrganizationGroup, EIp, FIp.</li> <li><code>*_types.go</code> files describe Spec and Status fields; <code>*_extend.go</code> adds loader and helper methods.</li> </ul> <p><pre><code>api/kube-dc.com/v1/\n\u251c\u2500\u2500 organization_types.go\n\u251c\u2500\u2500 project_types.go\n\u251c\u2500\u2500 organizationgroup_types.go\n\u251c\u2500\u2500 eip_types.go\n\u251c\u2500\u2500 fip_types.go\n\u251c\u2500\u2500 organization_extend.go\n\u2514\u2500\u2500 project_extend.go\n</code></pre> \u3010F:api/kube-dc.com/v1/organization_types.go\u2020L1-L80\u3011\u3010F:api/kube-dc.com/v1/project_types.go\u2020L1-L80\u3011</p>"},{"location":"CODEBASE_CONTEXT/#controllers-internalcontroller","title":"Controllers (internal/controller)","text":"<ul> <li>OrganizationReconciler: Manages Organization CR, delegates to internal/organization for Keycloak realm, auth config, roles, and secrets.</li> <li>ProjectReconciler: Manages Project CR, orchestrates namespace, VPC, subnet, SNAT, EIP, keypairs, roles, and DNS via internal/project.</li> <li>OrganizationGroupReconciler: Syncs OrganizationGroup CR, handling Keycloak groups and Kubernetes rolebindings.</li> <li>EIpReconciler / FIpReconciler: Reconcile external/Floating IP CRs.</li> <li>ServiceReconciler: Reconciles <code>ServiceTypeLoadBalancer</code> Services and their Endpoints; loads Project context; manages external IPs via <code>NewSvcLbEIpRes</code> and OVN-based load balancers via <code>NewLoadBalancerRes</code> in <code>service_controller.go</code>.   \u3010F:internal/controller/core/service_controller.go\u2020L52-L83\u3011\u3010F:internal/controller/core/service_controller.go\u2020L116-L140\u3011</li> </ul> <p><pre><code>internal/controller/\n\u251c\u2500\u2500 kube-dc.com/\n\u2502   \u251c\u2500\u2500 organization_controller.go\n\u2502   \u251c\u2500\u2500 project_controller.go\n\u2502   \u251c\u2500\u2500 organizatongroup_controller.go\n\u2502   \u251c\u2500\u2500 eip_controller.go\n\u2502   \u2514\u2500\u2500 fip_controller.go\n\u2514\u2500\u2500 core/\n    \u2514\u2500\u2500 service_controller.go\n</code></pre> \u3010F:internal/controller/kube-dc.com/organization_controller.go\u2020L1-L30\u3011\u3010F:internal/controller/core/service_controller.go\u2020L1-L20\u3011</p>"},{"location":"CODEBASE_CONTEXT/#business-logic-internal-packages","title":"Business Logic (internal packages)","text":"<ul> <li>internal/organization: Orchestrates Organization CR synchronization by invoking resource controllers:</li> <li><code>organization.go</code>: Sync/Delete pipeline calling NewKeycloakRealm, NewKubeAuthConfig, NewRealmRole, NewRealmAccessSeret to manage Keycloak realms, Kubernetes auth secrets, realm roles, and access secrets.     \u3010F:internal/organization/organization.go\u2020L12-L58\u3011\u3010F:internal/organization/organization.go\u2020L61-L102\u3011</li> <li>internal/project: Orchestrates Project CR lifecycle, provisioning namespaces, networking, and identities:</li> <li><code>project.go</code>: Sync/Delete pipeline calling NewProjectNamespace, NewProjectVpc, NewProjectEip, NewProjectSubnet, NewProjectNad, NewProjectSnat, NewProjectKeyPairSeret, NewProjectAuthKeySecret, NewProjectKeycloakRole, NewProjectRole, NewProjectRoleBinding, NewProjectVpcDns.     \u3010F:internal/project/project.go\u2020L13-L58\u3011\u3010F:internal/project/project.go\u2020L59-L137\u3011</li> <li>internal/organizationgroup: Manages Keycloak group and Kubernetes bindings per project.</li> <li>internal/service_lb: Implements Service LoadBalancer logic using OVN and EIp CRD:</li> <li><code>service_lb.go</code>: Defines <code>LBResource</code> which configures OVN logical router/switch load balancers (VIPs\u2192backends) via <code>NewLoadBalancerRes</code>, with <code>Sync</code>/<code>Delete</code> methods to mutate OVN NB DB.</li> <li><code>eip_res.go</code>: Defines <code>NewSvcLbEIpRes</code> to reconcile external IP addresses (EIp CRD) for services, based on annotations or project gateway, with functions to Get/Create/Delete and update status.   \u3010F:internal/service_lb/service_lb.go\u2020L30-L41\u3011\u3010F:internal/service_lb/eip_res.go\u2020L18-L27\u3011</li> <li>internal/objmgr: Generic resource manager abstractions for creating/updating Kubernetes objects.</li> <li>internal/utils: Common utilities (random names, JSON copy, resource processor).</li> </ul>"},{"location":"CODEBASE_CONTEXT/#external-integrations","title":"External Integrations","text":"<ul> <li>Keycloak via gocloak for identity management.</li> <li>OVN via kube-ovn client for software\u2011defined networking.</li> <li>NetworkAttachmentDefinitions via CNI client for custom network attachments.</li> </ul>"},{"location":"CODEBASE_CONTEXT/#metrics-healthchecks-leader-election","title":"Metrics, Healthchecks &amp; Leader Election","text":"<ul> <li>Exposes secure metrics endpoint with authentication filters.</li> <li>Readiness and liveness probes via <code>/healthz</code> and <code>/readyz</code>.</li> <li>Optional leader election for HA controller managers.</li> </ul>"},{"location":"CODEBASE_CONTEXT/#installation-via-clusterdev-infrastructure-as-code","title":"Installation via cluster.dev Infrastructure as Code","text":"<p>Kube-DC leverages the cluster.dev IaC framework (v0.9.7) to provision and deploy its control plane and dependencies.</p> <p>Under <code>installer/kube-dc</code>: - stack.yaml: Defines a <code>Stack</code> using the <code>templates/kube-dc/</code> StackTemplate to orchestrate installation units.   <pre><code>name: cluster\ntemplate: \"./templates/kube-dc/\"\nkind: Stack\n</code></pre>   \u3010F:installer/kube-dc/stack.yaml\u2020L1-L4\u3011\u3010F:go.mod\u2020L16\u3011 - project.yaml: Defines a <code>Project</code> for cluster.dev, setting owner organization and project defaults.   <pre><code>name: dev\nkind: Project\n</code></pre>   \u3010F:installer/kube-dc/project.yaml\u2020L1-L3\u3011 - templates/kube-dc/template.yaml: StackTemplate with sequential units: Terraform install, password generators, CRDs, Helm charts (kube-ovn, multus-cni, kubevirt, Keycloak, cert-manager, ingress-nginx, monitoring stack, kube-dc core), and custom shell hooks.   \u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L17-L23\u3011</p> <p>The installer docs demonstrate bootstrapping cluster.dev CLI and deploying the stack: - Bootstrapping cluster.dev: <code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh</code>   \u3010F:docs/quickstart-hetzner.md\u2020L175-L176\u3011 - High-level install step: \u201cKube-DC Installation: Use cluster.dev to deploy Kube-DC components\u201d   \u3010F:docs/quickstart-overview.md\u2020L104-L105\u3011</p>"},{"location":"CODEBASE_CONTEXT/#cicd-testing","title":"CI/CD &amp; Testing","text":""},{"location":"CODEBASE_CONTEXT/#github-actions-workflows","title":"GitHub Actions workflows","text":"<ul> <li>release.yaml: on tag pushes, builds and pushes Helm charts via <code>hack/build.sh</code> inside Alpine/helm container.   \u3010F:.github/workflows/release.yaml\u2020L1-L20\u3011</li> <li>sync_to_public_repo.yaml: on <code>main</code> changes to charts/examples/docs/installer/hack, syncs to the public kube-dc-public repo.   \u3010F:.github/workflows/sync_to_public_repo.yaml\u2020L1-L55\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#local-ci-via-makefile-go-code","title":"Local CI via Makefile (Go code)","text":"<ul> <li><code>make test</code>: run unit tests with envtest and coverage.</li> <li><code>make test-e2e</code>: run end-to-end tests via Kind.</li> <li><code>make lint</code>, <code>make fmt</code>, <code>make vet</code>: lint, format, and vet Go code.</li> <li><code>make build</code>: compile the controller manager binary.   \u3010F:Makefile\u2020L14-L23\u3011\u3010F:Makefile\u2020L27-L38\u3011\u3010F:Makefile\u2020L95-L114\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#frontend-ci-reacttypescript","title":"Frontend CI (React/TypeScript)","text":"<ul> <li><code>npm run ci-checks</code>: type-check, ESLint lint, and Jest coverage tests.</li> <li>Additional scripts: <code>start:dev</code>, <code>build</code>, <code>test</code>, <code>storybook</code>, <code>bundle-profile:analyze</code>.   \u3010F:ui/frontend/package.json\u2020L21-L26\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#backend-ci-nodejsexpress","title":"Backend CI (Node.js/Express)","text":"<ul> <li><code>npm run lint</code>: run ESLint for backend controllers.   \u3010F:ui/backend/package.json\u2020L28-L33\u3011</li> </ul>"},{"location":"CODEBASE_CONTEXT/#usage","title":"Usage","text":"<p>Refer to this file for project structure insights to avoid redundant codebase exploration.</p>"},{"location":"architecture-multi-tenancy/","title":"Multi-Tenancy &amp; RBAC","text":"<p>Kube-DC implements a comprehensive multi-tenant architecture that leverages Kubernetes namespaces and Keycloak for identity and access management. This document explains how organizations, projects, and groups in Kube-DC are mapped to Kubernetes and Keycloak objects.</p>"},{"location":"architecture-multi-tenancy/#core-components-and-mapping-structure","title":"Core Components and Mapping Structure","text":"<p>The following diagram illustrates the mapping between Kube-DC structures and the underlying Kubernetes and Keycloak components:</p> <pre><code>graph TD\n    User[User 1] --&gt;|Authenticates| KC[Keycloak Realm]\n    User --&gt;|Obtains Group and Role| KCG[Keycloak Group]\n    User --&gt;|Obtains Group and Role| KCR[Keycloak Role]\n\n    ORG[Organization] --&gt;|Maps to| ORGNS[Organization Namespace]\n\n    ORGNS --&gt;|Contains| ORGGRP[Organization Group]\n\n    PROJ[Project A] --&gt;|Maps to| PNS[Project A NS]\n    PROJ2[Project B] --&gt;|Maps to| PNS2[Project B NS]\n\n    ORGGRP --&gt;|Maps to| K8GRPCRD[Group CRD]\n    ORGGRP --&gt;|Maps to| KCGRP[Keycloak Group]\n\n    KCGRP --&gt;|Maps to| K8SROLE[K8s RoleBinding]\n    KCR --&gt;|Maps to| K8SROLE\n\n    K8GRPCRD --&gt;|Defines permissions for| PNS\n    K8GRPCRD --&gt;|Defines permissions for| PNS2\n\n    KK[Keycloak Client Role] --&gt;|KK to K8s Role Mapping| K8R[K8s Role]</code></pre>"},{"location":"architecture-multi-tenancy/#organization-structure","title":"Organization Structure","text":""},{"location":"architecture-multi-tenancy/#organization","title":"Organization","text":"<p>An Organization is the top-level entity in Kube-DC that represents a company, department, or team.</p> <p>Example Organization YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Organization\nmetadata:\n  name: shalb\n  namespace: shalb\nspec: \n  description: \"Shalb organization\"\n  email: \"arti@shalb.com\"\n</code></pre> <p>Mapping:</p> <ul> <li>Each Organization maps to a dedicated Kubernetes namespace with the same name</li> <li>A corresponding Keycloak Client is created for the organization</li> <li>The Organization serves as a logical grouping for Projects and OrganizationGroups</li> </ul>"},{"location":"architecture-multi-tenancy/#project","title":"Project","text":"<p>A Project represents a logical grouping of resources within an Organization. Projects help segregate workloads and manage access control.</p> <p>Example Project YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre> <p>Mapping:</p> <ul> <li>Each Project maps to a dedicated Kubernetes namespace in the format: <code>{organization}-{project}</code> (e.g., <code>shalb-demo</code>)</li> <li>Projects receive their own network CIDR block for resource isolation</li> <li>Kubernetes namespaces provide the boundary for resource quotas and access control</li> </ul>"},{"location":"architecture-multi-tenancy/#organizationgroup","title":"OrganizationGroup","text":"<p>An OrganizationGroup maps users to roles within specific projects, defining what actions they can perform.</p> <p>Example OrganizationGroup YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - admin\n  - project: \"prod\"\n    roles:\n    - resource-manager\n</code></pre> <p>Mapping:</p> <ul> <li>OrganizationGroups are implemented as Kubernetes Custom Resource Definitions (CRDs)</li> <li>Each OrganizationGroup maps to a Keycloak Group</li> <li>The permissions defined in OrganizationGroups determine the Kubernetes RoleBindings that grant access to resources</li> <li>Different roles can be assigned for different projects</li> </ul>"},{"location":"architecture-multi-tenancy/#authentication-and-authorization-flow","title":"Authentication and Authorization Flow","text":"<p>User Authentication:</p> <ul> <li>Users authenticate through Keycloak</li> <li>Upon successful authentication, users receive JSON Web Tokens (JWTs)</li> </ul> <p>Group and Role Assignment:</p> <ul> <li>Users are assigned to Keycloak Groups based on their OrganizationGroup membership</li> <li>Keycloak maps these groups to corresponding roles</li> </ul> <p>Kubernetes Authorization:</p> <ul> <li>The Kubernetes API server validates the user's JWT</li> <li>RoleBindings determine what actions the user can perform within each namespace</li> <li>Resource access is controlled at the Project (namespace) level</li> </ul> <p>Resource Access:</p> <ul> <li>Users can only access resources in projects where they have appropriate role assignments</li> <li>Actions are restricted based on the permissions defined in their roles</li> </ul>"},{"location":"architecture-multi-tenancy/#role-based-access-control","title":"Role-Based Access Control","text":"<p>Kube-DC provides several built-in roles that can be assigned to users via OrganizationGroups:</p> <ul> <li>Admin: Full access to all resources within a project</li> <li>Resource Manager: Can create and manage resources, but cannot modify project settings</li> <li>Viewer: Read-only access to project resources</li> </ul> <p>Example Role YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Role\nmetadata:\n  name: resource-manager\n  namespace: shalb\nspec:\n  rules:\n  - apiGroups: [\"*\"]\n    resources: [\"pods\", \"services\", \"deployments\", \"statefulsets\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"kubevirt.io\"]\n    resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"architecture-multi-tenancy/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture-multi-tenancy/#kubernetes-components","title":"Kubernetes Components","text":"<ul> <li>Namespaces: Used to isolate Organizations and Projects</li> <li>RBAC: Role-Based Access Control for managing permissions</li> <li>CRDs: Custom Resource Definitions for Kube-DC specific resources</li> <li>NetworkPolicies: Ensure network isolation between Projects</li> </ul>"},{"location":"architecture-multi-tenancy/#keycloak-integration","title":"Keycloak Integration","text":"<ul> <li>Realm: Represents the authentication domain</li> <li>Clients: Each Organization has a dedicated client</li> <li>Groups: Map to OrganizationGroups in Kube-DC</li> <li>Roles: Define permissions that can be assigned to users</li> <li>Role Mappings: Connect Keycloak roles to Kubernetes RBAC</li> </ul>"},{"location":"architecture-multi-tenancy/#practical-application","title":"Practical Application","text":"<p>When a user is added to an organization group in Kube-DC:</p> <ol> <li>The corresponding Keycloak group membership is created</li> <li>The user inherits roles based on the group's permissions</li> <li>When the user accesses the Kubernetes API, their JWT contains the group and role information</li> <li>Kubernetes RBAC evaluates the JWT against RoleBindings to determine access</li> <li>The user can operate only within the boundaries of their assigned permissions</li> </ol> <p>This multi-layered approach ensures secure isolation between tenants while providing fine-grained access control within each project.</p>"},{"location":"architecture-networking/","title":"Networking (Kube-OVN, VLANs)","text":"<p>Kube-DC provides advanced networking capabilities through Kube-OVN, enabling multi-tenant network isolation, external connectivity, and flexible service exposure. This document explains the key networking components and how they operate within the Kube-DC architecture.</p>"},{"location":"architecture-networking/#network-architecture-overview","title":"Network Architecture Overview","text":"<p>Kube-DC's networking architecture is built on Kube-OVN, which supports both overlay and underlay networks and provides Virtual Private Cloud (VPC) capabilities for tenant isolation.</p> <pre><code>graph TB\n    Internet((Internet)) &lt;--&gt;|Ingress/Egress| FW[VPC Firewall]\n\n    subgraph \"Project A (Namespace + VPC)\"\n        PROJ_A_EIP[Project A EIP] &lt;--&gt; FW\n        PROJ_A_EIP --&gt;|Default NAT GW| PROJ_A_NET[Project A Network]\n\n        SVC_LB_A[Service LoadBalancer&lt;br/&gt;with annotation] --&gt;|Uses| PROJ_A_EIP\n        SVC_LB_A --&gt;|Routes to| POD_A[Pods]\n        SVC_LB_A --&gt;|Routes to| VM_A[VMs]\n\n        FIP_A[Floating IP] --&gt;|Maps to| VM_A_TARGET[Specific VM/Pod]\n    end\n\n    subgraph \"Project B (Namespace + VPC)\"\n        PROJ_B_EIP[Project B EIP] &lt;--&gt; FW\n        PROJ_B_EIP --&gt;|Default NAT GW| PROJ_B_NET[Project B Network]\n\n        SVC_LB_B[Service LoadBalancer&lt;br/&gt;with annotation] --&gt;|Uses| PROJ_B_EIP\n        SVC_LB_B --&gt;|Routes to| POD_B[Pods]\n        SVC_LB_B --&gt;|Routes to| VM_B[VMs]\n\n        DEDICATED_EIP[Dedicated EIP] &lt;--&gt; FW\n        SVC_LB_B_DEDICATED[Service LoadBalancer&lt;br/&gt;with dedicated EIP] --&gt;|Uses| DEDICATED_EIP\n    end</code></pre> <p>In this architecture: - Each project gets its own namespace and VPC - Each project receives its own EIP that acts as a NAT gateway for outbound traffic - Service LoadBalancers can route traffic to both Pods and VMs - Service LoadBalancers can use either the project's default EIP (via annotation) or a dedicated EIP - Floating IPs can map specific VMs or Pods to External IPs for direct access</p>"},{"location":"architecture-networking/#network-elements","title":"Network Elements","text":""},{"location":"architecture-networking/#user-visible-network-resources","title":"User-Visible Network Resources","text":""},{"location":"architecture-networking/#external-ip-eip","title":"External IP (EIP)","text":"<p>External IPs provide connectivity from the public internet to resources within Kube-DC. Each EIP is allocated from the provider network.</p> <p>Example EIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: ssh-arti\n  namespace: shalb-demo\nspec: {}  \n</code></pre>"},{"location":"architecture-networking/#floating-ip-fip","title":"Floating IP (FIP)","text":"<p>Floating IPs map an internal IP address (of a VM or pod) to an External IP, enabling direct access to specific resources.</p> <p>Example FIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: fedora-arti\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171\n  eip: ssh-arti\n</code></pre>"},{"location":"architecture-networking/#kubernetes-service","title":"Kubernetes Service","text":"<p>Standard Kubernetes Services for in-cluster service discovery and load balancing.</p>"},{"location":"architecture-networking/#service-type-loadbalancer","title":"Service Type LoadBalancer","text":"<p>Creates and maps an EIP to a service that routes traffic to pods or VMs. Can use either a dedicated EIP or the project's default EIP.</p> <p>Example Service LoadBalancer YAML with default gateway EIP:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n</code></pre> <p>Example Service LoadBalancer for VM SSH access:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: debian\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222\n      targetPort: 22\n</code></pre>"},{"location":"architecture-networking/#internal-network-resources","title":"Internal Network Resources","text":""},{"location":"architecture-networking/#dnat-rule","title":"DNAT Rule","text":"<p>Destination Network Address Translation rules proxy requests from the internet through an EIP to resources within the VPC network. These are created automatically when an EIP is associated with a resource.</p>"},{"location":"architecture-networking/#snat","title":"SNAT","text":"<p>Source Network Address Translation is used for outbound connections from VPC subnets through EIPs, allowing resources within the VPC to communicate with the internet.</p>"},{"location":"architecture-networking/#project-network-provisioning","title":"Project Network Provisioning","text":"<p>When a new project is created in Kube-DC:</p> <ol> <li>The project is allocated a dedicated subnet from the VPC CIDR range</li> <li>Each project connected to the internet receives an EIP</li> <li>All project outbound traffic is routed through its assigned EIP</li> <li>Project-specific network policies are applied for isolation</li> </ol> <p>Example project creation with CIDR allocation:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre>"},{"location":"architecture-networking/#load-balancer-implementation","title":"Load Balancer Implementation","text":"<p>Kube-DC uses a specialized implementation for Service LoadBalancers:</p> <ul> <li>When a Service with type <code>LoadBalancer</code> is created, an OVS-based LoadBalancer routes traffic to service endpoints</li> <li>Endpoints can be Kubernetes pods or KubeVirt VMs</li> <li>The LoadBalancer can use either:</li> <li>The project's default gateway EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"</code>)</li> <li>A dedicated EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-eip: \"eip-name\"</code>)</li> </ul>"},{"location":"architecture-networking/#automatic-external-endpoints-v0134","title":"Automatic External Endpoints (v0.1.34+)","text":"<p>Kube-DC automatically creates external endpoints for LoadBalancer services to enable cross-VPC communication.</p> <p>When a LoadBalancer receives an external IP, the controller creates: - External Service (<code>&lt;service-name&gt;-ext</code>): Headless service - Endpoints (<code>&lt;service-name&gt;-ext</code>): Points to the LoadBalancer's external IP</p> <p>This solves cross-VPC access by providing stable DNS names (e.g., <code>etcd-lb-ext.shalb-envoy.svc.cluster.local</code>) instead of hardcoded IPs. Endpoints are automatically updated when IPs change and deleted with the LoadBalancer service.</p> <p>External endpoints are labeled with <code>kube-dc.com/managed-by: service-lb-controller</code>.</p>"},{"location":"architecture-networking/#kube-ovn-for-vpc-management","title":"Kube-OVN for VPC Management","text":"<p>Kube-OVN is a key component of Kube-DC's networking architecture, providing the foundation for multi-tenant network isolation through VPC networks.</p>"},{"location":"architecture-networking/#vpc-isolation","title":"VPC Isolation","text":"<p>Different VPC networks are independent of each other and can be separately configured with: - Subnet CIDRs - Routing policies - Security policies - Outbound gateways - EIP allocations</p>"},{"location":"architecture-networking/#overlay-vs-underlay-networks","title":"Overlay vs. Underlay Networks","text":"<p>Kube-DC supports both networking approaches:</p>"},{"location":"architecture-networking/#overlay-networks","title":"Overlay Networks","text":"<ul> <li>Software-defined networks that encapsulate packets</li> <li>Provide maximum flexibility for network segmentation</li> <li>Independent of physical network topology</li> <li>Managed entirely by Kube-OVN</li> <li>Ideal for multi-tenant environments</li> </ul>"},{"location":"architecture-networking/#underlay-networks","title":"Underlay Networks","text":"<ul> <li>Direct mapping to physical network infrastructure</li> <li>Better performance with reduced encapsulation overhead</li> <li>Requires coordination with physical network infrastructure</li> <li>Physical switches handle data-plane forwarding</li> <li>Cannot be isolated by VPCs as they are managed by physical switches</li> </ul>"},{"location":"architecture-networking/#network-security","title":"Network Security","text":"<p>Kube-DC implements multiple layers of network security:</p> <p>Project Isolation</p> <ul> <li>Each project receives its own subnet</li> <li>Traffic between projects is controlled by network policies</li> </ul> <p>VPC Segmentation</p> <ul> <li>Projects can be placed in different VPCs for stricter isolation</li> <li>Each VPC has its own network stack and routing tables</li> </ul> <p>Kubernetes Network Policies</p> <ul> <li>Fine-grained control over ingress and egress traffic</li> <li>Can be applied at the namespace, pod, or service level</li> </ul> <p>Subnet ACLs</p> <ul> <li>Control traffic at the subnet level</li> <li>Provide an additional layer of security beyond network policies</li> </ul>"},{"location":"architecture-overview/","title":"Overall Architecture","text":"<p>Kube-DC provides a comprehensive multi-tenant cloud infrastructure platform built on Kubernetes and enhanced with enterprise-grade features like virtualization, networking, and identity management.</p>"},{"location":"architecture-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC architecture consists of several key components that work together to deliver a complete cloud platform:</p> <p></p>"},{"location":"architecture-overview/#architectural-layers","title":"Architectural Layers","text":"<p>Kube-DC is organized into main architectural layers:</p> <pre><code>graph TD\n    K8s[Kubernetes] --&gt; KubeVirt[KubeVirt]    \n    K8s --&gt; KubeOVN[Kube-OVN]    \n    K8s --&gt; Keycloak[Keycloak]    \n    K8s --&gt; LBController[Kube-DC LB Controller]    \n    K8s --&gt; MultiTenant[Multi-Tenant Controller]\n\n    KubeVirt --&gt;|Provides| VMs[Virtual Machines]\n    KubeOVN --&gt;|Manages| Networking[Network VLANs/VPCs]\n    Keycloak --&gt;|Controls| IAM[Identity &amp; Access]\n    LBController --&gt;|Enables| LoadBalancing[Load Balancing, Floating IPs]\n    MultiTenant --&gt;|Organizes| Resources[Organization and Projects]</code></pre> <p>Infrastructure Layer</p> <ul> <li>Bare metal servers or cloud infrastructure</li> <li>Kubernetes core services</li> <li>Storage subsystems</li> </ul> <p>Virtualization Layer</p> <ul> <li>KubeVirt for VM provisioning and management</li> <li>Container workloads</li> <li>Hybrid application support</li> </ul> <p>Networking Layer</p> <ul> <li>Kube-OVN for software-defined networking</li> <li>Multi-tenant network isolation</li> <li>External IP addressing and service exposure</li> </ul> <p>Management Layer</p> <ul> <li>Multi-tenancy resource organization</li> <li>Identity and access management via Keycloak</li> <li>User interface and API access</li> </ul>"},{"location":"architecture-overview/#multi-tenant-organization","title":"Multi-Tenant Organization","text":"<p>Kube-DC introduces a hierarchical resource organization model:</p> <ul> <li>Organizations - Top-level entities representing companies or teams</li> <li>Projects - Logical groupings of resources within an organization</li> <li>Groups - Collections of users with defined roles and permissions</li> </ul> <p>This multi-tenant structure maps to Kubernetes and Keycloak components to provide isolation and access control. For detailed information on the multi-tenancy architecture, see the Multi-Tenancy &amp; RBAC documentation.</p>"},{"location":"architecture-overview/#network-architecture","title":"Network Architecture","text":"<p>Kube-DC leverages Kube-OVN to provide advanced networking capabilities:</p> <ul> <li>Virtual Private Clouds (VPCs) for network isolation</li> <li>External and Floating IPs for service exposure</li> <li>Load balancing and service routing</li> </ul> <p>For detailed information on the networking architecture, see the Networking (Kube-OVN, VLANs) documentation.</p>"},{"location":"architecture-overview/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC integrates KubeVirt to enable VM workloads alongside containers:</p> <ul> <li>VM lifecycle management through Kubernetes APIs</li> <li>Hardware passthrough capabilities</li> <li>Mixed container and VM environments</li> </ul> <p>For detailed information on the virtualization architecture, see the Virtualization (KubeVirt) documentation.</p>"},{"location":"architecture-overview/#key-benefits","title":"Key Benefits","text":"<ul> <li>Multi-tenant isolation: Secure separation between organizations and projects</li> <li>Unified management: Single platform for VMs and containers</li> <li>Network flexibility: Advanced SDN capabilities with Kube-OVN</li> <li>Enterprise security: Integrated identity management with Keycloak</li> <li>API-driven architecture: Consistent interfaces for automation and integration</li> </ul>"},{"location":"architecture-virtualization/","title":"Virtualization (KubeVirt)","text":"<p>Kube-DC leverages KubeVirt to provide powerful virtual machine capabilities alongside traditional container workloads. This document covers the virtualization architecture, features, and how VMs are managed within the platform.</p>"},{"location":"architecture-virtualization/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC's virtualization layer is built on KubeVirt, which extends Kubernetes to support virtual machine workloads. This architecture enables consistent management of both containers and VMs through the same API and tooling.</p> <pre><code>graph TD\n    K8s[Kubernetes API] --&gt; KV[KubeVirt Controller]\n    K8s --&gt; CDI[Containerized Data Importer]\n\n    KV --&gt; VMI[VM Instances]\n    CDI --&gt; DV[Data Volumes]\n\n    VMI --&gt; POD[VM Pods]\n    DV --&gt; PVC[Persistent Volumes]\n\n    subgraph \"VM Management\"\n        KV\n        VMI\n        POD\n    end\n\n    subgraph \"Storage Management\"\n        CDI\n        DV\n        PVC\n    end\n\n    UI[Kube-DC Dashboard] --&gt; K8s\n    CLI[kubectl/virtctl] --&gt; K8s</code></pre>"},{"location":"architecture-virtualization/#core-components","title":"Core Components","text":""},{"location":"architecture-virtualization/#kubevirt-controller","title":"KubeVirt Controller","text":"<p>The KubeVirt controller manages the lifecycle of virtual machines by:</p> <ul> <li>Translating VM specifications into Kubernetes resources</li> <li>Scheduling VMs on appropriate nodes</li> <li>Managing VM state (start, stop, pause, resume)</li> <li>Providing VM migration capabilities</li> <li>Handling VM monitoring and health checks</li> </ul>"},{"location":"architecture-virtualization/#containerized-data-importer-cdi","title":"Containerized Data Importer (CDI)","text":"<p>CDI handles storage provisioning for VMs by:</p> <ul> <li>Creating and managing Data Volumes</li> <li>Importing disk images from HTTP/S3 sources</li> <li>Converting disk formats as needed</li> <li>Cloning existing volumes</li> </ul>"},{"location":"architecture-virtualization/#data-volumes","title":"Data Volumes","text":"<p>Data Volumes serve as the storage backbone for VMs, providing:</p> <ul> <li>Storage allocation for VM disks</li> <li>Integration with Kubernetes storage classes</li> <li>Automated provisioning and cleanup</li> </ul>"},{"location":"architecture-virtualization/#vm-management-in-kube-dc","title":"VM Management in Kube-DC","text":""},{"location":"architecture-virtualization/#vm-creation-and-configuration","title":"VM Creation and Configuration","text":"<p>Kube-DC allows users to create VMs through YAML definitions or the web UI. VM configurations include:</p> <p>Example VM Definition:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\n  namespace: demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: default/ovn-demo\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n        cpu:\n          cores: 2\n        memory:\n          guest: 4G\n      volumes:\n      - dataVolume:\n          name: ubuntu-base-img\n        name: root-volume\n</code></pre>"},{"location":"architecture-virtualization/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Kube-DC provides templates for a variety of operating systems:</p> <ul> <li>Ubuntu (20.04, 22.04, 24.04)</li> <li>Debian</li> <li>CentOS/RHEL</li> <li>Fedora</li> <li>Alpine Linux</li> <li>FreeBSD</li> <li>openSUSE</li> <li>Minimal images (cirros)</li> </ul>"},{"location":"architecture-virtualization/#network-integration","title":"Network Integration","text":"<p>VMs in Kube-DC are integrated with the same network architecture as containers:</p> <ul> <li>Each VM can connect to VPC networks via Multus CNI</li> <li>VMs receive IP addresses from the project's CIDR block</li> <li>Network policies apply to VMs just like containers</li> <li>VMs can use floating IPs and load balancer services</li> </ul>"},{"location":"architecture-virtualization/#storage-management","title":"Storage Management","text":"<p>Kube-DC provides flexible storage options for VMs:</p> <ul> <li>Support for multiple storage classes</li> <li>Persistent storage using Kubernetes PVCs</li> <li>Live volume resizing</li> <li>Volume snapshots and cloning</li> </ul>"},{"location":"architecture-virtualization/#vm-customization","title":"VM Customization","text":"<p>VMs can be customized through cloud-init configurations:</p> <pre><code>cloudInitNoCloud:\n  userData: |-\n    #cloud-config\n    chpasswd: { expire: False }\n    password: securepassword\n    ssh_pwauth: True\n    package_update: true\n    package_upgrade: true\n    packages:\n    - qemu-guest-agent\n    runcmd:\n    - [ systemctl, start, qemu-guest-agent ]\n</code></pre> <p>This allows for: - Setting initial passwords - SSH key distribution - Software installation - Custom scripts execution - Network configuration</p>"},{"location":"architecture-virtualization/#health-monitoring","title":"Health Monitoring","text":"<p>VMs in Kube-DC support health checks through:</p> <pre><code>readinessProbe:\n  guestAgentPing: {}\n  failureThreshold: 10\n  initialDelaySeconds: 20\n  periodSeconds: 10\n</code></pre> <p>Health checks ensure: - VM is properly booted - Guest agent is responsive - Cloud-init has completed - Custom health check scripts pass</p>"},{"location":"architecture-virtualization/#web-ui-management","title":"Web UI Management","text":"<p>Kube-DC provides an intuitive web interface for VM management:</p> <p></p>"},{"location":"architecture-virtualization/#vm-dashboard-features","title":"VM Dashboard Features","text":"<p>The VM dashboard provides:</p> <ul> <li>VM Status Monitoring: Running status, uptime, and conditions</li> <li>Performance Metrics: Real-time CPU, memory, and storage usage</li> <li>VM Details: OS version, network configuration, and node placement</li> <li>Console Access: Direct web-based console access to VMs</li> <li>SSH Terminal: Direct SSH access from the browser</li> <li>Network Information: IP addresses and VPC subnet details</li> </ul>"},{"location":"architecture-virtualization/#vm-lifecycle-management","title":"VM Lifecycle Management","text":"<p>Through the UI, administrators and users can:</p> <ul> <li>Create VMs from templates or custom images</li> <li>Start, stop, pause, and restart VMs</li> <li>Adjust resource allocations (CPU, memory)</li> <li>Take snapshots for backup purposes</li> <li>Clone VMs to create new instances</li> <li>Migrate VMs between nodes</li> </ul>"},{"location":"architecture-virtualization/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture-virtualization/#gpu-passthrough","title":"GPU Passthrough","text":"<p>Kube-DC supports GPU passthrough for high-performance computing and AI workloads:</p> <pre><code>domain:\n  devices:\n    gpus:\n    - deviceName: nvidia.com/GP102GL_Tesla_P40\n      name: gpu1\n</code></pre>"},{"location":"architecture-virtualization/#live-migration","title":"Live Migration","text":"<p>VMs can be migrated between nodes without downtime:</p> <pre><code>spec:\n  strategy:\n    type: LiveMigrate\n</code></pre>"},{"location":"architecture-virtualization/#vm-snapshots","title":"VM Snapshots","text":"<p>Kube-DC supports VM snapshots for point-in-time recovery:</p> <pre><code>apiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: my-vm-snapshot\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: my-vm\n</code></pre>"},{"location":"architecture-virtualization/#vm-templates","title":"VM Templates","text":"<p>Organization administrators can create standardized VM templates for their users, ensuring consistent deployments and reducing configuration errors.</p>"},{"location":"architecture-virtualization/#integration-with-multi-tenancy","title":"Integration with Multi-Tenancy","text":"<p>VMs in Kube-DC operate within the same multi-tenant architecture as containers:</p> <ul> <li>VMs are created within specific projects</li> <li>Organization and project permissions control VM access</li> <li>Network isolation is enforced between projects</li> <li>VM metrics are included in project billing and quotas</li> </ul>"},{"location":"architecture-virtualization/#best-practices","title":"Best Practices","text":""},{"location":"architecture-virtualization/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Allocate sufficient memory for the guest OS (minimum 1GB for most Linux distributions)</li> <li>Consider CPU overcommit ratios when planning node capacity</li> <li>Use appropriate storage classes for VM performance requirements</li> </ul>"},{"location":"architecture-virtualization/#vm-optimization","title":"VM Optimization","text":"<ul> <li>Install guest agents for improved integration</li> <li>Use cloud-init for automated VM configuration</li> <li>Configure readiness probes for proper health monitoring</li> <li>Use virtio drivers for improved performance</li> </ul>"},{"location":"architecture-virtualization/#conclusion","title":"Conclusion","text":"<p>Kube-DC's integration of KubeVirt provides a seamless experience for managing both VMs and containers in a single platform. This unified approach simplifies infrastructure management, improves resource utilization, and enables hybrid application architectures that combine the benefits of both virtualization and containerization.</p>"},{"location":"community-support/","title":"Community &amp; Support","text":"<p>Kube-DC has multiple channels for support, community engagement, and professional services. Choose the option that best fits your needs.</p>"},{"location":"community-support/#community-support_1","title":"Community Support","text":""},{"location":"community-support/#github-discussions","title":"GitHub Discussions","text":"<ul> <li>Ask questions and engage with the community</li> <li>Share your experiences and solutions</li> <li>Report bugs and request features</li> <li>Access to public roadmap and project updates</li> <li>Visit GitHub Discussions</li> </ul>"},{"location":"community-support/#slack-community","title":"Slack Community","text":"<p>Join our active Slack community to:</p> <ul> <li>Get real-time help from community members</li> <li>Connect with other Kube-DC users</li> <li>Share your use cases and solutions</li> <li>Stay updated on latest developments</li> <li>Join Kube-DC Slack</li> </ul>"},{"location":"community-support/#documentation","title":"Documentation","text":"<ul> <li>Comprehensive guides and tutorials</li> <li>API reference documentation</li> <li>Best practices and examples</li> <li>Browse Documentation</li> </ul>"},{"location":"community-support/#professional-services","title":"Professional Services","text":""},{"location":"community-support/#commercial-support","title":"Commercial Support","text":"<p>We offer various tiers of commercial support:</p>"},{"location":"community-support/#basic-support","title":"Basic Support","text":"<ul> <li>Business hours support (9/5)</li> <li>Email support</li> <li>24-hour response time</li> <li>Bug fixes and security updates</li> <li>Access to knowledge base</li> </ul>"},{"location":"community-support/#enterprise-support","title":"Enterprise Support","text":"<ul> <li>24/7 support coverage</li> <li>Priority response (2-hour SLA for critical issues)</li> <li>Direct access to engineering team</li> <li>Custom feature development</li> <li>Dedicated support engineer</li> <li>Regular health checks and reviews</li> </ul>"},{"location":"community-support/#professional-services_1","title":"Professional Services","text":""},{"location":"community-support/#implementation-services","title":"Implementation Services","text":"<ul> <li>Architecture design and review</li> <li>Production deployment assistance</li> <li>Migration planning and execution</li> <li>Performance optimization</li> <li>Security hardening</li> </ul>"},{"location":"community-support/#training","title":"Training","text":"<ul> <li>Admin and operator training</li> <li>Developer workshops</li> <li>Custom training programs</li> <li>Certification programs</li> </ul>"},{"location":"community-support/#consulting","title":"Consulting","text":"<ul> <li>Technical architecture consulting</li> <li>Scalability planning</li> <li>High availability design</li> <li>Security assessment</li> <li>Performance optimization</li> <li>Custom integration development</li> </ul>"},{"location":"community-support/#getting-support","title":"Getting Support","text":""},{"location":"community-support/#for-community-support","title":"For Community Support","text":"<ol> <li>Check the documentation</li> <li>Search existing GitHub Issues</li> <li>Join our Slack community</li> <li>Post on GitHub Discussions</li> </ol>"},{"location":"community-support/#for-commercial-support","title":"For Commercial Support","text":"<p>Contact our sales team:</p> <ul> <li>Email: support@kube-dc.com</li> <li>Website: https://kube-dc.com/</li> <li>Phone: +380632441621</li> </ul>"},{"location":"community-support/#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Check our Contributing Guide to learn how you can:</p> <ul> <li>Submit bug reports and feature requests</li> <li>Contribute code</li> <li>Improve documentation</li> <li>Share use cases and examples</li> </ul>"},{"location":"controller_diagram/","title":"Controller Architecture Diagram","text":"<p>A high-level view of Kube-DC controller components (excluding UI) and external dependencies.</p> <pre><code>flowchart TB\n  subgraph Installer\n    CD[cluster.dev IaC]\n  end\n\n  subgraph K8sCluster[\"Kubernetes Cluster &amp; CRDs\"]\n    CRDs[[\"Org, Project, OrgGroup, EIp, FIp CRDs\"]]\n  end\n\n  subgraph Manager[\"Controller Manager\"]\n    OR(OrganizationReconciler)\n    PR(ProjectReconciler)\n    OGR(OrganizationGroupReconciler)\n    EIP(EIpReconciler)\n    FIP(FIpReconciler)\n    SR(ServiceReconciler)\n  end\n\n  subgraph Logic[\"Business Logic Packages\"]\n    OGi[\"internal/organization\"]\n    PI[\"internal/project\"]\n    OGG[\"internal/organizationgroup\"]\n    SLP[\"internal/service_lb\"]\n    OBJ[\"internal/objmgr\"]\n    UTL[\"internal/utils\"]\n  end\n\n  subgraph Ext[\"External Dependencies\"]\n    KC[Keycloak]\n    KO[Kube-OVN]\n    KV[KubeVirt]\n    ML[Multus CNI]\n    CM[Cert-Manager]\n    PM[Prometheus &amp; Loki]\n  end\n\n  CD --&gt; CRDs\n  CRDs --&gt; OR &amp; PR &amp; OGR &amp; EIP &amp; FIP &amp; SR\n\n  OR --&gt; OGi\n  PR --&gt; PI\n  OGR --&gt; OGG\n  EIP --&gt; SLP\n  FIP --&gt; SLP\n  SR --&gt; SLP\n\n  SLP --&gt; KO\n  OGi --&gt; KC\n  PI --&gt; KO &amp; KV &amp; ML\n  PI --&gt; PM &amp; CM\n  PI --&gt; KC\n\n  style CRDs fill:#f9f,stroke:#333,stroke-width:2px\n  style Manager fill:#bbf,stroke:#333,stroke-width:2px\n  style Logic fill:#bfb,stroke:#333,stroke-width:2px\n  style Ext fill:#ffb,stroke:#333,stroke-width:2px\n  style Installer fill:#fbb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"controller_diagram/#networking-integration-kube-ovn-multus","title":"Networking Integration (Kube-OVN &amp; Multus)","text":"<p>Below is a focused diagram showing how Kube-OVN and Multus CNI are installed and integrated via the Project NetworkAttachmentDefinition.</p> <pre><code>flowchart LR\n  subgraph Installer\n    KOV[\"Kube-OVN Helm Chart\"]\n    MULT[\"Multus CNI Helm Chart\"]\n  end\n\n  KOV --&gt; MULT\n\n  subgraph CNIInfra[\"CNI Infrastructure\"]\n    OVN[\"ovn-daemon (kube-ovn)\"]\n    MPods[\"Multus Pods\"]\n  end\n\n  MULT --&gt; MPods\n  KOV --&gt; OVN\n  OVN &amp; MPods --&gt; CNIInfra\n\n  NewNAD[\"NewProjectNad Controller\"]\n  NADCRD[\"NetworkAttachmentDefinition CR\"]\n  CNIConfig[\"Spec.Config: {type:'kube-ovn', server_socket:'/run/openvswitch/kube-ovn-daemon.sock', provider:&lt;proj&gt;} \"]\n  PodAttach[\"Pod annotation 'k8s.v1.cni.cncf.io/networks' = NAD\"]\n\n  NewNAD --&gt; NADCRD\n  NADCRD --&gt; CNIConfig\n  CNIConfig --&gt; PodAttach\n  PodAttach --&gt; CNIInfra\n\n  style Installer fill:#fbb,stroke:#333,stroke-width:1px\n  style CNIInfra fill:#ffb,stroke:#333,stroke-width:1px\n  style NewNAD fill:#bfb,stroke:#333,stroke-width:1px\n  style NADCRD fill:#f9f,stroke:#333,stroke-width:1px</code></pre> <p>Referenced code: - Scheme registration: \u3010F:cmd/main.go\u2020L57-L60\u3011 - NAD controller: \u3010F:internal/project/res_nad.go\u2020L12-L27\u3011   - Installer sequence: \u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L94-L102\u3011\u3010F:installer/kube-dc/templates/kube-dc/template.yaml\u2020L119-L127\u3011</p>"},{"location":"controller_diagram/#eip-fip-serviceloadbalancer-networking-flows","title":"EIP, FIP &amp; ServiceLoadBalancer Networking Flows","text":"<pre><code>flowchart TD\n  subgraph ProjectNet[\"Project Networking Controllers\"]\n    EIPdef[\"NewProjectEip (Default Gateway EIP)\"]\n    EIPcr[NewProjectEip CR]\n    EIPsync[EIpReconciler]\n    FIPsync[FIpReconciler]\n    LBsync[ServiceReconciler]\n  end\n\n  subgraph OVNNB[\"OVN Northbound DB &amp; OVS\"]\n    OVNNBdb[ovn-nb.db]\n    OVSOCK[ovs-db socket]\n  end\n\n  EIPdef --&gt; EIPcr\n  EIPcr --&gt; EIPsync\n  EIPsync --&gt; OVNNBdb\n\n  FIPsync --&gt;|Sync EIP + Floating IP| OVNNBdb\n\n  LBsync --&gt;|Ensure external IP via EIp CR| OVNNBdb\n  LBsync --&gt;|Configure Virtual IPs in LB| OVNNBdb\n\n  OVNNBdb --&gt; OVSOCK\n\n  classDef flow fill:#eef,stroke:#666,stroke-width:1px;\n  class EIPdef,EIPcr,EIPsync,FIPsync,LBsync flow;</code></pre>"},{"location":"controller_diagram/#detailed-network-stack-implementation","title":"Detailed Network Stack Implementation","text":"<ol> <li>Project VPC &amp; Subnet provisioning (<code>internal/project/res_vpc.go</code>)</li> <li>Creates an OVN Virtual Private Cloud via <code>OvnVpc</code> CR and logical switch.</li> <li>NetworkAttachmentDefinition (<code>internal/project/res_nad.go</code>)</li> <li>Defines a Multus NAD with CNI config for <code>kube-ovn</code>, pointing at the OVS socket and project provider.</li> <li>SNAT Rule (<code>internal/project/res_snat.go</code>)</li> <li>Installs an <code>OvnSnatRule</code> to translate pod-source IPs to the project gateway EIP for outbound internet.</li> <li>Default Gateway EIP (<code>internal/project/res_eip_default.go</code>)</li> <li>Ensures a project-scoped <code>EIp</code> CR representing the default gateway external IP, created via <code>NewEipDefault</code>.</li> <li>Floating IP (FIp) (<code>internal/fip/res_eip.go</code> &amp; <code>FIpReconciler</code>)</li> <li>Syncs or creates EIp owned by FIp, then updates <code>FIp.Status.ExternalIP</code> after attaching the EIp to pods via OVN.</li> <li>Service LoadBalancer (<code>internal/service_lb/service_lb.go</code>, <code>internal/service_lb/eip_res.go</code>, <code>ServiceReconciler</code>)</li> <li><code>NewSvcLbEIpRes</code> allocates or binds an external IP for the Service.</li> <li><code>NewLoadBalancerRes</code> uses OVN NB client to define load balancer VIP\u2192backend mappings and injects rules into logical router/switch.</li> <li>Extra External Subnets (<code>internal/project/res_vpc.go</code>)</li> <li>Adds <code>ExtraExternalSubnets</code> field to <code>Vpc.Spec</code> when <code>project.Spec.EgressNetworkType</code> differs from the default external subnet, enabling multi-network external connectivity.    <pre><code>if externalNetwork.Name != defaultExternalSubnet.Name {\n    vpc.Spec.ExtraExternalSubnets = []string{externalNetwork.Name}\n}\n</code></pre>    \u3010F:internal/project/res_vpc.go\u2020L45-L52\u3011</li> </ol> <p>-Refer to code for detailed behavior: - Preamble and flag parsing: \u3010F:cmd/main.go\u2020L117-L131\u3011 - NAD CNI config: \u3010F:internal/project/res_nad.go\u2020L14-L31\u3011 - SNAT via OVN: \u3010F:internal/project/res_snat.go\u2020L14-L45\u3011 - Default EIP creation: \u3010F:internal/project/res_eip_default.go\u2020L15-L42\u3011 - FIp EIP sync: \u3010F:internal/fip/res_eip.go\u2020L25-L50\u3011 - Service LB orchestration: \u3010F:internal/service_lb/service_lb.go\u2020L30-L58\u3011\u3010F:internal/service_lb/eip_res.go\u2020L18-L40\u3011</p>"},{"location":"controller_diagram/#public-vs-cloud-external-networking","title":"Public vs Cloud External Networking","text":"<p>Kube-DC supports two external network types: public (direct public IPs) and cloud (cloud-provider-backed). The type influences EIP/FIP provisioning and SNAT rules:</p> <p><pre><code>// ExternalNetworkType defines how external networks are treated:\ntype ExternalNetworkType string\nconst (\n  ExternalNetworkTypePublic ExternalNetworkType = \"public\"\n  ExternalNetworkTypeCloud  ExternalNetworkType = \"cloud\"\n)\n\n// MasterConfig defaults per resource if not overridden:\nDefaultGwNetworkType, DefaultEipNetworkType,\nDefaultFipNetworkType, DefaultSvcLbNetworkType\n</code></pre> \u3010F:api/kube-dc.com/v1/types.go\u2020L1-L18\u3011</p>"},{"location":"controller_diagram/#project-egress-network-selection","title":"Project Egress Network Selection","text":"<p>The project spec may set <code>egressNetworkType</code> to choose the external subnet for VPC/SNAT/EIP.</p> <p><code>go // GenerateProjectVpc picks externalSubnet based on project.Spec.EgressNetworkType: externalNetwork, _ := utils.SelectBestExternalSubnet(ctx, cli, project.Spec.EgressNetworkType)</code>\u3010F:internal/project/res_vpc.go\u2020L55-L61\u3011</p>"},{"location":"controller_diagram/#snat-rules-for-outbound-traffic","title":"SNAT Rules for Outbound Traffic","text":"<p>SNAT rules ensure pod egress to internet through the gateway EIP:</p> <p><code>go // NewProjectSnat creates OvnSnatRule linking project namespace to gateway EIP base.GeneratedObject = &amp;kubeovn.OvnSnatRule{   Spec: OvnSnatRuleSpec{     OvnEip: DefaultOvnEipName(project, externalSubnet.Name),     Vpc:    projectNamespace,     VpcSubnet: SubnetName(project),   }, }</code>\u3010F:internal/project/res_snat.go\u2020L14-L45\u3011</p>"},{"location":"controller_diagram/#default-gateway-eip-vs-floating-ip","title":"Default Gateway EIP vs Floating IP","text":"<ul> <li>Default Gateway EIP: A single EIp CR per project created by <code>NewProjectEip</code> when no explicit EIP exists. Used for SNAT and default outbound.</li> <li>Floating IP (FIp): EIp allocated per FIp CR to attach public IPs to specific workloads.</li> </ul> <p><code>go // NewProjectEip ensures default project gateway EIp exists WithGetFunction(func(...) {   eip, err := resourcesProcessor.GetProjectGwEip()   if IsNotFound(err) {     newEip, _ := NewEipDefault(...)     base.GeneratedObject = newEip   } })</code>\u3010F:internal/project/res_eip_default.go\u2020L15-L37\u3011</p> <p><code>go // SyncEip for FIp: derives EIp name from FIp and creates/gets it // then FIpReconciler attaches exclusive ownership in OVN</code>\u3010F:internal/fip/res_eip.go\u2020L25-L40\u3011</p>"},{"location":"controller_diagram/#service-loadbalancer-external-ip-binding","title":"Service LoadBalancer External IP Binding","text":"<p>ServiceReconciler uses annotations or defaults to bind EIp to Services:</p> <p>```go // Get or create EIp for Service LB via NewSvcLbEIpRes eipSyncer := NewSvcLbEIpRes(ctx, cli, svc, project) eipSyncer.Sync(ctx)</p> <p>// Configure OVN LB VRRP rules via NewLoadBalancerRes lbRes := NewLoadBalancerRes(ctx, cli, svc, endpoints, eipSyncer.Found(), project) lbRes.Sync(ctx) ```\u3010F:internal/service_lb/eip_res.go\u2020L18-L40\u3011\u3010F:internal/service_lb/service_lb.go\u2020L75-L98\u3011</p>"},{"location":"core-features/","title":"Core Features","text":"<p>Kube-DC extends Kubernetes with a robust set of features designed for enterprise data center operations. This page provides detailed technical specifications and use cases for each of Kube-DC's core capabilities.</p> <p>Looking for a Architectural details? Visit our architectural overview.</p>"},{"location":"core-features/#organization-management","title":"Organization Management","text":"<p>Foundation for Multi-Tenancy</p> <p>Organization Management provides the foundation for Kube-DC's multi-tenant capabilities, enabling complete isolation between different users and groups.</p> <p>Kube-DC's multi-tenant architecture allows service providers to host multiple organizations with complete isolation and customization.</p> <p>Capabilities:</p> <ul> <li>Multi-Organization Support: Host multiple organizations on a single Kube-DC installation with complete logical separation</li> <li>Custom SSO Integration: Each organization can configure its own identity provider:<ul> <li>Google Workspace / Gmail</li> <li>Microsoft Active Directory / Azure AD</li> <li>GitHub</li> <li>GitLab</li> <li>LDAP</li> <li>SAML 2.0 providers</li> <li>OpenID Connect providers</li> </ul> </li> <li>Hierarchical Group Management: Create and manage groups within organizations with inheritance of permissions</li> <li>Flexible RBAC: Assign fine-grained permissions to groups for specific projects or resources</li> <li>Organizational Quotas: Set resource limits at the organization level to ensure fair resource allocation</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Host multiple client organizations with separate authentication systems</li> <li>Enterprise IT: Separate departments with different authentication requirements</li> <li>Educational Institutions: Provide isolated environments for different departments or research groups</li> </ul>"},{"location":"core-features/#namespace-as-a-service","title":"Namespace as a Service","text":"<p>Projects and Workloads</p> <p>Namespaces in Kube-DC function as projects, providing isolated environments for deploying and managing diverse workloads.</p> <p>Every project in Kube-DC is allocated its own Kubernetes namespace with extended capabilities for running both containers and virtual machines.</p> <p>Capabilities:</p> <ul> <li>Unified Management: Deploy and manage both VMs and containers from a single interface</li> <li>Project Isolation: Complete network and resource isolation between projects</li> <li>Resource Quotas: Set limits on CPU, memory, storage, and other resources per project</li> <li>Integrated Dashboard: View and manage all workloads through a unified web interface</li> <li>Custom Templates: Create and use templates for quick deployment of common workloads</li> </ul> <p>Real-World Applications</p> <ul> <li>Application Modernization: Run legacy VMs alongside containerized microservices</li> <li>Development Environments: Provide isolated environments for development, testing, and staging</li> <li>Mixed Workloads: Support teams that require both traditional and cloud-native infrastructure</li> </ul>"},{"location":"core-features/#network-management","title":"Network Management","text":"<p>Advanced Connectivity</p> <p>Kube-DC's network capabilities enable sophisticated connectivity options while maintaining isolation between projects.</p> <p>Kube-DC provides advanced networking capabilities that bridge traditional data center networking with cloud-native concepts.</p> <p>Capabilities:</p> <ul> <li>Dedicated VPC per Project: Each project gets its own virtual network environment</li> <li>VLAN Integration: Connect to physical network infrastructure using VLANs</li> <li>Software-Defined Networking: Create overlay networks with software-defined control</li> <li>Network Peering: Connect project networks with each other or with external networks</li> <li>NAT and Internet Gateway: Control outbound and inbound internet access per project</li> <li>External IP Assignment: Assign public IPs directly to VMs or Kubernetes services</li> <li>Load Balancer Integration: Create and manage load balancers for services and VMs</li> <li>Network Policies: Define granular rules for network traffic filtering</li> <li>DNS Management: Automatic DNS for services and VMs with custom domain support</li> </ul> <p>Real-World Applications</p> <ul> <li>Hybrid Cloud Deployments: Extend on-premises networks to containerized workloads</li> <li>Multi-Tier Applications: Create complex network topologies for enterprise applications</li> <li>Secure Isolation: Create zero-trust network environments with fine-grained control</li> </ul>"},{"location":"core-features/#virtualization","title":"Virtualization","text":"<p>KubeVirt Integration</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities fully integrated with Kubernetes.</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities integrated with Kubernetes.</p> <p>Capabilities:</p> <ul> <li>Hardware Vendor Support: Compatible with major hardware vendors' servers and components</li> <li>GPU Passthrough: Support for Nvidia GPU passthrough to virtual machines</li> <li>ARM Support: Run VMs on ARM-based infrastructure</li> <li>Web Console: Access VM consoles directly through the web UI</li> <li>SSH Integration: SSH access management with key authentication</li> <li>Live Migration: Move running VMs between nodes without downtime</li> <li>Snapshots: Create point-in-time snapshots of VM volumes</li> <li>VM Templates: Create and use templates for rapid VM provisioning</li> <li>Custom Boot Options: Configure boot order, firmware settings, and UEFI support</li> <li>VM Import/Export: Import existing VMs from other platforms</li> </ul> <p>Real-World Applications</p> <ul> <li>Legacy Application Support: Run applications that require traditional VMs</li> <li>Windows Workloads: Host Windows servers alongside Linux containers</li> <li>GPU-Accelerated Computing: Provide GPU resources for AI/ML or rendering workloads</li> <li>Specialized Operating Systems: Run operating systems not supported in containers</li> </ul>"},{"location":"core-features/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>API-Driven Architecture</p> <p>Kube-DC's API-driven approach enables automation and integration with popular infrastructure tools.</p> <p>Kube-DC leverages and extends the Kubernetes API to enable comprehensive infrastructure automation.</p> <p>Capabilities:</p> <ul> <li>Native Kubernetes API: Manage all Kube-DC resources using standard Kubernetes tools</li> <li>Custom Resource Definitions (CRDs): Extended Kubernetes objects for managing organizations, projects, VMs, and more</li> <li>GitOps Compatible: Deploy and manage infrastructure using GitOps workflows</li> </ul> <p>Real-World Applications</p> <ul> <li>Automated Infrastructure: Create fully automated infrastructure provisioning workflows</li> <li>Self-Service Portals: Build custom self-service interfaces using the Kube-DC API</li> <li>CI/CD Integration: Include infrastructure provisioning in CI/CD pipelines</li> <li>Multi-Cloud Management: Manage Kube-DC resources alongside other cloud resources</li> </ul>"},{"location":"core-features/#integrated-flexible-billing","title":"Integrated Flexible Billing","text":"<p>Cost Management</p> <p>Track, allocate, and manage costs across all resources with Kube-DC's comprehensive billing capabilities.</p> <p>Kube-DC includes comprehensive resource tracking and billing capabilities suitable for both service providers and internal IT organizations.</p> <p>Capabilities:</p> <ul> <li>Resource Metering: Track usage of CPU, memory, storage, GPU, and network resources</li> <li>Custom Pricing Models: Define pricing tiers for different resource types and customers</li> <li>Project-Based Billing: Track and bill resource usage at the project level</li> <li>Cost Allocation: Assign costs to organizational units, projects, or individual resources</li> <li>Quota Enforcement: Automatically enforce resource limits based on billing status</li> <li>Usage Reporting: Generate detailed usage reports for analysis and billing</li> <li>Billing API: Integrate with external billing systems through a comprehensive API</li> <li>Chargeback Models: Support for various internal chargeback models for enterprise use</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Bill customers for exact resource usage</li> <li>Enterprise IT: Implement internal chargeback or showback for departmental resource usage</li> <li>Resource Optimization: Identify resource usage patterns and optimize costs</li> </ul>"},{"location":"core-features/#management-services","title":"Management Services","text":"<p>Value-Added Services</p> <p>Extend Kube-DC's capabilities by offering managed services on top of the core platform.</p> <p>Kube-DC provides a platform for delivering managed services on top of its infrastructure.</p> <p>Capabilities:</p> <p>Database as a Service: Deploy and manage databases with automated operations</p> <ul> <li>PostgreSQL</li> <li>MySQL/MariaDB</li> <li>Microsoft SQL Server</li> <li>And more</li> </ul> <p>Object Storage: S3-compatible storage with multi-tenancy support</p> <p>NoSQL Databases: Managed NoSQL database offerings</p> <ul> <li>Redis</li> <li>MongoDB</li> <li>Elasticsearch/OpenSearch</li> </ul> <p>AI/ML Platform: Infrastructure for deploying and serving AI/ML models</p> <ul> <li>LLM serving</li> <li>Model training infrastructure</li> <li>GPU resource allocation</li> </ul> <p>Backup Services: Automated backup solutions for VMs and containers Monitoring as a Service: Multi-tenant monitoring solutions Service Catalog: Self-service provisioning of common services</p> <p>Real-World Applications</p> <ul> <li>Internal Platform Team: Provide managed services to development teams</li> <li>Managed Service Providers: Offer value-added services beyond basic infrastructure</li> <li>AI/ML Operations: Provide specialized infrastructure for data science teams</li> </ul>"},{"location":"internal-billing-integration/","title":"Internal Billing Integration Documentation","text":""},{"location":"internal-billing-integration/#overview","title":"Overview","text":"<p>This document describes the internal architecture and implementation details of the billing API integration within the Kube-DC platform. This integration provides organization-level billing management and project-specific cost analysis through the Kube-DC UI.</p>"},{"location":"internal-billing-integration/#architecture-overview","title":"Architecture Overview","text":""},{"location":"internal-billing-integration/#system-components","title":"System Components","text":"<pre><code>graph TB\n    subgraph \"Kube-DC Frontend\"\n        UI[Billing UI Component]\n        Auth[JWT Authentication]\n    end\n\n    subgraph \"Kube-DC Backend\"\n        Proxy[Billing Proxy Controller]\n        TokenSvc[Token Service]\n        AuthZ[Authorization Layer]\n    end\n\n    subgraph \"Billing Service\"\n        API[Billing API]\n        DB[(PostgreSQL)]\n    end\n\n    UI --&gt; Auth\n    Auth --&gt; Proxy\n    Proxy --&gt; TokenSvc\n    TokenSvc --&gt; AuthZ\n    AuthZ --&gt; API\n    API --&gt; DB</code></pre>"},{"location":"internal-billing-integration/#authentication-authorization-flow","title":"Authentication &amp; Authorization Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant BillingAPI\n\n    User-&gt;&gt;Frontend: Access Billing Page\n    Frontend-&gt;&gt;Backend: GET /api/billing/project/{ns}/overview\n    Note over Frontend,Backend: JWT Token in Authorization header\n\n    Backend-&gt;&gt;Backend: Extract &amp; validate JWT token\n    Backend-&gt;&gt;Backend: Check namespace permissions\n\n    alt Valid token &amp; authorized namespace\n        Backend-&gt;&gt;BillingAPI: GET /api/project/{ns}/overview\n        BillingAPI-&gt;&gt;Backend: Billing data response\n        Backend-&gt;&gt;Frontend: Authorized data\n        Frontend-&gt;&gt;User: Display billing information\n    else Invalid token or unauthorized\n        Backend-&gt;&gt;Frontend: 401/403 Error\n        Frontend-&gt;&gt;User: Access denied message\n    end</code></pre>"},{"location":"internal-billing-integration/#implementation-details","title":"Implementation Details","text":""},{"location":"internal-billing-integration/#backend-integration","title":"Backend Integration","text":""},{"location":"internal-billing-integration/#file-structure","title":"File Structure","text":"<pre><code>ui/backend/\n\u251c\u2500\u2500 controllers/billing/\n\u2502   \u2514\u2500\u2500 billingController.js     # Main billing proxy controller\n\u251c\u2500\u2500 routes/\n\u2502   \u2514\u2500\u2500 billing.js              # Billing API routes\n\u251c\u2500\u2500 utils/\n\u2502   \u2514\u2500\u2500 logger.js               # Logging utility\n\u2514\u2500\u2500 app.js                      # Main app with billing routes\n</code></pre>"},{"location":"internal-billing-integration/#key-components","title":"Key Components","text":"<p>Billing Controller (<code>controllers/billing/billingController.js</code>) - Proxies requests to internal billing service - Implements JWT token validation - Enforces namespace-based authorization - Handles error responses and logging</p> <p>Authentication Flow <pre><code>// Token extraction\nconst token = tokenService.getToken(req);\n\n// JWT decoding and validation\nconst decodedToken = decodeJWT(token);\nconst userNamespaces = decodedToken.namespaces || [];\n\n// Authorization check\nif (!userNamespaces.includes(namespace)) {\n  return sendErrorResponse(res, 403, 'Access denied to namespace');\n}\n</code></pre></p> <p>Service Communication - Internal service URL: <code>billing-dashboard-svc.billing.svc.cluster.local:5000</code> - Uses Kubernetes service discovery - No external network access required</p>"},{"location":"internal-billing-integration/#frontend-integration","title":"Frontend Integration","text":""},{"location":"internal-billing-integration/#file-structure_1","title":"File Structure","text":"<pre><code>ui/frontend/src/app/ManageOrganization/\n\u251c\u2500\u2500 Billing/\n\u2502   \u2514\u2500\u2500 Billing.tsx             # Main billing component\n\u251c\u2500\u2500 OrganizationRoutes.tsx      # Route definitions\n\u251c\u2500\u2500 OrganizationSidebar.tsx     # Navigation sidebar\n\u2514\u2500\u2500 OrganizationLayout.tsx      # Layout logic\n</code></pre>"},{"location":"internal-billing-integration/#key-features","title":"Key Features","text":"<p>Billing Component (<code>Billing/Billing.tsx</code>) - Uses PatternFly design system - Displays project billing summaries in table format - Implements loading states and error handling - Follows existing Kube-DC UI patterns</p> <p>Security Implementation <pre><code>// Namespace extraction from JWT\nconst getUserNamespaces = React.useMemo(() =&gt; {\n  if (!token) return [];\n  try {\n    const decodedToken = decodeJWT(token);\n    return decodedToken.namespaces || [];\n  } catch (error) {\n    return [];\n  }\n}, [token]);\n\n// API calls with authentication\nconst response = await fetch(`/api/billing/project/${namespace}/overview`, {\n  headers: {\n    'Authorization': `Bearer ${token}`,\n    'Content-Type': 'application/json'\n  },\n  credentials: 'include'\n});\n</code></pre></p>"},{"location":"internal-billing-integration/#api-endpoints","title":"API Endpoints","text":""},{"location":"internal-billing-integration/#available-endpoints","title":"Available Endpoints","text":"Method Endpoint Description Authentication GET <code>/api/billing/health</code> Service health check Required GET <code>/api/billing/projects</code> List accessible projects Required GET <code>/api/billing/project/:namespace/overview</code> Project billing details Required + Namespace access"},{"location":"internal-billing-integration/#response-format","title":"Response Format","text":"<p>Project Overview Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"namespace\": \"project-name\",\n    \"billing_summary\": {\n      \"current_month_total\": 1672.31,\n      \"last_day_spend\": 109.16,\n      \"billing_period\": \"2025-09\",\n      \"total_cost_per_hour\": 2.32\n    },\n    \"compute_instances\": {\n      \"running_pods\": 10,\n      \"running_vms\": 1,\n      \"cpu_cores\": 2.4,\n      \"memory_gib\": 8.8,\n      \"cost_per_hour\": 0.15\n    },\n    \"cost_breakdown\": {\n      \"cpu_cost\": 89.45,\n      \"memory_cost\": 45.23,\n      \"storage_cost\": 12.67,\n      \"network_cost\": 0.00,\n      \"public_ip_cost\": 0.00\n    }\n  },\n  \"timestamp\": \"2025-10-01T15:25:00.000Z\"\n}\n</code></pre></p>"},{"location":"internal-billing-integration/#security-model","title":"Security Model","text":""},{"location":"internal-billing-integration/#jwt-token-structure","title":"JWT Token Structure","text":"<pre><code>{\n  \"org\": \"organization-name\",\n  \"namespaces\": [\"project-1\", \"project-2\"],\n  \"groups\": [\"org-admin\", \"user\"],\n  \"exp\": 1696176000,\n  \"iat\": 1696089600\n}\n</code></pre>"},{"location":"internal-billing-integration/#authorization-levels","title":"Authorization Levels","text":"Role Access Level Permissions <code>org-admin</code> Organization-wide All projects in organization <code>project-user</code> Project-specific Only assigned namespaces <code>guest</code> No access No billing data access"},{"location":"internal-billing-integration/#security-layers","title":"Security Layers","text":"<ol> <li>OIDC Authentication - External identity provider</li> <li>JWT Token Validation - Backend token verification</li> <li>Namespace Authorization - Per-project access control</li> <li>Network Security - Internal service communication only</li> </ol>"},{"location":"internal-billing-integration/#configuration","title":"Configuration","text":""},{"location":"internal-billing-integration/#environment-variables","title":"Environment Variables","text":"<p>Backend Configuration <pre><code># Billing service endpoint (internal)\nBILLING_API_URL=http://billing-dashboard-svc.billing.svc.cluster.local:5000\n\n# Logging level\nLOG_LEVEL=info\n</code></pre></p> <p>Frontend Configuration Uses existing Kube-DC ConfigMap pattern: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dc-frontend-config\ndata:\n  env.js: |\n    window.backendURL = 'https://backend.stage.kube-dc.com';\n    window.frontendURL = 'https://console.stage.kube-dc.com';\n    window.keycloakURL = 'https://login.stage.kube-dc.com';\n</code></pre></p>"},{"location":"internal-billing-integration/#network-policies","title":"Network Policies","text":"<pre><code># Internal service communication\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: kube-dc-billing-access\nspec:\n  podSelector:\n    matchLabels:\n      app: kube-dc-backend\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: billing\n    ports:\n    - protocol: TCP\n      port: 5000\n</code></pre>"},{"location":"internal-billing-integration/#error-handling","title":"Error Handling","text":""},{"location":"internal-billing-integration/#http-status-codes","title":"HTTP Status Codes","text":"Code Description Cause 200 Success Request completed successfully 401 Unauthorized Missing or invalid JWT token 403 Forbidden Valid token, insufficient permissions 503 Service Unavailable Billing service unreachable 500 Internal Server Error Unexpected server error"},{"location":"internal-billing-integration/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"success\": false,\n  \"error\": \"Access denied to namespace\",\n  \"details\": {\n    \"namespace\": \"requested-project\",\n    \"availableNamespaces\": [\"project-1\", \"project-2\"]\n  },\n  \"timestamp\": \"2025-10-01T15:25:00.000Z\"\n}\n</code></pre>"},{"location":"internal-billing-integration/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"internal-billing-integration/#log-levels","title":"Log Levels","text":"<ul> <li>INFO: Normal operations, API calls</li> <li>WARN: Authentication failures, permission denials  </li> <li>ERROR: Service errors, network issues</li> <li>DEBUG: Detailed request/response data (development only)</li> </ul>"},{"location":"internal-billing-integration/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li>API response times</li> <li>Authentication failure rates</li> <li>Service availability</li> <li>Error rates by endpoint</li> <li>Namespace access patterns</li> </ul>"},{"location":"internal-billing-integration/#development-guidelines","title":"Development Guidelines","text":""},{"location":"internal-billing-integration/#code-standards","title":"Code Standards","text":"<ul> <li>Follow existing Kube-DC patterns</li> <li>Use PatternFly components for UI consistency</li> <li>Implement proper error handling</li> <li>Add comprehensive logging</li> <li>Write JSDoc comments for public methods</li> </ul>"},{"location":"internal-billing-integration/#testing-approach","title":"Testing Approach","text":"<ul> <li>Unit tests for controller logic</li> <li>Integration tests for API endpoints</li> <li>Frontend component tests</li> <li>End-to-end authentication flows</li> </ul>"},{"location":"internal-billing-integration/#deployment-process","title":"Deployment Process","text":"<ol> <li>Backend changes deployed via Helm chart</li> <li>Frontend changes built into container image</li> <li>Configuration updates via ConfigMaps</li> <li>Rolling deployment with health checks</li> </ol>"},{"location":"internal-billing-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"internal-billing-integration/#common-issues","title":"Common Issues","text":"<p>\"Authentication token required\" - Check JWT token presence in request headers - Verify token format (Bearer scheme) - Confirm OIDC authentication is working</p> <p>\"Access denied to namespace\" - Verify user has access to requested project - Check JWT token namespace claims - Confirm RBAC configuration</p> <p>\"Billing service unreachable\" - Check billing service pod status - Verify network connectivity - Confirm service DNS resolution</p>"},{"location":"internal-billing-integration/#debug-commands","title":"Debug Commands","text":"<pre><code># Check service status\nkubectl get pods -n kube-dc\nkubectl get pods -n billing\n\n# Check service connectivity\nkubectl exec -n kube-dc deployment/kube-dc-backend -- \\\n  curl -v http://billing-dashboard-svc.billing.svc.cluster.local:5000/api/health\n\n# Check logs\nkubectl logs -n kube-dc deployment/kube-dc-backend\nkubectl logs -n billing deployment/billing-dashboard\n</code></pre>"},{"location":"internal-billing-integration/#future-enhancements","title":"Future Enhancements","text":""},{"location":"internal-billing-integration/#planned-features","title":"Planned Features","text":"<ul> <li>Cost trend analysis and forecasting</li> <li>Budget alerts and notifications</li> <li>Resource optimization recommendations</li> <li>Detailed cost attribution reports</li> <li>Integration with cloud provider billing APIs</li> </ul>"},{"location":"internal-billing-integration/#technical-improvements","title":"Technical Improvements","text":"<ul> <li>Response caching for performance</li> <li>Real-time cost updates via WebSocket</li> <li>Advanced filtering and search capabilities</li> <li>Export functionality for billing reports</li> <li>Integration with monitoring systems</li> </ul> <p>This document is maintained by the Kube-DC development team. For questions or updates, please contact the platform team.</p>"},{"location":"managing-os-images/","title":"Managing OS Images in Kube-DC","text":"<p>This guide explains how to manage operating system images in the Kube-DC platform, including adding new OS options, modifying existing configurations, and updating the system.</p>"},{"location":"managing-os-images/#overview","title":"Overview","text":"<p>OS images in Kube-DC are configured through a Kubernetes ConfigMap that defines: - Available operating systems in the VM creation UI - Default resource requirements (memory, CPU, storage) - Firmware and virtualization settings - Cloud-init configurations - Image URLs and user credentials</p>"},{"location":"managing-os-images/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Helm Chart    \u2502\u2500\u2500\u2500\u25b6\u2502   ConfigMap      \u2502\u2500\u2500\u2500\u25b6\u2502  Backend API    \u2502\n\u2502   Template      \u2502    \u2502 images-configmap \u2502    \u2502 /os-images      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                       \u2502  Frontend UI    \u2502\n                       \u2502 Create VM Modal \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"managing-os-images/#configmap-structure","title":"ConfigMap Structure","text":"<p>The OS images are defined in <code>/charts/kube-dc/templates/os-images-configmap.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: images-configmap\n  namespace: {{ .Release.Namespace }}\ndata:\n  images.yaml: |\n    images:\n      - OS_NAME: \"Ubuntu 24.04\"\n        CLOUD_USER: ubuntu\n        OS_IMAGE_URL: \"https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\"\n        MIN_MEMORY: \"1G\"\n        MIN_VCPU: \"1\"\n        MIN_STORAGE: \"20G\"\n        FIRMWARE_TYPE: \"bios\"\n        MACHINE_TYPE: \"q35\"\n        FEATURES: \"acpi\"\n        CLOUD_INIT: |\n          #cloud-config\n          package_update: true\n          packages:\n            - qemu-guest-agent\n</code></pre>"},{"location":"managing-os-images/#configuration-fields","title":"Configuration Fields","text":""},{"location":"managing-os-images/#required-fields","title":"Required Fields","text":"Field Description Example <code>OS_NAME</code> Display name in UI dropdown <code>\"Ubuntu 24.04\"</code> <code>CLOUD_USER</code> Default SSH user for the OS <code>ubuntu</code> <code>OS_IMAGE_URL</code> HTTP URL to the disk image <code>https://example.com/image.qcow2</code>"},{"location":"managing-os-images/#resource-requirements","title":"Resource Requirements","text":"Field Description Example Notes <code>MIN_MEMORY</code> Minimum RAM requirement <code>\"8G\"</code>, <code>\"512M\"</code> Supports G/M suffixes <code>MIN_VCPU</code> Minimum CPU cores <code>\"2\"</code> String format <code>MIN_STORAGE</code> Minimum disk size <code>\"60G\"</code> Supports G suffix"},{"location":"managing-os-images/#virtualization-settings","title":"Virtualization Settings","text":"Field Description Options Notes <code>FIRMWARE_TYPE</code> Boot firmware <code>\"bios\"</code>, <code>\"efi\"</code> EFI required for Windows 11 <code>MACHINE_TYPE</code> QEMU machine type <code>\"q35\"</code>, <code>\"pc-q35-rhel8.6.0\"</code> Specific types for compatibility <code>FEATURES</code> Virtualization features <code>\"acpi\"</code>, <code>\"hyperv,acpi,apic,smm,tpm\"</code> Comma-separated list"},{"location":"managing-os-images/#supported-features","title":"Supported Features","text":"<ul> <li><code>acpi</code> - Advanced Configuration and Power Interface</li> <li><code>apic</code> - Advanced Programmable Interrupt Controller  </li> <li><code>hyperv</code> - Microsoft Hyper-V enlightenments</li> <li><code>smm</code> - System Management Mode</li> <li><code>tpm</code> - Trusted Platform Module (required for Windows 11)</li> </ul>"},{"location":"managing-os-images/#os-specific-configurations","title":"OS-Specific Configurations","text":""},{"location":"managing-os-images/#linux-distributions","title":"Linux Distributions","text":"<p>Ubuntu/Debian: <pre><code>- OS_NAME: \"Ubuntu 24.04\"\n  CLOUD_USER: ubuntu\n  MIN_MEMORY: \"1G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"20G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n</code></pre></p> <p>CentOS/RHEL: <pre><code>- OS_NAME: \"CentOS Stream 9\"\n  CLOUD_USER: centos\n  MIN_MEMORY: \"2G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"20G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n</code></pre></p>"},{"location":"managing-os-images/#windows-systems","title":"Windows Systems","text":"<p>Windows 11: <pre><code>- OS_NAME: \"Windows 11 Enterprise\"\n  CLOUD_USER: Administrator\n  MIN_MEMORY: \"8G\"\n  MIN_VCPU: \"4\"\n  MIN_STORAGE: \"60G\"\n  FIRMWARE_TYPE: \"efi\"\n  MACHINE_TYPE: \"pc-q35-rhel8.6.0\"\n  FEATURES: \"hyperv,acpi,apic,smm,tpm\"\n</code></pre></p>"},{"location":"managing-os-images/#adding-a-new-os-image","title":"Adding a New OS Image","text":""},{"location":"managing-os-images/#step-1-prepare-the-image","title":"Step 1: Prepare the Image","text":"<ol> <li>Obtain the disk image (qcow2, raw, or vmdk format)</li> <li>Host the image on an HTTP server accessible to your cluster</li> <li>Test the image to ensure it boots correctly</li> </ol>"},{"location":"managing-os-images/#step-2-update-the-configmap","title":"Step 2: Update the ConfigMap","text":"<p>Edit <code>/charts/kube-dc/templates/os-images-configmap.yaml</code>:</p> <pre><code># Add your new OS entry\n- OS_NAME: \"Fedora 40\"\n  CLOUD_USER: fedora\n  OS_IMAGE_URL: \"https://download.fedoraproject.org/pub/fedora/linux/releases/40/Cloud/x86_64/images/Fedora-Cloud-Base-40-1.14.x86_64.qcow2\"\n  MIN_MEMORY: \"2G\"\n  MIN_VCPU: \"1\"\n  MIN_STORAGE: \"25G\"\n  FIRMWARE_TYPE: \"bios\"\n  MACHINE_TYPE: \"q35\"\n  FEATURES: \"acpi\"\n  CLOUD_INIT: |\n    #cloud-config\n    package_update: true\n    packages:\n      - qemu-guest-agent\n    runcmd:\n      - systemctl enable --now qemu-guest-agent\n</code></pre>"},{"location":"managing-os-images/#step-3-deploy-the-changes","title":"Step 3: Deploy the Changes","text":"<p>Option A: Helm Upgrade (Recommended) <pre><code># From the project root\nhelm upgrade kube-dc ./charts/kube-dc -n kube-dc\n</code></pre></p> <p>Option B: Direct ConfigMap Update <pre><code># Apply the ConfigMap directly\nkubectl apply -f charts/kube-dc/templates/os-images-configmap.yaml\n</code></pre></p>"},{"location":"managing-os-images/#step-4-reload-the-backend","title":"Step 4: Reload the Backend","text":"<p>The backend caches OS images for performance. After updating the ConfigMap:</p> <pre><code># Restart the backend to reload the cache\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n\n# Or wait for the cache TTL (30 seconds) to expire\n</code></pre>"},{"location":"managing-os-images/#modifying-existing-os-images","title":"Modifying Existing OS Images","text":""},{"location":"managing-os-images/#inline-editing","title":"Inline Editing","text":"<p>You can modify the ConfigMap directly in Kubernetes:</p> <pre><code># Edit the ConfigMap in your cluster\nkubectl edit configmap images-configmap -n kube-dc\n</code></pre> <p>Example: Increase Windows 11 memory requirement: <pre><code># Change from:\nMIN_MEMORY: \"8G\"\n# To:\nMIN_MEMORY: \"16G\"\n</code></pre></p> <p>After saving, restart the backend: <pre><code>kubectl rollout restart deployment/kube-dc-backend -n kube-dc\n</code></pre></p>"},{"location":"managing-os-images/#updating-image-urls","title":"Updating Image URLs","text":"<p>If an image URL changes or becomes unavailable:</p> <ol> <li> <p>Update the ConfigMap: <pre><code># Old URL\nOS_IMAGE_URL: \"https://old-server.com/ubuntu-24.04.qcow2\"\n# New URL  \nOS_IMAGE_URL: \"https://new-server.com/ubuntu-24.04.qcow2\"\n</code></pre></p> </li> <li> <p>Apply changes: <pre><code>kubectl apply -f charts/kube-dc/templates/os-images-configmap.yaml\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n</code></pre></p> </li> </ol>"},{"location":"managing-os-images/#testing-changes","title":"Testing Changes","text":""},{"location":"managing-os-images/#verify-configmap-update","title":"Verify ConfigMap Update","text":"<pre><code># Check the ConfigMap was updated\nkubectl get configmap images-configmap -n kube-dc -o yaml\n\n# Test the API endpoint\ncurl -s \"https://backend.stage.kube-dc.com/api/create-vm/your-namespace/os-images\" | jq '.[].OS_NAME'\n</code></pre>"},{"location":"managing-os-images/#test-in-ui","title":"Test in UI","text":"<ol> <li>Open the Kube-DC web interface</li> <li>Navigate to Create VM</li> <li>Check the Operation System dropdown</li> <li>Verify your new OS appears with correct parameters</li> <li>Select the OS and confirm memory/CPU/storage auto-populate</li> </ol>"},{"location":"managing-os-images/#troubleshooting","title":"Troubleshooting","text":""},{"location":"managing-os-images/#os-not-appearing-in-ui","title":"OS Not Appearing in UI","text":"<p>Check the ConfigMap: <pre><code>kubectl describe configmap images-configmap -n kube-dc\n</code></pre></p> <p>Verify backend logs: <pre><code>kubectl logs -n kube-dc deployment/kube-dc-backend --tail=50\n</code></pre></p> <p>Common issues: - YAML syntax errors in ConfigMap - Backend cache not refreshed - Network connectivity to image URL</p>"},{"location":"managing-os-images/#vm-creation-fails","title":"VM Creation Fails","text":"<p>Check image accessibility: <pre><code># Test if the image URL is reachable\ncurl -I \"https://your-image-url.com/image.qcow2\"\n</code></pre></p> <p>Verify resource requirements: - Ensure cluster has sufficient resources - Check storage class availability - Verify network policies allow image downloads</p>"},{"location":"managing-os-images/#backend-cache-issues","title":"Backend Cache Issues","text":"<p>The backend caches OS images for 30 seconds. To force refresh:</p> <pre><code># Restart backend pods\nkubectl rollout restart deployment/kube-dc-backend -n kube-dc\n\n# Or wait for cache expiration (30 seconds)\n</code></pre>"},{"location":"managing-os-images/#best-practices","title":"Best Practices","text":""},{"location":"managing-os-images/#image-management","title":"Image Management","text":"<ol> <li>Use stable URLs - Avoid URLs that change frequently</li> <li>Host images reliably - Use CDNs or reliable hosting</li> <li>Test images - Verify images boot before adding to production</li> <li>Document changes - Keep track of image versions and changes</li> </ol>"},{"location":"managing-os-images/#resource-requirements_1","title":"Resource Requirements","text":"<ol> <li>Set realistic minimums - Don't under-provision resources</li> <li>Consider workload - Different use cases need different resources  </li> <li>Test performance - Verify VMs perform well with set resources</li> </ol>"},{"location":"managing-os-images/#security","title":"Security","text":"<ol> <li>Verify image sources - Only use trusted image providers</li> <li>Scan images - Check for vulnerabilities before deployment</li> <li>Use HTTPS - Always use secure URLs for image downloads</li> <li>Regular updates - Keep OS images updated with security patches</li> </ol>"},{"location":"managing-os-images/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"managing-os-images/#custom-cloud-init","title":"Custom Cloud-Init","text":"<p>For complex initialization requirements:</p> <pre><code>CLOUD_INIT: |\n  #cloud-config\n  users:\n    - name: admin\n      groups: sudo\n      shell: /bin/bash\n      sudo: ALL=(ALL) NOPASSWD:ALL\n  packages:\n    - docker.io\n    - nginx\n  runcmd:\n    - systemctl enable docker\n    - systemctl start docker\n    - docker run -d -p 80:80 nginx\n</code></pre>"},{"location":"managing-os-images/#windows-specific-settings","title":"Windows-Specific Settings","text":"<p>For Windows VMs, additional configuration may be needed:</p> <pre><code># Windows Server 2022\n- OS_NAME: \"Windows Server 2022\"\n  CLOUD_USER: Administrator\n  MIN_MEMORY: \"4G\"\n  MIN_VCPU: \"2\"\n  MIN_STORAGE: \"80G\"\n  FIRMWARE_TYPE: \"efi\"\n  MACHINE_TYPE: \"pc-q35-rhel8.6.0\"\n  FEATURES: \"hyperv,acpi,apic,smm,tpm\"\n  BOOT_ORDER: \"cdrom,disk\"\n  ADDITIONAL_DISKS: \"virtio-drivers\"\n</code></pre>"},{"location":"managing-os-images/#api-reference","title":"API Reference","text":"<p>The OS images are served via the backend API:</p> <p>Endpoint: <code>GET /api/create-vm/{namespace}/os-images</code></p> <p>Response format: <pre><code>[\n  {\n    \"OS_NAME\": \"Ubuntu 24.04\",\n    \"CLOUD_USER\": \"ubuntu\",\n    \"OS_IMAGE_URL\": \"https://cloud-images.ubuntu.com/...\",\n    \"MIN_MEMORY\": \"1G\",\n    \"MIN_VCPU\": \"1\",\n    \"MIN_STORAGE\": \"20G\",\n    \"FIRMWARE_TYPE\": \"bios\",\n    \"MACHINE_TYPE\": \"q35\",\n    \"FEATURES\": \"acpi\",\n    \"CLOUD_INIT\": \"#cloud-config\\n...\"\n  }\n]\n</code></pre></p>"},{"location":"managing-os-images/#support","title":"Support","text":"<p>For additional help: - Check the Kube-DC documentation - Review troubleshooting guides - Open an issue in the project repository</p>"},{"location":"product-backlog/","title":"Kube-DC Product Backlog","text":"<p>This document outlines the current product backlog for the Kube-DC project, organized by epics and features.</p>"},{"location":"product-backlog/#active-epics","title":"\ud83d\ude80 Active Epics","text":""},{"location":"product-backlog/#epic-windows-support","title":"[Epic] Windows Support","text":"<p>Status: In Progress</p> <ul> <li>\u2705 [Platform] Windows QCOW and documented pipeline on preparing golden image</li> <li>Complete Windows 11 VM implementation with VirtIO drivers</li> <li>HTTP-based ISO serving infrastructure</li> <li> <p>Documentation for Windows VM setup and configuration</p> </li> <li> <p>\ud83d\udd04 Fix issues on metrics from Windows VMs</p> </li> <li>Address Windows-specific monitoring and metrics collection</li> <li>Ensure proper guest agent integration for Windows</li> </ul>"},{"location":"product-backlog/#epic-vmware-migration","title":"[Epic] VMware Migration","text":"<p>Status: Research Phase</p> <ul> <li>\ud83d\udd0d VMware vSphere migration research (CDI, vjailbreak)</li> <li>Investigate migration tools and methodologies</li> <li>Evaluate CDI (Containerized Data Importer) for VM migration</li> <li>Research vjailbreak and other migration utilities</li> </ul>"},{"location":"product-backlog/#epic-organization-management","title":"[Epic] Organization Management","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udccb UI for project and roles</li> <li>Implement project management interface</li> <li>Role-based access control UI components</li> <li> <p>User and group management interfaces</p> </li> <li> <p>\ud83c\udfa8 Customize Login Page</p> </li> <li>Branding and customization options</li> <li>Organization-specific login themes</li> </ul>"},{"location":"product-backlog/#epic-ui-implementation-on-backend","title":"[Epic] UI Implementation on Backend","text":"<p>Status: Multiple Items in Progress</p> <ul> <li>\u274c UI Clone Disks - Not working</li> <li>Fix disk cloning functionality in UI</li> <li> <p>Ensure proper CDI integration</p> </li> <li> <p>\ud83d\udccb UI Create VM from PVC/DataVolume</p> </li> <li>Interface for VM creation from existing storage</li> <li> <p>DataVolume selection and configuration</p> </li> <li> <p>\ud83c\udf10 UI Add VM Static IP</p> </li> <li>Static IP assignment interface</li> <li> <p>Network configuration management</p> </li> <li> <p>\ud83c\udf10 UI Add VM FIP (Floating IP)</p> </li> <li>Floating IP assignment and management</li> <li> <p>Integration with FIP CRD resources</p> </li> <li> <p>\u2696\ufe0f UI Add Load Balancer Setup</p> </li> <li>Load balancer configuration interface</li> <li> <p>Service exposure management</p> </li> <li> <p>\ud83d\udd04 UI Migrate/Clone VM (rook/ceph)</p> </li> <li>VM migration interface with Rook/Ceph backend</li> <li> <p>Live migration capabilities</p> </li> <li> <p>\ud83d\udc65 UI VM Groups</p> </li> <li>VM grouping and management features</li> <li>Bulk operations on VM groups</li> </ul>"},{"location":"product-backlog/#epic-installer","title":"[Epic] Installer","text":"<p>Status: Enhancement Phase</p> <ul> <li>\ud83d\uddc4\ufe0f Postgres DB for Keycloak and Billing to be dedicated in installer stack</li> <li>Separate PostgreSQL deployment for Keycloak</li> <li> <p>Database isolation and management</p> </li> <li> <p>\u26a1 Simplify Installer</p> </li> <li>Streamline installation process</li> <li> <p>Reduce complexity and dependencies</p> </li> <li> <p>\ud83d\udda5\ufe0f Single host install</p> </li> <li>Support for single-node deployments</li> <li> <p>All-in-one installation option</p> </li> <li> <p>\ud83d\udd27 Test on VMware vSX</p> </li> <li>Validation on VMware infrastructure</li> <li> <p>Compatibility testing and documentation</p> </li> <li> <p>\ud83d\udd10 Fix hardcoded passwords in loki.yaml</p> </li> <li>Security improvement for Loki configuration</li> <li>Dynamic password generation</li> </ul>"},{"location":"product-backlog/#epic-licensing","title":"[Epic] Licensing","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udcc4 License for Node Limits per installation</li> <li>Implement licensing system</li> <li>Node-based licensing model</li> <li>License validation and enforcement</li> </ul>"},{"location":"product-backlog/#epic-billing","title":"[Epic] Billing","text":"<p>Status: Planning</p> <ul> <li>\ud83d\udcb0 Billing system implementation</li> <li>Usage tracking and billing</li> <li>Integration with licensing system</li> <li>Cost management features</li> </ul>"},{"location":"product-backlog/#epic-observability","title":"[Epic] Observability","text":"<p>Status: Enhancement Phase</p> <ul> <li>\ud83d\udcca Logs</li> <li>Centralized logging improvements</li> <li> <p>Log aggregation and analysis</p> </li> <li> <p>\ud83d\udcc8 Metrics</p> </li> <li>Enhanced monitoring and metrics collection</li> <li> <p>Performance dashboards</p> </li> <li> <p>\ud83d\udea8 Alerts</p> </li> <li>Alerting system implementation</li> <li>Notification and escalation policies</li> </ul>"},{"location":"product-backlog/#epic-gpu-support","title":"[Epic] GPU Support","text":"<p>Status: Research Phase</p> <ul> <li>\ud83c\udfae Evaluate Project HAMI</li> <li>GPU sharing and management solution</li> <li>Integration assessment with KubeVirt</li> </ul>"},{"location":"product-backlog/#epic-managed-services","title":"[Epic] Managed Services","text":"<p>Status: Planning</p> <ul> <li>\u2638\ufe0f K8s CAPI (Cluster API)</li> <li>Kubernetes cluster management</li> <li> <p>Multi-cluster operations</p> </li> <li> <p>\u2638\ufe0f K8s Vcluster</p> </li> <li>Virtual cluster implementation</li> <li> <p>Tenant isolation improvements</p> </li> <li> <p>\ud83d\uddc4\ufe0f Rook S3</p> </li> <li>Object storage services</li> <li> <p>S3-compatible storage backend</p> </li> <li> <p>\ud83d\uddc4\ufe0f RDS Percona Operators</p> </li> <li>Database-as-a-Service implementation</li> <li>MySQL/PostgreSQL managed services</li> </ul>"},{"location":"product-backlog/#epic-kubevirt-enhancements","title":"[Epic] KubeVirt Enhancements","text":"<p>Status: High Priority</p> <ul> <li>\ud83d\udd04 CDI Cloning (Priority)</li> <li>VM disk cloning capabilities</li> <li> <p>Efficient storage management</p> </li> <li> <p>\ud83d\udd25 CPU, Memory, GPU Hotplug</p> </li> <li>Dynamic resource allocation</li> <li>Live resource scaling for VMs</li> </ul>"},{"location":"product-backlog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":""},{"location":"product-backlog/#critical-bugs","title":"Critical Bugs","text":"<ul> <li>\ud83d\udd27 UI get_kubeconfig.sh namespace issue</li> <li>Fix namespace handling in kubeconfig generation</li> <li> <p>Ensure proper authentication and authorization</p> </li> <li> <p>\ud83e\uddf9 Fix issue with stale jobs with pshell</p> </li> <li>Clean up orphaned pshell jobs</li> <li>Improve job lifecycle management</li> </ul>"},{"location":"product-backlog/#priority-matrix","title":"\ud83d\udcca Priority Matrix","text":""},{"location":"product-backlog/#high-priority","title":"High Priority","text":"<ol> <li>CDI Cloning functionality</li> <li>Windows metrics fixes</li> <li>UI disk cloning repairs</li> <li>Critical bug fixes (kubeconfig, pshell jobs)</li> </ol>"},{"location":"product-backlog/#medium-priority","title":"Medium Priority","text":"<ol> <li>VMware migration research</li> <li>GPU support evaluation</li> <li>Installer simplification</li> <li>Observability enhancements</li> </ol>"},{"location":"product-backlog/#low-priority","title":"Low Priority","text":"<ol> <li>Licensing system</li> <li>Billing implementation</li> <li>Managed services expansion</li> <li>UI enhancements (VM groups, static IP)</li> </ol> <p>Last Updated: September 2025 Document Owner: Kube-DC Product Team</p>"},{"location":"project_resources/","title":"Project Resources Documentation","text":"<p>This document provides a comprehensive overview of all resources created by the Kube-DC Project controller, their finalizers, ownership patterns, and deletion dependencies.</p>"},{"location":"project_resources/#resource-creation-order","title":"Resource Creation Order","text":"<p>When a Project is created, resources are synchronized in this order:</p> <ol> <li>Namespace - Project namespace (<code>{org}-{project}</code>)</li> <li>VPC - Kube-OVN Virtual Private Cloud</li> <li>EIp (Default Gateway) - External IP for project gateway</li> <li>Subnet - Kube-OVN subnet for project pods</li> <li>NetworkAttachmentDefinition - CNI network configuration</li> <li>OvnSnatRule - SNAT rule for outbound traffic</li> <li>Secrets - SSH keypairs and authorized keys</li> <li>RBAC - Roles and RoleBindings</li> <li>VpcDns - DNS configuration for VPC</li> </ol>"},{"location":"project_resources/#detailed-resource-breakdown","title":"Detailed Resource Breakdown","text":""},{"location":"project_resources/#1-namespace","title":"1. Namespace","text":"<ul> <li>Resource: <code>v1.Namespace</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: None (managed by Kubernetes)</li> <li>Created by: <code>NewProjectNamespace()</code> in <code>internal/project/res_namespace.go</code></li> <li>Dependencies: None (first resource created)</li> </ul>"},{"location":"project_resources/#2-vpc-virtual-private-cloud","title":"2. VPC (Virtual Private Cloud)","text":"<ul> <li>Resource: <code>kubeovn.io/v1.Vpc</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectVpc()</code> in <code>internal/project/res_vpc.go</code></li> <li>Dependencies: Namespace must exist</li> <li> <p>Configuration:</p> </li> <li> <p>Static routes to external subnets</p> </li> <li>Extra external subnets based on <code>egressNetworkType</code></li> </ul>"},{"location":"project_resources/#3-eip-external-ip-default-gateway","title":"3. EIp (External IP - Default Gateway)","text":"<ul> <li>Resource: <code>kube-dc.com/v1.EIp</code></li> <li>Name: <code>default-gw</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>eip.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectEip()</code> in <code>internal/project/res_eip_default.go</code></li> <li>Dependencies: Namespace must exist</li> <li> <p>Ownership States:</p> </li> <li> <p><code>Released</code>: No active owners (initial state)</p> </li> <li><code>Shared</code>: Has SNAT rule and/or LoadBalancer services as owners</li> <li><code>Exclusive</code>: Used by FIp resources</li> </ul>"},{"location":"project_resources/#4-ovneip-underlying-ovn-external-ip","title":"4. OvnEip (Underlying OVN External IP)","text":"<ul> <li>Resource: <code>kubeovn.io/v1.OvnEip</code></li> <li>Name: <code>{organization}-{project}-{external-subnet}</code> (e.g., <code>shalb-envoy-ext-public</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: EIp controller via <code>NewOvEipRes()</code> in <code>internal/eip/ovn_eip_res.go</code></li> <li>Dependencies: EIp must exist, external subnet must be available</li> <li> <p>Labels:</p> </li> <li> <p><code>network.kube-dc.com/eip</code>: <code>{namespace}.{eip-name}</code></p> </li> <li> <p>Annotations:</p> </li> <li> <p><code>kube-dc.com/ovn-eip-created-by-eip</code>: <code>{namespace}.{eip-name}</code></p> </li> </ul>"},{"location":"project_resources/#5-subnet","title":"5. Subnet","text":"<ul> <li>Resource: <code>kubeovn.io/v1.Subnet</code></li> <li>Name: <code>{organization}-{project}-default</code> (e.g., <code>shalb-envoy-default</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectSubnet()</code> in <code>internal/project/res_subnet.go</code></li> <li>Dependencies: VPC must exist</li> <li> <p>Configuration:</p> </li> <li> <p>CIDR block from project spec</p> </li> <li>Associated with project VPC</li> <li>Gateway IP (first IP in CIDR)</li> </ul>"},{"location":"project_resources/#6-networkattachmentdefinition","title":"6. NetworkAttachmentDefinition","text":"<ul> <li>Resource: <code>k8s.cni.cncf.io/v1.NetworkAttachmentDefinition</code></li> <li>Name: <code>default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectNad()</code> in <code>internal/project/res_nad.go</code></li> <li>Dependencies: Subnet must exist</li> <li>Configuration: Kube-OVN CNI configuration pointing to project subnet</li> </ul>"},{"location":"project_resources/#7-ovnsnatrule","title":"7. OvnSnatRule","text":"<ul> <li>Resource: <code>kubeovn.io/v1.OvnSnatRule</code></li> <li>Name: <code>{organization}-{project}</code> (e.g., <code>shalb-envoy</code>)</li> <li>Finalizer: <code>kubeovn.io/kube-ovn-controller</code></li> <li>Created by: <code>NewProjectSnat()</code> in <code>internal/project/res_snat.go</code></li> <li>Dependencies: OvnEip must exist and have IP assigned</li> <li> <p>Configuration:</p> </li> <li> <p>Links project subnet to external IP</p> </li> <li>Enables outbound internet access for pods</li> </ul>"},{"location":"project_resources/#8-secrets","title":"8. Secrets","text":""},{"location":"project_resources/#ssh-key-pair-secret","title":"SSH Key Pair Secret","text":"<ul> <li>Resource: <code>v1.Secret</code></li> <li>Name: <code>ssh-keypair-default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectKeyPairSeret()</code> in <code>internal/project/res_secret.go</code></li> <li>Content: Generated SSH public/private key pair</li> </ul>"},{"location":"project_resources/#authorized-keys-secret","title":"Authorized Keys Secret","text":"<ul> <li>Resource: <code>v1.Secret</code></li> <li>Name: <code>authorized-keys-default</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectAuthKeySecret()</code> in <code>internal/project/res_secret.go</code></li> <li>Content: SSH public keys for VM access</li> </ul>"},{"location":"project_resources/#9-rbac-resources","title":"9. RBAC Resources","text":""},{"location":"project_resources/#role","title":"Role","text":"<ul> <li>Resource: <code>rbac.authorization.k8s.io/v1.Role</code></li> <li>Name: <code>admin</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectRole()</code> in <code>internal/project/res_role.go</code></li> <li>Permissions: Full access to project resources (pods, services, VMs, etc.)</li> </ul>"},{"location":"project_resources/#rolebinding","title":"RoleBinding","text":"<ul> <li>Resource: <code>rbac.authorization.k8s.io/v1.RoleBinding</code></li> <li>Name: <code>org-admin</code></li> <li>Namespace: <code>{organization}-{project}</code></li> <li>Finalizer: <code>project.kube-dc.com/finalizer</code></li> <li>Created by: <code>NewProjectRoleBinding()</code> in <code>internal/project/res_role_binding.go</code></li> <li>Subject: <code>{organization}:org-admin</code> group</li> </ul>"},{"location":"project_resources/#10-vpcdns-service","title":"10. VpcDns Service","text":"<ul> <li>Resource: <code>v1.Service</code></li> <li>Name: <code>slr-vpc-dns-{organization}-{project}</code></li> <li>Namespace: <code>kube-system</code></li> <li>Finalizer: None (managed by service controller)</li> <li>Created by: <code>NewProjectVpcDns()</code> in <code>internal/project/res_vpc_dns.go</code></li> <li>Purpose: DNS resolution for VPC</li> </ul>"},{"location":"project_resources/#finalizers-summary","title":"Finalizers Summary","text":"Resource Type Finalizer Controller Project <code>project.kube-dc.com/finalizer</code> kube-dc-manager EIp <code>eip.kube-dc.com/finalizer</code> kube-dc-manager OvnEip <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller Vpc <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller Subnet <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller OvnSnatRule <code>kubeovn.io/kube-ovn-controller</code> kube-ovn-controller NetworkAttachmentDefinition <code>project.kube-dc.com/finalizer</code> kube-dc-manager Secrets <code>project.kube-dc.com/finalizer</code> kube-dc-manager Role <code>project.kube-dc.com/finalizer</code> kube-dc-manager RoleBinding <code>project.kube-dc.com/finalizer</code> kube-dc-manager"},{"location":"project_resources/#deletion-order-and-dependencies","title":"Deletion Order and Dependencies","text":"<p>When a Project is deleted, resources must be removed in reverse dependency order:</p>"},{"location":"project_resources/#phase-1-application-resources","title":"Phase 1: Application Resources","text":"<ol> <li>Pods, Services, VMs - User workloads (deleted by users/operators)</li> <li>FIp resources - Floating IPs (if any exist)</li> </ol>"},{"location":"project_resources/#phase-2-snat-and-networking","title":"Phase 2: SNAT and Networking","text":"<ol> <li>OvnSnatRule - Must be deleted before OvnEip</li> <li>OvnEip - Must be deleted before EIp and Subnet</li> </ol>"},{"location":"project_resources/#phase-3-project-infrastructure","title":"Phase 3: Project Infrastructure","text":"<ol> <li>EIp - External IP resource</li> <li>NetworkAttachmentDefinition - CNI configuration</li> <li>Secrets - SSH keys and authorized keys</li> <li>RBAC - Roles and RoleBindings</li> <li>Subnet - Must be deleted before VPC</li> <li>VPC - Virtual Private Cloud</li> <li>VpcDns - DNS service</li> <li>Namespace - Project namespace (last)</li> </ol>"},{"location":"project_resources/#common-deletion-issues","title":"Common Deletion Issues","text":""},{"location":"project_resources/#stuck-finalizers","title":"Stuck Finalizers","text":"<ul> <li>OvnEip: May get stuck if SNAT rule deletion fails</li> <li>Subnet: May get stuck if pods are still running</li> <li>EIp: May get stuck if project is deleted before EIp controller processes it</li> </ul>"},{"location":"project_resources/#manual-cleanup-commands","title":"Manual Cleanup Commands","text":"<pre><code># Remove stuck finalizers (use with caution)\nkubectl patch ovn-snat-rule {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch ovn-eip {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch subnet {name} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch eip {name} -n {namespace} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch project {name} -n {org-namespace} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\n</code></pre>"},{"location":"project_resources/#eip-ownership-patterns","title":"EIp Ownership Patterns","text":""},{"location":"project_resources/#ownership-states","title":"Ownership States","text":"<ul> <li>Released: <code>ownershipType: Released</code>, <code>owners: []</code> - No active users</li> <li>Shared: <code>ownershipType: Shared</code>, <code>owners: [...]</code> - Multiple users (SNAT + Services)</li> <li>Exclusive: <code>ownershipType: Exclusive</code>, <code>owners: [single]</code> - Single FIp owner</li> </ul>"},{"location":"project_resources/#owner-types","title":"Owner Types","text":"<ul> <li><code>Snat</code>: SNAT rule using the EIP for outbound traffic</li> <li><code>ServiceLb</code>: LoadBalancer service using the EIP</li> <li><code>FIp</code>: Floating IP using the EIP exclusively</li> </ul>"},{"location":"project_resources/#ownership-transitions","title":"Ownership Transitions","text":"<pre><code>Released \u2192 Shared (first owner added)\nShared \u2192 Released (last owner removed)\nReleased \u2192 Exclusive (FIp claims EIP)\nExclusive \u2192 Released (FIp releases EIP)\n</code></pre>"},{"location":"project_resources/#organization-limits","title":"Organization Limits","text":"<p>Organizations have a configurable limit on the number of ready projects they can contain (default: 3). This limit is enforced by the Project controller during reconciliation.</p> <ul> <li>Configuration: Set via <code>MasterConfig.OrganizationProjectsLimit</code></li> <li>Enforcement: Projects exceeding the limit will not be reconciled until space becomes available</li> <li>Status: Projects blocked by limits show as not ready but remain in the cluster</li> </ul>"},{"location":"project_resources/#enhanced-limit-enforcement-v0131-dev1","title":"Enhanced Limit Enforcement (v0.1.31-dev1+)","text":"<p>The Project controller now provides comprehensive feedback when organization limits are hit:</p> <p>Detailed Logging: - Organization project status with ready/pending counts and project names - Clear error messages with organization, namespace, and limit context - Debug-level logs showing available slots and project lists</p> <p>Status Conditions: Projects blocked by limits receive a <code>LimitCheck</code> condition: <pre><code>status:\n  ready: false\n  conditions:\n  - type: LimitCheck\n    status: \"False\"\n    reason: LimitExceeded\n    message: \"organization limit (3 projects) reached - ready projects: 3 (limit: 3)\"\n    lastTransitionTime: \"2025-01-19T16:45:00Z\"\n</code></pre></p> <p>Automatic Retry: - Projects are automatically requeued every 30 seconds - Reconciliation proceeds when limit space becomes available - No manual intervention required</p> <p>Projects can use different external network types: - cloud: Uses <code>ext-cloud</code> subnet (default) - public: Uses <code>ext-public</code> subnet (real public IPs)</p> <p>The <code>egressNetworkType</code> in Project spec determines which external subnet is used for the default gateway EIP.</p>"},{"location":"quickstart-hetzner/","title":"Master-Worker Setup on Hetzner Dedicated Servers","text":"<p>This guide provides step-by-step instructions for deploying a Kube-DC cluster with a master and worker node setup on Hetzner Dedicated Servers. This deployment leverages Hetzner's vSwitch and additional subnets to provide enterprise-grade networking capabilities for floating IPs and load balancers.</p>"},{"location":"quickstart-hetzner/#prerequisites","title":"Prerequisites","text":"<ol> <li>At least two Hetzner Dedicated Servers</li> <li>Access to Hetzner Robot interface</li> <li>A Hetzner vSwitch configured for your servers (see Hetzner vSwitch documentation)</li> <li>An additional subnet allocated through Hetzner Robot for external IPs and load balancers</li> <li>Wildcard domain ex: *.dev.kube-dc.com shoud be set to main public ip of master node.</li> </ol>"},{"location":"quickstart-hetzner/#server-configuration","title":"Server Configuration","text":""},{"location":"quickstart-hetzner/#1-prepare-servers","title":"1. Prepare Servers","text":"<p>Ensure your Hetzner Dedicated Servers meet these minimum requirements: - Master Node: 4+ CPU cores, 16+ GB RAM - Worker Node: 4+ CPU cores, 16+ GB RAM</p> <p>Install Ubuntu 24.04 LTS on all servers through the Hetzner Robot interface.</p>"},{"location":"quickstart-hetzner/#2-configure-vswitch","title":"2. Configure vSwitch","text":"<p>In the Hetzner Robot interface:</p> <ol> <li>Create a vSwitch if you don't have one already</li> <li>Add your servers to the vSwitch   </li> <li>Request an additional subnet to be used for external IPs (Floating IPs)</li> <li>Assign the subnet to your vSwitch:   </li> </ol> <p>You will get two vlan ids, one for the local network(in example 4012) and one for the external subnet with public ips(in example 4011).</p>"},{"location":"quickstart-hetzner/#network-configuration","title":"Network Configuration","text":""},{"location":"quickstart-hetzner/#1-configure-network-interfaces","title":"1. Configure Network Interfaces","text":"<p>SSH into each server and configure the networking using Netplan. Backup default netplan config: <pre><code>mkdir /root/tmp/\nmv /etc/netplan/*.yaml /root/tmp/\n</code></pre> Create new config(<code>/etc/netplan/60-kube-dc.yaml</code>) Replace values with <code>example</code> by values from default file(see it in <code>/root/tmp/</code>):</p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s31f6_example:  # Primary network interface name (get it from default netplan config)\n      addresses:\n        - 22.22.22.2_example/24  # Primary IP address and subnet mask (get it from default netplan config)\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 22.22.22.1_example  # Gateway IP address (get it from default netplan config)\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      routing-policy:\n        - from: 22.22.22.2_example  # Source-based routing for traffic from gateway (Primary IP)\n          table: 100  # Custom routing table ID\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    enp0s31f6.4012_example:  # VLAN interface name (format: interface.vlan_id, see your VLAN in https://robot.hetzner.com/vswitch/index)\n      id: 4012_example  # VLAN ID (must match your Hetzner vSwitch ID, same vlan_id)\n      link: enp0s31f6_example  # Parent interface for VLAN (same interface from default netplan config)\n      mtu: 1460  # Maximum Transmission Unit size\n      addresses:\n        - 192.168.100.2/22  # Master node IP on private network (This for master node setup)\n       #- 192.168.100.3/22  # Worker node IP                    (This for master node setup)\n</code></pre> <p>Apply the configuration:</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"quickstart-hetzner/#2-system-optimization","title":"2. System Optimization","text":"<p>Downgrade kernel (due to a bug in kernel https://github.com/k3s-io/k3s/issues/11175):</p> <pre><code>sudo apt -y update\nsudo apt install linux-image-6.8.0-52-generic linux-headers-6.8.0-52-generic\n# Remove previous kernel\nsudo apt-get remove --purge linux-image-6.8.0-58-generic linux-headers-6.8.0-58-generic\n# Reboot\nsudo reboot\n</code></pre> <p>On all nodes, update, upgrade, and install required software:</p> <pre><code>sudo apt -y install unzip iptables linux-headers-$(uname -r)\n</code></pre> <p>Update to the latest kernel version:</p> <pre><code>sudo apt -y install linux-generic\nsudo reboot\n</code></pre> <p>After the server reboots, verify your kernel version:</p> <pre><code>uname -r\n</code></pre> <p>Optimize system settings by adding to <code>/etc/sysctl.conf</code>:</p> <pre><code># Increase inotify limits\nfs.inotify.max_user_watches=1524288\nfs.inotify.max_user_instances=4024\n\n# Enable packet forwarding\nnet.ipv4.ip_forward = 1\n</code></pre> <p>Ensure the nf_conntrack module is loaded:</p> <pre><code># Check if the module is loaded\nlsmod | grep nf_conntrack\n\n# If not loaded, load it manually\nsudo modprobe nf_conntrack\n\n# To ensure it's loaded on boot, add it to /etc/modules\necho \"nf_conntrack\" | sudo tee -a /etc/modules\n</code></pre> <p>Apply the changes:</p> <pre><code>sudo sysctl -p\n</code></pre> <p>Disable systemd-resolved to prevent DNS conflicts:</p> <pre><code>sudo systemctl stop systemd-resolved\nsudo systemctl disable systemd-resolved\nsudo rm /etc/resolv.conf\necho \"nameserver 8.8.8.8\" | sudo tee /etc/resolv.conf\necho \"nameserver 8.8.4.4\" | sudo tee -a /etc/resolv.conf\n</code></pre> <p>Update the hosts file on each server with the private IPs:</p> <pre><code># On Master Node\necho \"192.168.100.2 kube-dc-master-1\" | sudo tee -a /etc/hosts\n# On Worker Node\necho \"192.168.100.3 kube-dc-worker-1\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"quickstart-hetzner/#kubernetes-installation","title":"Kubernetes Installation","text":""},{"location":"quickstart-hetzner/#1-install-clusterdev","title":"1. Install Cluster.dev","text":"<p>On the master node, install Cluster.dev:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\n</code></pre>"},{"location":"quickstart-hetzner/#2-configure-and-install-rke2-on-master-node","title":"2. Configure and Install RKE2 on Master Node","text":"<p>Install kubectl:</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Create RKE2 configuration (replace the external IP with your server's public IP):</p> <pre><code>sudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\nnode-name: kube-dc-master-1\ndisable-cloud-controller: true\ndisable: rke2-ingress-nginx\ncni: none\ncluster-cidr: \"10.100.0.0/16\"\nservice-cidr: \"10.101.0.0/16\"\ncluster-dns: \"10.101.0.11\"\nnode-label:\n  - kube-dc-manager=true\n  - kube-ovn/role=master\nkube-apiserver-arg: \n  - authentication-config=/etc/rancher/auth-conf.yaml\ndebug: true\nnode-external-ip: 22.22.22.2_example # Primary IP address (get it from default netplan config)\ntls-san:\n  - kube-api.yourdomain.com\n  - 192.168.100.2 # Master node IP on private network (This for master node setup)\nadvertise-address: 192.168.100.2 # Master node IP on private network (This for master node setup)\nnode-ip: 192.168.100.2 # Master node IP on private network (This for master node setup)\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/auth-conf.yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt: []\nEOF\nsudo chmod 666 /etc/rancher/auth-conf.yaml\n</code></pre> <p>Install RKE2 server:</p> <pre><code>export INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"server\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\n</code></pre> <p>You can check the installation logs here:</p> <pre><code>sudo journalctl -u rke2-server -f\n</code></pre> <p>Configure kubectl:</p> <pre><code>mkdir -p ~/.kube\nsudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config\nsudo chown $(id -u):$(id -g) ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre> <p>Verify the cluster status:</p> <pre><code>kubectl get nodes\n# If you see this output then you can proceed:\nNAME               STATUS     ROLES\nkube-dc-master-1   NotReady   control-plane,etcd,master\n</code></pre>"},{"location":"quickstart-hetzner/#4-join-worker-node-to-the-cluster","title":"4. Join Worker Node to the Cluster","text":"<p>Get the join token from the master node:</p> <pre><code># on master node\nsudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre> <p>On the worker node, create the RKE2 configuration (replace TOKEN with the token from the master node):</p> <pre><code># on worker node\nsudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\ntoken: &lt;TOKEN&gt;\nserver: https://192.168.100.2:9345 # Master node local IP\nnode-name: kube-dc-worker-1\nnode-ip: 192.168.100.3\nEOF\n</code></pre> <p>Install RKE2 agent:</p> <pre><code># on worker node\nexport INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"agent\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-agent.service\nsudo systemctl start rke2-agent.service\n</code></pre> <p>Monitor the agent service:</p> <pre><code># on worker node\nsudo journalctl -u rke2-agent -f\n</code></pre> <p>Verify on the master node that the worker joined successfully:</p> <pre><code># on master node\nkubectl get nodes\n</code></pre>"},{"location":"quickstart-hetzner/#install-kube-dc-components-on-master-node","title":"Install Kube-DC Components on Master Node","text":""},{"location":"quickstart-hetzner/#1-create-clusterdev-project-configuration","title":"1. Create Cluster.dev Project Configuration","text":"<p>On the master node, create a project configuration file:</p> <pre><code>mkdir -p ~/kube-dc-hetzner\ncat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/project.yaml\nkind: Project\nname: kube-dc-hetzner\nbackend: \"default\"\nvariables:\n  kubeconfig: ~/.kube/config\n  debug: true\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#2-create-clusterdev-stack-configuration","title":"2. Create Cluster.dev Stack Configuration","text":"<p>Create the stack configuration file(replace <code>example</code> by appropriate values):</p> <pre><code>cat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/stack.yaml\nname: cluster\ntemplate: https://github.com/kube-dc/kube-dc-public//installer/kube-dc/templates/kube-dc?ref=main\nkind: Stack\nbackend: default\nvariables:\n  debug: \"true\"\n  kubeconfig: /root/.kube/config # Change for your username path to RKE kubeconfig\n\n  cluster_config:\n    pod_cidr: \"10.100.0.0/16\"\n    svc_cidr: \"10.101.0.0/16\"\n    join_cidr: \"100.64.0.0/16\"\n    cluster_dns: \"10.101.0.11\"\n    default_external_network:\n      nodes_list: # list of nodes, where 4011 vlan (external network) is accessible\n        - kube-dc-master-1\n        - kube-dc-worker-1\n      name: external4011_example # VLAN interface for this name you can find here https://robot.hetzner.com/vswitch/index\n      vlan_id: \"4011_example\" # VLAN interface id, see your VLAN in https://robot.hetzner.com/vswitch/index\n      interface: \"enp0s31f6_example\" # Parent interface for VLAN (same interface from default netplan config)\n      cidr: \"33.33.33.33_example/29\" # External subnet provided by Hetzner (should see during VLAN creation here https://robot.hetzner.com/vswitch/index)\n      gateway: 33.33.33.34_example # Gateway for external subnet (should see during VLAN creation here https://robot.hetzner.com/vswitch/index)\n      mtu: \"1400\"\n\n  node_external_ip: 22.22.22.2_example # Primary IP address (get it from default netplan config). Wildcard *.dev.kube-dc.com shoud be faced on this ip\n\n\n  email: \"noreply@example.com\"\n  domain: \"dev.example-kube-dc.com\"\n  install_terraform: true\n\n  create_default:\n    organization:\n      name: example\n      description: \"My test org my-org 1\"\n      email: \"example@example.com\"\n    project:\n      name: demo\n      cidr_block: \"10.1.0.0/16\"\n\n  monitoring:\n    prom_storage: 20Gi\n    retention_size: 17GiB\n    retention: 365d\n\n  versions:\n    kube_dc: \"v0.1.21\" # release version\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#3-deploy-kube-dc","title":"3. Deploy Kube-DC","text":"<p>Run Cluster.dev to deploy Kube-DC components:</p> <pre><code>cd ~/kube-dc-hetzner\ncdev apply\n</code></pre> <p>This process will take 15-20 minutes to complete. You can monitor the deployment progress in the terminal output.</p>"},{"location":"quickstart-hetzner/#4-verify-installation","title":"4. Verify Installation","text":"<p>After successful deployment, you will receive console and login credentials for deployment admin user. Also if you have created some default organization youll get organization admin credentials. Example:</p> <pre><code>keycloak_user = admin\norganization_admin_username = admin\norganization_name = example\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n example -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n example -o jsonpath='{.data.url}' | base64 -d\nconsole_url = https://console.dev.kube-dc.com\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#post-installation-steps","title":"Post-Installation Steps","text":""},{"location":"quickstart-hetzner/#1-access-kube-dc-ui-using-default-organization-credentials","title":"1. Access Kube-DC UI using default organization credentials","text":"<p>After the installation completes, the Kube-DC UI should be accessible at <code>https://console.yourdomain.com</code>. In cdev output there are output for default organization, project and admin user for default organization(use <code>retrieve_organization_password</code> to login):</p> <pre><code>console_url = https://console.dev.kube-dc.com\norganization_admin_username = admin\norganization_name = example\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n example -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n example -o jsonpath='{.data.url}' | base64 -d\n</code></pre>"},{"location":"quickstart-hetzner/#2-keep-credentials-for-keycloak-master-admin-user","title":"2. Keep credentials for Keycloak master admin user","text":"<p>You can save global Keycloak credentials if you need to manage Keycloak as super-admin.</p> <p>Master admin user credentials:</p> <pre><code>keycloak_user = admin\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during the installation:</p> <ol> <li> <p>Check the RKE2 server/agent logs:    <pre><code>sudo journalctl -u rke2-server -f  # On master\nsudo journalctl -u rke2-agent -f   # On worker\n</code></pre></p> </li> <li> <p>Check the Kube-OVN logs:    <pre><code>kubectl logs -n kube-system -l app=kube-ovn-controller\n</code></pre></p> </li> <li> <p>Verify network connectivity between nodes on the private network:    <pre><code>ping 192.168.100.2  # From worker node\nping 192.168.100.3  # From master node\n</code></pre></p> </li> </ol> <p>For additional help, consult the Kube-DC community support resources.</p>"},{"location":"quickstart-overview/","title":"Kube-DC Installation Overview","text":"<p>This document provides a technical overview of the Kube-DC installation process, with detailed explanations of key configuration files and their parameters.</p>"},{"location":"quickstart-overview/#installation-methods","title":"Installation Methods","text":"<p>Kube-DC can be installed in several ways:</p> <ul> <li>Master-Worker deployment: Recommended starting point for new deployments</li> <li>Multi-node HA cluster: For production environments</li> </ul> <p>Start with actual tested deployment on Hetzner Bare Metal Servers.</p>"},{"location":"quickstart-overview/#prerequisites","title":"Prerequisites","text":"<p>Before installing Kube-DC, ensure your system meets the following requirements:</p> <ul> <li>Hardware: Minimum 4 CPU cores, 8GB RAM per node</li> <li>Operating System: Ubuntu 20.04 LTS or newer (24.04 LTS recommended)</li> <li>Network: Dedicated network interface for VM traffic with VLAN support</li> <li>Storage: Local or network storage with support for dynamic provisioning</li> <li>Kubernetes: Version 1.31+ if installing on existing cluster</li> </ul>"},{"location":"quickstart-overview/#network-configuration","title":"Network Configuration","text":"<p>Kube-DC requires proper network configuration for optimal performance. The key requirement is that your external network must be routed through a VLAN to enable advanced networking features.</p>"},{"location":"quickstart-overview/#external-network-requirements","title":"External Network Requirements","text":"<p>Kube-DC networking is built on top of Kube-OVN and requires the following network configuration:</p> <ul> <li>VLAN-capable network interface: A dedicated network interface with VLAN support</li> <li>External subnet with routing: An external subnet that's properly routed to your infrastructure</li> <li>Static IP configuration: Static IP addressing (no DHCP) to ensure network stability</li> </ul> <p>This configuration allows Kube-DC to implement:</p> <ul> <li>Floating IP allocation: Dynamically assign public IPs to workloads</li> <li>Load balancer with external IPs: Distribute traffic to services with public visibility</li> <li>Default gateway per project: Isolate network traffic between projects</li> </ul> <p>All of these features work as a wrapper on top of Kube-OVN, providing enterprise-grade networking capabilities for your infrastructure.</p>"},{"location":"quickstart-overview/#example-network-configuration","title":"Example Network Configuration","text":"<p>Below is an example Netplan configuration with detailed comments for a VLAN-enabled network:</p> <pre><code>network:\n  version: 2  # Netplan version\n  renderer: networkd  # Network renderer to use\n  ethernets:\n    eth0:  # Primary network interface name (check your actual interface name)\n      addresses:\n        - 192.168.1.2/24  # Primary IP address and subnet mask\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 192.168.1.1  # Gateway IP address\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    eth0.100:  # VLAN interface (format: interface.vlan_id)\n      id: 100  # VLAN ID\n      link: eth0  # Parent interface for VLAN \n      mtu: 1500  # Recommended MTU for your network\n      addresses:\n        - 10.100.0.2/24  # Private IP on the VLAN network\n</code></pre> <p>Important</p> <p>Do not use DHCP for the VLAN interface as it would break the initial Kube-OVN setup. Always use static IP configuration.</p>"},{"location":"quickstart-overview/#networking-components","title":"Networking Components","text":"<p>The Kube-DC network setup consists of several key components that work together:</p> <ol> <li>Kube-OVN: Core CNI providing overlay and underlay networking</li> <li>Multus CNI: Enables multiple network interfaces for pods</li> <li>VLAN Integration: Connects Kubernetes networking to physical infrastructure</li> </ol>"},{"location":"quickstart-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC installer deploys the following core components:</p> <ol> <li>Kube-OVN: Advanced networking solution that provides overlay and underlay networking</li> <li>Multus CNI: CNI that enables attaching multiple network interfaces to pods</li> <li>KubeVirt: Virtualization layer for running VMs on Kubernetes</li> <li>Keycloak: Identity and access management solution</li> <li>Cert-Manager: Certificate management for TLS</li> <li>Ingress-NGINX: Ingress controller for external access</li> <li>Prometheus &amp; Loki: Monitoring and logging stack</li> <li>Kube-DC Core: The core management components for Kube-DC</li> </ol>"},{"location":"quickstart-overview/#installation-process-overview","title":"Installation Process Overview","text":"<p>The installation process follows these high-level steps:</p> <ol> <li>System Preparation: Configure network, optimize system settings, and install prerequisites</li> <li>Kubernetes Installation: Install RKE2 on master and worker nodes</li> <li>Kube-DC Installation: Use cluster.dev to deploy Kube-DC components</li> <li>Post-Installation Setup: Configure authentication, networking, and initial organization</li> </ol> <p>For detailed step-by-step instructions, refer to: - Master-Worker Setup (Dedicated Servers)</p>"},{"location":"todo/","title":"Todo List","text":"<ul> <li> Implement default NetworkPolicy creation in the Project controller. The controller should create a 'default-deny-all' NetworkPolicy in the project namespace upon project creation. This requires:</li> <li>Adding a <code>res_network_policy.go</code> file in <code>internal/project</code>.</li> <li>Updating <code>internal/project/project.go</code> to call the new function.</li> <li>Rebuilding and deploying the controller image (<code>make docker-build deploy</code>).</li> <li> <p>Re-enabling the e2e test in <code>tests/e2e/project_test.go</code>.</p> </li> <li> <p> Fix Project Controller Deletion Deadlock</p> </li> <li>Issue: The <code>Project</code> controller gets stuck in a deadlock when deleting a <code>Project</code> that has an empty <code>spec.egressNetworkType</code>. The deletion logic in <code>internal/project/res_vpc.go</code> incorrectly attempts to re-generate the <code>kube-ovn</code> VPC if it's not found, which fails because <code>utils.SelectBestExternalSubnet</code> requires an <code>egressNetworkType</code>.</li> <li>Fix: In <code>internal/project/res_vpc.go</code>, inside the <code>NewProjectVpc</code> function, modify the <code>IsNotFound</code> error handling. Before attempting to regenerate the VPC, add a check to see if the project has a <code>DeletionTimestamp</code>. If it does, the function should return the <code>NotFound</code> error directly, as this is an expected condition during cleanup, and regeneration should not occur.</li> </ul>"},{"location":"tutorial-ip-and-lb/","title":"Managing IPs and Load Balancers","text":"<p>This tutorial walks you through managing External IPs (EIPs), Floating IPs (FIPs), Load Balancers, and deploying namespace-scoped Ingress controllers in Kube-DC using kubectl with YAML manifests.</p>"},{"location":"tutorial-ip-and-lb/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Access to a Kube-DC cluster</li> <li>The <code>kubectl</code> command-line tool installed</li> <li>Helm installed (for Ingress controller deployment)</li> <li>A project with the necessary permissions to create network resources</li> </ul>"},{"location":"tutorial-ip-and-lb/#understanding-network-resources-in-kube-dc","title":"Understanding Network Resources in Kube-DC","text":"<p>Kube-DC's networking is built on Kube-OVN and includes several key components:</p> <ol> <li>External IP (EIP): Public IP addresses that provide connectivity from the internet to resources within Kube-DC</li> <li>Floating IP (FIP): Maps an internal IP address (of a VM or pod) to an External IP</li> <li>Service LoadBalancer: Creates and maps an EIP to a service that routes traffic to pods or VMs</li> <li>Ingress Controller: Provides HTTP/HTTPS routing to services within a namespace</li> </ol>"},{"location":"tutorial-ip-and-lb/#managing-external-ips-eips","title":"Managing External IPs (EIPs)","text":"<p>Each project in Kube-DC automatically receives a default EIP that acts as a NAT gateway for outbound traffic. You can also create additional EIPs for specific services.</p>"},{"location":"tutorial-ip-and-lb/#eip-allocation-algorithm","title":"EIP Allocation Algorithm","text":"<p>When an EIP is created, Kube-DC follows a specific algorithm to allocate the external subnet:</p> <ol> <li>Check Default Subnet Compatibility</li> <li>The system first checks if the EIP's required external network type matches the default external subnet type</li> <li>If they match, it looks for a free OEIP in that subnet</li> <li>If a free OEIP is found, it's connected to the EIP</li> <li> <p>If no free OEIP exists, a new one is created</p> </li> <li> <p>Check Connected Subnets</p> </li> <li>If the required network type is different from the default, the system takes a list of external subnets already connected to the project's VPC</li> <li>It retrieves all free OEIPs from these connected subnets</li> <li> <p>If at least one free OEIP is found, it's connected to the EIP</p> </li> <li> <p>Select Best Available Subnet</p> </li> <li>If no connected subnets have free IPs, the system takes a complete list of available external networks</li> <li>Networks are sorted by the number of available IPs (descending order)</li> <li> <p>The network with the most free addresses is selected</p> </li> <li> <p>Connect New Subnet to VPC</p> </li> <li>The selected subnet is connected to the project's VPC</li> <li>The system waits for the OEIP resource to be created</li> <li> <p>Once created, the OEIP is connected to the EIP</p> </li> <li> <p>Error Handling</p> </li> <li>If no networks with free IPs are available, the operation fails with an error</li> </ol> <p>This algorithm ensures optimal IP address utilization while providing the flexibility to support different external network types.</p>"},{"location":"tutorial-ip-and-lb/#creating-an-eip-using-kubectl","title":"Creating an EIP Using kubectl","text":"<p>For automation or GitOps workflows, you can create EIPs using kubectl and YAML manifests.</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: web-server-eip\n  namespace: shalb-demo\nspec: {}\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f eip.yaml\n</code></pre> <p>Check the status:</p> <pre><code>kubectl get eip -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#managing-floating-ips-fips","title":"Managing Floating IPs (FIPs)","text":"<p>Floating IPs map an internal IP address (of a VM or pod) to an External IP, enabling direct access to specific resources.</p>"},{"location":"tutorial-ip-and-lb/#creating-a-fip-using-kubectl","title":"Creating a FIP Using kubectl","text":"<p>Create a FIP manifest:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: database-vm-fip\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171  # Internal IP of your VM or pod\n  eip: web-server-eip     # Name of an existing EIP\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f fip.yaml\n</code></pre> <p>Check the status:</p> <pre><code>kubectl get fip -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#configuring-load-balancers","title":"Configuring Load Balancers","text":"<p>Load Balancers in Kube-DC are implemented as Kubernetes Services of type LoadBalancer, with specific annotations to control their behavior.</p>"},{"location":"tutorial-ip-and-lb/#creating-a-load-balancer-for-pods-using-kubectl","title":"Creating a Load Balancer for Pods Using kubectl","text":"<p>Create a Service manifest:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"  # Use project's default EIP\n    # service.nlb.kube-dc.com/bind-on-eip: \"web-server-eip\"  # Or use a dedicated EIP\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f service-lb.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#creating-a-load-balancer-for-vm-ssh-access","title":"Creating a Load Balancer for VM SSH Access","text":"<p>You can also create a Load Balancer to expose SSH access to a virtual machine:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: debian  # Target VM name\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222  # External port\n      targetPort: 22  # Internal port (SSH)\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f vm-ssh-lb.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#automatic-external-endpoints","title":"Automatic External Endpoints","text":"<p>Kube-DC automatically creates external endpoints for every LoadBalancer service, providing stable DNS-based access across VPC boundaries.</p> <p>When a LoadBalancer service gets an external IP, the controller creates: - External Service: <code>&lt;service-name&gt;-ext</code> (headless service) - Endpoints: Points to the LoadBalancer's external IP</p> <p>This enables cross-VPC communication using stable DNS names like <code>etcd-lb-ext.shalb-demo.svc.cluster.local:2379</code> instead of hardcoded IP addresses.</p> <p>Verify external endpoints:</p> <pre><code># List all external endpoints\nkubectl get endpoints -A --selector=kube-dc.com/managed-by=service-lb-controller\n\n# Check specific service\nkubectl get svc,endpoints -n shalb-demo nginx-service-lb-ext\n</code></pre>"},{"location":"tutorial-ip-and-lb/#managing-network-resources-with-kubectl","title":"Managing Network Resources with kubectl","text":"<p>Check the status of network resources:</p> <pre><code># List all External IPs\nkubectl get eip -n shalb-demo\n\n# List all Floating IPs\nkubectl get fip -n shalb-demo\n\n# Get details about a specific LoadBalancer service\nkubectl describe service nginx-service-lb -n shalb-demo\n\n# Check if your LoadBalancer has an external IP assigned\nkubectl get service -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#deploying-namespace-scoped-ingress-controllers","title":"Deploying Namespace-Scoped Ingress Controllers","text":"<p>For more advanced HTTP/HTTPS routing capabilities, you can deploy an ingress-nginx controller scoped to your specific namespace. This allows you to have complete control over the Ingress resources in your project.</p>"},{"location":"tutorial-ip-and-lb/#understanding-namespace-scoped-ingress","title":"Understanding Namespace-Scoped Ingress","text":"<p>A namespace-scoped ingress controller: - Only watches for Ingress resources in the specified namespace - Doesn't interfere with other controllers in the cluster - Uses your project's networking resources (like the default EIP) - Provides advanced routing, SSL termination, and load balancing</p>"},{"location":"tutorial-ip-and-lb/#deploying-ingress-nginx-controller-with-helm","title":"Deploying ingress-nginx Controller with Helm","text":""},{"location":"tutorial-ip-and-lb/#step-1-create-a-valuesyaml-file-for-your-configuration","title":"Step 1: Create a values.yaml file for your configuration","text":"<pre><code>controller:\n  ingressClassResource:\n    enabled: false  # Disables the default IngressClass creation\n  ingressClass: \"\"  # No default IngressClass\n  scope:\n    enabled: true  # Enables namespace-scoped mode\n    namespace: shalb-demo  # Restricts the controller to this namespace\n  watchIngressWithoutClass: false\n  admissionWebhooks:\n    enabled: false\n  service:\n    annotations:\n      service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nrbac:\n  create: true\n  scope: true\n\ndefaultBackend:\n  enabled: false  # Disables the default backend\n</code></pre> <p>Save this as <code>ingress-values.yaml</code>.</p>"},{"location":"tutorial-ip-and-lb/#step-2-add-the-ingress-nginx-helm-repository-if-not-already-added","title":"Step 2: Add the ingress-nginx Helm repository (if not already added)","text":"<pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-3-install-the-ingress-nginx-controller","title":"Step 3: Install the ingress-nginx controller","text":"<pre><code>helm upgrade --install ingress ingress-nginx/ingress-nginx \\\n  --namespace shalb-demo \\\n  --values ingress-values.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-4-verify-the-installation","title":"Step 4: Verify the installation","text":"<pre><code>kubectl get pods -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\nkubectl get svc -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\n</code></pre> <p>The controller pod should be running, and the service should have an external IP assigned.</p>"},{"location":"tutorial-ip-and-lb/#creating-an-ingress-resource","title":"Creating an Ingress Resource","text":"<p>Once your ingress controller is running, you can create Ingress resources to route traffic to your services:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: shalb-demo\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: example.kube-dc.com  # Replace with your domain\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f example-ingress.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#configuring-ssltls-with-cert-manager","title":"Configuring SSL/TLS with cert-manager","text":"<p>For secure HTTPS connections, you can deploy cert-manager to automatically obtain and manage certificates:</p>"},{"location":"tutorial-ip-and-lb/#step-1-install-cert-manager-in-your-namespace","title":"Step 1: Install cert-manager in your namespace","text":"<pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm install cert-manager jetstack/cert-manager \\\n  --namespace shalb-demo \\\n  --set installCRDs=true \\\n  --set namespace=shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-2-create-an-issuer-or-clusterissuer","title":"Step 2: Create an Issuer or ClusterIssuer","text":"<p>For Let's Encrypt:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: shalb-demo\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: your-email@example.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f issuer.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-3-update-your-ingress-with-tls-configuration","title":"Step 3: Update your Ingress with TLS configuration","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: shalb-demo\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    cert-manager.io/issuer: \"letsencrypt-prod\"\nspec:\n  tls:\n  - hosts:\n    - example.kube-dc.com\n    secretName: example-tls\n  rules:\n  - host: example.kube-dc.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f example-ingress-tls.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#putting-it-all-together-exposing-a-web-application","title":"Putting It All Together: Exposing a Web Application","text":"<p>Let's walk through a complete example of deploying a web application and exposing it to the internet:</p>"},{"location":"tutorial-ip-and-lb/#step-1-deploy-an-nginx-pod","title":"Step 1: Deploy an Nginx Pod","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: shalb-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-2-create-a-service-for-the-deployment","title":"Step 2: Create a Service for the Deployment","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: shalb-demo\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f nginx-service.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-3-deploy-a-namespace-scoped-ingress-controller","title":"Step 3: Deploy a Namespace-Scoped Ingress Controller","text":"<p>Follow the steps in the \"Deploying Namespace-Scoped Ingress Controllers\" section to deploy your ingress controller.</p>"},{"location":"tutorial-ip-and-lb/#step-4-create-an-ingress-resource","title":"Step 4: Create an Ingress Resource","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-ingress\n  namespace: shalb-demo\nspec:\n  rules:\n  - host: nginx.example.com  # Replace with your domain\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f nginx-ingress.yaml\n</code></pre>"},{"location":"tutorial-ip-and-lb/#step-5-verify-access","title":"Step 5: Verify Access","text":"<p>Get the external IP of your ingress controller's service:</p> <pre><code>kubectl get svc -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\n</code></pre> <p>Configure your DNS to point your domain (e.g., nginx.example.com) to this IP address. Once DNS propagates, you should be able to access your Nginx service using the domain.</p>"},{"location":"tutorial-ip-and-lb/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Naming: Use descriptive names for your network resources</li> <li>EIP Conservation: When possible, use the project's default EIP with annotations rather than creating dedicated EIPs</li> <li>Security: Limit exposed ports to only what's necessary and use HTTPS with valid certificates</li> <li>Monitoring: Regularly check the status of your network resources</li> <li>Documentation: Document which services are exposed on which domains/paths</li> </ol>"},{"location":"tutorial-ip-and-lb/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-ip-and-lb/#common-issues","title":"Common Issues","text":"<ol> <li>EIP Not Allocated: EIP creation may take a few moments. Check status with <code>kubectl get eip</code></li> <li>LoadBalancer Pending: External IP allocation may take time. Check with <code>kubectl describe service</code></li> <li>Cannot Connect to Service: Verify that the service's selector matches your pod labels</li> <li>Ingress Not Routing Traffic: Check ingress controller logs and ingress resource status</li> </ol>"},{"location":"tutorial-ip-and-lb/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Check EIP status and details\nkubectl describe eip web-server-eip -n shalb-demo\n\n# Check FIP status and details\nkubectl describe fip database-vm-fip -n shalb-demo\n\n# Check LoadBalancer service events\nkubectl describe service nginx-service-lb -n shalb-demo\n\n# Check if pods are selected by the service\nkubectl get pods -l app=nginx -n shalb-demo\n\n# Check ingress controller logs\nkubectl logs -n shalb-demo -l app.kubernetes.io/name=ingress-nginx\n\n# Check ingress status\nkubectl describe ingress nginx-ingress -n shalb-demo\n</code></pre>"},{"location":"tutorial-ip-and-lb/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to manage External IPs (EIPs), Floating IPs (FIPs), LoadBalancer services, and namespace-scoped Ingress controllers in Kube-DC. These networking resources provide flexible options for exposing your applications and VMs to external traffic, with Ingress controllers offering advanced HTTP/HTTPS routing capabilities for your web applications.</p>"},{"location":"tutorial-kubeconfig/","title":"Obtaining and Using Kubeconfig in Your Local Console","text":"<p>This guide explains how to obtain and configure a kubeconfig file for the kube-dc platform to use in your local development environment.</p>"},{"location":"tutorial-kubeconfig/#overview","title":"Overview","text":"<p>The kubeconfig file is essential for authenticating with the Kubernetes API server. In kube-dc, authentication is handled through Keycloak, which provides secure token-based access.</p> <p>This tutorial covers: - Setting up the authentication script - Generating a kubeconfig file - Using kubeconfig with kubectl - Troubleshooting common issues</p>"},{"location":"tutorial-kubeconfig/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Access to a kube-dc organization and project</li> <li>Your Keycloak username and password</li> <li><code>kubectl</code> installed on your local machine</li> <li><code>curl</code> and <code>jq</code> utilities installed (required for token operations)</li> </ul>"},{"location":"tutorial-kubeconfig/#using-the-authentication-helper-script","title":"Using the Authentication Helper Script","text":"<p>kube-dc provides a helper script that simplifies the kubeconfig generation process.</p>"},{"location":"tutorial-kubeconfig/#step-1-download-and-run-the-authentication-script","title":"Step 1: Download and Run the Authentication Script","text":"<p>You can download the authentication script directly from the public repository:</p> <pre><code># Create a directory for the script\nmkdir -p ~/.kube-dc/bin\n\n# Download the script\ncurl -o ~/.kube-dc/bin/kdc_get_kubeconfig.sh https://raw.githubusercontent.com/kube-dc/kube-dc-public/main/hack/auth/kdc_get_kubeconfig.sh\n\n# Make it executable\nchmod +x ~/.kube-dc/bin/kdc_get_kubeconfig.sh\n\n# Run the authentication script with your organization and project name\n~/.kube-dc/bin/kdc_get_kubeconfig.sh your-org/your-project\n</code></pre> <p>Alternatively, if you have the entire repository cloned:</p> <pre><code># If you have the repository already cloned\ncd kube-dc\n./hack/auth/kdc_get_kubeconfig.sh your-org/your-project\n</code></pre> <p>The script will prompt you for the following information: - Keycloak endpoint URL (e.g., <code>https://login.dev.kube-dc.com</code>) - Organization name (your Keycloak realm) - Kubernetes API server URL (e.g., <code>https://kube-api.dev.kube-dc.com:6443</code>) - Cluster name (usually <code>kube-dc</code>) - User name (your Keycloak username) - Context name (usually <code>kube-dc</code>) - CA certificate (you can provide this as a file, paste it directly, or skip for insecure mode)</p>"},{"location":"tutorial-kubeconfig/#step-2-activate-the-generated-configuration","title":"Step 2: Activate the Generated Configuration","text":"<p>After the script completes, activate the configuration:</p> <pre><code>source ~/.kube-dc/your-org-your-project/activate.sh\n</code></pre> <p>This will:</p> <ol> <li>Set the <code>KUBECONFIG</code> environment variable to point to your new configuration</li> <li>Source the environment variables from the <code>.env</code> file</li> <li>Add the <code>kn</code> alias for namespace switching</li> <li>Display instructions for using kubectl</li> </ol> <p>You can now use <code>kubectl</code> commands as usual:</p> <pre><code>kubectl get pods\n</code></pre> <p>On first use, you'll be prompted to enter your Keycloak username and password. The script will obtain tokens and cache them for subsequent commands.</p>"},{"location":"tutorial-kubeconfig/#step-3-test-your-connection","title":"Step 3: Test Your Connection","text":"<p>Test that your kubeconfig works correctly:</p> <pre><code>kubectl get pods\n</code></pre> <p>On first use, you'll be prompted to enter your Keycloak username and password. The script will obtain tokens and cache them for subsequent commands.</p>"},{"location":"tutorial-kubeconfig/#using-the-namespace-switcher","title":"Using the Namespace Switcher","text":"<p>The <code>kn</code> tool is installed automatically during setup and is configured as an alias when you activate the environment. It allows you to easily view and switch between namespaces that your token has permissions to access.</p>"},{"location":"tutorial-kubeconfig/#features","title":"Features","text":"<ul> <li>Intelligent Operation: When used with a kube-dc context, it reads namespace permissions directly from your JWT token</li> <li>Interactive Selection: Run <code>kn</code> without arguments to see a list of available namespaces</li> <li>Direct Selection: Specify a namespace with <code>kn my-namespace</code></li> <li>Fallback Mode: If not in a kube-dc context, falls back to <code>kubens</code> or basic kubectl namespace commands</li> </ul>"},{"location":"tutorial-kubeconfig/#examples","title":"Examples","text":"<pre><code># List available namespaces and select interactively\nkn\n\n# Switch directly to a specific namespace\nkn shalb-demo\n</code></pre>"},{"location":"tutorial-kubeconfig/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-kubeconfig/#authentication-issues","title":"Authentication Issues","text":"<p>If you're experiencing authentication problems:</p> <ol> <li> <p>Token expiration: The refresh token may have expired. Delete the <code>.refresh_token</code> file in your kubeconfig directory and try again:    <pre><code>rm ~/.kube-dc/*-*/scripts/.refresh_token\n</code></pre></p> </li> <li> <p>Invalid credentials: Ensure you're using the correct username and password for your Keycloak account.</p> </li> <li> <p>Connection issues: Verify your network can reach the Keycloak and API servers.</p> </li> </ol>"},{"location":"tutorial-kubeconfig/#permission-issues","title":"Permission Issues","text":"<p>If you can authenticate but receive permission errors:</p> <ol> <li> <p>Namespace access: Ensure you're using the correct namespace in your context. Your namespace should be in the format <code>organization-project</code>.</p> </li> <li> <p>Role assignment: Contact your organization administrator to verify you have the appropriate roles assigned in Keycloak.</p> </li> <li> <p>Resource-specific permissions: Check that your role has permissions for the specific resources you're trying to access.</p> </li> </ol>"},{"location":"tutorial-kubeconfig/#security-considerations","title":"Security Considerations","text":"<ul> <li>Keep your kubeconfig file secure (600 permissions)</li> <li>Never share your refresh or access tokens</li> <li>Be cautious when using <code>insecure-skip-tls-verify: true</code> in production environments</li> <li>If your credentials may be compromised, contact your administrator to revoke your tokens</li> </ul>"},{"location":"tutorial-kubeconfig/#next-steps","title":"Next Steps","text":"<ul> <li>User and Group Management: Learn about role-based access control</li> <li>Tutorial: Virtual Machines: Deploy your first VM</li> <li>Examples: Explore example manifests for various resources</li> </ul>"},{"location":"tutorial-networking-external/","title":"Additional External Network Configuration","text":"<p>This guide explains how to add additional external networks to Kube-DC alongside the default cloud network.</p>"},{"location":"tutorial-networking-external/#overview","title":"Overview","text":"<p>The configuration demonstrates how to add a second external network (public) to an existing Kube-DC setup that already has a cloud external network, using multiple VLANs on a single physical interface per node.</p>"},{"location":"tutorial-networking-external/#network-types-explained-by-example","title":"Network Types Explained by Example","text":""},{"location":"tutorial-networking-external/#cloud-network-egressnetworktype-cloud","title":"Cloud Network (<code>egressNetworkType: cloud</code>)","text":"<ul> <li>Purpose: Default external network for most workloads</li> <li>Subnet: <code>ext-cloud</code> (100.65.0.0/16) on VLAN 4013</li> <li>Use Cases: </li> <li>General internet access for applications</li> <li>Standard egress traffic from project workloads</li> <li>Cost-effective external connectivity</li> <li>IP Pool: Large address space (65,000+ IPs available)</li> </ul>"},{"location":"tutorial-networking-external/#public-network-egressnetworktype-public","title":"Public Network (<code>egressNetworkType: public</code>)","text":"<ul> <li>Purpose: Premium external network for specialized workloads</li> <li>Subnet: <code>ext-public</code> (168.119.17.48/28) on VLAN 4011</li> <li> <p>Use Cases:</p> </li> <li> <p>Production services requiring dedicated public IPs</p> </li> <li>Load balancers and ingress controllers</li> <li>Services needing specific public IP ranges or routing</li> <li>IP Pool: Limited address space with real ipv4 addresses (16 IPs total)</li> </ul>"},{"location":"tutorial-networking-external/#architecture","title":"Architecture","text":"<pre><code>Physical Interface (enp0s31f6)\n\u251c\u2500\u2500 VLAN 4013 (Cloud Network) - 100.65.0.0/16 (ext-cloud)\n\u2514\u2500\u2500 VLAN 4011 (Public Network) - 168.119.17.48/28 (ext-public)\n</code></pre>"},{"location":"tutorial-networking-external/#example-cluster-usage","title":"Example Cluster Usage","text":"<ul> <li>shalb-demo project: Uses <code>egressNetworkType: cloud</code> \u2192 EIP: 100.65.0.102 (development/testing)</li> <li>shalb-dev project: Uses <code>egressNetworkType: public</code> \u2192 EIP: 168.119.17.51 (development with public access)</li> <li>shalb-envoy project: Uses <code>egressNetworkType: public</code> \u2192 EIPs: 168.119.17.52, 168.119.17.54 (production load balancer)</li> </ul>"},{"location":"tutorial-networking-external/#choosing-the-right-network","title":"Choosing the Right Network","text":"<p>Use Cloud Network when: - Need basic internet connectivity - Don't require specific public IP ranges</p> <p>Use Public Network when: - Need dedicated public IP addresses - Have specific routing or compliance requirements - Running load balancers or ingress controllers</p>"},{"location":"tutorial-networking-external/#ovsovn-modifications-applied","title":"OVS/OVN Modifications Applied","text":""},{"location":"tutorial-networking-external/#1-ovs-bridge-configuration","title":"1. OVS Bridge Configuration","text":"<p>The system automatically creates the necessary OVS infrastructure:</p> <p>Bridge: <code>br-ext-cloud</code> - Physical interface <code>enp0s31f6</code> attached with VLAN trunking - Trunk VLANs: <code>[0, 4011, 4013]</code> - Patch ports for both external networks:   - <code>patch-localnet.ext-cloud-to-br-int</code> \u2194 <code>patch-br-int-to-localnet.ext-cloud</code>   - <code>patch-localnet.ext-public-to-br-int</code> \u2194 <code>patch-br-int-to-localnet.ext-public</code></p>"},{"location":"tutorial-networking-external/#2-ovn-logical-switches","title":"2. OVN Logical Switches","text":"<p>Two logical switches are created automatically: - <code>ext-cloud</code> (for VLAN 4013) - <code>ext-public</code> (for VLAN 4011)</p>"},{"location":"tutorial-networking-external/#3-providernetwork-status","title":"3. ProviderNetwork Status","text":"<p>The existing ProviderNetwork <code>ext-cloud</code> is updated to include both VLANs: <pre><code>status:\n  vlans: [\"vlan4013\", \"vlan4011\"]\n  ready: true\n  readyNodes:\n  - kube-dc-master-1\n  - kube-dc-worker-1\n</code></pre></p>"},{"location":"tutorial-networking-external/#configuration-steps","title":"Configuration Steps","text":""},{"location":"tutorial-networking-external/#1-apply-vlan-configuration","title":"1. Apply VLAN Configuration","text":"<pre><code>kubectl apply -f examples/networking/additional-external-network.yaml\n</code></pre>"},{"location":"tutorial-networking-external/#2-verify-configuration","title":"2. Verify Configuration","text":"<pre><code># Check ProviderNetwork VLANs\nkubectl get provider-network ext-cloud -o jsonpath='{.status.vlans}'\n# Expected output: [\"vlan4013\",\"vlan4011\"]\n\n# Check external subnets\nkubectl get subnets ext-cloud ext-public\n# Expected: ext-cloud (100.65.0.0/16) and ext-public (168.119.17.48/28)\n\n# Check EIP assignments\nkubectl get eips -A\n# Shows which projects are using which external networks\n\n# Check OVS bridge configuration\nkubectl exec -n kube-system [ovs-pod] -- ovs-vsctl show | grep -A 10 \"br-ext-cloud\"\n\n# Check OVN logical switches\nkubectl exec -n kube-system [ovn-central-pod] -- ovn-nbctl ls-list | grep ext\n</code></pre>"},{"location":"tutorial-networking-external/#3-test-with-project","title":"3. Test with Project","text":"<p>Create projects to test both network types:</p> <p>Project using Cloud Network: <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: test-project-cloud\n  namespace: test-org\nspec:\n  cidrBlock: 10.200.0.0/24\n  egressNetworkType: cloud  # Uses ext-cloud subnet (100.65.0.0/16)\n</code></pre></p> <p>Project using Public Network: <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: test-project-public\n  namespace: test-org\nspec:\n  cidrBlock: 10.201.0.0/24\n  egressNetworkType: public  # Uses ext-public subnet (168.119.17.48/28)\n</code></pre></p>"},{"location":"tutorial-networking-external/#key-points","title":"Key Points","text":"<ol> <li>Single ProviderNetwork: Use one ProviderNetwork per physical interface with multiple VLANs attached</li> <li>Automatic Configuration: OVS bridges, patch ports, and OVN logical switches are created automatically</li> <li>VLAN Trunking: The physical interface supports multiple VLANs simultaneously</li> <li>No Manual OVS Changes: All OVS/OVN modifications are handled by Kube-DC controllers</li> </ol>"},{"location":"tutorial-networking-external/#prerequisites","title":"Prerequisites","text":"<ul> <li>Physical network infrastructure supporting VLAN trunking</li> <li>vSwitch configured with appropriate VLAN IDs</li> </ul>"},{"location":"tutorial-networking-external/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial-networking-external/#check-vlan-interface-on-nodes","title":"Check VLAN Interface on Nodes","text":"<pre><code># On cluster nodes\nip link show enp0s31f6.4011\nip addr show enp0s31f6.4011\n</code></pre>"},{"location":"tutorial-networking-external/#check-ovn-resources","title":"Check OVN Resources","text":"<pre><code># Check OVN-EIP resources\nkubectl get ovn-eip | grep ext-public\n\n# Check subnet status\nkubectl get subnet ext-public -o yaml\n</code></pre>"},{"location":"tutorial-networking-external/#test-connectivity","title":"Test Connectivity","text":"<pre><code># Test from pod\nkubectl exec -n [namespace] [pod] -- wget -qO- http://httpbin.org/ip\n</code></pre>"},{"location":"tutorial-user-groups/","title":"User and Group Management","text":"<p>This guide explains how to set up and manage users, groups, and roles in Kube-DC using Kubernetes RBAC and Keycloak integration.</p>"},{"location":"tutorial-user-groups/#overview","title":"Overview","text":"<p>Kube-DC implements a multi-tenant access control system that combines:</p> <ul> <li>Kubernetes RBAC: Handles resource-level permissions within namespaces</li> <li>Organization Groups: Manages project-level access across namespaces</li> <li>Keycloak Integration: Provides user authentication and group management</li> </ul>"},{"location":"tutorial-user-groups/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you're working from an Organization Admin perspective. You'll need:</p> <ul> <li>Access to the Kube-DC cluster with organization admin privileges</li> <li><code>kubectl</code> configured to access your cluster with organization admin privileges</li> <li>Access to the Keycloak organization admin console</li> </ul> <p>Before You Begin</p> <p>During organization and project creation you will get a namespace with organization name <code>&lt;orgname&gt;</code> created and project namespace with <code>&lt;orgname&gt;-&lt;projectname&gt;</code> pattern.</p>"},{"location":"tutorial-user-groups/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"tutorial-user-groups/#creating-project-roles","title":"Creating Project Roles","text":"<p>Create a Kubernetes Role to define permissions within a project namespace. These roles dictate what actions users can perform on specific resources.</p> <pre><code>apiVersion: rbac.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: shalb-demo  # Replace with your project namespace\n  name: resource-manager\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\", \"services\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"daemonsets\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n</code></pre> <p>Apply the role to your namespace using:</p> <pre><code>kubectl apply -f role.yaml\n</code></pre> <p>Role Scope</p> <p>Remember that Roles are namespace-scoped. If you need permissions across multiple namespaces, you need to create a separate Role in each Project namespace.</p>"},{"location":"tutorial-user-groups/#creating-organization-groups","title":"Creating Organization Groups","text":"<p>Create an OrganizationGroup Custom Resource (CR) to define group permissions across projects.</p> <p>Key Points</p> <ul> <li>The OrganizationGroup CR automatically creates a corresponding group in Keycloak</li> <li>This CR must be created in the organization namespace, not the project namespace</li> <li>Role bindings would be created by this CR</li> </ul> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb  # namespace of the organization (not the project)\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - resource-manager\n  # Additional projects and roles can be added:\n  # - project: \"prod\"\n  #   roles:\n  #   - resource-manager\n</code></pre> <p>Apply the group configuration:</p> <pre><code>kubectl apply -f organization-group.yaml\n</code></pre>"},{"location":"tutorial-user-groups/#managing-users-in-keycloak","title":"Managing Users in Keycloak","text":""},{"location":"tutorial-user-groups/#access-keycloak-admin-console","title":"Access Keycloak Admin Console","text":"<p>Retrieve Keycloak access credentials from your organization namespace:</p> <pre><code>kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.user}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\n</code></pre> <p>Remember</p> <p>Replace <code>shalb</code> with your own organization namespace in the commands above.</p>"},{"location":"tutorial-user-groups/#create-and-configure-users","title":"Create and Configure Users","text":"<ol> <li>Log in to the Keycloak admin console using the retrieved credentials</li> <li>Navigate to Users \u2192 Add User </li> <li>Fill in the required user information</li> <li>Set up initial password in the Credentials tab</li> <li>Add the user to the appropriate group (e.g., \"app-manager\") via the Groups tab </li> </ol> <p>User Group Mapping</p> <p>Any groups created via OrganizationGroup CRs will appear automatically in Keycloak. Changes to group membership in Keycloak are synchronized with Kubernetes RBAC.</p>"},{"location":"tutorial-user-groups/#accessing-kube-dc-ui","title":"Accessing Kube-DC UI","text":"<ol> <li>Navigate to the Kube-DC UI login page</li> <li>Log in using the credentials created in Keycloak</li> <li>Verify access to assigned project resources</li> </ol> <p>Permissions Troubleshooting</p> <p>If a user cannot access expected resources: - Verify they're assigned to the correct groups in Keycloak - Check that the OrganizationGroup CR includes the correct projects and roles - Ensure the underlying Kubernetes Roles have appropriate permissions - Examine the Keycloak logs for authentication issues</p> <p>Permission changes may take up to 5 minutes to propagate through the system.</p>"},{"location":"tutorial-virtual-machines/","title":"Deploying VMs &amp; Containers","text":"<p>This tutorial walks you through deploying virtual machines and containers in Kube-DC. You'll learn both the UI-based approach and how to use kubectl with YAML manifests.</p>"},{"location":"tutorial-virtual-machines/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Access to a Kube-DC cluster</li> <li>The <code>kubectl</code> command-line tool installed</li> <li>The <code>virtctl</code> plugin installed for KubeVirt (optional, but recommended)</li> <li>A project with the necessary permissions to create VMs and containers</li> </ul>"},{"location":"tutorial-virtual-machines/#understanding-vm-components-in-kube-dc","title":"Understanding VM Components in Kube-DC","text":"<p>Kube-DC's virtualization is powered by KubeVirt and consists of several components:</p> <ol> <li>VirtualMachine (VM): Defines the VM configuration and lifecycle</li> <li>DataVolume: Manages the VM's disk image(s)</li> <li>VirtualMachineInstance (VMI): Represents a running instance of a VM</li> </ol>"},{"location":"tutorial-virtual-machines/#creating-a-vm-using-the-kube-dc-ui","title":"Creating a VM Using the Kube-DC UI","text":""},{"location":"tutorial-virtual-machines/#step-1-navigate-to-vm-creation","title":"Step 1: Navigate to VM Creation","text":"<ol> <li>Log in to the Kube-DC dashboard</li> <li>Select your project from the dropdown menu (e.g., \"demo\")</li> <li>Navigate to \"Virtual Machines\" in the left sidebar</li> <li>Click the \"+\" button to create a new VM</li> </ol>"},{"location":"tutorial-virtual-machines/#step-2-configure-basic-vm-parameters","title":"Step 2: Configure Basic VM Parameters","text":"<p>In the VM creation wizard, specify the basic parameters:</p> <ol> <li>VM Name: Enter a name for your VM (e.g., \"new-vm-name\")</li> <li>Operation System: Select from the dropdown (e.g., \"Ubuntu 24.04\")</li> <li>Advanced Options: Expand this section if you want to customize the image source</li> </ol> <p></p>"},{"location":"tutorial-virtual-machines/#step-3-configure-vm-resources","title":"Step 3: Configure VM Resources","text":"<p>Continue configuring the VM:</p> <ol> <li>Number of vCPUs: Select the number of virtual CPUs</li> <li>RAM (GB): Specify the amount of memory</li> <li>Subnet: Choose the network for your VM</li> <li>Root Storage Size (GB): Set the disk size</li> <li>Root Storage Type: Select the storage class</li> </ol>"},{"location":"tutorial-virtual-machines/#step-4-review-and-create","title":"Step 4: Review and Create","text":"<ol> <li>Click \"Next\" to proceed to the review page</li> <li>Review the generated VM configuration</li> <li>The UI shows the actual YAML that will be applied</li> <li>Click \"Finish\" to create the VM</li> </ol>"},{"location":"tutorial-virtual-machines/#step-5-monitor-vm-creation","title":"Step 5: Monitor VM Creation","text":"<p>After creation:</p> <ol> <li>You'll be redirected to the VM list</li> <li>Wait for the VM to reach \"Running\" state</li> <li>Note the assigned IP address</li> </ol>"},{"location":"tutorial-virtual-machines/#managing-vms-via-the-ui","title":"Managing VMs via the UI","text":""},{"location":"tutorial-virtual-machines/#viewing-vm-details","title":"Viewing VM Details","text":"<p>Click on a VM name to view its details page, which includes:</p> <ol> <li>Guest OS: Information about the operating system</li> <li>VM Details: Status, VPC subnet, and node placement</li> <li>Performance Metrics: Real-time CPU, memory, and storage usage</li> <li>Conditions: Agent connection and other status indicators</li> </ol> <p></p>"},{"location":"tutorial-virtual-machines/#accessing-vm-console","title":"Accessing VM Console","text":"<p>From the VM details page, you have two options:</p> <ol> <li>Launch Remote Console: Opens a graphical console in your browser</li> <li>Launch SSH Terminal: Opens a web-based SSH terminal</li> </ol> <p>These options provide direct access to your VM without requiring SSH client configuration.</p>"},{"location":"tutorial-virtual-machines/#vm-actions","title":"VM Actions","text":"<p>The UI supports common VM management actions:</p> <ul> <li>Start/Stop: Control the VM power state</li> <li>Restart: Reboot the VM</li> <li>Delete: Remove the VM and its resources</li> <li>Configure: Modify VM settings</li> </ul>"},{"location":"tutorial-virtual-machines/#creating-a-vm-using-kubectl-manifests","title":"Creating a VM Using kubectl Manifests","text":"<p>For automation or GitOps workflows, you can create VMs using kubectl and YAML manifests.</p>"},{"location":"tutorial-virtual-machines/#step-1-create-datavolume","title":"Step 1: Create DataVolume","text":"<p>First, create a DataVolume to serve as the VM's disk:</p> <pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: ubuntu-vm-disk\n  namespace: shalb-demo\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\n</code></pre> <p>Apply this manifest:</p> <pre><code>kubectl apply -f ubuntu-datavolume.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#step-2-create-the-vm-definition","title":"Step 2: Create the VM Definition","text":"<p>Create a VM manifest:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\n  namespace: shalb-demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: shalb-demo/default\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n        cpu:\n          cores: 2\n        memory:\n          guest: 4G\n      volumes:\n      - dataVolume:\n          name: ubuntu-vm-disk\n        name: root-volume\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |-\n            #cloud-config\n            chpasswd: { expire: False }\n            password: temppassword\n            ssh_pwauth: True\n            package_update: true\n            package_upgrade: true\n            packages:\n            - qemu-guest-agent\n            runcmd:\n            - [ systemctl, enable, qemu-guest-agent ]\n            - [ systemctl, start, qemu-guest-agent ]\n</code></pre> <p>Apply the VM manifest:</p> <pre><code>kubectl apply -f ubuntu-vm.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#step-3-monitor-vm-status","title":"Step 3: Monitor VM Status","text":"<p>Check the status of your VM:</p> <pre><code>kubectl get virtualmachines -n shalb-demo\nkubectl get virtualmachineinstances -n shalb-demo\n</code></pre>"},{"location":"tutorial-virtual-machines/#vm-examples-for-different-operating-systems","title":"VM Examples for Different Operating Systems","text":"<p>Kube-DC supports various operating systems. Here are examples for the most common ones:</p>"},{"location":"tutorial-virtual-machines/#debian","title":"Debian","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: debian-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 14G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-generic-amd64.qcow2\n---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: debian-vm\n  namespace: shalb-demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: shalb-demo/default\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n        cpu:\n          cores: 1\n        memory:\n          guest: 2G\n      volumes:\n      - dataVolume:\n          name: debian-base-img\n        name: root-volume\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |-\n            #cloud-config\n            chpasswd: { expire: False }\n            password: temppassword\n            ssh_pwauth: True\n            package_update: true\n            packages:\n            - qemu-guest-agent\n            runcmd:\n            - [ systemctl, start, qemu-guest-agent ]\n</code></pre>"},{"location":"tutorial-virtual-machines/#alpine-linux","title":"Alpine Linux","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: alpine-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 2G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://dl-cdn.alpinelinux.org/alpine/v3.19/releases/cloud/nocloud_alpine-3.19.1-x86_64-bios-cloudinit-r0.qcow2\n</code></pre>"},{"location":"tutorial-virtual-machines/#centos-stream-9","title":"CentOS Stream 9","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: centos-base-img\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10G\n    storageClassName: local-path\n  source:\n    http:\n      url: https://cloud.centos.org/centos/9-stream/x86_64/images/CentOS-Stream-GenericCloud-9-latest.x86_64.qcow2\n</code></pre> <p>Note: CentOS Stream 9 requires additional SELinux configuration to enable guest agent SSH key injection. The OS configuration includes proper SELinux booleans and contexts to allow guest agent operations. See the CentOS example in <code>examples/virtual-machine/centos-8.yaml</code> for the complete cloud-init configuration with SELinux setup.</p>"},{"location":"tutorial-virtual-machines/#virtual-machine-health-checks","title":"Virtual Machine Health Checks","text":"<p>Kube-DC supports VM health checks to ensure your VMs are running properly:</p> <pre><code>spec:\n  template:\n    spec:\n      readinessProbe:\n        guestAgentPing: {}\n        failureThreshold: 10\n        initialDelaySeconds: 20\n        periodSeconds: 10\n        timeoutSeconds: 5\n      livenessProbe:\n        failureThreshold: 10\n        initialDelaySeconds: 120\n        periodSeconds: 20\n        timeoutSeconds: 5\n        httpGet:\n          port: 80\n</code></pre>"},{"location":"tutorial-virtual-machines/#exposing-vm-services","title":"Exposing VM Services","text":""},{"location":"tutorial-virtual-machines/#creating-a-service-for-vm","title":"Creating a Service for VM","text":"<p>To expose a service running on your VM:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh-service\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: ubuntu-vm\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222\n      targetPort: 22\n</code></pre> <p>Apply this service:</p> <pre><code>kubectl apply -f vm-service.yaml\n</code></pre>"},{"location":"tutorial-virtual-machines/#using-floating-ips-for-vms","title":"Using Floating IPs for VMs","text":"<p>You can assign a floating IP to your VM for direct external access:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: ubuntu-vm-fip\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171\n  eip: vm-eip\n</code></pre> <p>First, ensure you have an EIP:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: vm-eip\n  namespace: shalb-demo\nspec: {}\n</code></pre>"},{"location":"tutorial-virtual-machines/#deploying-containers-alongside-vms","title":"Deploying Containers Alongside VMs","text":"<p>Kube-DC allows you to run containers alongside VMs. Here's how to deploy a simple Nginx container:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: shalb-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n</code></pre> <p>Create a service for the Nginx deployment:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-virtual-machines/#best-practices-for-vm-management","title":"Best Practices for VM Management","text":""},{"location":"tutorial-virtual-machines/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Allocate appropriate resources based on the OS and workload requirements</li> <li>Monitor VM performance to adjust resources as needed</li> <li>Use resource quotas to prevent resource exhaustion</li> </ul>"},{"location":"tutorial-virtual-machines/#security","title":"Security","text":"<ul> <li>Change default passwords immediately</li> <li>Use SSH keys instead of passwords when possible</li> <li>Keep guest OS updated with security patches</li> <li>Apply network policies to control VM traffic</li> </ul>"},{"location":"tutorial-virtual-machines/#efficiency","title":"Efficiency","text":"<ul> <li>Use cloud-init for automated VM configuration</li> <li>Create VM templates for standardized deployments</li> <li>Use the smallest OS image that meets your requirements</li> </ul>"},{"location":"tutorial-virtual-machines/#troubleshooting-vms","title":"Troubleshooting VMs","text":""},{"location":"tutorial-virtual-machines/#common-issues","title":"Common Issues","text":"<ol> <li> <p>VM stuck in provisioning: Check DataVolume status and events    <pre><code>kubectl get datavolume -n shalb-demo\nkubectl describe datavolume ubuntu-vm-disk -n shalb-demo\n</code></pre></p> </li> <li> <p>VM not accessible via network: Verify network configuration    <pre><code>kubectl get vmi -n shalb-demo -o jsonpath='{.items[*].status.interfaces[*].ipAddress}'\n</code></pre></p> </li> <li> <p>Cloud-init not running: Check cloud-init logs inside the VM    <pre><code># Inside the VM\nsudo cat /var/log/cloud-init.log\n</code></pre></p> </li> </ol>"},{"location":"tutorial-virtual-machines/#accessing-vm-logs","title":"Accessing VM Logs","text":"<pre><code>kubectl get events -n shalb-demo\nvirtctl console ubuntu-vm -n shalb-demo\nvirtctl logs ubuntu-vm -n shalb-demo\n</code></pre>"},{"location":"tutorial-virtual-machines/#advanced-kubevirt-features","title":"Advanced KubeVirt Features","text":"<p>For more advanced features, refer to the KubeVirt documentation:</p> <ul> <li>VM Snapshots</li> <li>Live Migration</li> <li>GPU Passthrough</li> <li>Storage Management</li> </ul>"},{"location":"tutorial-virtual-machines/#conclusion","title":"Conclusion","text":"<p>You've now learned how to deploy and manage VMs and containers in Kube-DC using both the intuitive UI and kubectl manifests. This hybrid approach allows you to choose the most appropriate method for your workflow, whether you prefer interactive management or automation through GitOps practices.</p>"},{"location":"tutorial-windows-vm/","title":"Windows 11 VM Tutorial - Complete Setup Guide","text":"<p>This comprehensive guide covers the complete process of setting up Windows 11 VMs in KubeVirt, from infrastructure setup to golden image creation and deployment.</p>"},{"location":"tutorial-windows-vm/#overview","title":"Overview","text":"<p>This tutorial provides two deployment methods:</p> <ol> <li>Golden Image Deployment (Recommended) - Deploy pre-configured VMs in 5-10 minutes</li> <li>Fresh Installation - Create custom Windows installations with full control</li> </ol>"},{"location":"tutorial-windows-vm/#prerequisites","title":"Prerequisites","text":"<ul> <li>KubeVirt and CDI installed and running</li> <li>Multus CNI with OVN network configured</li> <li>StorageClass <code>local-path</code> available</li> <li>Ingress controller for HTTP access to ISOs</li> </ul>"},{"location":"tutorial-windows-vm/#step-1-create-iso-hosting-environment","title":"Step 1: Create ISO Hosting Environment","text":""},{"location":"tutorial-windows-vm/#11-create-dedicated-namespace","title":"1.1 Create Dedicated Namespace","text":"<pre><code># hack/windows/iso-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: iso\n</code></pre>"},{"location":"tutorial-windows-vm/#12-create-storage-for-isos","title":"1.2 Create Storage for ISOs","text":"<pre><code># hack/windows/iso-storage-pvc-iso-ns.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: iso-storage\n  namespace: iso\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi  # Increased for Windows ISO + golden images\n  storageClassName: local-path\n</code></pre>"},{"location":"tutorial-windows-vm/#13-http-server-for-isos","title":"1.3 HTTP Server for ISOs","text":"<pre><code># hack/windows/nginx-iso-server-iso-ns.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-iso-server\n  namespace: iso\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-iso-server\n  template:\n    metadata:\n      labels:\n        app: nginx-iso-server\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: iso-storage\n          mountPath: /usr/share/nginx/html\n        - name: nginx-config\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: iso-storage\n        persistentVolumeClaim:\n          claimName: iso-storage\n      - name: nginx-config\n        configMap:\n          name: nginx-iso-config\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-iso-config\n  namespace: iso\ndata:\n  default.conf: |\n    server {\n        listen 80;\n        server_name _;\n        root /usr/share/nginx/html;\n\n        location / {\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n        }\n\n        location ~* \\.(iso|img|qcow2)$ {\n            add_header Content-Type application/octet-stream;\n            add_header Cache-Control \"public, max-age=3600\";\n        }\n    }\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-iso-server\n  namespace: iso\nspec:\n  selector:\n    app: nginx-iso-server\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre>"},{"location":"tutorial-windows-vm/#14-ingress-configuration-with-tls","title":"1.4 Ingress Configuration with TLS","text":"<pre><code># hack/windows/iso-ingress-iso-ns.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: iso-server-ingress\n  namespace: iso\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod-http\n    nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"3600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"3600\"\n    nginx.ingress.kubernetes.io/proxy-buffering: \"off\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - iso.stage.kube-dc.com\n    secretName: iso-server-tls\n  rules:\n  - host: iso.stage.kube-dc.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: iso-server\n            port:\n              number: 80\n</code></pre>"},{"location":"tutorial-windows-vm/#15-deploy-infrastructure","title":"1.5 Deploy Infrastructure","text":"<pre><code># Deploy all infrastructure components\nkubectl apply -f hack/windows/iso-namespace.yaml\nkubectl apply -f hack/windows/iso-storage-pvc-iso-ns.yaml\nkubectl apply -f hack/windows/nginx-iso-server-iso-ns.yaml\nkubectl apply -f hack/windows/iso-ingress-iso-ns.yaml\n\n# Verify deployment\nkubectl get pods -n iso\nkubectl get ingress -n iso\n</code></pre>"},{"location":"tutorial-windows-vm/#step-2-download-and-upload-windows-iso","title":"Step 2: Download and Upload Windows ISO","text":""},{"location":"tutorial-windows-vm/#21-download-windows-11-enterprise-iso","title":"2.1 Download Windows 11 Enterprise ISO","text":"<ol> <li>Visit: https://www.microsoft.com/en-us/evalcenter/download-windows-11-enterprise</li> <li>Select: ISO \u2013 Enterprise download 64-bit edition (90-day evaluation)  </li> <li>Download the ISO file (approximately 5.4GB)</li> </ol>"},{"location":"tutorial-windows-vm/#22-download-virtio-drivers","title":"2.2 Download VirtIO Drivers","text":"<pre><code># Download latest VirtIO drivers\nwget https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso\n</code></pre>"},{"location":"tutorial-windows-vm/#23-upload-files-to-cluster","title":"2.3 Upload Files to Cluster","text":"<pre><code># Create temporary upload pod\nkubectl apply -f hack/windows/iso-upload-pod.yaml\n\n# Wait for pod to be ready\nkubectl wait --for=condition=Ready pod/iso-upload-pod -n iso --timeout=60s\n\n# Upload Windows 11 ISO (replace with your actual filename)\nkubectl cp ~/Win11_24H2_EnglishInternational_x64.iso iso/iso-upload-pod:/storage/win11-x64.iso\n\n# Upload VirtIO drivers\nkubectl cp ~/virtio-win.iso iso/iso-upload-pod:/storage/virtio-win.iso\n\n# Upload OpenSSH installation script\nkubectl cp hack/windows/install-openssh-windows.ps1 iso/iso-upload-pod:/storage/install-openssh-windows.ps1\n\n# Clean up upload pod\nkubectl delete pod iso-upload-pod -n iso --wait=true\n\n# Verify files are accessible\ncurl -I https://iso.stage.kube-dc.com/win11-x64.iso\ncurl -I https://iso.stage.kube-dc.com/virtio-win.iso\n</code></pre>"},{"location":"tutorial-windows-vm/#step-3-fresh-windows-installation","title":"Step 3: Fresh Windows Installation","text":""},{"location":"tutorial-windows-vm/#31-deploy-installation-vm","title":"3.1 Deploy Installation VM","text":"<p>Use the complete VM manifest that includes all required DataVolumes:</p> <pre><code># Deploy Windows VM with installation ISOs\nkubectl apply -f hack/windows/windows11-vm.yaml\n\n# Monitor DataVolume download progress\nkubectl get dv -n shalb-dev\n\n# Check VM status\nkubectl get vm,vmi -n shalb-dev | grep windows11\n</code></pre>"},{"location":"tutorial-windows-vm/#32-windows-installation-process","title":"3.2 Windows Installation Process","text":"<pre><code># Access VM console via VNC\nvirtctl vnc windows11-vm -n shalb-dev\n\n# Or use VNC proxy\nvirtctl vnc windows11-vm -n shalb-dev --proxy-only --port 5900\n# Then connect VNC client to localhost:5900\n</code></pre> <p>Installation Steps:</p> <ol> <li> <p>Boot from Windows ISO: VM boots from Windows installer (bootOrder: 1)</p> </li> <li> <p>Load VirtIO Drivers: </p> </li> <li>When prompted for disk drivers, click \"Load driver\"</li> <li>Browse to VirtIO drivers CDROM</li> <li>Navigate to <code>/amd64/w11/</code> folder</li> <li>Install VirtIO SCSI controller drivers (for disk access)</li> <li> <p>DO NOT install network drivers yet (to create local account)</p> </li> <li> <p>Install Windows: </p> </li> <li>Select the 60GB VirtIO disk for installation</li> <li> <p>Complete Windows 11 setup with local account</p> </li> <li> <p>Post-Installation:</p> </li> <li> <p>Install remaining VirtIO drivers from CDROM (network, balloon, RNG)</p> </li> <li>Install QEMU Guest Agent from VirtIO drivers CDROM</li> <li>Run Windows Updates</li> </ol>"},{"location":"tutorial-windows-vm/#33-configure-ssh-and-rdp","title":"3.3 Configure SSH and RDP","text":"<p>After Windows installation, configure SSH and RDP access:</p> <pre><code># Method 1: Download and run script\nInvoke-WebRequest -Uri \"https://iso.stage.kube-dc.com/install-openssh-windows.ps1\" -OutFile \"install-openssh-windows.ps1\"\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n.\\install-openssh-windows.ps1\n\n# Method 2: Direct execution (bypass execution policy)\nPowerShell -ExecutionPolicy Bypass -Command \"Invoke-Expression (Invoke-WebRequest -Uri 'https://iso.stage.kube-dc.com/install-openssh-windows.ps1').Content\"\n</code></pre> <p>Script Features:</p> <ul> <li>\u2705 Installs OpenSSH Server using Windows capabilities</li> <li>\u2705 Configures SSH service for automatic startup  </li> <li>\u2705 Opens SSH port 22 in Windows Firewall (all network profiles)</li> <li>\u2705 Enables Remote Desktop (port 3389)</li> <li>\u2705 Enables ICMP ping (IPv4 and IPv6)</li> <li>\u2705 Provides detailed verification and status reporting</li> </ul>"},{"location":"tutorial-windows-vm/#step-4-create-golden-image","title":"Step 4: Create Golden Image","text":""},{"location":"tutorial-windows-vm/#41-prepare-vm-for-golden-image","title":"4.1 Prepare VM for Golden Image","text":"<pre><code># 1. Inside Windows VM, run Sysprep (optional but recommended)\n# Navigate to: C:\\Windows\\System32\\Sysprep\\sysprep.exe\n# Options: Generalize, Enter System Out-of-Box Experience (OOBE), Shutdown\n\n# 2. Stop the source VM (CRITICAL for export)\nkubectl patch vm windows11-vm -n shalb-dev --type merge -p '{\"spec\":{\"runStrategy\":\"Halted\"}}'\nkubectl wait --for=delete vmi/windows11-vm -n shalb-dev --timeout=300s\n</code></pre>"},{"location":"tutorial-windows-vm/#42-export-to-qcow2-golden-image","title":"4.2 Export to QCOW2 Golden Image","text":"<pre><code># hack/windows/export-golden-image.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: export-golden-image\n  namespace: shalb-dev\nspec:\n  restartPolicy: Never\n  containers:\n    - name: exporter\n      image: ubuntu:22.04\n      command: [\"sh\", \"-c\"]\n      args:\n        - |\n          set -e\n          apt-get update &amp;&amp; apt-get install -y qemu-utils curl\n          cd /pvc\n\n          echo \"=== Disk Information ===\"\n          DISK_SIZE_BYTES=$(qemu-img info --output=json disk.img | grep '\"virtual-size\"' | cut -d: -f2 | tr -d ' ,')\n          DISK_SIZE_GB=$((DISK_SIZE_BYTES / 1024 / 1024 / 1024))\n          echo \"Source disk: ${DISK_SIZE_GB}GB (${DISK_SIZE_BYTES} bytes)\"\n\n          echo \"=== Converting to compressed QCOW2 ===\"\n          qemu-img convert -p -O qcow2 -c disk.img windows11-x64-golden.qcow2\n\n          echo \"=== Final Golden Image ===\"\n          ls -lh windows11-x64-golden.qcow2\n          qemu-img info windows11-x64-golden.qcow2\n\n          echo \"=== Upload to ISO server ===\"\n          # Upload to ISO server storage\n          curl -T windows11-x64-golden.qcow2 http://nginx-iso-server.iso.svc.cluster.local/windows11-x64-golden.qcow2 || echo \"Upload failed, manual copy required\"\n\n          echo \"=== Export completed, sleeping for inspection ===\"\n          sleep 3600\n      volumeMounts:\n        - name: source-disk\n          mountPath: /pvc\n  volumes:\n    - name: source-disk\n      persistentVolumeClaim:\n        claimName: windows11-disk\n</code></pre>"},{"location":"tutorial-windows-vm/#43-export-process","title":"4.3 Export Process","text":"<pre><code># Export golden image\nkubectl apply -f hack/windows/export-golden-image.yaml\nkubectl wait --for=condition=Ready pod/export-golden-image -n shalb-dev --timeout=120s\n\n# Monitor export progress\nkubectl logs -n shalb-dev export-golden-image -f\n\n# Manual copy to ISO server (if curl upload fails)\nkubectl cp shalb-dev/export-golden-image:/pvc/windows11-x64-golden.qcow2 /tmp/\nkubectl cp /tmp/windows11-x64-golden.qcow2 iso/iso-upload-pod:/storage/\n\n# Verify golden image is available\ncurl -I https://iso.stage.kube-dc.com/windows11-x64-golden.qcow2\n\n# Clean up export pod\nkubectl delete pod export-golden-image -n shalb-dev --wait=true\n</code></pre>"},{"location":"tutorial-windows-vm/#step-5-deploy-from-golden-image","title":"Step 5: Deploy from Golden Image","text":""},{"location":"tutorial-windows-vm/#51-golden-image-deployment-recommended","title":"5.1 Golden Image Deployment (Recommended)","text":"<pre><code># Deploy VM from golden image\nkubectl apply -f hack/windows/win11-x64.yaml\n\n# Create SSH key secret for key injection\nkubectl create secret generic authorized-keys-default \\\n  --from-file=key1=~/.ssh/id_rsa.pub \\\n  -n shalb-dev\n\n# Monitor deployment\nkubectl get vm,vmi,dv -n shalb-dev | grep win11-x64\n\n# Get VM IP when ready\nkubectl get vmi win11-x64 -n shalb-dev -o jsonpath='{.status.interfaces[0].ipAddress}'\n\n# SSH to VM (once guest agent is ready)\nssh kube-dc@&lt;vm-ip&gt;\n</code></pre>"},{"location":"tutorial-windows-vm/#52-golden-image-benefits","title":"5.2 Golden Image Benefits","text":"Aspect Golden Image Fresh Install Deployment Time 5-10 minutes 30+ minutes Download Size 21.3GB compressed 5.4GB + drivers Configuration Pre-configured Manual setup required SSH/RDP Ready immediately Requires script execution VirtIO Drivers Pre-installed Manual installation Use Case Production deployment Custom configurations"},{"location":"tutorial-windows-vm/#step-6-troubleshooting","title":"Step 6: Troubleshooting","text":""},{"location":"tutorial-windows-vm/#61-common-issues","title":"6.1 Common Issues","text":"<p>DataVolume stuck in ImportScheduled: <pre><code># Check CDI importer pods\nkubectl get pods -n cdi\nkubectl logs -n cdi &lt;importer-pod&gt;\n\n# Check storage provisioner\nkubectl get pods -n local-path-storage\n</code></pre></p> <p>VM won't start - CPU resources: <pre><code># Check node resources\nkubectl describe nodes | grep -A 10 \"Allocated resources\"\n\n# Reduce VM CPU if needed\nkubectl patch vm &lt;vm-name&gt; -n &lt;namespace&gt; --type merge -p '{\"spec\":{\"template\":{\"spec\":{\"domain\":{\"cpu\":{\"cores\":2}}}}}}'\n</code></pre></p> <p>SSH not working: <pre><code># Check guest agent connection\nkubectl describe vmi &lt;vm-name&gt; -n &lt;namespace&gt; | grep -i agent\n\n# Test network connectivity\nkubectl run test-pod --image=nicolaka/netshoot --rm -it -- ping &lt;vm-ip&gt;\nkubectl run test-pod --image=nicolaka/netshoot --rm -it -- nc -zv &lt;vm-ip&gt; 22\n</code></pre></p> <p>Storage issues with local-path: <pre><code># Check local-path provisioner\nkubectl get pods -n local-path-storage\nkubectl logs -n local-path-storage &lt;provisioner-pod&gt;\n\n# Note: local-path doesn't support Block mode, use Filesystem mode\n</code></pre></p>"},{"location":"tutorial-windows-vm/#62-verification-commands","title":"6.2 Verification Commands","text":"<pre><code># Check all Windows VMs\nkubectl get vm -A | grep -i win\n\n# Check DataVolume progress\nkubectl get dv -n &lt;namespace&gt;\n\n# Access VM console\nvirtctl vnc &lt;vm-name&gt; -n &lt;namespace&gt;\n\n# Check VM resource usage\nkubectl top pods -n &lt;namespace&gt; | grep virt-launcher\n</code></pre>"},{"location":"tutorial-windows-vm/#required-manifests-summary","title":"Required Manifests Summary","text":"<p>Infrastructure (Step 1): - <code>hack/windows/iso-namespace.yaml</code> - ISO namespace - <code>hack/windows/iso-storage-pvc-iso-ns.yaml</code> - Storage for ISOs - <code>hack/windows/nginx-iso-server-iso-ns.yaml</code> - HTTP server - <code>hack/windows/iso-ingress-iso-ns.yaml</code> - Ingress configuration</p> <p>Utilities: - <code>hack/windows/iso-upload-pod.yaml</code> - Upload files to cluster - <code>hack/windows/install-openssh-windows.ps1</code> - SSH/RDP configuration script</p> <p>VM Deployment: - <code>hack/windows/windows11-vm.yaml</code> - Fresh installation VM - <code>hack/windows/win11-x64.yaml</code> - Golden image deployment</p> <p>Golden Image Creation: - <code>hack/windows/export-golden-image.yaml</code> - Export VM to QCOW2 - <code>hack/windows/windows11-enterprise-golden-datasource.yaml</code> - DataSource for cloning</p> <p>Optional: - <code>hack/windows/windows11-from-datasource.yaml</code> - Deploy from DataSource (same namespace) - <code>hack/windows/pvc-init-windows11-disk.yaml</code> - Fix disk size issues if needed</p>"},{"location":"tutorial-windows-vm/#available-resources","title":"Available Resources","text":"<p>Once deployed, the following resources are available:</p> <ul> <li>Windows 11 ISO: <code>https://iso.stage.kube-dc.com/win11-x64.iso</code> (5.4GB)</li> <li>VirtIO Drivers: <code>https://iso.stage.kube-dc.com/virtio-win.iso</code> (700MB)</li> <li>SSH Script: <code>https://iso.stage.kube-dc.com/install-openssh-windows.ps1</code> (5KB)</li> <li>Golden Image: <code>https://iso.stage.kube-dc.com/windows11-x64-golden.qcow2</code> (21.3GB)</li> </ul>"},{"location":"tutorial-windows-vm/#security-considerations","title":"Security Considerations","text":"<ul> <li>SSH Keys: Use KubeVirt accessCredentials for secure key injection {{ ... }}</li> <li>Network Policies: Implement Kubernetes network policies for VM isolation</li> <li>Sysprep: Run before creating golden images to avoid SID conflicts</li> <li>Firewall: Script configures Windows Firewall appropriately for SSH, RDP, and ICMP</li> </ul> <p>\u2705 Complete Windows 11 VM infrastructure with golden image support ready for production use!</p>"},{"location":"user-groups/","title":"User and Group Management","text":"<p>This guide explains how to set up and manage users, groups, and roles in Kube-DC using Kubernetes RBAC and Keycloak integration.</p>"},{"location":"user-groups/#overview","title":"Overview","text":"<p>Kube-DC implements a multi-tenant access control system that combines:</p> <ul> <li>Kubernetes RBAC: Handles resource-level permissions within namespaces</li> <li>Organization Groups: Manages project-level access across namespaces</li> <li>Keycloak Integration: Provides user authentication and group management</li> </ul>"},{"location":"user-groups/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you're working from an Organization Admin perspective. You'll need:</p> <ul> <li>Access to the Kube-DC cluster with organization admin privileges</li> <li><code>kubectl</code> configured to access your cluster with organization admin privileges</li> <li>Access to the Keycloak organization admin console</li> </ul> <p>Before You Begin</p> <p>During organization and project creation you will get a namespace with organization name <code>&lt;orgname&gt;</code> created and project namespace with <code>&lt;orgname&gt;-&lt;projectname&gt;</code> pattern.</p>"},{"location":"user-groups/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"user-groups/#creating-project-roles","title":"Creating Project Roles","text":"<p>Create a Kubernetes Role to define permissions within a project namespace. These roles dictate what actions users can perform on specific resources.</p> <pre><code>apiVersion: rbac.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: shalb-demo  # Replace with your project namespace\n  name: resource-manager\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\", \"services\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"daemonsets\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"create\", \"watch\", \"delete\"]\n</code></pre> <p>Apply the role to your namespace using:</p> <pre><code>kubectl apply -f role.yaml\n</code></pre> <p>Role Scope</p> <p>Remember that Roles are namespace-scoped. If you need permissions across multiple namespaces, you need to create a separate Role in each Project namespace.</p>"},{"location":"user-groups/#creating-organization-groups","title":"Creating Organization Groups","text":"<p>Create an OrganizationGroup Custom Resource (CR) to define group permissions across projects.</p> <p>Key Points</p> <ul> <li>The OrganizationGroup CR automatically creates a corresponding group in Keycloak</li> <li>This CR must be created in the organization namespace, not the project namespace</li> <li>Role bindings would be created by this CR</li> </ul> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb  # namespace of the organization (not the project)\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - resource-manager\n  # Additional projects and roles can be added:\n  # - project: \"prod\"\n  #   roles:\n  #   - resource-manager\n</code></pre> <p>Apply the group configuration:</p> <pre><code>kubectl apply -f organization-group.yaml\n</code></pre>"},{"location":"user-groups/#managing-users-in-keycloak","title":"Managing Users in Keycloak","text":""},{"location":"user-groups/#access-keycloak-admin-console","title":"Access Keycloak Admin Console","text":"<p>Retrieve Keycloak access credentials from your organization namespace:</p> <pre><code>kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.user}' | base64 -d\nkubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\n</code></pre> <p>Remember</p> <p>Replace <code>shalb</code> with your own organization namespace in the commands above.</p>"},{"location":"user-groups/#create-and-configure-users","title":"Create and Configure Users","text":"<ol> <li>Log in to the Keycloak admin console using the retrieved credentials</li> <li>Navigate to Users \u2192 Add User </li> <li>Fill in the required user information</li> <li>Set up initial password in the Credentials tab</li> <li>Add the user to the appropriate group (e.g., \"app-manager\") via the Groups tab </li> </ol> <p>User Group Mapping</p> <p>Any groups created via OrganizationGroup CRs will appear automatically in Keycloak. Changes to group membership in Keycloak are synchronized with Kubernetes RBAC.</p>"},{"location":"user-groups/#accessing-kube-dc-ui","title":"Accessing Kube-DC UI","text":"<ol> <li>Navigate to the Kube-DC UI login page</li> <li>Log in using the credentials created in Keycloak</li> <li>Verify access to assigned project resources</li> </ol> <p>Permissions Troubleshooting</p> <p>If a user cannot access expected resources: - Verify they're assigned to the correct groups in Keycloak - Check that the OrganizationGroup CR includes the correct projects and roles - Ensure the underlying Kubernetes Roles have appropriate permissions - Examine the Keycloak logs for authentication issues</p> <p>Permission changes may take up to 5 minutes to propagate through the system.</p>"}]}