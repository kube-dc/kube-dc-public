{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What Is Kube-DC?","text":"<p>Kube-DC is an advanced, enterprise-grade platform that transforms Kubernetes into a comprehensive Data Center solution supporting both virtual machines and containerized workloads. It provides organizations with a unified management interface for all their infrastructure needs, from multi-tenancy and virtualization to networking and billing.</p>"},{"location":"#overview","title":"Overview","text":"<p>Kube-DC bridges the gap between traditional virtualization and modern container orchestration, allowing teams to run both legacy workloads and cloud-native applications on the same platform. By leveraging Kubernetes as the foundation, Kube-DC inherits its robust ecosystem while extending functionality to support enterprise requirements.</p> <p></p>"},{"location":"#key-features-at-a-glance","title":"Key Features at a Glance","text":"<p>Kube-DC offers a comprehensive set of features designed for modern data center operations:</p> <ul> <li>Multi-Tenancy - Host multiple organizations with isolated environments and custom SSO integration</li> <li>Unified Workload Management - Run both VMs and containers on the same platform</li> <li>Advanced Networking - VPC per project, VLAN support, and software-defined networking</li> <li>Enterprise Virtualization - KubeVirt integration with GPU passthrough and live migration</li> <li>Infrastructure as Code - Kubernetes-native APIs with support for Terraform, Ansible, and more</li> <li>Integrated Billing - Track and allocate costs for all resources</li> <li>Managed Services Platform - Deploy databases, storage, and AI/ML infrastructure</li> </ul> <p>For detailed information about each feature, including capabilities and use cases, visit the Core Features page.</p>"},{"location":"#why-choose-kube-dc","title":"Why Choose Kube-DC?","text":""},{"location":"#for-enterprise-it","title":"For Enterprise IT","text":"<ul> <li>Run legacy VMs alongside modern containers</li> <li>Implement chargeback models for departmental resource usage</li> <li>Provide self-service infrastructure while maintaining governance</li> <li>Reduce operational costs by consolidating virtualization and container platforms</li> </ul>"},{"location":"#for-service-providers","title":"For Service Providers","text":"<ul> <li>Offer multi-tenant infrastructure with complete isolation</li> <li>Provide value-added services beyond basic IaaS</li> <li>Implement flexible billing based on actual resource usage</li> <li>Support diverse customer workloads on a single platform</li> </ul>"},{"location":"#for-devops-teams","title":"For DevOps Teams","text":"<ul> <li>Unify VM and container management workflows</li> <li>Implement infrastructure as code for all resources</li> <li>Integrate with existing CI/CD pipelines</li> <li>Enable developer self-service while maintaining control</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to explore Kube-DC? Check out our Quick Start guides to begin your journey.</p> <p>For a deeper understanding of the underlying architecture and concepts, visit the Architecture &amp; Concepts section.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Kube-DC is built with a focus on community collaboration. Visit our Community &amp; Support page to learn how to get involved, report issues, or seek assistance.</p>"},{"location":"architecture-multi-tenancy/","title":"Multi-Tenancy &amp; RBAC","text":"<p>Kube-DC implements a comprehensive multi-tenant architecture that leverages Kubernetes namespaces and Keycloak for identity and access management. This document explains how organizations, projects, and groups in Kube-DC are mapped to Kubernetes and Keycloak objects.</p>"},{"location":"architecture-multi-tenancy/#core-components-and-mapping-structure","title":"Core Components and Mapping Structure","text":"<p>The following diagram illustrates the mapping between Kube-DC structures and the underlying Kubernetes and Keycloak components:</p> <pre><code>graph TD\n    User[User 1] --&gt;|Authenticates| KC[Keycloak Realm]\n    User --&gt;|Obtains Group and Role| KCG[Keycloak Group]\n    User --&gt;|Obtains Group and Role| KCR[Keycloak Role]\n\n    ORG[Organization] --&gt;|Maps to| ORGNS[Organization Namespace]\n\n    ORGNS --&gt;|Contains| ORGGRP[Organization Group]\n\n    PROJ[Project A] --&gt;|Maps to| PNS[Project A NS]\n    PROJ2[Project B] --&gt;|Maps to| PNS2[Project B NS]\n\n    ORGGRP --&gt;|Maps to| K8GRPCRD[Group CRD]\n    ORGGRP --&gt;|Maps to| KCGRP[Keycloak Group]\n\n    KCGRP --&gt;|Maps to| K8SROLE[K8s RoleBinding]\n    KCR --&gt;|Maps to| K8SROLE\n\n    K8GRPCRD --&gt;|Defines permissions for| PNS\n    K8GRPCRD --&gt;|Defines permissions for| PNS2\n\n    KK[Keycloak Client Role] --&gt;|KK to K8s Role Mapping| K8R[K8s Role]</code></pre>"},{"location":"architecture-multi-tenancy/#organization-structure","title":"Organization Structure","text":""},{"location":"architecture-multi-tenancy/#organization","title":"Organization","text":"<p>An Organization is the top-level entity in Kube-DC that represents a company, department, or team.</p> <p>Example Organization YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Organization\nmetadata:\n  name: shalb\n  namespace: shalb\nspec: \n  description: \"Shalb organization\"\n  email: \"arti@shalb.com\"\n</code></pre> <p>Mapping: - Each Organization maps to a dedicated Kubernetes namespace with the same name - A corresponding Keycloak Client is created for the organization - The Organization serves as a logical grouping for Projects and OrganizationGroups</p>"},{"location":"architecture-multi-tenancy/#project","title":"Project","text":"<p>A Project represents a logical grouping of resources within an Organization. Projects help segregate workloads and manage access control.</p> <p>Example Project YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre> <p>Mapping: - Each Project maps to a dedicated Kubernetes namespace in the format: <code>{organization}-{project}</code> (e.g., <code>shalb-demo</code>) - Projects receive their own network CIDR block for resource isolation - Kubernetes namespaces provide the boundary for resource quotas and access control</p>"},{"location":"architecture-multi-tenancy/#organizationgroup","title":"OrganizationGroup","text":"<p>An OrganizationGroup maps users to roles within specific projects, defining what actions they can perform.</p> <p>Example OrganizationGroup YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: OrganizationGroup\nmetadata:\n  name: \"app-manager\"\n  namespace: shalb\nspec:\n  permissions:\n  - project: \"demo\"\n    roles:\n    - admin\n  - project: \"prod\"\n    roles:\n    - resource-manager\n</code></pre> <p>Mapping: - OrganizationGroups are implemented as Kubernetes Custom Resource Definitions (CRDs) - Each OrganizationGroup maps to a Keycloak Group - The permissions defined in OrganizationGroups determine the Kubernetes RoleBindings that grant access to resources - Different roles can be assigned for different projects</p>"},{"location":"architecture-multi-tenancy/#authentication-and-authorization-flow","title":"Authentication and Authorization Flow","text":"<ol> <li>User Authentication:</li> <li>Users authenticate through Keycloak</li> <li> <p>Upon successful authentication, users receive JSON Web Tokens (JWTs)</p> </li> <li> <p>Group and Role Assignment:</p> </li> <li>Users are assigned to Keycloak Groups based on their OrganizationGroup membership</li> <li> <p>Keycloak maps these groups to corresponding roles</p> </li> <li> <p>Kubernetes Authorization:</p> </li> <li>The Kubernetes API server validates the user's JWT</li> <li>RoleBindings determine what actions the user can perform within each namespace</li> <li> <p>Resource access is controlled at the Project (namespace) level</p> </li> <li> <p>Resource Access:</p> </li> <li>Users can only access resources in projects where they have appropriate role assignments</li> <li>Actions are restricted based on the permissions defined in their roles</li> </ol>"},{"location":"architecture-multi-tenancy/#role-based-access-control","title":"Role-Based Access Control","text":"<p>Kube-DC provides several built-in roles that can be assigned to users via OrganizationGroups:</p> <ul> <li>Admin: Full access to all resources within a project</li> <li>Resource Manager: Can create and manage resources, but cannot modify project settings</li> <li>Viewer: Read-only access to project resources</li> </ul> <p>Example Role YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Role\nmetadata:\n  name: resource-manager\n  namespace: shalb\nspec:\n  rules:\n  - apiGroups: [\"*\"]\n    resources: [\"pods\", \"services\", \"deployments\", \"statefulsets\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"kubevirt.io\"]\n    resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"architecture-multi-tenancy/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture-multi-tenancy/#kubernetes-components","title":"Kubernetes Components","text":"<ul> <li>Namespaces: Used to isolate Organizations and Projects</li> <li>RBAC: Role-Based Access Control for managing permissions</li> <li>CRDs: Custom Resource Definitions for Kube-DC specific resources</li> <li>NetworkPolicies: Ensure network isolation between Projects</li> </ul>"},{"location":"architecture-multi-tenancy/#keycloak-integration","title":"Keycloak Integration","text":"<ul> <li>Realm: Represents the authentication domain</li> <li>Clients: Each Organization has a dedicated client</li> <li>Groups: Map to OrganizationGroups in Kube-DC</li> <li>Roles: Define permissions that can be assigned to users</li> <li>Role Mappings: Connect Keycloak roles to Kubernetes RBAC</li> </ul>"},{"location":"architecture-multi-tenancy/#practical-application","title":"Practical Application","text":"<p>When a user is added to an organization group in Kube-DC:</p> <ol> <li>The corresponding Keycloak group membership is created</li> <li>The user inherits roles based on the group's permissions</li> <li>When the user accesses the Kubernetes API, their JWT contains the group and role information</li> <li>Kubernetes RBAC evaluates the JWT against RoleBindings to determine access</li> <li>The user can operate only within the boundaries of their assigned permissions</li> </ol> <p>This multi-layered approach ensures secure isolation between tenants while providing fine-grained access control within each project.</p>"},{"location":"architecture-networking/","title":"Networking (Kube-OVN, VLANs)","text":"<p>Kube-DC provides advanced networking capabilities through Kube-OVN, enabling multi-tenant network isolation, external connectivity, and flexible service exposure. This document explains the key networking components and how they operate within the Kube-DC architecture.</p>"},{"location":"architecture-networking/#network-architecture-overview","title":"Network Architecture Overview","text":"<p>Kube-DC's networking architecture is built on Kube-OVN, which supports both overlay and underlay networks and provides Virtual Private Cloud (VPC) capabilities for tenant isolation.</p> <pre><code>graph TB\n    Internet((Internet)) &lt;--&gt;|Ingress/Egress| FW[VPC Firewall]\n\n    subgraph \"Project A (Namespace + VPC)\"\n        PROJ_A_EIP[Project A EIP] &lt;--&gt; FW\n        PROJ_A_EIP --&gt;|Default NAT GW| PROJ_A_NET[Project A Network]\n\n        SVC_LB_A[Service LoadBalancer&lt;br/&gt;with annotation] --&gt;|Uses| PROJ_A_EIP\n        SVC_LB_A --&gt;|Routes to| POD_A[Pods]\n        SVC_LB_A --&gt;|Routes to| VM_A[VMs]\n\n        FIP_A[Floating IP] --&gt;|Maps to| VM_A_TARGET[Specific VM/Pod]\n    end\n\n    subgraph \"Project B (Namespace + VPC)\"\n        PROJ_B_EIP[Project B EIP] &lt;--&gt; FW\n        PROJ_B_EIP --&gt;|Default NAT GW| PROJ_B_NET[Project B Network]\n\n        SVC_LB_B[Service LoadBalancer&lt;br/&gt;with annotation] --&gt;|Uses| PROJ_B_EIP\n        SVC_LB_B --&gt;|Routes to| POD_B[Pods]\n        SVC_LB_B --&gt;|Routes to| VM_B[VMs]\n\n        DEDICATED_EIP[Dedicated EIP] &lt;--&gt; FW\n        SVC_LB_B_DEDICATED[Service LoadBalancer&lt;br/&gt;with dedicated EIP] --&gt;|Uses| DEDICATED_EIP\n    end</code></pre> <p>In this architecture: - Each project gets its own namespace and VPC - Each project receives its own EIP that acts as a NAT gateway for outbound traffic - Service LoadBalancers can route traffic to both Pods and VMs - Service LoadBalancers can use either the project's default EIP (via annotation) or a dedicated EIP - Floating IPs can map specific VMs or Pods to External IPs for direct access</p>"},{"location":"architecture-networking/#network-elements","title":"Network Elements","text":""},{"location":"architecture-networking/#user-visible-network-resources","title":"User-Visible Network Resources","text":""},{"location":"architecture-networking/#external-ip-eip","title":"External IP (EIP)","text":"<p>External IPs provide connectivity from the public internet to resources within Kube-DC. Each EIP is allocated from the provider network.</p> <p>Example EIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: EIp\nmetadata:\n  name: ssh-arti\n  namespace: shalb-demo\nspec: {}  \n</code></pre>"},{"location":"architecture-networking/#floating-ip-fip","title":"Floating IP (FIP)","text":"<p>Floating IPs map an internal IP address (of a VM or pod) to an External IP, enabling direct access to specific resources.</p> <p>Example FIP YAML:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: FIp\nmetadata:\n  name: fedora-arti\n  namespace: shalb-demo\nspec:\n  ipAddress: 10.0.10.171\n  eip: ssh-arti\n</code></pre>"},{"location":"architecture-networking/#kubernetes-service","title":"Kubernetes Service","text":"<p>Standard Kubernetes Services for in-cluster service discovery and load balancing.</p>"},{"location":"architecture-networking/#service-type-loadbalancer","title":"Service Type LoadBalancer","text":"<p>Creates and maps an EIP to a service that routes traffic to pods or VMs. Can use either a dedicated EIP or the project's default EIP.</p> <p>Example Service LoadBalancer YAML with default gateway EIP:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n</code></pre> <p>Example Service LoadBalancer for VM SSH access:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vm-ssh\n  namespace: shalb-demo\n  annotations:\n    service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    vm.kubevirt.io/name: debian\n  ports:\n    - name: ssh\n      protocol: TCP\n      port: 2222\n      targetPort: 22\n</code></pre>"},{"location":"architecture-networking/#internal-network-resources","title":"Internal Network Resources","text":""},{"location":"architecture-networking/#dnat-rule","title":"DNAT Rule","text":"<p>Destination Network Address Translation rules proxy requests from the internet through an EIP to resources within the VPC network. These are created automatically when an EIP is associated with a resource.</p>"},{"location":"architecture-networking/#snat","title":"SNAT","text":"<p>Source Network Address Translation is used for outbound connections from VPC subnets through EIPs, allowing resources within the VPC to communicate with the internet.</p>"},{"location":"architecture-networking/#project-network-provisioning","title":"Project Network Provisioning","text":"<p>When a new project is created in Kube-DC:</p> <ol> <li>The project is allocated a dedicated subnet from the VPC CIDR range</li> <li>Each project connected to the internet receives an EIP</li> <li>All project outbound traffic is routed through its assigned EIP</li> <li>Project-specific network policies are applied for isolation</li> </ol> <p>Example project creation with CIDR allocation:</p> <pre><code>apiVersion: kube-dc.com/v1\nkind: Project\nmetadata:\n  name: demo\n  namespace: shalb\nspec:\n  cidrBlock: \"10.0.10.0/24\"\n</code></pre>"},{"location":"architecture-networking/#load-balancer-implementation","title":"Load Balancer Implementation","text":"<p>Kube-DC uses a specialized implementation for Service LoadBalancers:</p> <ul> <li>When a Service with type <code>LoadBalancer</code> is created, an OVS-based LoadBalancer routes traffic to service endpoints</li> <li>Endpoints can be Kubernetes pods or KubeVirt VMs</li> <li>The LoadBalancer can use either:</li> <li>The project's default gateway EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-default-gw-eip: \"true\"</code>)</li> <li>A dedicated EIP (with annotation <code>service.nlb.kube-dc.com/bind-on-eip: \"eip-name\"</code>)</li> </ul>"},{"location":"architecture-networking/#kube-ovn-for-vpc-management","title":"Kube-OVN for VPC Management","text":"<p>Kube-OVN is a key component of Kube-DC's networking architecture, providing the foundation for multi-tenant network isolation through VPC networks.</p>"},{"location":"architecture-networking/#vpc-isolation","title":"VPC Isolation","text":"<p>Different VPC networks are independent of each other and can be separately configured with: - Subnet CIDRs - Routing policies - Security policies - Outbound gateways - EIP allocations</p>"},{"location":"architecture-networking/#overlay-vs-underlay-networks","title":"Overlay vs. Underlay Networks","text":"<p>Kube-DC supports both networking approaches:</p>"},{"location":"architecture-networking/#overlay-networks","title":"Overlay Networks","text":"<ul> <li>Software-defined networks that encapsulate packets</li> <li>Provide maximum flexibility for network segmentation</li> <li>Independent of physical network topology</li> <li>Managed entirely by Kube-OVN</li> <li>Ideal for multi-tenant environments</li> </ul>"},{"location":"architecture-networking/#underlay-networks","title":"Underlay Networks","text":"<ul> <li>Direct mapping to physical network infrastructure</li> <li>Better performance with reduced encapsulation overhead</li> <li>Requires coordination with physical network infrastructure</li> <li>Physical switches handle data-plane forwarding</li> <li>Cannot be isolated by VPCs as they are managed by physical switches</li> </ul>"},{"location":"architecture-networking/#network-security","title":"Network Security","text":"<p>Kube-DC implements multiple layers of network security:</p> <ol> <li>Project Isolation</li> <li>Each project receives its own subnet</li> <li> <p>Traffic between projects is controlled by network policies</p> </li> <li> <p>VPC Segmentation</p> </li> <li>Projects can be placed in different VPCs for stricter isolation</li> <li> <p>Each VPC has its own network stack and routing tables</p> </li> <li> <p>Kubernetes Network Policies</p> </li> <li>Fine-grained control over ingress and egress traffic</li> <li> <p>Can be applied at the namespace, pod, or service level</p> </li> <li> <p>Subnet ACLs</p> </li> <li>Control traffic at the subnet level</li> <li>Provide an additional layer of security beyond network policies</li> </ol>"},{"location":"architecture-overview/","title":"Overall Architecture","text":"<p>Kube-DC provides a comprehensive multi-tenant cloud infrastructure platform built on Kubernetes and enhanced with enterprise-grade features like virtualization, networking, and identity management.</p>"},{"location":"architecture-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC architecture consists of several key components that work together to deliver a complete cloud platform:</p> <pre><code>graph TD\n    K8s[Kubernetes] --&gt; KubeVirt[KubeVirt]    \n    K8s --&gt; KubeOVN[Kube-OVN]    \n    K8s --&gt; Keycloak[Keycloak]    \n    K8s --&gt; LBController[Kube-DC LB Controller]    \n    K8s --&gt; MultiTenant[Multi-Tenant Controller]\n\n    KubeVirt --&gt;|Provides| VMs[Virtual Machines]\n    KubeOVN --&gt;|Manages| Networking[Network VLANs/VPCs]\n    Keycloak --&gt;|Controls| IAM[Identity &amp; Access]\n    LBController --&gt;|Enables| LoadBalancing[Load Balancing]\n    MultiTenant --&gt;|Organizes| Resources[Resources]</code></pre>"},{"location":"architecture-overview/#architectural-layers","title":"Architectural Layers","text":"<p>Kube-DC is organized into four main architectural layers:</p> <ol> <li>Infrastructure Layer</li> <li>Bare metal servers or cloud infrastructure</li> <li>Kubernetes core services</li> <li> <p>Storage subsystems</p> </li> <li> <p>Virtualization Layer</p> </li> <li>KubeVirt for VM provisioning and management</li> <li>Container workloads</li> <li> <p>Hybrid application support</p> </li> <li> <p>Networking Layer</p> </li> <li>Kube-OVN for software-defined networking</li> <li>Multi-tenant network isolation</li> <li> <p>External IP addressing and service exposure</p> </li> <li> <p>Management Layer</p> </li> <li>Multi-tenancy resource organization</li> <li>Identity and access management via Keycloak</li> <li>User interface and API access</li> </ol>"},{"location":"architecture-overview/#multi-tenant-organization","title":"Multi-Tenant Organization","text":"<p>Kube-DC introduces a hierarchical resource organization model:</p> <ul> <li>Organizations - Top-level entities representing companies or teams</li> <li>Projects - Logical groupings of resources within an organization</li> <li>Groups - Collections of users with defined roles and permissions</li> </ul> <p>This multi-tenant structure maps to Kubernetes and Keycloak components to provide isolation and access control. For detailed information on the multi-tenancy architecture, see the Multi-Tenancy &amp; RBAC documentation.</p>"},{"location":"architecture-overview/#network-architecture","title":"Network Architecture","text":"<p>Kube-DC leverages Kube-OVN to provide advanced networking capabilities:</p> <ul> <li>Virtual Private Clouds (VPCs) for network isolation</li> <li>External and Floating IPs for service exposure</li> <li>Load balancing and service routing</li> </ul> <p>For detailed information on the networking architecture, see the Networking (Kube-OVN, VLANs) documentation.</p>"},{"location":"architecture-overview/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC integrates KubeVirt to enable VM workloads alongside containers:</p> <ul> <li>VM lifecycle management through Kubernetes APIs</li> <li>Hardware passthrough capabilities</li> <li>Mixed container and VM environments</li> </ul> <p>For detailed information on the virtualization architecture, see the Virtualization (KubeVirt) documentation.</p>"},{"location":"architecture-overview/#key-benefits","title":"Key Benefits","text":"<ul> <li>Multi-tenant isolation: Secure separation between organizations and projects</li> <li>Unified management: Single platform for VMs and containers</li> <li>Network flexibility: Advanced SDN capabilities with Kube-OVN</li> <li>Enterprise security: Integrated identity management with Keycloak</li> <li>API-driven architecture: Consistent interfaces for automation and integration</li> </ul>"},{"location":"architecture-virtualization/","title":"Virtualization (KubeVirt)","text":"<p>Kube-DC leverages KubeVirt to provide powerful virtual machine capabilities alongside traditional container workloads. This document covers the virtualization architecture, features, and how VMs are managed within the platform.</p>"},{"location":"architecture-virtualization/#virtualization-architecture","title":"Virtualization Architecture","text":"<p>Kube-DC's virtualization layer is built on KubeVirt, which extends Kubernetes to support virtual machine workloads. This architecture enables consistent management of both containers and VMs through the same API and tooling.</p> <pre><code>graph TD\n    K8s[Kubernetes API] --&gt; KV[KubeVirt Controller]\n    K8s --&gt; CDI[Containerized Data Importer]\n\n    KV --&gt; VMI[VM Instances]\n    CDI --&gt; DV[Data Volumes]\n\n    VMI --&gt; POD[VM Pods]\n    DV --&gt; PVC[Persistent Volumes]\n\n    subgraph \"VM Management\"\n        KV\n        VMI\n        POD\n    end\n\n    subgraph \"Storage Management\"\n        CDI\n        DV\n        PVC\n    end\n\n    UI[Kube-DC Dashboard] --&gt; K8s\n    CLI[kubectl/virtctl] --&gt; K8s</code></pre>"},{"location":"architecture-virtualization/#core-components","title":"Core Components","text":""},{"location":"architecture-virtualization/#kubevirt-controller","title":"KubeVirt Controller","text":"<p>The KubeVirt controller manages the lifecycle of virtual machines by:</p> <ul> <li>Translating VM specifications into Kubernetes resources</li> <li>Scheduling VMs on appropriate nodes</li> <li>Managing VM state (start, stop, pause, resume)</li> <li>Providing VM migration capabilities</li> <li>Handling VM monitoring and health checks</li> </ul>"},{"location":"architecture-virtualization/#containerized-data-importer-cdi","title":"Containerized Data Importer (CDI)","text":"<p>CDI handles storage provisioning for VMs by:</p> <ul> <li>Creating and managing Data Volumes</li> <li>Importing disk images from HTTP/S3 sources</li> <li>Converting disk formats as needed</li> <li>Cloning existing volumes</li> </ul>"},{"location":"architecture-virtualization/#data-volumes","title":"Data Volumes","text":"<p>Data Volumes serve as the storage backbone for VMs, providing:</p> <ul> <li>Storage allocation for VM disks</li> <li>Integration with Kubernetes storage classes</li> <li>Automated provisioning and cleanup</li> </ul>"},{"location":"architecture-virtualization/#vm-management-in-kube-dc","title":"VM Management in Kube-DC","text":""},{"location":"architecture-virtualization/#vm-creation-and-configuration","title":"VM Creation and Configuration","text":"<p>Kube-DC allows users to create VMs through YAML definitions or the web UI. VM configurations include:</p> <p>Example VM Definition:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: ubuntu-vm\n  namespace: demo\nspec:\n  running: true\n  template:\n    spec:\n      networks:\n      - name: vpc_net_0\n        multus:\n          default: true\n          networkName: default/ovn-demo\n      domain:\n        devices:\n          interfaces:\n            - name: vpc_net_0\n              bridge: {}\n          disks:\n          - disk: \n              bus: virtio\n            name: root-volume\n        cpu:\n          cores: 2\n        memory:\n          guest: 4G\n      volumes:\n      - dataVolume:\n          name: ubuntu-base-img\n        name: root-volume\n</code></pre>"},{"location":"architecture-virtualization/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Kube-DC provides templates for a variety of operating systems:</p> <ul> <li>Ubuntu (20.04, 22.04, 24.04)</li> <li>Debian</li> <li>CentOS/RHEL</li> <li>Fedora</li> <li>Alpine Linux</li> <li>FreeBSD</li> <li>openSUSE</li> <li>Minimal images (cirros)</li> </ul>"},{"location":"architecture-virtualization/#network-integration","title":"Network Integration","text":"<p>VMs in Kube-DC are integrated with the same network architecture as containers:</p> <ul> <li>Each VM can connect to VPC networks via Multus CNI</li> <li>VMs receive IP addresses from the project's CIDR block</li> <li>Network policies apply to VMs just like containers</li> <li>VMs can use floating IPs and load balancer services</li> </ul>"},{"location":"architecture-virtualization/#storage-management","title":"Storage Management","text":"<p>Kube-DC provides flexible storage options for VMs:</p> <ul> <li>Support for multiple storage classes</li> <li>Persistent storage using Kubernetes PVCs</li> <li>Live volume resizing</li> <li>Volume snapshots and cloning</li> </ul>"},{"location":"architecture-virtualization/#vm-customization","title":"VM Customization","text":"<p>VMs can be customized through cloud-init configurations:</p> <pre><code>cloudInitNoCloud:\n  userData: |-\n    #cloud-config\n    chpasswd: { expire: False }\n    password: securepassword\n    ssh_pwauth: True\n    package_update: true\n    package_upgrade: true\n    packages:\n    - qemu-guest-agent\n    runcmd:\n    - [ systemctl, start, qemu-guest-agent ]\n</code></pre> <p>This allows for: - Setting initial passwords - SSH key distribution - Software installation - Custom scripts execution - Network configuration</p>"},{"location":"architecture-virtualization/#health-monitoring","title":"Health Monitoring","text":"<p>VMs in Kube-DC support health checks through:</p> <pre><code>readinessProbe:\n  guestAgentPing: {}\n  failureThreshold: 10\n  initialDelaySeconds: 20\n  periodSeconds: 10\n</code></pre> <p>Health checks ensure: - VM is properly booted - Guest agent is responsive - Cloud-init has completed - Custom health check scripts pass</p>"},{"location":"architecture-virtualization/#web-ui-management","title":"Web UI Management","text":"<p>Kube-DC provides an intuitive web interface for VM management:</p> <p></p>"},{"location":"architecture-virtualization/#vm-dashboard-features","title":"VM Dashboard Features","text":"<p>The VM dashboard provides:</p> <ul> <li>VM Status Monitoring: Running status, uptime, and conditions</li> <li>Performance Metrics: Real-time CPU, memory, and storage usage</li> <li>VM Details: OS version, network configuration, and node placement</li> <li>Console Access: Direct web-based console access to VMs</li> <li>SSH Terminal: Direct SSH access from the browser</li> <li>Network Information: IP addresses and VPC subnet details</li> </ul>"},{"location":"architecture-virtualization/#vm-lifecycle-management","title":"VM Lifecycle Management","text":"<p>Through the UI, administrators and users can:</p> <ul> <li>Create VMs from templates or custom images</li> <li>Start, stop, pause, and restart VMs</li> <li>Adjust resource allocations (CPU, memory)</li> <li>Take snapshots for backup purposes</li> <li>Clone VMs to create new instances</li> <li>Migrate VMs between nodes</li> </ul>"},{"location":"architecture-virtualization/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture-virtualization/#gpu-passthrough","title":"GPU Passthrough","text":"<p>Kube-DC supports GPU passthrough for high-performance computing and AI workloads:</p> <pre><code>domain:\n  devices:\n    gpus:\n    - deviceName: nvidia.com/GP102GL_Tesla_P40\n      name: gpu1\n</code></pre>"},{"location":"architecture-virtualization/#live-migration","title":"Live Migration","text":"<p>VMs can be migrated between nodes without downtime:</p> <pre><code>spec:\n  strategy:\n    type: LiveMigrate\n</code></pre>"},{"location":"architecture-virtualization/#vm-snapshots","title":"VM Snapshots","text":"<p>Kube-DC supports VM snapshots for point-in-time recovery:</p> <pre><code>apiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: my-vm-snapshot\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: my-vm\n</code></pre>"},{"location":"architecture-virtualization/#vm-templates","title":"VM Templates","text":"<p>Organization administrators can create standardized VM templates for their users, ensuring consistent deployments and reducing configuration errors.</p>"},{"location":"architecture-virtualization/#integration-with-multi-tenancy","title":"Integration with Multi-Tenancy","text":"<p>VMs in Kube-DC operate within the same multi-tenant architecture as containers:</p> <ul> <li>VMs are created within specific projects</li> <li>Organization and project permissions control VM access</li> <li>Resource quotas apply to VM resources</li> <li>Network isolation is enforced between projects</li> <li>VM metrics are included in project billing and quotas</li> </ul>"},{"location":"architecture-virtualization/#best-practices","title":"Best Practices","text":""},{"location":"architecture-virtualization/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Allocate sufficient memory for the guest OS (minimum 1GB for most Linux distributions)</li> <li>Consider CPU overcommit ratios when planning node capacity</li> <li>Use appropriate storage classes for VM performance requirements</li> </ul>"},{"location":"architecture-virtualization/#vm-optimization","title":"VM Optimization","text":"<ul> <li>Install guest agents for improved integration</li> <li>Use cloud-init for automated VM configuration</li> <li>Configure readiness probes for proper health monitoring</li> <li>Use virtio drivers for improved performance</li> </ul>"},{"location":"architecture-virtualization/#security-considerations","title":"Security Considerations","text":"<ul> <li>Use SSH keys instead of passwords when possible</li> <li>Apply security updates automatically through cloud-init</li> <li>Use network policies to restrict VM communication</li> <li>Consider using encrypted storage for sensitive workloads</li> </ul>"},{"location":"architecture-virtualization/#conclusion","title":"Conclusion","text":"<p>Kube-DC's integration of KubeVirt provides a seamless experience for managing both VMs and containers in a single platform. This unified approach simplifies infrastructure management, improves resource utilization, and enables hybrid application architectures that combine the benefits of both virtualization and containerization.</p>"},{"location":"core-features/","title":"Core Features","text":"<p>Kube-DC extends Kubernetes with a robust set of features designed for enterprise data center operations. This page provides detailed technical specifications and use cases for each of Kube-DC's core capabilities.</p> <p>Looking for a high-level overview? Visit our introduction page.</p>"},{"location":"core-features/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Organization Management</li> <li>Namespace as a Service</li> <li>Network Management</li> <li>Virtualization</li> <li>Infrastructure as Code</li> <li>Integrated Flexible Billing</li> <li>Management Services</li> </ul>"},{"location":"core-features/#organization-management","title":"Organization Management","text":"<p>Foundation for Multi-Tenancy</p> <p>Organization Management provides the foundation for Kube-DC's multi-tenant capabilities, enabling complete isolation between different users and groups.</p> <p>Kube-DC's multi-tenant architecture allows service providers to host multiple organizations with complete isolation and customization.</p> <p>Capabilities:</p> <ul> <li>Multi-Organization Support: Host multiple organizations on a single Kube-DC installation with complete logical separation</li> <li>Custom SSO Integration: Each organization can configure its own identity provider:<ul> <li>Google Workspace / Gmail</li> <li>Microsoft Active Directory / Azure AD</li> <li>GitHub</li> <li>GitLab</li> <li>LDAP</li> <li>SAML 2.0 providers</li> <li>OpenID Connect providers</li> </ul> </li> <li>Hierarchical Group Management: Create and manage groups within organizations with inheritance of permissions</li> <li>Flexible RBAC: Assign fine-grained permissions to groups for specific projects or resources</li> <li>Organizational Quotas: Set resource limits at the organization level to ensure fair resource allocation</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Host multiple client organizations with separate authentication systems</li> <li>Enterprise IT: Separate departments with different authentication requirements</li> <li>Educational Institutions: Provide isolated environments for different departments or research groups</li> </ul>"},{"location":"core-features/#namespace-as-a-service","title":"Namespace as a Service","text":"<p>Projects and Workloads</p> <p>Namespaces in Kube-DC function as projects, providing isolated environments for deploying and managing diverse workloads.</p> <p>Every project in Kube-DC is allocated its own Kubernetes namespace with extended capabilities for running both containers and virtual machines.</p> <p>Capabilities:</p> <ul> <li>Unified Management: Deploy and manage both VMs and containers from a single interface</li> <li>Project Isolation: Complete network and resource isolation between projects</li> <li>Resource Quotas: Set limits on CPU, memory, storage, and other resources per project</li> <li>Integrated Dashboard: View and manage all workloads through a unified web interface</li> <li>Custom Templates: Create and use templates for quick deployment of common workloads</li> </ul> <p>Real-World Applications</p> <ul> <li>Application Modernization: Run legacy VMs alongside containerized microservices</li> <li>Development Environments: Provide isolated environments for development, testing, and staging</li> <li>Mixed Workloads: Support teams that require both traditional and cloud-native infrastructure</li> </ul>"},{"location":"core-features/#network-management","title":"Network Management","text":"<p>Advanced Connectivity</p> <p>Kube-DC's network capabilities enable sophisticated connectivity options while maintaining isolation between projects.</p> <p>Kube-DC provides advanced networking capabilities that bridge traditional data center networking with cloud-native concepts.</p> <p>Capabilities:</p> <ul> <li>Dedicated VPC per Project: Each project gets its own virtual network environment</li> <li>VLAN Integration: Connect to physical network infrastructure using VLANs</li> <li>Software-Defined Networking: Create overlay networks with software-defined control</li> <li>Network Peering: Connect project networks with each other or with external networks</li> <li>NAT and Internet Gateway: Control outbound and inbound internet access per project</li> <li>External IP Assignment: Assign public IPs directly to VMs or Kubernetes services</li> <li>Load Balancer Integration: Create and manage load balancers for services and VMs</li> <li>Network Policies: Define granular rules for network traffic filtering</li> <li>DNS Management: Automatic DNS for services and VMs with custom domain support</li> </ul> <p>Real-World Applications</p> <ul> <li>Hybrid Cloud Deployments: Extend on-premises networks to containerized workloads</li> <li>Multi-Tier Applications: Create complex network topologies for enterprise applications</li> <li>Secure Isolation: Create zero-trust network environments with fine-grained control</li> </ul>"},{"location":"core-features/#virtualization","title":"Virtualization","text":"<p>KubeVirt Integration</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities fully integrated with Kubernetes.</p> <p>Built on KubeVirt, Kube-DC provides enterprise-grade virtualization capabilities integrated with Kubernetes.</p> <p>Capabilities:</p> <ul> <li>Hardware Vendor Support: Compatible with major hardware vendors' servers and components</li> <li>GPU Passthrough: Support for Nvidia GPU passthrough to virtual machines</li> <li>ARM Support: Run VMs on ARM-based infrastructure</li> <li>Web Console: Access VM consoles directly through the web UI</li> <li>SSH Integration: SSH access management with key authentication</li> <li>Live Migration: Move running VMs between nodes without downtime</li> <li>Snapshots: Create point-in-time snapshots of VM volumes</li> <li>VM Templates: Create and use templates for rapid VM provisioning</li> <li>Custom Boot Options: Configure boot order, firmware settings, and UEFI support</li> <li>VM Import/Export: Import existing VMs from other platforms</li> </ul> <p>Real-World Applications</p> <ul> <li>Legacy Application Support: Run applications that require traditional VMs</li> <li>Windows Workloads: Host Windows servers alongside Linux containers</li> <li>GPU-Accelerated Computing: Provide GPU resources for AI/ML or rendering workloads</li> <li>Specialized Operating Systems: Run operating systems not supported in containers</li> </ul>"},{"location":"core-features/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>API-Driven Architecture</p> <p>Kube-DC's API-driven approach enables automation and integration with popular infrastructure tools.</p> <p>Kube-DC leverages and extends the Kubernetes API to enable comprehensive infrastructure automation.</p> <p>Capabilities:</p> <ul> <li>Native Kubernetes API: Manage all Kube-DC resources using standard Kubernetes tools</li> <li>Custom Resource Definitions (CRDs): Extended Kubernetes objects for managing organizations, projects, VMs, and more</li> <li>GitOps Compatible: Deploy and manage infrastructure using GitOps workflows</li> <li>Terraform Provider: Official Terraform provider for Kube-DC resources</li> <li>Ansible Integration: Ansible modules for managing Kube-DC resources</li> <li>Crossplane Support: Use Crossplane to provision and manage Kube-DC resources</li> <li>Pulumi Provider: Programmatically manage Kube-DC using multiple languages</li> </ul> <p>Real-World Applications</p> <ul> <li>Automated Infrastructure: Create fully automated infrastructure provisioning workflows</li> <li>Self-Service Portals: Build custom self-service interfaces using the Kube-DC API</li> <li>CI/CD Integration: Include infrastructure provisioning in CI/CD pipelines</li> <li>Multi-Cloud Management: Manage Kube-DC resources alongside other cloud resources</li> </ul>"},{"location":"core-features/#integrated-flexible-billing","title":"Integrated Flexible Billing","text":"<p>Cost Management</p> <p>Track, allocate, and manage costs across all resources with Kube-DC's comprehensive billing capabilities.</p> <p>Kube-DC includes comprehensive resource tracking and billing capabilities suitable for both service providers and internal IT organizations.</p> <p>Capabilities:</p> <ul> <li>Resource Metering: Track usage of CPU, memory, storage, GPU, and network resources</li> <li>Custom Pricing Models: Define pricing tiers for different resource types and customers</li> <li>Project-Based Billing: Track and bill resource usage at the project level</li> <li>Cost Allocation: Assign costs to organizational units, projects, or individual resources</li> <li>Quota Enforcement: Automatically enforce resource limits based on billing status</li> <li>Usage Reporting: Generate detailed usage reports for analysis and billing</li> <li>Billing API: Integrate with external billing systems through a comprehensive API</li> <li>Chargeback Models: Support for various internal chargeback models for enterprise use</li> </ul> <p>Real-World Applications</p> <ul> <li>Managed Service Providers: Bill customers for exact resource usage</li> <li>Enterprise IT: Implement internal chargeback or showback for departmental resource usage</li> <li>Resource Optimization: Identify resource usage patterns and optimize costs</li> </ul>"},{"location":"core-features/#management-services","title":"Management Services","text":"<p>Value-Added Services</p> <p>Extend Kube-DC's capabilities by offering managed services on top of the core platform.</p> <p>Kube-DC provides a platform for delivering managed services on top of its infrastructure.</p> <p>Capabilities:</p> <ul> <li>Database as a Service: Deploy and manage databases with automated operations</li> <li>PostgreSQL</li> <li>MySQL/MariaDB</li> <li>Microsoft SQL Server</li> <li>And more</li> <li>Object Storage: S3-compatible storage with multi-tenancy support</li> <li>NoSQL Databases: Managed NoSQL database offerings</li> <li>Redis</li> <li>MongoDB</li> <li>Elasticsearch/OpenSearch</li> <li>AI/ML Platform: Infrastructure for deploying and serving AI/ML models</li> <li>LLM serving</li> <li>Model training infrastructure</li> <li>GPU resource allocation</li> <li>Backup Services: Automated backup solutions for VMs and containers</li> <li>Monitoring as a Service: Multi-tenant monitoring solutions</li> <li>Service Catalog: Self-service provisioning of common services</li> </ul> <p>Real-World Applications</p> <ul> <li>Internal Platform Team: Provide managed services to development teams</li> <li>Managed Service Providers: Offer value-added services beyond basic infrastructure</li> <li>AI/ML Operations: Provide specialized infrastructure for data science teams</li> </ul>"},{"location":"quick-start/","title":"Quick start","text":"<ol> <li>Configure network. Ubuntu 24 netplan example (all nodes): </li> </ol> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s31f6:\n      addresses:\n        - 22.22.22.2/24\n      routes:\n        - to: 0.0.0.0/0\n          via: 22.22.22.1\n          on-link: true\n          metric: 100\n      routing-policy:\n        - from: 22.22.22.1\n          table: 100\n      nameservers:\n        addresses:\n          - 8.8.8.8\n          - 8.8.4.4\n  vlans:\n    enp0s31f6.4012:\n      id: 4012\n      link: enp0s31f6\n      mtu: 1460\n      addresses:\n        - 192.168.100.2/22\n</code></pre> <ol> <li>Update, upgrade, install soft:</li> </ol> <pre><code>sudo apt -y update\nsudo apt -y upgrade\nsudo apt -y install unzip iptables\n</code></pre> <ol> <li>Disable IPv6 and increase inotify limits. Add to <code>/etc/sysctl.conf</code> (all nodes, optional)</li> </ol> <pre><code>net.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\nfs.inotify.max_user_watches=1524288\nfs.inotify.max_user_instances=4024\n</code></pre> <p>and run </p> <pre><code>sudo sysctl -p\n</code></pre> <ol> <li>Disable <code>resolved</code>: </li> </ol> <pre><code>systemctl stop systemd-resolved\nsystemctl disable systemd-resolved\nrm /etc/resolv.conf\necho \"nameserver 8.8.8.8\" &gt; /etc/resolv.conf\necho \"nameserver 8.8.4.4\" &gt;&gt; /etc/resolv.conf\n</code></pre> <ol> <li>Edit <code>/etc/hosts</code> and remove all ipv6:</li> </ol> <pre><code>127.0.0.1 localhost.localdomain localhost\n22.22.22.3 kube-dc-master-1\n</code></pre> <ol> <li>Clone git repo (initial master node): </li> </ol> <pre><code>git clone https://github.com/shalb/kube-dc.git\ncd kube-dc\n</code></pre> <ol> <li>Check passwordless connection to all other nodes.</li> </ol> <pre><code>ssh root@22.22.22.3\n</code></pre> <ol> <li>Install <code>cluster.dev</code>. https://docs.cluster.dev/installation-upgrade/</li> </ol> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\n</code></pre> <ol> <li>Configure and install rke2 cluster initial node.    8.1 Install <code>kubectl</code> <pre><code>\n</code></pre></li> </ol> <p>8.2 Create rke2 config file <code>/etc/rancher/rke2/config.yaml</code>: <pre><code># run from root or sudo -s\nmkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml\nnode-name: kube-dc-master-1\ndisable-cloud-controller: true\ndisable: rke2-ingress-nginx\ncni: none\ncluster-cidr: \"10.100.0.0/16\"\nservice-cidr: \"10.101.0.0/16\"\ncluster-dns: \"10.101.0.11\"\nnode-label:\n  - kube-dc-manager=true\n  - kube-ovn/role=master\nkube-apiserver-arg: \n  - authentication-config=/etc/rancher/auth-conf.yaml\ndebug: true\nnode-external-ip: 138.201.253.201\ntls-san:\n  - kube-api.dev.kube-dc.com\n  - 138.201.253.201\nadvertise-address: 138.201.253.201\nnode-ip: 192.168.100.2\nEOF\n</code></pre></p> <p>8.2 Create kubernetes auth file: <pre><code># run from root or sudo -s\ncat &lt;&lt;EOF &gt; /etc/rancher/auth-conf.yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt: []\nEOF\nchmod 666 /etc/rancher/auth-conf.yaml\n</code></pre>   8.3 Install rke2: </p> <pre><code># run from root or sudo -s\nexport INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\" # https://docs.rke2.io/release-notes/v1.32.X (required kubernetes v1.31 or later)\nexport INSTALL_RKE2_TYPE=\"server\"\n\ncurl -sfL https://get.rke2.io | sh -\n\nsystemctl enable rke2-server.service\nsystemctl start rke2-server.service\n\njournalctl -u rke2-server -f\n</code></pre> <p>8.4 Get kubeconfig and check cluster access <pre><code># run from your user\nsudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config\nsudo chown \"$(whoami):$(whoami)\" ~/.kube/config\n\nkubectl get node\n# kube-dc-master-1   NotReady   control-plane,etcd,master   10m   v1.32.1+rke2r1\n# NotReady node it's ok\n</code></pre></p> <ol> <li>Configure and join master node.   9.1 Get join token (on init master node): <pre><code>sudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre>   9.2 Create rke2 config file <code>/etc/rancher/rke2/config.yaml</code>: <pre><code># run from root or sudo -s\nmkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml\ntoken: &lt;TOKEN&gt;\nserver: https://138.201.253.201:9345\nnode-name: kube-dc-master-2\ndisable-cloud-controller: true\ndisable: rke2-ingress-nginx\ncluster-cidr: \"10.100.0.0/16\"\nservice-cidr: \"10.101.0.0/16\"\ncluster-dns: \"10.101.0.11\"\nnode-label:\n  - kube-ovn/role=master\ndebug: true\nnode-external-ip: 88.99.29.250\ntls-san:\n  - kube-api.dev.kube-dc.com\n  - 88.99.29.250\nadvertise-address: 88.99.29.250\nnode-ip: 192.168.100.3\nEOF\n</code></pre></li> </ol> <p>8.3 Install rke2: </p> <pre><code># run from root or sudo -s\nexport INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\" # https://docs.rke2.io/release-notes/v1.32.X (required kubernetes v1.31 or later)\nexport INSTALL_RKE2_TYPE=\"server\"\n\ncurl -sfL https://get.rke2.io | sh -\n\nsystemctl enable rke2-server.service\nsystemctl start rke2-server.service\n\njournalctl -u rke2-server -f\n</code></pre> <ol> <li>Configure and join worker node.   9.1 Get join token (on init master node): <pre><code>sudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre>   9.2 Create rke2 config file: <pre><code># run from root or sudo -s\nmkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml\ntoken: &lt;TOKEN&gt;\nserver: https://192.168.100.2:9345\nnode-name: kube-dc-worker-1\nnode-ip: 192.168.100.3\nEOF\n</code></pre></li> </ol> <p>8.3 Install rke2: </p> <pre><code># run from root or sudo -s\nexport INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\" # https://docs.rke2.io/release-notes/v1.32.X (required kubernetes v1.31 or later)\nexport INSTALL_RKE2_TYPE=\"agent\"\n\ncurl -sfL https://get.rke2.io | sh -\n\nsystemctl enable rke2-agent.service\nsystemctl start rke2-agent.service\n\njournalctl -u rke2-agent -f\n</code></pre> <ol> <li>Install kube-dc stack</li> </ol> <p>In installer folder <code>inatsller/kube-dc/</code>, edit stack.yaml like this:</p> <p><pre><code>name: cluster\ntemplate: \"./templates/kube-dc/\"\nkind: Stack\nbackend: default\nvariables:\n  debug: \"true\"\n  kubeconfig: /home/arti/.kube/config\n\n  monitoring:\n    prom_storage: 20Gi\n    retention_size: 17GiB\n    retention: 365d\n\n  cluster_config:\n    pod_cidr: \"10.100.0.0/16\"\n    svc_cidr: \"10.101.0.0/16\"\n    join_cidr: \"100.64.0.0/16\"\n    cluster_dns: \"10.101.0.11\"\n    default_external_network:\n      nodes_list: # list of nodes, where 4011 vlan is accessible\n        - kube-dc-master-1\n        - kube-dc-worker-1\n      name: external4011\n      vlan_id: \"4011\"\n      interface: \"enp0s31f6\"\n      cidr: \"167.235.85.112/29\"\n      gateway: 167.235.85.113\n      mtu: \"1400\"\n\n  node_external_ip: 138.201.253.201 # wildcard *.dev.kube-dc.com shoud be faced on this ip\n\n  email: \"noreply@shalb.com\"\n  domain: \"dev.kube-dc.com\"\n  install_terraform: true\n\n  create_default:\n    organization:\n      name: shalb\n      description: \"My test org my-org 1\"\n      email: \"arti@shalb.com\"\n    project:\n      name: demo\n      cidr_block: \"10.1.0.0/16\"\n\n\n  versions:\n    kube_dc: \"v0.1.20\" # release version\n    rke2: \"v1.32.1+rke2r1\"\n</code></pre> install: </p> <pre><code>cdev apply\n</code></pre>"},{"location":"quickstart-hetzner/","title":"Master-Worker Setup on Hetzner Dedicated Servers","text":"<p>This guide provides step-by-step instructions for deploying a Kube-DC cluster with a master and worker node setup on Hetzner Dedicated Servers. This deployment leverages Hetzner's vSwitch and additional subnets to provide enterprise-grade networking capabilities for floating IPs and load balancers.</p>"},{"location":"quickstart-hetzner/#prerequisites","title":"Prerequisites","text":"<ol> <li>At least two Hetzner Dedicated Servers</li> <li>Access to Hetzner Robot interface</li> <li>A Hetzner vSwitch configured for your servers (see Hetzner vSwitch documentation)</li> <li>An additional subnet allocated through Hetzner Robot for external IPs and load balancers</li> <li>Wildcard domain ex: *.dev.kube-dc.com shoud be set to main public ip of master node.</li> </ol>"},{"location":"quickstart-hetzner/#server-configuration","title":"Server Configuration","text":""},{"location":"quickstart-hetzner/#1-prepare-servers","title":"1. Prepare Servers","text":"<p>Ensure your Hetzner Dedicated Servers meet these minimum requirements: - Master Node: 4+ CPU cores, 16+ GB RAM - Worker Node: 4+ CPU cores, 16+ GB RAM</p> <p>Install Ubuntu 24.04 LTS on all servers through the Hetzner Robot interface.</p>"},{"location":"quickstart-hetzner/#2-configure-vswitch","title":"2. Configure vSwitch","text":"<p>In the Hetzner Robot interface:</p> <ol> <li>Create a vSwitch if you don't have one already</li> <li>Add your servers to the vSwitch</li> <li>Request an additional subnet to be used for external IPs (Floating IPs)</li> <li>Assign the subnet to your vSwitch</li> </ol> <p>You will get two vlan ids, one for the local network(in example 4012) and one for the external subnet with public ips(in example 4011).</p>"},{"location":"quickstart-hetzner/#network-configuration","title":"Network Configuration","text":""},{"location":"quickstart-hetzner/#1-configure-network-interfaces","title":"1. Configure Network Interfaces","text":"<p>SSH into each server and configure the networking using Netplan. Edit <code>/etc/netplan/60-kube-dc.yaml</code>:</p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s31f6:  # Primary network interface name (check your actual interface name)\n      addresses:\n        - 22.22.22.2/24  # Primary IP address and subnet mask (main IP provided by Herzner)\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 22.22.22.1  # Gateway IP address (Also provided by Hernzer)\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      routing-policy:\n        - from: 22.22.22.2  # Source-based routing for traffic from gateway\n          table: 100  # Custom routing table ID\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    enp0s31f6.4012:  # VLAN interface name (format: interface.vlan_id)\n      id: 4012  # VLAN ID (must match your Hetzner vSwitch ID)\n      link: enp0s31f6  # Parent interface for VLAN \n      mtu: 1460  # Maximum Transmission Unit size\n      addresses:\n        - 192.168.100.2/22  # Master node IP on private network\n        # Use 192.168.100.3/22 for worker node in its config\n</code></pre> <p>Apply the configuration:</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"quickstart-hetzner/#2-system-optimization","title":"2. System Optimization","text":"<p>On all nodes, update, upgrade, and install required software:</p> <pre><code>sudo apt -y update\nsudo apt -y upgrade\nsudo apt -y install unzip iptables\n</code></pre> <p>Optimize system settings by adding to <code>/etc/sysctl.conf</code>:</p> <pre><code># Disable IPv6\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\n\n# Increase inotify limits\nfs.inotify.max_user_watches=1524288\nfs.inotify.max_user_instances=4024\n\n# Network optimization for Kubernetes\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n</code></pre> <p>Apply the changes:</p> <pre><code>sudo sysctl -p\n</code></pre> <p>Disable systemd-resolved to prevent DNS conflicts:</p> <pre><code>sudo systemctl stop systemd-resolved\nsudo systemctl disable systemd-resolved\nsudo rm /etc/resolv.conf\necho \"nameserver 8.8.8.8\" | sudo tee /etc/resolv.conf\necho \"nameserver 8.8.4.4\" | sudo tee -a /etc/resolv.conf\n</code></pre> <p>Update the hosts file on each server with the private IPs:</p> <pre><code># On Master Node\necho \"192.168.100.2 kube-dc-master-1\" | sudo tee -a /etc/hosts\n# On Worker Node\necho \"192.168.100.3 kube-dc-worker-1\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"quickstart-hetzner/#kubernetes-installation","title":"Kubernetes Installation","text":""},{"location":"quickstart-hetzner/#1-install-clusterdev","title":"1. Install Cluster.dev","text":"<p>On the master node, install Cluster.dev:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\n</code></pre>"},{"location":"quickstart-hetzner/#2-configure-and-install-rke2-on-master-node","title":"2. Configure and Install RKE2 on Master Node","text":"<p>Install kubectl:</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Create RKE2 configuration (replace the external IP with your server's public IP):</p> <pre><code>sudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\nnode-name: kube-dc-master-1\ndisable-cloud-controller: true\ndisable: rke2-ingress-nginx\ncni: none\ncluster-cidr: \"10.100.0.0/16\"\nservice-cidr: \"10.101.0.0/16\"\ncluster-dns: \"10.101.0.11\"\nnode-label:\n  - kube-dc-manager=true\n  - kube-ovn/role=master\nkube-apiserver-arg: \n  - authentication-config=/etc/rancher/auth-conf.yaml\ndebug: true\nnode-external-ip: YOUR_MASTER_PUBLIC_IP # Main IP provided by Hetzner for server\ntls-san:\n  - kube-api.yourdomain.com\n  - YOUR_MASTER_PUBLIC_IP\nadvertise-address: YOUR_MASTER_PUBLIC_IP\nnode-ip: 192.168.100.2\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/auth-conf.yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt: []\nEOF\nsudo chmod 666 /etc/rancher/auth-conf.yaml\n</code></pre> <p>Install RKE2 server:</p> <pre><code>export INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"server\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-server.service\nsudo systemctl start rke2-server.service\n</code></pre> <p>Check the installation progress:</p> <pre><code>sudo journalctl -u rke2-server -f\n</code></pre> <p>Configure kubectl:</p> <pre><code>mkdir -p ~/.kube\nsudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config\nsudo chown $(id -u):$(id -g) ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre> <p>Verify the cluster status:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"quickstart-hetzner/#4-join-worker-node-to-the-cluster","title":"4. Join Worker Node to the Cluster","text":"<p>Get the join token from the master node:</p> <pre><code>sudo cat /var/lib/rancher/rke2/server/node-token\n</code></pre> <p>On the worker node, create the RKE2 configuration (replace TOKEN with the token from the master node):</p> <pre><code>sudo mkdir -p /etc/rancher/rke2/\n\ncat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml\ntoken: &lt;TOKEN&gt;\nserver: https://192.168.100.2:9345 # Master node local IP\nnode-name: kube-dc-worker-1\nnode-ip: 192.168.100.3\nEOF\n</code></pre> <p>Install RKE2 agent:</p> <pre><code>export INSTALL_RKE2_VERSION=\"v1.32.1+rke2r1\"\nexport INSTALL_RKE2_TYPE=\"agent\"\ncurl -sfL https://get.rke2.io | sh -\nsudo systemctl enable rke2-agent.service\nsudo systemctl start rke2-agent.service\n</code></pre> <p>Monitor the agent service:</p> <pre><code>sudo journalctl -u rke2-agent -f\n</code></pre> <p>Verify on the master node that the worker joined successfully:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"quickstart-hetzner/#install-kube-dc-components","title":"Install Kube-DC Components","text":""},{"location":"quickstart-hetzner/#1-create-clusterdev-project-configuration","title":"1. Create Cluster.dev Project Configuration","text":"<p>On the master node, create a project configuration file:</p> <pre><code>mkdir -p ~/kube-dc-hetzner\ncat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/project.yaml\nkind: Project\nname: kube-dc-hetzner\nvariables:\n  kubeconfig: ~/.kube/config\n  debug: true\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#2-create-clusterdev-stack-configuration","title":"2. Create Cluster.dev Stack Configuration","text":"<p>Create the stack configuration file:</p> <pre><code>cat &lt;&lt;EOF &gt; ~/kube-dc-hetzner/stack.yaml\nname: cluster\ntemplate: https://github.com/kube-dc/kube-dc-public//installer/kube-dc?ref=main\nkind: Stack\nbackend: default\nvariables:\n  debug: \"true\"\n  kubeconfig: /home/arti/.kube/config # Change for your username path to RKE kubeconfig\n\n  cluster_config:\n    pod_cidr: \"10.100.0.0/16\"\n    svc_cidr: \"10.101.0.0/16\"\n    join_cidr: \"100.64.0.0/16\"\n    cluster_dns: \"10.101.0.11\"\n    default_external_network:\n      nodes_list: # list of nodes, where 4011 vlan (external network) is accessible\n        - kube-dc-master-1\n        - kube-dc-worker-1\n      name: external4011\n      vlan_id: \"4011\"\n      interface: \"enp0s31f6\"\n      cidr: \"167.235.85.112/29\" # External subnet provided by Hetzner\n      gateway: 167.235.85.113 # Gateway for external subnet\n      mtu: \"1400\"\n\n  node_external_ip: 22.22.22.2 # wildcard *.dev.kube-dc.com shoud be faced on this ip\n\n  email: \"noreply@shalb.com\"\n  domain: \"dev.kube-dc.com\"\n  install_terraform: true\n\n  create_default:\n    organization:\n      name: shalb\n      description: \"My test org my-org 1\"\n      email: \"arti@shalb.com\"\n    project:\n      name: demo\n      cidr_block: \"10.1.0.0/16\"\n\n  monitoring:\n    prom_storage: 20Gi\n    retention_size: 17GiB\n    retention: 365d\n\n  versions:\n    kube_dc: \"v0.1.20\" # release version\n    rke2: \"v1.32.1+rke2r1\"\nEOF\n</code></pre>"},{"location":"quickstart-hetzner/#3-deploy-kube-dc","title":"3. Deploy Kube-DC","text":"<p>Run Cluster.dev to deploy Kube-DC components:</p> <pre><code>cd ~/kube-dc-hetzner\ncdev apply\n</code></pre> <p>This process will take 15-20 minutes to complete. You can monitor the deployment progress in the terminal output.</p>"},{"location":"quickstart-hetzner/#4-verify-installation","title":"4. Verify Installation","text":"<p>After successful deployment, you will receive console and login credentials for deployment admin user. Also if you have created some default organization youll get organization admin credentials. Example:</p> <pre><code>keycloak_user = admin\norganization_admin_username = admin\norganization_name = shalb\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\nconsole_url = https://console.dev.kube-dc.com\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#post-installation-steps","title":"Post-Installation Steps","text":""},{"location":"quickstart-hetzner/#1-access-kube-dc-ui-using-default-organization-credentials","title":"1. Access Kube-DC UI using default organization credentials","text":"<p>After the installation completes, the Kube-DC UI should be accessible at <code>https://console.yourdomain.com</code>. In cdev output there are output for default organization, project and admin user for default organization:</p> <pre><code>console_url = https://console.dev.kube-dc.com\norganization_admin_username = admin\norganization_name = shalb\nproject_name = demo\nretrieve_organization_password = kubectl get secret realm-access -n shalb -o jsonpath='{.data.password}' | base64 -d\nretrieve_organization_realm_url = kubectl get secret realm-access -n shalb -o jsonpath='{.data.url}' | base64 -d\n</code></pre>"},{"location":"quickstart-hetzner/#2-keep-credentials-for-keycloak-master-admin-user","title":"2. Keep credentials for Keycloak master admin user","text":"<p>You can save global Keycloak credentials if you need to manage other organizations as super-admin. Master admin user credentials:</p> <pre><code>keycloak_user = admin\nkeycloak_password = XXXXXXXX\nkeycloak_url = https://login.dev.kube-dc.com\n</code></pre>"},{"location":"quickstart-hetzner/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during the installation:</p> <ol> <li> <p>Check the RKE2 server/agent logs:    <pre><code>sudo journalctl -u rke2-server -f  # On master\nsudo journalctl -u rke2-agent -f   # On worker\n</code></pre></p> </li> <li> <p>Check the Kube-OVN logs:    <pre><code>kubectl logs -n kube-system -l app=kube-ovn-controller\n</code></pre></p> </li> <li> <p>Verify network connectivity between nodes on the private network:    <pre><code>ping 192.168.100.2  # From worker node\nping 192.168.100.3  # From master node\n</code></pre></p> </li> </ol> <p>For additional help, consult the Kube-DC community support resources.</p>"},{"location":"quickstart-overview/","title":"Kube-DC Installation Overview","text":"<p>This document provides a technical overview of the Kube-DC installation process, with detailed explanations of key configuration files and their parameters.</p>"},{"location":"quickstart-overview/#installation-methods","title":"Installation Methods","text":"<p>Kube-DC can be installed in several ways:</p> <ul> <li>Master-Worker deployment: Recommended starting point for new deployments</li> <li>Multi-node HA cluster: For production environments</li> <li>On top of existing Kubernetes: For extending an existing cluster</li> </ul>"},{"location":"quickstart-overview/#prerequisites","title":"Prerequisites","text":"<p>Before installing Kube-DC, ensure your system meets the following requirements:</p> <ul> <li>Hardware: Minimum 4 CPU cores, 8GB RAM per node</li> <li>Operating System: Ubuntu 20.04 LTS or newer (24.04 LTS recommended)</li> <li>Network: Dedicated network interface for VM traffic with VLAN support</li> <li>Storage: Local or network storage with support for dynamic provisioning</li> <li>Kubernetes: Version 1.31+ if installing on existing cluster</li> </ul>"},{"location":"quickstart-overview/#network-configuration","title":"Network Configuration","text":"<p>Kube-DC requires proper network configuration for optimal performance. The key requirement is that your external network must be routed through a VLAN to enable advanced networking features.</p>"},{"location":"quickstart-overview/#external-network-requirements","title":"External Network Requirements","text":"<p>Kube-DC networking is built on top of Kube-OVN and requires the following network configuration:</p> <ul> <li>VLAN-capable network interface: A dedicated network interface with VLAN support</li> <li>External subnet with routing: An external subnet that's properly routed to your infrastructure</li> <li>Static IP configuration: Static IP addressing (no DHCP) to ensure network stability</li> </ul> <p>This configuration allows Kube-DC to implement:</p> <ul> <li>Floating IP allocation: Dynamically assign public IPs to workloads</li> <li>Load balancer with external IPs: Distribute traffic to services with public visibility</li> <li>Default gateway per project: Isolate network traffic between projects</li> </ul> <p>All of these features work as a wrapper on top of Kube-OVN, providing enterprise-grade networking capabilities for your infrastructure.</p>"},{"location":"quickstart-overview/#example-network-configuration","title":"Example Network Configuration","text":"<p>Below is an example Netplan configuration with detailed comments for a VLAN-enabled network:</p> <pre><code>network:\n  version: 2  # Netplan version\n  renderer: networkd  # Network renderer to use\n  ethernets:\n    eth0:  # Primary network interface name (check your actual interface name)\n      addresses:\n        - 192.168.1.2/24  # Primary IP address and subnet mask\n      routes:\n        - to: 0.0.0.0/0  # Default route for all traffic\n          via: 192.168.1.1  # Gateway IP address\n          on-link: true  # Indicates the gateway is directly reachable\n          metric: 100  # Route priority (lower = higher priority)\n      nameservers:\n        addresses:\n          - 8.8.8.8  # Primary DNS server (Google)\n          - 8.8.4.4  # Secondary DNS server (Google)\n  vlans:\n    eth0.100:  # VLAN interface (format: interface.vlan_id)\n      id: 100  # VLAN ID\n      link: eth0  # Parent interface for VLAN \n      mtu: 1500  # Recommended MTU for your network\n      addresses:\n        - 10.100.0.2/24  # Private IP on the VLAN network\n</code></pre> <p>Important</p> <p>Do not use DHCP for the VLAN interface as it would break the initial Kube-OVN setup. Always use static IP configuration.</p>"},{"location":"quickstart-overview/#networking-components","title":"Networking Components","text":"<p>The Kube-DC network setup consists of several key components that work together:</p> <ol> <li>Kube-OVN: Core CNI providing overlay and underlay networking</li> <li>Multus CNI: Enables multiple network interfaces for pods</li> <li>VLAN Integration: Connects Kubernetes networking to physical infrastructure</li> </ol>"},{"location":"quickstart-overview/#external-network-configuration","title":"External Network Configuration","text":"<p>Kube-OVN is configured with the following settings to enable external connectivity through VLAN routing:</p> <pre><code># Subnet configuration for external connectivity\napiVersion: kubeovn.io/v1\nkind: Subnet\nmetadata:\n  name: external-network  # Network name\n  labels:\n    network.kube-dc.com/allow-projects: \"all\"  # Allow all projects to use this network\nspec:\n  protocol: IPv4  # IP protocol version\n  cidrBlock: 203.0.113.0/24  # Your allocated external subnet\n  gateway: 203.0.113.1  # Gateway IP (first IP in your external subnet)\n  vlan: vlan100  # VLAN ID reference matching your network configuration\n  mtu: 1500  # MTU size optimized for your network\n</code></pre> <pre><code># VLAN configuration\napiVersion: kubeovn.io/v1\nkind: Vlan\nmetadata:\n  name: vlan4012  # VLAN name reference\nspec:\n  id: 4012  # VLAN ID matching your Hetzner vSwitch ID\n  provider: external-network  # Provider name\n</code></pre>"},{"location":"quickstart-overview/#system-optimization","title":"System Optimization","text":"<p>For optimal performance, several system settings should be configured:</p>"},{"location":"quickstart-overview/#system-control-parameters","title":"System Control Parameters","text":"<p>Add the following to <code>/etc/sysctl.conf</code>:</p> <pre><code># Disable IPv6 to prevent issues with dual-stack networks\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\n\n# Increase inotify limits for Kubernetes workloads\nfs.inotify.max_user_watches=1524288  # Number of watches per user\nfs.inotify.max_user_instances=4024   # Number of watch instances per user\n</code></pre>"},{"location":"quickstart-overview/#dns-configuration","title":"DNS Configuration","text":"<p>Disable <code>systemd-resolved</code> to prevent conflicts with container DNS:</p> <pre><code>systemctl stop systemd-resolved\nsystemctl disable systemd-resolved\nrm /etc/resolv.conf\necho \"nameserver 8.8.8.8\" &gt; /etc/resolv.conf\necho \"nameserver 8.8.4.4\" &gt;&gt; /etc/resolv.conf\n</code></pre>"},{"location":"quickstart-overview/#rke2-configuration","title":"RKE2 Configuration","text":"<p>RKE2 (Rancher Kubernetes Engine 2) is the preferred Kubernetes distribution for Kube-DC. The configuration differs depending on the node role:</p>"},{"location":"quickstart-overview/#initial-master-node-configuration","title":"Initial Master Node Configuration","text":"<p>Create <code>/etc/rancher/rke2/config.yaml</code> with the following parameters:</p> <pre><code>node-name: kube-dc-master-1  # Unique node name\ndisable-cloud-controller: true  # Disable default cloud controller as Kube-DC uses its own\ndisable: rke2-ingress-nginx  # Disable default ingress as Kube-DC provides its own\ncni: none  # Disable default CNI as Kube-DC uses Kube-OVN\ncluster-cidr: \"10.100.0.0/16\"  # Pod network CIDR range\nservice-cidr: \"10.101.0.0/16\"  # Service network CIDR range\ncluster-dns: \"10.101.0.11\"  # Cluster DNS service IP\nnode-label:  # Node labels used by components to identify master nodes\n  - kube-dc-manager=true  # Identifies this node for management components\n  - kube-ovn/role=master  # Identifies this node for Kube-OVN control plane\nkube-apiserver-arg:  # Additional API server arguments\n  - authentication-config=/etc/rancher/auth-conf.yaml  # Path to auth config for custom authentication\ndebug: true  # Enable debug logging\nnode-external-ip: 138.201.253.201  # External IP for this node\ntls-san:  # Subject Alternative Names for TLS certificates\n  - kube-api.dev.kube-dc.com  # DNS name for API server\n  - 138.201.253.201  # IP address for API server\nadvertise-address: 138.201.253.201  # IP address to advertise for API server\nnode-ip: 192.168.100.2  # Internal cluster IP address\n</code></pre>"},{"location":"quickstart-overview/#authentication-configuration","title":"Authentication Configuration","text":"<p>Create <code>/etc/rancher/auth-conf.yaml</code> to configure Kubernetes API authentication:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt: []  # Empty JWT configuration allows for extending authentication later\n</code></pre>"},{"location":"quickstart-overview/#clusterdev-installation","title":"Cluster.dev Installation","text":"<p>Kube-DC uses Cluster.dev as the deployment tool for installing and managing components. Install it with:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\n</code></pre>"},{"location":"quickstart-overview/#core-components","title":"Core Components","text":"<p>The Kube-DC installer deploys the following core components:</p> <ol> <li>Kube-OVN: Advanced networking solution that provides overlay and underlay networking</li> <li>Multus CNI: CNI that enables attaching multiple network interfaces to pods</li> <li>KubeVirt: Virtualization layer for running VMs on Kubernetes</li> <li>Keycloak: Identity and access management solution</li> <li>Cert-Manager: Certificate management for TLS</li> <li>Ingress-NGINX: Ingress controller for external access</li> <li>Prometheus &amp; Loki: Monitoring and logging stack</li> <li>Kube-DC Core: The core management components for Kube-DC</li> </ol>"},{"location":"quickstart-overview/#installation-process-overview","title":"Installation Process Overview","text":"<p>The installation process follows these high-level steps:</p> <ol> <li>System Preparation: Configure network, optimize system settings, and install prerequisites</li> <li>Kubernetes Installation: Install RKE2 on master and worker nodes</li> <li>Kube-DC Installation: Use cluster.dev to deploy Kube-DC components</li> <li>Post-Installation Setup: Configure authentication, networking, and initial organization</li> </ol> <p>For detailed step-by-step instructions, refer to: - Master-Worker Setup (Dedicated Servers) - Minimal HA Setup (Bare Metal) - Installing on Existing K8s</p>"},{"location":"quickstart-overview/#version-compatibility","title":"Version Compatibility","text":"<p>The main components of Kube-DC have the following version compatibility:</p> Component Minimum Version Recommended Version Kubernetes 1.31.0 1.32.1 KubeVirt 1.2.0 1.3.0 Kube-OVN 1.12.0 1.13.2 Keycloak 24.0.0 24.3.0"}]}